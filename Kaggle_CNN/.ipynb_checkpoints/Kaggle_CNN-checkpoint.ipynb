{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "num_classes = 15\n",
    "epochs = 200\n",
    "image_size = (224,224)\n",
    "lr = 1e-5\n",
    "patience = 30\n",
    "validation_split = 0.1\n",
    "drop_rate = 0.0\n",
    "\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "# save_dir = os.path.join(os.getcwd(), 'saved_models_1556113658')\n",
    "model_name = 'keras_kaggle_cnn_trained_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make tf allocate gpu memory as needed\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "# assign the session with the previous setting as the backend of Keras\n",
    "tf.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Using real-time data augmentation.')\n",
    "\n",
    "\n",
    "# train_datagen = ImageDataGenerator(\n",
    "#     rescale=1./255,\n",
    "# #     rotation_range=30,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "# #     width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "# #     height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "# #         shear_range=0.2,\n",
    "# #         zoom_range=0.2,\n",
    "#     horizontal_flip=True,  # randomly flip images\n",
    "#     validation_split=validation_split,\n",
    "#     vertical_flip=False\n",
    "#     )\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='constant',\n",
    "    cval=0)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'train',\n",
    "        shuffle=True,\n",
    "        seed=42,\n",
    "        target_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_datagen = ImageDataGenerator()\n",
    "# valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "        'train',\n",
    "        shuffle=False,\n",
    "        seed=42,\n",
    "        target_size=image_size,\n",
    "        batch_size=1,\n",
    "        class_mode='categorical',\n",
    "        subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator() \n",
    "# test_datagen = ImageDataGenerator(rescale=1./255) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "i,l = next(train_generator)\n",
    "print('i.shape= ', i.shape)\n",
    "print(l[0])\n",
    "ax = plt.imshow(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dict = train_generator.class_indices\n",
    "training_dict = list(training_dict)\n",
    "training_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_dict = {\n",
    "    'CALsuburb': 9,\n",
    "    'PARoffice': 7,\n",
    "    'bedroom': 12,\n",
    "    'coast': 10,\n",
    "    'forest': 4,\n",
    "    'highway': 14,\n",
    "    'industrial': 2,\n",
    "    'insidecity': 3,\n",
    "    'kitchen': 0,\n",
    "    'livingroom': 5,\n",
    "    'mountain': 8,\n",
    "    'opencountry': 6,\n",
    "    'store': 11,\n",
    "    'street': 1,\n",
    "    'tallbuilding': 13 }\n",
    "kaggle_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_generator.n)\n",
    "print(len(list(train_generator.class_indices)))\n",
    "print(validation_generator.n)\n",
    "print(len(list(validation_generator.class_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### print('train shape:', image_size)\n",
    "print(train_generator.n, 'train samples')\n",
    "print(validation_generator.n, 'test samples')\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), input_shape=(image_size[0], image_size[1],3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(drop_rate))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(drop_rate))\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(drop_rate))\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(drop_rate))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "model.add(Dense(num_classes))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "print(model.summary())\n",
    "input\n",
    "# initiate Adam optimizer\n",
    "opt = keras.optimizers.Adam(lr=lr)\n",
    "\n",
    "# Let's train the model using Adam\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Use ModelCheckpoint to save model and weights\n",
    "# if os.path.exists(save_dir):\n",
    "#     timestamp = 'saved_models_{:d}'.format(int(time.time()))\n",
    "#     print(\"file or directory with the name of \", save_dir, ' exists. use modified name : ', timestamp)\n",
    "#     save_dir = os.path.join(os.getcwd(), timestamp)\n",
    "#     os.makedirs(save_dir)\n",
    "# else:\n",
    "#     os.makedirs(save_dir)\n",
    "\n",
    "# model_path = os.path.join(save_dir, model_name)\n",
    "\n",
    "# checkpoint = ModelCheckpoint(model_path, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "\n",
    "# Use ModelCheckpoint to save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "checkpoint = ModelCheckpoint(model_path, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "\n",
    "# earlystop\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=patience, verbose=1)\n",
    "\n",
    "\n",
    "STEPS_PER_EPOCH = train_generator.n // train_generator.batch_size\n",
    "VALIDATION_STEP = validation_generator.n // validation_generator.batch_size\n",
    "print('STEPS_PER_EPOCH = ', STEPS_PER_EPOCH)\n",
    "print('VALIDATION_STEP = ', VALIDATION_STEP)\n",
    "model_history = model.fit_generator(\n",
    "                                    generator=train_generator,\n",
    "                                    steps_per_epoch = STEPS_PER_EPOCH,\n",
    "                                    validation_data = validation_generator, \n",
    "                                    validation_steps = VALIDATION_STEP,\n",
    "                                    callbacks=[earlystop,checkpoint],\n",
    "                                    workers=4,\n",
    "                                    epochs = epochs\n",
    "                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading our save model\n",
    "print(\"Loading trained model\")\n",
    "model = load_model(model_path)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate_generator(generator=validation_generator)\n",
    "\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import PIL\n",
    "from PIL import Image\n",
    "test_dir='testset/'\n",
    "\n",
    "ID = []\n",
    "CLASS = []\n",
    "\n",
    "files = [join(test_dir, f) for f in listdir(test_dir) if isfile(join(test_dir, f))]\n",
    "\n",
    "for file in files:\n",
    "#     print(os.path.splitext(file)[0].split(\"/\")[1])\n",
    "#     print(file)\n",
    "    ID.append(os.path.splitext(file)[0].split(\"/\")[1])\n",
    "    image = Image.open(file)\n",
    "    image = image.resize(image_size)\n",
    "\n",
    "    image = np.array(image)\n",
    "\n",
    "#     image = image / 255.\n",
    "\n",
    "    if image.shape == image_size:\n",
    "        gray2RGB = image.reshape(image.shape[0]*image.shape[1],1)\n",
    "        temp = np.hstack((gray2RGB,gray2RGB))\n",
    "        gray2RGB = np.hstack((temp,gray2RGB))\n",
    "        image = gray2RGB.reshape((image.shape[0],image.shape[1],3))\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "    else:\n",
    "#         print(\"file: \", file, \" image shape = \", image.shape)\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "    class_id = np.argmax(model.predict(image, batch_size=1))\n",
    "    \n",
    "    CLASS.append(kaggle_dict[training_dict[class_id]])\n",
    "    \n",
    "# len(ID),len(CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = 1\n",
    "# ID[idx],CLASS[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from os import listdir\n",
    "# from os.path import isfile, join\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# from matplotlib import pyplot as plt\n",
    "# import pandas as pd\n",
    "# import PIL\n",
    "# from PIL import Image\n",
    "\n",
    "\n",
    "# file = '/home/jovyan/my-workspace/Kaggle_CNN/testset/d9d914fae34406fb2e6e2d3c97fe33b9319733fd121b00ed2acff8be772f1e7e.jpg'\n",
    "\n",
    "# image = Image.open(file)\n",
    "# image = image.resize(image_size)\n",
    "# plt.imshow(image)\n",
    "# image = np.array(image)\n",
    "# image = image / 255.\n",
    "\n",
    "# gray2RGB = image.reshape(image.shape[0]*image.shape[1],1)\n",
    "# temp = np.hstack((gray2RGB,gray2RGB))\n",
    "# gray2RGB = np.hstack((temp,gray2RGB))\n",
    "# image = gray2RGB.reshape((image.shape[0],image.shape[1],3))\n",
    "# image = np.expand_dims(image, axis=0)\n",
    "# class_id = kaggle_dict[training_dict[np.argmax(model.predict(image, batch_size=1))]]\n",
    "# print(\"class= \", class_id)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = pd.DataFrame({\"ID\":ID, \"class\":CLASS})\n",
    "solution.to_csv(\"submission.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
