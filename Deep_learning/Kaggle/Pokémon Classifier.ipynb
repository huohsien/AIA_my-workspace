{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_class = 6\n",
    "# how many data per batch to load\n",
    "batch_size = 10000\n",
    "# data split ratio\n",
    "train_ratio = 0.7\n",
    "test_ratio = 0.01\n",
    "\n",
    "n_epochs = 1000\n",
    "\n",
    "lr=0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "Counter({'class_0': 1611, 'class_2': 1478, 'class_5': 1411, 'class_1': 920, 'class_3': 889, 'class_4': 851})\n",
      "After:\n",
      "Counter({'class_0': 1611, 0: 1611, 1: 1611, 2: 1611, 3: 1611, 4: 1611, 5: 1611, 'class_2': 1478, 'class_5': 1411, 'class_1': 920, 'class_3': 889, 'class_4': 851})\n"
     ]
    }
   ],
   "source": [
    "# make number of data for each class equal\n",
    "#\n",
    "from collections import Counter\n",
    "\n",
    "class_counter = Counter()\n",
    "\n",
    "class_names =['class_' + str(i) for i in range(number_of_class)]\n",
    "for class_name in class_names:\n",
    "    class_counter[class_name] = 0\n",
    "for i in train['class']:\n",
    "    class_counter['class_' + str(i)] += 1\n",
    "\n",
    "print('Before:')\n",
    "print(class_counter)\n",
    "\n",
    "max_count = -np.Inf\n",
    "for i in range(number_of_class):\n",
    "    if class_counter['class_' + str(i)] > max_count:\n",
    "        max_count = class_counter['class_' + str(i)]\n",
    "\n",
    "train_classified = [train[train['class'] == i] for i in range(number_of_class)]\n",
    "\n",
    "for i in range(number_of_class):\n",
    "    num_need_resample = max_count - class_counter['class_' + str(i)]\n",
    "    num_resample_batch = num_need_resample // class_counter['class_' + str(i)]\n",
    "    num_resample_leftover = num_need_resample % class_counter['class_' + str(i)]\n",
    "    for j in range(num_resample_batch):\n",
    "        add_df = train_classified[i]\n",
    "        train =  pd.concat([train, add_df[0:dist_class[i][1]]], ignore_index=True)\n",
    "        train =  train.append(df_to_be_added)\n",
    "        \n",
    "    df_to_be_added = train_classified[i][:num_resample_leftover]\n",
    "    train =  train.append(df_to_be_added)\n",
    "\n",
    "class_names =[i for i in range(number_of_class)]\n",
    "for class_name in class_names:\n",
    "    class_counter[class_name] = 0\n",
    "for i in train['class']:\n",
    "    class_counter[i] += 1\n",
    "\n",
    "print('After:')\n",
    "print(class_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat((train.loc[:,'appearedTimeOfDay':'cooc_151'],\n",
    "                      test.loc[:,'appearedTimeOfDay':'cooc_151']))\n",
    "id = test['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data.applymap(lambda x: 1.0 if x == True else x)\n",
    "all_data = all_data.applymap(lambda x: 0.0 if x == False else x)\n",
    "all_data = pd.get_dummies(all_data)\n",
    "# numeric_feats = df.dtypes[df.dtypes != \"object\"].index\n",
    "# df = df[numeric_feats]\n",
    "apearedHour = all_data['appearedHour']\n",
    "appearedMinute = all_data['appearedMinute']\n",
    "appearedTimeDayCycle = apearedHour * 60 + appearedMinute\n",
    "appearedTimeDayCycle = np.sin(appearedTimeDayCycle / (24 * 60) * 2 * np.pi)\n",
    "# print('appearedTimeDayCycle= ',appearedTimeDayCycle)\n",
    "all_data = all_data.drop(['appearedHour'], axis=1)\n",
    "all_data = all_data.drop(['appearedMinute'], axis=1)\n",
    "all_data['appearedTimeDayCycle'] = appearedTimeDayCycle\n",
    "\n",
    "# df = df.drop(['temperature'], axis=1)\n",
    "# df = df.drop(['windSpeed'], axis=1)\n",
    "# df = df.drop(['pressure'], axis=1)\n",
    "# df = df.drop(['gymIn100m'], axis=1)\n",
    "# df = df.drop(['gymIn250m'], axis=1)\n",
    "# df = df.drop(['gymIn500m'], axis=1)\n",
    "# df = df.drop(['gymIn1000m'], axis=1)\n",
    "# df = df.drop(['gymIn2500m'], axis=1)\n",
    "# df = df.drop(['gymIn5000m'], axis=1)\n",
    "# df = df.drop(['rural'], axis=1)\n",
    "# df = df.drop(['midurban'], axis=1)\n",
    "# df = df.drop(['suburban'], axis=1)\n",
    "# df = df.drop(['urban'], axis=1)\n",
    "# df = df.drop(['pokestopIn100m'], axis=1)\n",
    "# df = df.drop(['pokestopIn250m'], axis=1)\n",
    "# df = df.drop(['pokestopIn500m'], axis=1)\n",
    "# df = df.drop(['pokestopIn1000m'], axis=1)\n",
    "# df = df.drop(['pokestopIn2500m'], axis=1)\n",
    "# df = df.drop(['pokestopIn5000m'], axis=1)\n",
    "# df = df.drop(['terrainType'], axis=1)\n",
    "# df = df.drop(['closeToWater'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(all_data.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's look at the data :\n",
    "# matplotlib.rcParams['figure.figsize'] = (6.0, 6.0)\n",
    "\n",
    "# data = pd.DataFrame({\"x\":df['population_density'], \"y\":targets})\n",
    "\n",
    "# data.plot(x = \"x\", y = \"y\",kind = \"scatter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normailize to 0-1\n",
    "for k in all_data.columns.values:\n",
    "    if (all_data[k].max() - all_data[k].min()) > 0:\n",
    "        all_data[k] = (all_data[k] - all_data[k].min())/(all_data[k].max() - all_data[k].min())\n",
    "    else:\n",
    "        all_data[k] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9666 entries, 0 to 1088\n",
      "Columns: 297 entries, terrainType to appearedTimeDayCycle\n",
      "dtypes: float64(272), int64(25)\n",
      "memory usage: 22.0 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9666, 297)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = all_data[:train.shape[0]]\n",
    "features_test = all_data[train.shape[0]:]\n",
    "targets = train['class']\n",
    "\n",
    "features.info()\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = plt.hist(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6766, 297), (2871, 297), (29, 297))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# split the data into training and validation sets\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(features.values, targets.values, test_size = 1 - train_ratio, stratify=targets.values, random_state=0)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_valid, y_valid, test_size = test_ratio, stratify=y_valid, random_state=0)\n",
    "X_train.shape,X_valid.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_ratio = 0.98\n",
    "\n",
    "test_ratio = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.hist(y_test)\n",
    "# plt.hist(y_valid)\n",
    "a =plt.hist(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data_utils\n",
    "\n",
    "data_train = data_utils.TensorDataset(torch.from_numpy(X_train).type((torch.FloatTensor)), torch.from_numpy(y_train).type((torch.LongTensor)))\n",
    "data_valid = data_utils.TensorDataset(torch.from_numpy(X_valid).type((torch.FloatTensor)), torch.from_numpy(y_valid).type((torch.LongTensor)))\n",
    "data_test = data_utils.TensorDataset(torch.from_numpy(X_test).type((torch.FloatTensor)), torch.from_numpy(y_test).type((torch.LongTensor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# convert data to torch.FloatTensor\n",
    "\n",
    "loaders = {}\n",
    "loaders['train'] = torch.utils.data.DataLoader(data_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=1)\n",
    "\n",
    "loaders['valid'] = torch.utils.data.DataLoader(data_valid,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=1)\n",
    "loaders['test'] = torch.utils.data.DataLoader(data_test,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "#     print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the CNN architecture\n",
    "class Net(nn.Module):\n",
    "    ### TODO: choose an architecture, and complete the class\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(297, 128)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.fc3 = nn.Linear(32, 6)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "#         x = self.dropout(x)\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "#         x = self.dropout(x)\n",
    "        x = F.sigmoid(self.fc3(x))\n",
    "\n",
    "\n",
    "        return x\n",
    "\n",
    "#-#-# You do NOT have to modify the code below this line. #-#-#\n",
    "\n",
    "# instantiate the CNN\n",
    "model = Net()\n",
    "def init_weights(m):\n",
    "        print(m)\n",
    "        if type(m) == nn.Linear:\n",
    "            m.weight.data.fill_(1.0)\n",
    "            print(m.weight)\n",
    "            \n",
    "def init_ortho(m):\n",
    "    print()\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.orthogonal_(m.weight)\n",
    "        print(m.weight)\n",
    "\n",
    "# use the modules apply function to recursively apply the initialization\n",
    "# model.apply(init_ortho)\n",
    "\n",
    "# move tensors to GPU if CUDA is available\n",
    "if use_cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "### TODO: select loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "### TODO: select optimizer\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "# optimizer = optim.Adamax(model.parameters(), lr=0.01)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01,weight_decay= 1e-6, momentum = 0.9, nesterov = True)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.6/site-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.794421 \tValidation Loss: 1.804024 \t time: 0.3\n",
      "Validation loss decreased from inf to 1.804024. Model was saved\n",
      "Epoch: 2 \tTraining Loss: 1.804070 \tValidation Loss: 1.795591 \t time: 0.3\n",
      "Validation loss decreased from 1.804024 to 1.795591. Model was saved\n",
      "Epoch: 3 \tTraining Loss: 1.795605 \tValidation Loss: 1.791557 \t time: 0.2\n",
      "Validation loss decreased from 1.795591 to 1.791557. Model was saved\n",
      "Epoch: 4 \tTraining Loss: 1.791561 \tValidation Loss: 1.788002 \t time: 0.2\n",
      "Validation loss decreased from 1.791557 to 1.788002. Model was saved\n",
      "Epoch: 5 \tTraining Loss: 1.787946 \tValidation Loss: 1.781280 \t time: 0.3\n",
      "Validation loss decreased from 1.788002 to 1.781280. Model was saved\n",
      "Epoch: 6 \tTraining Loss: 1.781235 \tValidation Loss: 1.771095 \t time: 0.3\n",
      "Validation loss decreased from 1.781280 to 1.771095. Model was saved\n",
      "Epoch: 7 \tTraining Loss: 1.771232 \tValidation Loss: 1.758436 \t time: 0.3\n",
      "Validation loss decreased from 1.771095 to 1.758436. Model was saved\n",
      "Epoch: 8 \tTraining Loss: 1.758618 \tValidation Loss: 1.743872 \t time: 0.2\n",
      "Validation loss decreased from 1.758436 to 1.743872. Model was saved\n",
      "Epoch: 9 \tTraining Loss: 1.744074 \tValidation Loss: 1.725482 \t time: 0.2\n",
      "Validation loss decreased from 1.743872 to 1.725482. Model was saved\n",
      "Epoch: 10 \tTraining Loss: 1.725925 \tValidation Loss: 1.705853 \t time: 0.2\n",
      "Validation loss decreased from 1.725482 to 1.705853. Model was saved\n",
      "Epoch: 11 \tTraining Loss: 1.706388 \tValidation Loss: 1.685424 \t time: 0.2\n",
      "Validation loss decreased from 1.705853 to 1.685424. Model was saved\n",
      "Epoch: 12 \tTraining Loss: 1.685646 \tValidation Loss: 1.665609 \t time: 0.2\n",
      "Validation loss decreased from 1.685424 to 1.665609. Model was saved\n",
      "Epoch: 13 \tTraining Loss: 1.665579 \tValidation Loss: 1.647745 \t time: 0.2\n",
      "Validation loss decreased from 1.665609 to 1.647745. Model was saved\n",
      "Epoch: 14 \tTraining Loss: 1.646568 \tValidation Loss: 1.633612 \t time: 0.2\n",
      "Validation loss decreased from 1.647745 to 1.633612. Model was saved\n",
      "Epoch: 15 \tTraining Loss: 1.630798 \tValidation Loss: 1.623533 \t time: 0.3\n",
      "Validation loss decreased from 1.633612 to 1.623533. Model was saved\n",
      "Epoch: 16 \tTraining Loss: 1.617890 \tValidation Loss: 1.613256 \t time: 0.3\n",
      "Validation loss decreased from 1.623533 to 1.613256. Model was saved\n",
      "Epoch: 17 \tTraining Loss: 1.605924 \tValidation Loss: 1.603258 \t time: 0.3\n",
      "Validation loss decreased from 1.613256 to 1.603258. Model was saved\n",
      "Epoch: 18 \tTraining Loss: 1.594092 \tValidation Loss: 1.594444 \t time: 0.2\n",
      "Validation loss decreased from 1.603258 to 1.594444. Model was saved\n",
      "Epoch: 19 \tTraining Loss: 1.582969 \tValidation Loss: 1.585146 \t time: 0.3\n",
      "Validation loss decreased from 1.594444 to 1.585146. Model was saved\n",
      "Epoch: 20 \tTraining Loss: 1.573253 \tValidation Loss: 1.578215 \t time: 0.3\n",
      "Validation loss decreased from 1.585146 to 1.578215. Model was saved\n",
      "Epoch: 21 \tTraining Loss: 1.563990 \tValidation Loss: 1.572346 \t time: 0.2\n",
      "Validation loss decreased from 1.578215 to 1.572346. Model was saved\n",
      "Epoch: 22 \tTraining Loss: 1.555981 \tValidation Loss: 1.565334 \t time: 0.2\n",
      "Validation loss decreased from 1.572346 to 1.565334. Model was saved\n",
      "Epoch: 23 \tTraining Loss: 1.548682 \tValidation Loss: 1.561653 \t time: 0.3\n",
      "Validation loss decreased from 1.565334 to 1.561653. Model was saved\n",
      "Epoch: 24 \tTraining Loss: 1.541963 \tValidation Loss: 1.554946 \t time: 0.2\n",
      "Validation loss decreased from 1.561653 to 1.554946. Model was saved\n",
      "Epoch: 25 \tTraining Loss: 1.535618 \tValidation Loss: 1.552003 \t time: 0.3\n",
      "Validation loss decreased from 1.554946 to 1.552003. Model was saved\n",
      "Epoch: 26 \tTraining Loss: 1.528915 \tValidation Loss: 1.548099 \t time: 0.3\n",
      "Validation loss decreased from 1.552003 to 1.548099. Model was saved\n",
      "Epoch: 27 \tTraining Loss: 1.522986 \tValidation Loss: 1.542091 \t time: 0.3\n",
      "Validation loss decreased from 1.548099 to 1.542091. Model was saved\n",
      "Epoch: 28 \tTraining Loss: 1.517457 \tValidation Loss: 1.540116 \t time: 0.2\n",
      "Validation loss decreased from 1.542091 to 1.540116. Model was saved\n",
      "Epoch: 29 \tTraining Loss: 1.512204 \tValidation Loss: 1.532694 \t time: 0.3\n",
      "Validation loss decreased from 1.540116 to 1.532694. Model was saved\n",
      "Epoch: 30 \tTraining Loss: 1.506404 \tValidation Loss: 1.530907 \t time: 0.3\n",
      "Validation loss decreased from 1.532694 to 1.530907. Model was saved\n",
      "Epoch: 31 \tTraining Loss: 1.500686 \tValidation Loss: 1.526107 \t time: 0.3\n",
      "Validation loss decreased from 1.530907 to 1.526107. Model was saved\n",
      "Epoch: 32 \tTraining Loss: 1.495332 \tValidation Loss: 1.521686 \t time: 0.3\n",
      "Validation loss decreased from 1.526107 to 1.521686. Model was saved\n",
      "Epoch: 33 \tTraining Loss: 1.491150 \tValidation Loss: 1.522080 \t time: 0.3\n",
      "Epoch: 34 \tTraining Loss: 1.487593 \tValidation Loss: 1.514894 \t time: 0.3\n",
      "Validation loss decreased from 1.521686 to 1.514894. Model was saved\n",
      "Epoch: 35 \tTraining Loss: 1.483818 \tValidation Loss: 1.514766 \t time: 0.2\n",
      "Validation loss decreased from 1.514894 to 1.514766. Model was saved\n",
      "Epoch: 36 \tTraining Loss: 1.479307 \tValidation Loss: 1.509520 \t time: 0.3\n",
      "Validation loss decreased from 1.514766 to 1.509520. Model was saved\n",
      "Epoch: 37 \tTraining Loss: 1.474577 \tValidation Loss: 1.505929 \t time: 0.3\n",
      "Validation loss decreased from 1.509520 to 1.505929. Model was saved\n",
      "Epoch: 38 \tTraining Loss: 1.470564 \tValidation Loss: 1.505372 \t time: 0.2\n",
      "Validation loss decreased from 1.505929 to 1.505372. Model was saved\n",
      "Epoch: 39 \tTraining Loss: 1.467273 \tValidation Loss: 1.499422 \t time: 0.3\n",
      "Validation loss decreased from 1.505372 to 1.499422. Model was saved\n",
      "Epoch: 40 \tTraining Loss: 1.464203 \tValidation Loss: 1.501933 \t time: 0.3\n",
      "Epoch: 41 \tTraining Loss: 1.461659 \tValidation Loss: 1.494758 \t time: 0.3\n",
      "Validation loss decreased from 1.499422 to 1.494758. Model was saved\n",
      "Epoch: 42 \tTraining Loss: 1.456750 \tValidation Loss: 1.493148 \t time: 0.3\n",
      "Validation loss decreased from 1.494758 to 1.493148. Model was saved\n",
      "Epoch: 43 \tTraining Loss: 1.452507 \tValidation Loss: 1.492903 \t time: 0.2\n",
      "Validation loss decreased from 1.493148 to 1.492903. Model was saved\n",
      "Epoch: 44 \tTraining Loss: 1.449755 \tValidation Loss: 1.488412 \t time: 0.3\n",
      "Validation loss decreased from 1.492903 to 1.488412. Model was saved\n",
      "Epoch: 45 \tTraining Loss: 1.447219 \tValidation Loss: 1.488935 \t time: 0.3\n",
      "Epoch: 46 \tTraining Loss: 1.443335 \tValidation Loss: 1.486161 \t time: 0.3\n",
      "Validation loss decreased from 1.488412 to 1.486161. Model was saved\n",
      "Epoch: 47 \tTraining Loss: 1.439845 \tValidation Loss: 1.483512 \t time: 0.3\n",
      "Validation loss decreased from 1.486161 to 1.483512. Model was saved\n",
      "Epoch: 48 \tTraining Loss: 1.438130 \tValidation Loss: 1.485801 \t time: 0.3\n",
      "Epoch: 49 \tTraining Loss: 1.436235 \tValidation Loss: 1.480470 \t time: 0.3\n",
      "Validation loss decreased from 1.483512 to 1.480470. Model was saved\n",
      "Epoch: 50 \tTraining Loss: 1.433242 \tValidation Loss: 1.480761 \t time: 0.3\n",
      "Epoch: 51 \tTraining Loss: 1.429592 \tValidation Loss: 1.478611 \t time: 0.3\n",
      "Validation loss decreased from 1.480470 to 1.478611. Model was saved\n",
      "Epoch: 52 \tTraining Loss: 1.426678 \tValidation Loss: 1.475554 \t time: 0.2\n",
      "Validation loss decreased from 1.478611 to 1.475554. Model was saved\n",
      "Epoch: 53 \tTraining Loss: 1.424917 \tValidation Loss: 1.476892 \t time: 0.3\n",
      "Epoch: 54 \tTraining Loss: 1.422174 \tValidation Loss: 1.473145 \t time: 0.3\n",
      "Validation loss decreased from 1.475554 to 1.473145. Model was saved\n",
      "Epoch: 55 \tTraining Loss: 1.419336 \tValidation Loss: 1.471449 \t time: 0.3\n",
      "Validation loss decreased from 1.473145 to 1.471449. Model was saved\n",
      "Epoch: 56 \tTraining Loss: 1.416397 \tValidation Loss: 1.471948 \t time: 0.3\n",
      "Epoch: 57 \tTraining Loss: 1.414328 \tValidation Loss: 1.468262 \t time: 0.3\n",
      "Validation loss decreased from 1.471449 to 1.468262. Model was saved\n",
      "Epoch: 58 \tTraining Loss: 1.412967 \tValidation Loss: 1.473955 \t time: 0.2\n",
      "Epoch: 59 \tTraining Loss: 1.412595 \tValidation Loss: 1.465926 \t time: 0.3\n",
      "Validation loss decreased from 1.468262 to 1.465926. Model was saved\n",
      "Epoch: 60 \tTraining Loss: 1.411132 \tValidation Loss: 1.468003 \t time: 0.3\n",
      "Epoch: 61 \tTraining Loss: 1.405408 \tValidation Loss: 1.467397 \t time: 0.3\n",
      "Epoch: 62 \tTraining Loss: 1.403419 \tValidation Loss: 1.463182 \t time: 0.3\n",
      "Validation loss decreased from 1.465926 to 1.463182. Model was saved\n",
      "Epoch: 63 \tTraining Loss: 1.403269 \tValidation Loss: 1.465886 \t time: 0.3\n",
      "Epoch: 64 \tTraining Loss: 1.398787 \tValidation Loss: 1.463500 \t time: 0.3\n",
      "Epoch: 65 \tTraining Loss: 1.396078 \tValidation Loss: 1.460816 \t time: 0.3\n",
      "Validation loss decreased from 1.463182 to 1.460816. Model was saved\n",
      "Epoch: 66 \tTraining Loss: 1.395221 \tValidation Loss: 1.466018 \t time: 0.3\n",
      "Epoch: 67 \tTraining Loss: 1.393896 \tValidation Loss: 1.459802 \t time: 0.2\n",
      "Validation loss decreased from 1.460816 to 1.459802. Model was saved\n",
      "Epoch: 68 \tTraining Loss: 1.390805 \tValidation Loss: 1.459595 \t time: 0.3\n",
      "Validation loss decreased from 1.459802 to 1.459595. Model was saved\n",
      "Epoch: 69 \tTraining Loss: 1.387267 \tValidation Loss: 1.461051 \t time: 0.3\n",
      "Epoch: 70 \tTraining Loss: 1.385592 \tValidation Loss: 1.458587 \t time: 0.3\n",
      "Validation loss decreased from 1.459595 to 1.458587. Model was saved\n",
      "Epoch: 71 \tTraining Loss: 1.384879 \tValidation Loss: 1.460550 \t time: 0.3\n",
      "Epoch: 72 \tTraining Loss: 1.382054 \tValidation Loss: 1.456751 \t time: 0.3\n",
      "Validation loss decreased from 1.458587 to 1.456751. Model was saved\n",
      "Epoch: 73 \tTraining Loss: 1.378952 \tValidation Loss: 1.456919 \t time: 0.2\n",
      "Epoch: 74 \tTraining Loss: 1.377218 \tValidation Loss: 1.457491 \t time: 0.3\n",
      "Epoch: 75 \tTraining Loss: 1.375234 \tValidation Loss: 1.454861 \t time: 0.3\n",
      "Validation loss decreased from 1.456751 to 1.454861. Model was saved\n",
      "Epoch: 76 \tTraining Loss: 1.373663 \tValidation Loss: 1.457594 \t time: 0.3\n",
      "Epoch: 77 \tTraining Loss: 1.372386 \tValidation Loss: 1.453842 \t time: 0.3\n",
      "Validation loss decreased from 1.454861 to 1.453842. Model was saved\n",
      "Epoch: 78 \tTraining Loss: 1.369219 \tValidation Loss: 1.453594 \t time: 0.3\n",
      "Validation loss decreased from 1.453842 to 1.453594. Model was saved\n",
      "Epoch: 79 \tTraining Loss: 1.366778 \tValidation Loss: 1.453619 \t time: 0.3\n",
      "Epoch: 80 \tTraining Loss: 1.365615 \tValidation Loss: 1.451255 \t time: 0.3\n",
      "Validation loss decreased from 1.453594 to 1.451255. Model was saved\n",
      "Epoch: 81 \tTraining Loss: 1.363724 \tValidation Loss: 1.452278 \t time: 0.2\n",
      "Epoch: 82 \tTraining Loss: 1.361578 \tValidation Loss: 1.450448 \t time: 0.3\n",
      "Validation loss decreased from 1.451255 to 1.450448. Model was saved\n",
      "Epoch: 83 \tTraining Loss: 1.360286 \tValidation Loss: 1.450693 \t time: 0.2\n",
      "Epoch: 84 \tTraining Loss: 1.359026 \tValidation Loss: 1.449166 \t time: 0.2\n",
      "Validation loss decreased from 1.450448 to 1.449166. Model was saved\n",
      "Epoch: 85 \tTraining Loss: 1.356528 \tValidation Loss: 1.450032 \t time: 0.3\n",
      "Epoch: 86 \tTraining Loss: 1.354927 \tValidation Loss: 1.447478 \t time: 0.3\n",
      "Validation loss decreased from 1.449166 to 1.447478. Model was saved\n",
      "Epoch: 87 \tTraining Loss: 1.353783 \tValidation Loss: 1.448988 \t time: 0.3\n",
      "Epoch: 88 \tTraining Loss: 1.351924 \tValidation Loss: 1.446443 \t time: 0.2\n",
      "Validation loss decreased from 1.447478 to 1.446443. Model was saved\n",
      "Epoch: 89 \tTraining Loss: 1.349608 \tValidation Loss: 1.446074 \t time: 0.3\n",
      "Validation loss decreased from 1.446443 to 1.446074. Model was saved\n",
      "Epoch: 90 \tTraining Loss: 1.348214 \tValidation Loss: 1.445764 \t time: 0.3\n",
      "Validation loss decreased from 1.446074 to 1.445764. Model was saved\n",
      "Epoch: 91 \tTraining Loss: 1.346712 \tValidation Loss: 1.444759 \t time: 0.2\n",
      "Validation loss decreased from 1.445764 to 1.444759. Model was saved\n",
      "Epoch: 92 \tTraining Loss: 1.344914 \tValidation Loss: 1.444981 \t time: 0.2\n",
      "Epoch: 93 \tTraining Loss: 1.343409 \tValidation Loss: 1.443711 \t time: 0.3\n",
      "Validation loss decreased from 1.444759 to 1.443711. Model was saved\n",
      "Epoch: 94 \tTraining Loss: 1.342084 \tValidation Loss: 1.445369 \t time: 0.3\n",
      "Epoch: 95 \tTraining Loss: 1.341453 \tValidation Loss: 1.443457 \t time: 0.3\n",
      "Validation loss decreased from 1.443711 to 1.443457. Model was saved\n",
      "Epoch: 96 \tTraining Loss: 1.342076 \tValidation Loss: 1.453569 \t time: 0.3\n",
      "Epoch: 97 \tTraining Loss: 1.348355 \tValidation Loss: 1.444564 \t time: 0.3\n",
      "Epoch: 98 \tTraining Loss: 1.344702 \tValidation Loss: 1.443544 \t time: 0.3\n",
      "Epoch: 99 \tTraining Loss: 1.337805 \tValidation Loss: 1.445642 \t time: 0.3\n",
      "Epoch: 100 \tTraining Loss: 1.339077 \tValidation Loss: 1.439476 \t time: 0.2\n",
      "Validation loss decreased from 1.443457 to 1.439476. Model was saved\n",
      "Epoch: 101 \tTraining Loss: 1.334227 \tValidation Loss: 1.440530 \t time: 0.3\n",
      "Epoch: 102 \tTraining Loss: 1.335125 \tValidation Loss: 1.442562 \t time: 0.3\n",
      "Epoch: 103 \tTraining Loss: 1.331832 \tValidation Loss: 1.442679 \t time: 0.2\n",
      "Epoch: 104 \tTraining Loss: 1.331442 \tValidation Loss: 1.438738 \t time: 0.3\n",
      "Validation loss decreased from 1.439476 to 1.438738. Model was saved\n",
      "Epoch: 105 \tTraining Loss: 1.328839 \tValidation Loss: 1.438009 \t time: 0.3\n",
      "Validation loss decreased from 1.438738 to 1.438009. Model was saved\n",
      "Epoch: 106 \tTraining Loss: 1.326762 \tValidation Loss: 1.439303 \t time: 0.3\n",
      "Epoch: 107 \tTraining Loss: 1.325503 \tValidation Loss: 1.436881 \t time: 0.3\n",
      "Validation loss decreased from 1.438009 to 1.436881. Model was saved\n",
      "Epoch: 108 \tTraining Loss: 1.321797 \tValidation Loss: 1.436710 \t time: 0.3\n",
      "Validation loss decreased from 1.436881 to 1.436710. Model was saved\n",
      "Epoch: 109 \tTraining Loss: 1.322252 \tValidation Loss: 1.434609 \t time: 0.3\n",
      "Validation loss decreased from 1.436710 to 1.434609. Model was saved\n",
      "Epoch: 110 \tTraining Loss: 1.317753 \tValidation Loss: 1.433417 \t time: 0.3\n",
      "Validation loss decreased from 1.434609 to 1.433417. Model was saved\n",
      "Epoch: 111 \tTraining Loss: 1.317449 \tValidation Loss: 1.430765 \t time: 0.3\n",
      "Validation loss decreased from 1.433417 to 1.430765. Model was saved\n",
      "Epoch: 112 \tTraining Loss: 1.314543 \tValidation Loss: 1.428662 \t time: 0.2\n",
      "Validation loss decreased from 1.430765 to 1.428662. Model was saved\n",
      "Epoch: 113 \tTraining Loss: 1.311910 \tValidation Loss: 1.427029 \t time: 0.3\n",
      "Validation loss decreased from 1.428662 to 1.427029. Model was saved\n",
      "Epoch: 114 \tTraining Loss: 1.310762 \tValidation Loss: 1.424997 \t time: 0.3\n",
      "Validation loss decreased from 1.427029 to 1.424997. Model was saved\n",
      "Epoch: 115 \tTraining Loss: 1.307133 \tValidation Loss: 1.425218 \t time: 0.3\n",
      "Epoch: 116 \tTraining Loss: 1.306700 \tValidation Loss: 1.422936 \t time: 0.3\n",
      "Validation loss decreased from 1.424997 to 1.422936. Model was saved\n",
      "Epoch: 117 \tTraining Loss: 1.304101 \tValidation Loss: 1.422022 \t time: 0.3\n",
      "Validation loss decreased from 1.422936 to 1.422022. Model was saved\n",
      "Epoch: 118 \tTraining Loss: 1.302631 \tValidation Loss: 1.423242 \t time: 0.3\n",
      "Epoch: 119 \tTraining Loss: 1.302024 \tValidation Loss: 1.421191 \t time: 0.3\n",
      "Validation loss decreased from 1.422022 to 1.421191. Model was saved\n",
      "Epoch: 120 \tTraining Loss: 1.299523 \tValidation Loss: 1.421389 \t time: 0.3\n",
      "Epoch: 121 \tTraining Loss: 1.299057 \tValidation Loss: 1.422424 \t time: 0.3\n",
      "Epoch: 122 \tTraining Loss: 1.298011 \tValidation Loss: 1.421384 \t time: 0.3\n",
      "Epoch: 123 \tTraining Loss: 1.296319 \tValidation Loss: 1.420678 \t time: 0.3\n",
      "Validation loss decreased from 1.421191 to 1.420678. Model was saved\n",
      "Epoch: 124 \tTraining Loss: 1.296111 \tValidation Loss: 1.421870 \t time: 0.3\n",
      "Epoch: 125 \tTraining Loss: 1.295419 \tValidation Loss: 1.420823 \t time: 0.2\n",
      "Epoch: 126 \tTraining Loss: 1.293294 \tValidation Loss: 1.419617 \t time: 0.3\n",
      "Validation loss decreased from 1.420678 to 1.419617. Model was saved\n",
      "Epoch: 127 \tTraining Loss: 1.292787 \tValidation Loss: 1.420206 \t time: 0.3\n",
      "Epoch: 128 \tTraining Loss: 1.290427 \tValidation Loss: 1.419883 \t time: 0.2\n",
      "Epoch: 129 \tTraining Loss: 1.289050 \tValidation Loss: 1.419121 \t time: 0.3\n",
      "Validation loss decreased from 1.419617 to 1.419121. Model was saved\n",
      "Epoch: 130 \tTraining Loss: 1.288133 \tValidation Loss: 1.420059 \t time: 0.3\n",
      "Epoch: 131 \tTraining Loss: 1.286381 \tValidation Loss: 1.420473 \t time: 0.3\n",
      "Epoch: 132 \tTraining Loss: 1.285644 \tValidation Loss: 1.419056 \t time: 0.3\n",
      "Validation loss decreased from 1.419121 to 1.419056. Model was saved\n",
      "Epoch: 133 \tTraining Loss: 1.285004 \tValidation Loss: 1.419227 \t time: 0.3\n",
      "Epoch: 134 \tTraining Loss: 1.284342 \tValidation Loss: 1.420898 \t time: 0.2\n",
      "Epoch: 135 \tTraining Loss: 1.284350 \tValidation Loss: 1.421194 \t time: 0.3\n",
      "Epoch: 136 \tTraining Loss: 1.286050 \tValidation Loss: 1.421212 \t time: 0.3\n",
      "Epoch: 137 \tTraining Loss: 1.284513 \tValidation Loss: 1.425937 \t time: 0.3\n",
      "Epoch: 138 \tTraining Loss: 1.287326 \tValidation Loss: 1.416844 \t time: 0.3\n",
      "Validation loss decreased from 1.419056 to 1.416844. Model was saved\n",
      "Epoch: 139 \tTraining Loss: 1.279482 \tValidation Loss: 1.421507 \t time: 0.2\n",
      "Epoch: 140 \tTraining Loss: 1.286002 \tValidation Loss: 1.442391 \t time: 0.2\n",
      "Epoch: 141 \tTraining Loss: 1.307685 \tValidation Loss: 1.419078 \t time: 0.3\n",
      "Epoch: 142 \tTraining Loss: 1.280218 \tValidation Loss: 1.435212 \t time: 0.3\n",
      "Epoch: 143 \tTraining Loss: 1.314459 \tValidation Loss: 1.424712 \t time: 0.2\n",
      "Epoch: 144 \tTraining Loss: 1.288627 \tValidation Loss: 1.447134 \t time: 0.3\n",
      "Epoch: 145 \tTraining Loss: 1.312154 \tValidation Loss: 1.418854 \t time: 0.3\n",
      "Epoch: 146 \tTraining Loss: 1.282504 \tValidation Loss: 1.422560 \t time: 0.3\n",
      "Epoch: 147 \tTraining Loss: 1.299054 \tValidation Loss: 1.416980 \t time: 0.3\n",
      "Epoch: 148 \tTraining Loss: 1.286858 \tValidation Loss: 1.420822 \t time: 0.3\n",
      "Epoch: 149 \tTraining Loss: 1.280590 \tValidation Loss: 1.434166 \t time: 0.3\n",
      "Epoch: 150 \tTraining Loss: 1.292667 \tValidation Loss: 1.420507 \t time: 0.3\n",
      "Epoch: 151 \tTraining Loss: 1.277849 \tValidation Loss: 1.420176 \t time: 0.3\n",
      "Epoch: 152 \tTraining Loss: 1.282285 \tValidation Loss: 1.417117 \t time: 0.2\n",
      "Epoch: 153 \tTraining Loss: 1.277623 \tValidation Loss: 1.414315 \t time: 0.3\n",
      "Validation loss decreased from 1.416844 to 1.414315. Model was saved\n",
      "Epoch: 154 \tTraining Loss: 1.272916 \tValidation Loss: 1.420630 \t time: 0.3\n",
      "Epoch: 155 \tTraining Loss: 1.277643 \tValidation Loss: 1.418187 \t time: 0.2\n",
      "Epoch: 156 \tTraining Loss: 1.273409 \tValidation Loss: 1.413236 \t time: 0.3\n",
      "Validation loss decreased from 1.414315 to 1.413236. Model was saved\n",
      "Epoch: 157 \tTraining Loss: 1.268501 \tValidation Loss: 1.416858 \t time: 0.3\n",
      "Epoch: 158 \tTraining Loss: 1.273995 \tValidation Loss: 1.414970 \t time: 0.2\n",
      "Epoch: 159 \tTraining Loss: 1.265924 \tValidation Loss: 1.418856 \t time: 0.2\n",
      "Epoch: 160 \tTraining Loss: 1.268776 \tValidation Loss: 1.416052 \t time: 0.3\n",
      "Epoch: 161 \tTraining Loss: 1.265240 \tValidation Loss: 1.413629 \t time: 0.3\n",
      "Epoch: 162 \tTraining Loss: 1.263696 \tValidation Loss: 1.412205 \t time: 0.3\n",
      "Validation loss decreased from 1.413236 to 1.412205. Model was saved\n",
      "Epoch: 163 \tTraining Loss: 1.264440 \tValidation Loss: 1.410557 \t time: 0.3\n",
      "Validation loss decreased from 1.412205 to 1.410557. Model was saved\n",
      "Epoch: 164 \tTraining Loss: 1.260166 \tValidation Loss: 1.415066 \t time: 0.3\n",
      "Epoch: 165 \tTraining Loss: 1.262082 \tValidation Loss: 1.412136 \t time: 0.3\n",
      "Epoch: 166 \tTraining Loss: 1.259373 \tValidation Loss: 1.408626 \t time: 0.3\n",
      "Validation loss decreased from 1.410557 to 1.408626. Model was saved\n",
      "Epoch: 167 \tTraining Loss: 1.258061 \tValidation Loss: 1.409803 \t time: 0.3\n",
      "Epoch: 168 \tTraining Loss: 1.258942 \tValidation Loss: 1.409443 \t time: 0.2\n",
      "Epoch: 169 \tTraining Loss: 1.255912 \tValidation Loss: 1.411163 \t time: 0.3\n",
      "Epoch: 170 \tTraining Loss: 1.256484 \tValidation Loss: 1.409360 \t time: 0.3\n",
      "Epoch: 171 \tTraining Loss: 1.254975 \tValidation Loss: 1.407305 \t time: 0.2\n",
      "Validation loss decreased from 1.408626 to 1.407305. Model was saved\n",
      "Epoch: 172 \tTraining Loss: 1.253957 \tValidation Loss: 1.408182 \t time: 0.3\n",
      "Epoch: 173 \tTraining Loss: 1.254183 \tValidation Loss: 1.407596 \t time: 0.3\n",
      "Epoch: 174 \tTraining Loss: 1.252017 \tValidation Loss: 1.409513 \t time: 0.2\n",
      "Epoch: 175 \tTraining Loss: 1.252570 \tValidation Loss: 1.408431 \t time: 0.3\n",
      "Epoch: 176 \tTraining Loss: 1.250764 \tValidation Loss: 1.407191 \t time: 0.3\n",
      "Validation loss decreased from 1.407305 to 1.407191. Model was saved\n",
      "Epoch: 177 \tTraining Loss: 1.250643 \tValidation Loss: 1.406203 \t time: 0.2\n",
      "Validation loss decreased from 1.407191 to 1.406203. Model was saved\n",
      "Epoch: 178 \tTraining Loss: 1.249621 \tValidation Loss: 1.408142 \t time: 0.3\n",
      "Epoch: 179 \tTraining Loss: 1.249044 \tValidation Loss: 1.408544 \t time: 0.3\n",
      "Epoch: 180 \tTraining Loss: 1.248700 \tValidation Loss: 1.406201 \t time: 0.3\n",
      "Validation loss decreased from 1.406203 to 1.406201. Model was saved\n",
      "Epoch: 181 \tTraining Loss: 1.247473 \tValidation Loss: 1.405928 \t time: 0.3\n",
      "Validation loss decreased from 1.406201 to 1.405928. Model was saved\n",
      "Epoch: 182 \tTraining Loss: 1.247535 \tValidation Loss: 1.406471 \t time: 0.2\n",
      "Epoch: 183 \tTraining Loss: 1.246107 \tValidation Loss: 1.407224 \t time: 0.2\n",
      "Epoch: 184 \tTraining Loss: 1.246076 \tValidation Loss: 1.405258 \t time: 0.3\n",
      "Validation loss decreased from 1.405928 to 1.405258. Model was saved\n",
      "Epoch: 185 \tTraining Loss: 1.244998 \tValidation Loss: 1.404744 \t time: 0.3\n",
      "Validation loss decreased from 1.405258 to 1.404744. Model was saved\n",
      "Epoch: 186 \tTraining Loss: 1.244931 \tValidation Loss: 1.404779 \t time: 0.3\n",
      "Epoch: 187 \tTraining Loss: 1.243861 \tValidation Loss: 1.405994 \t time: 0.3\n",
      "Epoch: 188 \tTraining Loss: 1.243729 \tValidation Loss: 1.405358 \t time: 0.3\n",
      "Epoch: 189 \tTraining Loss: 1.242930 \tValidation Loss: 1.405074 \t time: 0.3\n",
      "Epoch: 190 \tTraining Loss: 1.242612 \tValidation Loss: 1.404565 \t time: 0.3\n",
      "Validation loss decreased from 1.404744 to 1.404565. Model was saved\n",
      "Epoch: 191 \tTraining Loss: 1.241904 \tValidation Loss: 1.405014 \t time: 0.3\n",
      "Epoch: 192 \tTraining Loss: 1.241436 \tValidation Loss: 1.404406 \t time: 0.2\n",
      "Validation loss decreased from 1.404565 to 1.404406. Model was saved\n",
      "Epoch: 193 \tTraining Loss: 1.240882 \tValidation Loss: 1.403355 \t time: 0.3\n",
      "Validation loss decreased from 1.404406 to 1.403355. Model was saved\n",
      "Epoch: 194 \tTraining Loss: 1.240373 \tValidation Loss: 1.402770 \t time: 0.2\n",
      "Validation loss decreased from 1.403355 to 1.402770. Model was saved\n",
      "Epoch: 195 \tTraining Loss: 1.239872 \tValidation Loss: 1.403211 \t time: 0.3\n",
      "Epoch: 196 \tTraining Loss: 1.239416 \tValidation Loss: 1.402718 \t time: 0.3\n",
      "Validation loss decreased from 1.402770 to 1.402718. Model was saved\n",
      "Epoch: 197 \tTraining Loss: 1.239018 \tValidation Loss: 1.402143 \t time: 0.2\n",
      "Validation loss decreased from 1.402718 to 1.402143. Model was saved\n",
      "Epoch: 198 \tTraining Loss: 1.238674 \tValidation Loss: 1.402020 \t time: 0.2\n",
      "Validation loss decreased from 1.402143 to 1.402020. Model was saved\n",
      "Epoch: 199 \tTraining Loss: 1.238574 \tValidation Loss: 1.402814 \t time: 0.2\n",
      "Epoch: 200 \tTraining Loss: 1.238577 \tValidation Loss: 1.402405 \t time: 0.3\n",
      "Epoch: 201 \tTraining Loss: 1.240036 \tValidation Loss: 1.402635 \t time: 0.3\n",
      "Epoch: 202 \tTraining Loss: 1.238235 \tValidation Loss: 1.401793 \t time: 0.3\n",
      "Validation loss decreased from 1.402020 to 1.401793. Model was saved\n",
      "Epoch: 203 \tTraining Loss: 1.236992 \tValidation Loss: 1.401824 \t time: 0.3\n",
      "Epoch: 204 \tTraining Loss: 1.235965 \tValidation Loss: 1.402619 \t time: 0.3\n",
      "Epoch: 205 \tTraining Loss: 1.236319 \tValidation Loss: 1.402365 \t time: 0.3\n",
      "Epoch: 206 \tTraining Loss: 1.236649 \tValidation Loss: 1.401797 \t time: 0.3\n",
      "Epoch: 207 \tTraining Loss: 1.235086 \tValidation Loss: 1.402599 \t time: 0.3\n",
      "Epoch: 208 \tTraining Loss: 1.234557 \tValidation Loss: 1.402539 \t time: 0.3\n",
      "Epoch: 209 \tTraining Loss: 1.235027 \tValidation Loss: 1.401185 \t time: 0.3\n",
      "Validation loss decreased from 1.401793 to 1.401185. Model was saved\n",
      "Epoch: 210 \tTraining Loss: 1.234155 \tValidation Loss: 1.401957 \t time: 0.3\n",
      "Epoch: 211 \tTraining Loss: 1.233297 \tValidation Loss: 1.402398 \t time: 0.3\n",
      "Epoch: 212 \tTraining Loss: 1.233415 \tValidation Loss: 1.401671 \t time: 0.3\n",
      "Epoch: 213 \tTraining Loss: 1.232906 \tValidation Loss: 1.401856 \t time: 0.3\n",
      "Epoch: 214 \tTraining Loss: 1.232283 \tValidation Loss: 1.401819 \t time: 0.3\n",
      "Epoch: 215 \tTraining Loss: 1.231911 \tValidation Loss: 1.402060 \t time: 0.2\n",
      "Epoch: 216 \tTraining Loss: 1.231790 \tValidation Loss: 1.402031 \t time: 0.3\n",
      "Epoch: 217 \tTraining Loss: 1.231220 \tValidation Loss: 1.401326 \t time: 0.3\n",
      "Epoch: 218 \tTraining Loss: 1.230673 \tValidation Loss: 1.400996 \t time: 0.3\n",
      "Validation loss decreased from 1.401185 to 1.400996. Model was saved\n",
      "Epoch: 219 \tTraining Loss: 1.230485 \tValidation Loss: 1.401219 \t time: 0.2\n",
      "Epoch: 220 \tTraining Loss: 1.230293 \tValidation Loss: 1.400881 \t time: 0.2\n",
      "Validation loss decreased from 1.400996 to 1.400881. Model was saved\n",
      "Epoch: 221 \tTraining Loss: 1.229748 \tValidation Loss: 1.400488 \t time: 0.2\n",
      "Validation loss decreased from 1.400881 to 1.400488. Model was saved\n",
      "Epoch: 222 \tTraining Loss: 1.229415 \tValidation Loss: 1.400954 \t time: 0.3\n",
      "Epoch: 223 \tTraining Loss: 1.229264 \tValidation Loss: 1.400703 \t time: 0.3\n",
      "Epoch: 224 \tTraining Loss: 1.228945 \tValidation Loss: 1.399983 \t time: 0.3\n",
      "Validation loss decreased from 1.400488 to 1.399983. Model was saved\n",
      "Epoch: 225 \tTraining Loss: 1.228648 \tValidation Loss: 1.400420 \t time: 0.2\n",
      "Epoch: 226 \tTraining Loss: 1.228258 \tValidation Loss: 1.400198 \t time: 0.3\n",
      "Epoch: 227 \tTraining Loss: 1.228037 \tValidation Loss: 1.399822 \t time: 0.2\n",
      "Validation loss decreased from 1.399983 to 1.399822. Model was saved\n",
      "Epoch: 228 \tTraining Loss: 1.227850 \tValidation Loss: 1.400316 \t time: 0.3\n",
      "Epoch: 229 \tTraining Loss: 1.227515 \tValidation Loss: 1.400091 \t time: 0.3\n",
      "Epoch: 230 \tTraining Loss: 1.227100 \tValidation Loss: 1.400028 \t time: 0.3\n",
      "Epoch: 231 \tTraining Loss: 1.226754 \tValidation Loss: 1.400139 \t time: 0.2\n",
      "Epoch: 232 \tTraining Loss: 1.226564 \tValidation Loss: 1.399317 \t time: 0.2\n",
      "Validation loss decreased from 1.399822 to 1.399317. Model was saved\n",
      "Epoch: 233 \tTraining Loss: 1.226346 \tValidation Loss: 1.399754 \t time: 0.2\n",
      "Epoch: 234 \tTraining Loss: 1.225993 \tValidation Loss: 1.399315 \t time: 0.2\n",
      "Validation loss decreased from 1.399317 to 1.399315. Model was saved\n",
      "Epoch: 235 \tTraining Loss: 1.225638 \tValidation Loss: 1.399199 \t time: 0.3\n",
      "Validation loss decreased from 1.399315 to 1.399199. Model was saved\n",
      "Epoch: 236 \tTraining Loss: 1.225307 \tValidation Loss: 1.399578 \t time: 0.3\n",
      "Epoch: 237 \tTraining Loss: 1.225000 \tValidation Loss: 1.399023 \t time: 0.2\n",
      "Validation loss decreased from 1.399199 to 1.399023. Model was saved\n",
      "Epoch: 238 \tTraining Loss: 1.224678 \tValidation Loss: 1.398862 \t time: 0.3\n",
      "Validation loss decreased from 1.399023 to 1.398862. Model was saved\n",
      "Epoch: 239 \tTraining Loss: 1.224473 \tValidation Loss: 1.398848 \t time: 0.3\n",
      "Validation loss decreased from 1.398862 to 1.398848. Model was saved\n",
      "Epoch: 240 \tTraining Loss: 1.224290 \tValidation Loss: 1.398340 \t time: 0.2\n",
      "Validation loss decreased from 1.398848 to 1.398340. Model was saved\n",
      "Epoch: 241 \tTraining Loss: 1.224046 \tValidation Loss: 1.398132 \t time: 0.3\n",
      "Validation loss decreased from 1.398340 to 1.398132. Model was saved\n",
      "Epoch: 242 \tTraining Loss: 1.223758 \tValidation Loss: 1.398562 \t time: 0.3\n",
      "Epoch: 243 \tTraining Loss: 1.223428 \tValidation Loss: 1.398012 \t time: 0.2\n",
      "Validation loss decreased from 1.398132 to 1.398012. Model was saved\n",
      "Epoch: 244 \tTraining Loss: 1.223078 \tValidation Loss: 1.398195 \t time: 0.3\n",
      "Epoch: 245 \tTraining Loss: 1.222648 \tValidation Loss: 1.397868 \t time: 0.3\n",
      "Validation loss decreased from 1.398012 to 1.397868. Model was saved\n",
      "Epoch: 246 \tTraining Loss: 1.222340 \tValidation Loss: 1.397607 \t time: 0.3\n",
      "Validation loss decreased from 1.397868 to 1.397607. Model was saved\n",
      "Epoch: 247 \tTraining Loss: 1.222053 \tValidation Loss: 1.397479 \t time: 0.2\n",
      "Validation loss decreased from 1.397607 to 1.397479. Model was saved\n",
      "Epoch: 248 \tTraining Loss: 1.221733 \tValidation Loss: 1.396872 \t time: 0.3\n",
      "Validation loss decreased from 1.397479 to 1.396872. Model was saved\n",
      "Epoch: 249 \tTraining Loss: 1.221360 \tValidation Loss: 1.396510 \t time: 0.2\n",
      "Validation loss decreased from 1.396872 to 1.396510. Model was saved\n",
      "Epoch: 250 \tTraining Loss: 1.220913 \tValidation Loss: 1.396498 \t time: 0.3\n",
      "Validation loss decreased from 1.396510 to 1.396498. Model was saved\n",
      "Epoch: 251 \tTraining Loss: 1.220548 \tValidation Loss: 1.396031 \t time: 0.3\n",
      "Validation loss decreased from 1.396498 to 1.396031. Model was saved\n",
      "Epoch: 252 \tTraining Loss: 1.220221 \tValidation Loss: 1.396024 \t time: 0.3\n",
      "Validation loss decreased from 1.396031 to 1.396024. Model was saved\n",
      "Epoch: 253 \tTraining Loss: 1.219932 \tValidation Loss: 1.395769 \t time: 0.3\n",
      "Validation loss decreased from 1.396024 to 1.395769. Model was saved\n",
      "Epoch: 254 \tTraining Loss: 1.219666 \tValidation Loss: 1.395063 \t time: 0.3\n",
      "Validation loss decreased from 1.395769 to 1.395063. Model was saved\n",
      "Epoch: 255 \tTraining Loss: 1.219349 \tValidation Loss: 1.394874 \t time: 0.3\n",
      "Validation loss decreased from 1.395063 to 1.394874. Model was saved\n",
      "Epoch: 256 \tTraining Loss: 1.219012 \tValidation Loss: 1.394501 \t time: 0.3\n",
      "Validation loss decreased from 1.394874 to 1.394501. Model was saved\n",
      "Epoch: 257 \tTraining Loss: 1.218665 \tValidation Loss: 1.394308 \t time: 0.3\n",
      "Validation loss decreased from 1.394501 to 1.394308. Model was saved\n",
      "Epoch: 258 \tTraining Loss: 1.218342 \tValidation Loss: 1.394254 \t time: 0.2\n",
      "Validation loss decreased from 1.394308 to 1.394254. Model was saved\n",
      "Epoch: 259 \tTraining Loss: 1.218076 \tValidation Loss: 1.394405 \t time: 0.3\n",
      "Epoch: 260 \tTraining Loss: 1.218221 \tValidation Loss: 1.394332 \t time: 0.3\n",
      "Epoch: 261 \tTraining Loss: 1.218865 \tValidation Loss: 1.396021 \t time: 0.3\n",
      "Epoch: 262 \tTraining Loss: 1.222986 \tValidation Loss: 1.393503 \t time: 0.3\n",
      "Validation loss decreased from 1.394254 to 1.393503. Model was saved\n",
      "Epoch: 263 \tTraining Loss: 1.218990 \tValidation Loss: 1.392507 \t time: 0.3\n",
      "Validation loss decreased from 1.393503 to 1.392507. Model was saved\n",
      "Epoch: 264 \tTraining Loss: 1.216659 \tValidation Loss: 1.392987 \t time: 0.2\n",
      "Epoch: 265 \tTraining Loss: 1.215951 \tValidation Loss: 1.393175 \t time: 0.3\n",
      "Epoch: 266 \tTraining Loss: 1.216794 \tValidation Loss: 1.394541 \t time: 0.3\n",
      "Epoch: 267 \tTraining Loss: 1.218254 \tValidation Loss: 1.390688 \t time: 0.3\n",
      "Validation loss decreased from 1.392507 to 1.390688. Model was saved\n",
      "Epoch: 268 \tTraining Loss: 1.214970 \tValidation Loss: 1.390651 \t time: 0.2\n",
      "Validation loss decreased from 1.390688 to 1.390651. Model was saved\n",
      "Epoch: 269 \tTraining Loss: 1.213365 \tValidation Loss: 1.393111 \t time: 0.2\n",
      "Epoch: 270 \tTraining Loss: 1.214471 \tValidation Loss: 1.390527 \t time: 0.3\n",
      "Validation loss decreased from 1.390651 to 1.390527. Model was saved\n",
      "Epoch: 271 \tTraining Loss: 1.212318 \tValidation Loss: 1.391339 \t time: 0.3\n",
      "Epoch: 272 \tTraining Loss: 1.212367 \tValidation Loss: 1.392196 \t time: 0.3\n",
      "Epoch: 273 \tTraining Loss: 1.213497 \tValidation Loss: 1.388643 \t time: 0.3\n",
      "Validation loss decreased from 1.390527 to 1.388643. Model was saved\n",
      "Epoch: 274 \tTraining Loss: 1.211235 \tValidation Loss: 1.389691 \t time: 0.3\n",
      "Epoch: 275 \tTraining Loss: 1.211488 \tValidation Loss: 1.392364 \t time: 0.3\n",
      "Epoch: 276 \tTraining Loss: 1.212835 \tValidation Loss: 1.387382 \t time: 0.2\n",
      "Validation loss decreased from 1.388643 to 1.387382. Model was saved\n",
      "Epoch: 277 \tTraining Loss: 1.211324 \tValidation Loss: 1.389621 \t time: 0.3\n",
      "Epoch: 278 \tTraining Loss: 1.212093 \tValidation Loss: 1.393083 \t time: 0.3\n",
      "Epoch: 279 \tTraining Loss: 1.213878 \tValidation Loss: 1.391406 \t time: 0.3\n",
      "Epoch: 280 \tTraining Loss: 1.210663 \tValidation Loss: 1.390523 \t time: 0.3\n",
      "Epoch: 281 \tTraining Loss: 1.213961 \tValidation Loss: 1.390492 \t time: 0.3\n",
      "Epoch: 282 \tTraining Loss: 1.212558 \tValidation Loss: 1.390505 \t time: 0.2\n",
      "Epoch: 283 \tTraining Loss: 1.211391 \tValidation Loss: 1.389604 \t time: 0.3\n",
      "Epoch: 284 \tTraining Loss: 1.213087 \tValidation Loss: 1.388339 \t time: 0.3\n",
      "Epoch: 285 \tTraining Loss: 1.209122 \tValidation Loss: 1.391382 \t time: 0.3\n",
      "Epoch: 286 \tTraining Loss: 1.210977 \tValidation Loss: 1.388904 \t time: 0.3\n",
      "Epoch: 287 \tTraining Loss: 1.210033 \tValidation Loss: 1.388519 \t time: 0.2\n",
      "Epoch: 288 \tTraining Loss: 1.210501 \tValidation Loss: 1.387301 \t time: 0.2\n",
      "Validation loss decreased from 1.387382 to 1.387301. Model was saved\n",
      "Epoch: 289 \tTraining Loss: 1.208000 \tValidation Loss: 1.389404 \t time: 0.3\n",
      "Epoch: 290 \tTraining Loss: 1.208783 \tValidation Loss: 1.388927 \t time: 0.3\n",
      "Epoch: 291 \tTraining Loss: 1.207603 \tValidation Loss: 1.387571 \t time: 0.2\n",
      "Epoch: 292 \tTraining Loss: 1.207518 \tValidation Loss: 1.387316 \t time: 0.3\n",
      "Epoch: 293 \tTraining Loss: 1.207045 \tValidation Loss: 1.388615 \t time: 0.2\n",
      "Epoch: 294 \tTraining Loss: 1.206967 \tValidation Loss: 1.388136 \t time: 0.2\n",
      "Epoch: 295 \tTraining Loss: 1.206581 \tValidation Loss: 1.387385 \t time: 0.2\n",
      "Epoch: 296 \tTraining Loss: 1.205748 \tValidation Loss: 1.387912 \t time: 0.2\n",
      "Epoch: 297 \tTraining Loss: 1.205829 \tValidation Loss: 1.388060 \t time: 0.2\n",
      "Epoch: 298 \tTraining Loss: 1.205304 \tValidation Loss: 1.387555 \t time: 0.2\n",
      "Epoch: 299 \tTraining Loss: 1.204768 \tValidation Loss: 1.387563 \t time: 0.3\n",
      "Epoch: 300 \tTraining Loss: 1.204689 \tValidation Loss: 1.389047 \t time: 0.3\n",
      "Epoch: 301 \tTraining Loss: 1.204901 \tValidation Loss: 1.389831 \t time: 0.3\n",
      "Epoch: 302 \tTraining Loss: 1.203971 \tValidation Loss: 1.388198 \t time: 0.3\n",
      "Epoch: 303 \tTraining Loss: 1.204093 \tValidation Loss: 1.387384 \t time: 0.3\n",
      "Epoch: 304 \tTraining Loss: 1.203877 \tValidation Loss: 1.387753 \t time: 0.2\n",
      "Epoch: 305 \tTraining Loss: 1.203328 \tValidation Loss: 1.388712 \t time: 0.3\n",
      "Epoch: 306 \tTraining Loss: 1.203092 \tValidation Loss: 1.388415 \t time: 0.2\n",
      "Epoch: 307 \tTraining Loss: 1.203266 \tValidation Loss: 1.388177 \t time: 0.3\n",
      "Epoch: 308 \tTraining Loss: 1.202451 \tValidation Loss: 1.388800 \t time: 0.3\n",
      "Epoch: 309 \tTraining Loss: 1.202730 \tValidation Loss: 1.388237 \t time: 0.3\n",
      "Epoch: 310 \tTraining Loss: 1.202284 \tValidation Loss: 1.387305 \t time: 0.3\n",
      "Epoch: 311 \tTraining Loss: 1.201817 \tValidation Loss: 1.387601 \t time: 0.3\n",
      "Epoch: 312 \tTraining Loss: 1.201899 \tValidation Loss: 1.388038 \t time: 0.2\n",
      "Epoch: 313 \tTraining Loss: 1.201748 \tValidation Loss: 1.387736 \t time: 0.3\n",
      "Epoch: 314 \tTraining Loss: 1.201341 \tValidation Loss: 1.387642 \t time: 0.3\n",
      "Epoch: 315 \tTraining Loss: 1.201305 \tValidation Loss: 1.387402 \t time: 0.3\n",
      "Epoch: 316 \tTraining Loss: 1.201145 \tValidation Loss: 1.387057 \t time: 0.3\n",
      "Validation loss decreased from 1.387301 to 1.387057. Model was saved\n",
      "Epoch: 317 \tTraining Loss: 1.200702 \tValidation Loss: 1.387660 \t time: 0.3\n",
      "Epoch: 318 \tTraining Loss: 1.200919 \tValidation Loss: 1.388120 \t time: 0.3\n",
      "Epoch: 319 \tTraining Loss: 1.200503 \tValidation Loss: 1.387721 \t time: 0.2\n",
      "Epoch: 320 \tTraining Loss: 1.200268 \tValidation Loss: 1.387184 \t time: 0.3\n",
      "Epoch: 321 \tTraining Loss: 1.200223 \tValidation Loss: 1.387444 \t time: 0.3\n",
      "Epoch: 322 \tTraining Loss: 1.199841 \tValidation Loss: 1.387594 \t time: 0.3\n",
      "Epoch: 323 \tTraining Loss: 1.199499 \tValidation Loss: 1.387390 \t time: 0.3\n",
      "Epoch: 324 \tTraining Loss: 1.199474 \tValidation Loss: 1.387140 \t time: 0.3\n",
      "Epoch: 325 \tTraining Loss: 1.199221 \tValidation Loss: 1.387338 \t time: 0.3\n",
      "Epoch: 326 \tTraining Loss: 1.198730 \tValidation Loss: 1.387069 \t time: 0.3\n",
      "Epoch: 327 \tTraining Loss: 1.198860 \tValidation Loss: 1.386799 \t time: 0.3\n",
      "Validation loss decreased from 1.387057 to 1.386799. Model was saved\n",
      "Epoch: 328 \tTraining Loss: 1.198841 \tValidation Loss: 1.386997 \t time: 0.3\n",
      "Epoch: 329 \tTraining Loss: 1.198320 \tValidation Loss: 1.387504 \t time: 0.3\n",
      "Epoch: 330 \tTraining Loss: 1.198448 \tValidation Loss: 1.386495 \t time: 0.3\n",
      "Validation loss decreased from 1.386799 to 1.386495. Model was saved\n",
      "Epoch: 331 \tTraining Loss: 1.198248 \tValidation Loss: 1.386557 \t time: 0.3\n",
      "Epoch: 332 \tTraining Loss: 1.197791 \tValidation Loss: 1.387949 \t time: 0.3\n",
      "Epoch: 333 \tTraining Loss: 1.197861 \tValidation Loss: 1.387224 \t time: 0.3\n",
      "Epoch: 334 \tTraining Loss: 1.197765 \tValidation Loss: 1.386317 \t time: 0.3\n",
      "Validation loss decreased from 1.386495 to 1.386317. Model was saved\n",
      "Epoch: 335 \tTraining Loss: 1.197463 \tValidation Loss: 1.386730 \t time: 0.2\n",
      "Epoch: 336 \tTraining Loss: 1.197554 \tValidation Loss: 1.386748 \t time: 0.3\n",
      "Epoch: 337 \tTraining Loss: 1.197493 \tValidation Loss: 1.386426 \t time: 0.3\n",
      "Epoch: 338 \tTraining Loss: 1.197114 \tValidation Loss: 1.387094 \t time: 0.3\n",
      "Epoch: 339 \tTraining Loss: 1.197195 \tValidation Loss: 1.387338 \t time: 0.2\n",
      "Epoch: 340 \tTraining Loss: 1.197026 \tValidation Loss: 1.387053 \t time: 0.3\n",
      "Epoch: 341 \tTraining Loss: 1.196711 \tValidation Loss: 1.387405 \t time: 0.3\n",
      "Epoch: 342 \tTraining Loss: 1.196885 \tValidation Loss: 1.388085 \t time: 0.2\n",
      "Epoch: 343 \tTraining Loss: 1.196707 \tValidation Loss: 1.388348 \t time: 0.3\n",
      "Epoch: 344 \tTraining Loss: 1.196404 \tValidation Loss: 1.388398 \t time: 0.2\n",
      "Epoch: 345 \tTraining Loss: 1.196526 \tValidation Loss: 1.388590 \t time: 0.2\n",
      "Epoch: 346 \tTraining Loss: 1.196286 \tValidation Loss: 1.388611 \t time: 0.2\n",
      "Epoch: 347 \tTraining Loss: 1.196029 \tValidation Loss: 1.388806 \t time: 0.3\n",
      "Epoch: 348 \tTraining Loss: 1.196135 \tValidation Loss: 1.389029 \t time: 0.3\n",
      "Epoch: 349 \tTraining Loss: 1.195859 \tValidation Loss: 1.389084 \t time: 0.3\n",
      "Epoch: 350 \tTraining Loss: 1.195668 \tValidation Loss: 1.389173 \t time: 0.3\n",
      "Epoch: 351 \tTraining Loss: 1.195707 \tValidation Loss: 1.389429 \t time: 0.3\n",
      "Epoch: 352 \tTraining Loss: 1.195437 \tValidation Loss: 1.389555 \t time: 0.2\n",
      "Epoch: 353 \tTraining Loss: 1.195198 \tValidation Loss: 1.389496 \t time: 0.3\n",
      "Epoch: 354 \tTraining Loss: 1.195213 \tValidation Loss: 1.389546 \t time: 0.3\n",
      "Epoch: 355 \tTraining Loss: 1.195004 \tValidation Loss: 1.389536 \t time: 0.2\n",
      "Epoch: 356 \tTraining Loss: 1.194748 \tValidation Loss: 1.389566 \t time: 0.3\n",
      "Epoch: 357 \tTraining Loss: 1.194737 \tValidation Loss: 1.389550 \t time: 0.3\n",
      "Epoch: 358 \tTraining Loss: 1.194569 \tValidation Loss: 1.389454 \t time: 0.3\n",
      "Epoch: 359 \tTraining Loss: 1.194345 \tValidation Loss: 1.389369 \t time: 0.2\n",
      "Epoch: 360 \tTraining Loss: 1.194346 \tValidation Loss: 1.389390 \t time: 0.3\n",
      "Epoch: 361 \tTraining Loss: 1.194220 \tValidation Loss: 1.389555 \t time: 0.2\n",
      "Epoch: 362 \tTraining Loss: 1.193981 \tValidation Loss: 1.389743 \t time: 0.3\n",
      "Epoch: 363 \tTraining Loss: 1.193950 \tValidation Loss: 1.389655 \t time: 0.3\n",
      "Epoch: 364 \tTraining Loss: 1.193902 \tValidation Loss: 1.389598 \t time: 0.3\n",
      "Epoch: 365 \tTraining Loss: 1.193624 \tValidation Loss: 1.389607 \t time: 0.3\n",
      "Epoch: 366 \tTraining Loss: 1.193479 \tValidation Loss: 1.389956 \t time: 0.2\n",
      "Epoch: 367 \tTraining Loss: 1.193405 \tValidation Loss: 1.390231 \t time: 0.2\n",
      "Epoch: 368 \tTraining Loss: 1.193231 \tValidation Loss: 1.390520 \t time: 0.3\n",
      "Epoch: 369 \tTraining Loss: 1.193072 \tValidation Loss: 1.390741 \t time: 0.2\n",
      "Epoch: 370 \tTraining Loss: 1.193027 \tValidation Loss: 1.390728 \t time: 0.2\n",
      "Epoch: 371 \tTraining Loss: 1.192912 \tValidation Loss: 1.390485 \t time: 0.3\n",
      "Epoch: 372 \tTraining Loss: 1.192765 \tValidation Loss: 1.390237 \t time: 0.3\n",
      "Epoch: 373 \tTraining Loss: 1.192680 \tValidation Loss: 1.389918 \t time: 0.3\n",
      "Epoch: 374 \tTraining Loss: 1.192539 \tValidation Loss: 1.389722 \t time: 0.3\n",
      "Epoch: 375 \tTraining Loss: 1.192402 \tValidation Loss: 1.389776 \t time: 0.3\n",
      "Epoch: 376 \tTraining Loss: 1.192315 \tValidation Loss: 1.389712 \t time: 0.2\n",
      "Epoch: 377 \tTraining Loss: 1.192202 \tValidation Loss: 1.389640 \t time: 0.3\n",
      "Epoch: 378 \tTraining Loss: 1.192090 \tValidation Loss: 1.389757 \t time: 0.3\n",
      "Epoch: 379 \tTraining Loss: 1.192013 \tValidation Loss: 1.389706 \t time: 0.3\n",
      "Epoch: 380 \tTraining Loss: 1.191943 \tValidation Loss: 1.389598 \t time: 0.2\n",
      "Epoch: 381 \tTraining Loss: 1.191843 \tValidation Loss: 1.389532 \t time: 0.3\n",
      "Epoch: 382 \tTraining Loss: 1.191731 \tValidation Loss: 1.389701 \t time: 0.3\n",
      "Epoch: 383 \tTraining Loss: 1.191658 \tValidation Loss: 1.389859 \t time: 0.3\n",
      "Epoch: 384 \tTraining Loss: 1.191593 \tValidation Loss: 1.389828 \t time: 0.3\n",
      "Epoch: 385 \tTraining Loss: 1.191513 \tValidation Loss: 1.390072 \t time: 0.3\n",
      "Epoch: 386 \tTraining Loss: 1.191447 \tValidation Loss: 1.390181 \t time: 0.3\n",
      "Epoch: 387 \tTraining Loss: 1.191401 \tValidation Loss: 1.390157 \t time: 0.3\n",
      "Epoch: 388 \tTraining Loss: 1.191346 \tValidation Loss: 1.390337 \t time: 0.3\n",
      "Epoch: 389 \tTraining Loss: 1.191288 \tValidation Loss: 1.390352 \t time: 0.3\n",
      "Epoch: 390 \tTraining Loss: 1.191231 \tValidation Loss: 1.390294 \t time: 0.3\n",
      "Epoch: 391 \tTraining Loss: 1.191164 \tValidation Loss: 1.390393 \t time: 0.3\n",
      "Epoch: 392 \tTraining Loss: 1.191105 \tValidation Loss: 1.390303 \t time: 0.3\n",
      "Epoch: 393 \tTraining Loss: 1.191045 \tValidation Loss: 1.390413 \t time: 0.3\n",
      "Epoch: 394 \tTraining Loss: 1.190984 \tValidation Loss: 1.390593 \t time: 0.3\n",
      "Epoch: 395 \tTraining Loss: 1.190927 \tValidation Loss: 1.390377 \t time: 0.3\n",
      "Epoch: 396 \tTraining Loss: 1.190864 \tValidation Loss: 1.390482 \t time: 0.3\n",
      "Epoch: 397 \tTraining Loss: 1.190793 \tValidation Loss: 1.390526 \t time: 0.3\n",
      "Epoch: 398 \tTraining Loss: 1.190728 \tValidation Loss: 1.390409 \t time: 0.2\n",
      "Epoch: 399 \tTraining Loss: 1.190661 \tValidation Loss: 1.390596 \t time: 0.3\n",
      "Epoch: 400 \tTraining Loss: 1.190587 \tValidation Loss: 1.390686 \t time: 0.3\n",
      "Epoch: 401 \tTraining Loss: 1.190513 \tValidation Loss: 1.390689 \t time: 0.2\n",
      "Epoch: 402 \tTraining Loss: 1.190446 \tValidation Loss: 1.390693 \t time: 0.2\n",
      "Epoch: 403 \tTraining Loss: 1.190385 \tValidation Loss: 1.390558 \t time: 0.3\n",
      "Epoch: 404 \tTraining Loss: 1.190326 \tValidation Loss: 1.390614 \t time: 0.3\n",
      "Epoch: 405 \tTraining Loss: 1.190256 \tValidation Loss: 1.390548 \t time: 0.3\n",
      "Epoch: 406 \tTraining Loss: 1.190162 \tValidation Loss: 1.390477 \t time: 0.3\n",
      "Epoch: 407 \tTraining Loss: 1.190076 \tValidation Loss: 1.390768 \t time: 0.3\n",
      "Epoch: 408 \tTraining Loss: 1.190030 \tValidation Loss: 1.390834 \t time: 0.3\n",
      "Epoch: 409 \tTraining Loss: 1.189983 \tValidation Loss: 1.390746 \t time: 0.3\n",
      "Epoch: 410 \tTraining Loss: 1.189919 \tValidation Loss: 1.390747 \t time: 0.3\n",
      "Epoch: 411 \tTraining Loss: 1.189885 \tValidation Loss: 1.390618 \t time: 0.3\n",
      "Epoch: 412 \tTraining Loss: 1.189815 \tValidation Loss: 1.390351 \t time: 0.3\n",
      "Epoch: 413 \tTraining Loss: 1.189777 \tValidation Loss: 1.390263 \t time: 0.2\n",
      "Epoch: 414 \tTraining Loss: 1.189731 \tValidation Loss: 1.390327 \t time: 0.3\n",
      "Epoch: 415 \tTraining Loss: 1.189679 \tValidation Loss: 1.390528 \t time: 0.3\n",
      "Epoch: 416 \tTraining Loss: 1.189629 \tValidation Loss: 1.390466 \t time: 0.3\n",
      "Epoch: 417 \tTraining Loss: 1.189580 \tValidation Loss: 1.390390 \t time: 0.3\n",
      "Epoch: 418 \tTraining Loss: 1.189543 \tValidation Loss: 1.390518 \t time: 0.3\n",
      "Epoch: 419 \tTraining Loss: 1.189506 \tValidation Loss: 1.390324 \t time: 0.3\n",
      "Epoch: 420 \tTraining Loss: 1.189464 \tValidation Loss: 1.390095 \t time: 0.3\n",
      "Epoch: 421 \tTraining Loss: 1.189423 \tValidation Loss: 1.390104 \t time: 0.3\n",
      "Epoch: 422 \tTraining Loss: 1.189366 \tValidation Loss: 1.390000 \t time: 0.3\n",
      "Epoch: 423 \tTraining Loss: 1.189312 \tValidation Loss: 1.389862 \t time: 0.3\n",
      "Epoch: 424 \tTraining Loss: 1.189266 \tValidation Loss: 1.389905 \t time: 0.2\n",
      "Epoch: 425 \tTraining Loss: 1.189200 \tValidation Loss: 1.390059 \t time: 0.2\n",
      "Epoch: 426 \tTraining Loss: 1.189131 \tValidation Loss: 1.390187 \t time: 0.3\n",
      "Epoch: 427 \tTraining Loss: 1.189079 \tValidation Loss: 1.390166 \t time: 0.2\n",
      "Epoch: 428 \tTraining Loss: 1.189031 \tValidation Loss: 1.390108 \t time: 0.2\n",
      "Epoch: 429 \tTraining Loss: 1.188983 \tValidation Loss: 1.390031 \t time: 0.3\n",
      "Epoch: 430 \tTraining Loss: 1.188923 \tValidation Loss: 1.389744 \t time: 0.3\n",
      "Epoch: 431 \tTraining Loss: 1.188855 \tValidation Loss: 1.389463 \t time: 0.2\n",
      "Epoch: 432 \tTraining Loss: 1.188790 \tValidation Loss: 1.389446 \t time: 0.3\n",
      "Epoch: 433 \tTraining Loss: 1.188718 \tValidation Loss: 1.389527 \t time: 0.3\n",
      "Epoch: 434 \tTraining Loss: 1.188658 \tValidation Loss: 1.389360 \t time: 0.3\n",
      "Epoch: 435 \tTraining Loss: 1.188544 \tValidation Loss: 1.389339 \t time: 0.3\n",
      "Epoch: 436 \tTraining Loss: 1.188410 \tValidation Loss: 1.389084 \t time: 0.2\n",
      "Epoch: 437 \tTraining Loss: 1.188369 \tValidation Loss: 1.388780 \t time: 0.3\n",
      "Epoch: 438 \tTraining Loss: 1.188339 \tValidation Loss: 1.388529 \t time: 0.3\n",
      "Epoch: 439 \tTraining Loss: 1.188303 \tValidation Loss: 1.388385 \t time: 0.3\n",
      "Epoch: 440 \tTraining Loss: 1.188251 \tValidation Loss: 1.388390 \t time: 0.3\n",
      "Epoch: 441 \tTraining Loss: 1.188183 \tValidation Loss: 1.388460 \t time: 0.3\n",
      "Epoch: 442 \tTraining Loss: 1.188112 \tValidation Loss: 1.388515 \t time: 0.3\n",
      "Epoch: 443 \tTraining Loss: 1.188025 \tValidation Loss: 1.388532 \t time: 0.3\n",
      "Epoch: 444 \tTraining Loss: 1.187944 \tValidation Loss: 1.388523 \t time: 0.3\n",
      "Epoch: 445 \tTraining Loss: 1.187870 \tValidation Loss: 1.388494 \t time: 0.3\n",
      "Epoch: 446 \tTraining Loss: 1.187768 \tValidation Loss: 1.388439 \t time: 0.3\n",
      "Epoch: 447 \tTraining Loss: 1.187673 \tValidation Loss: 1.388548 \t time: 0.3\n",
      "Epoch: 448 \tTraining Loss: 1.187601 \tValidation Loss: 1.388779 \t time: 0.3\n",
      "Epoch: 449 \tTraining Loss: 1.187502 \tValidation Loss: 1.389006 \t time: 0.2\n",
      "Epoch: 450 \tTraining Loss: 1.187403 \tValidation Loss: 1.389263 \t time: 0.3\n",
      "Epoch: 451 \tTraining Loss: 1.187357 \tValidation Loss: 1.389604 \t time: 0.3\n",
      "Epoch: 452 \tTraining Loss: 1.187321 \tValidation Loss: 1.389694 \t time: 0.2\n",
      "Epoch: 453 \tTraining Loss: 1.187279 \tValidation Loss: 1.389664 \t time: 0.3\n",
      "Epoch: 454 \tTraining Loss: 1.187232 \tValidation Loss: 1.389670 \t time: 0.3\n",
      "Epoch: 455 \tTraining Loss: 1.187188 \tValidation Loss: 1.389573 \t time: 0.3\n",
      "Epoch: 456 \tTraining Loss: 1.187156 \tValidation Loss: 1.389440 \t time: 0.2\n",
      "Epoch: 457 \tTraining Loss: 1.187114 \tValidation Loss: 1.389392 \t time: 0.3\n",
      "Epoch: 458 \tTraining Loss: 1.187074 \tValidation Loss: 1.389529 \t time: 0.3\n",
      "Epoch: 459 \tTraining Loss: 1.187038 \tValidation Loss: 1.389742 \t time: 0.3\n",
      "Epoch: 460 \tTraining Loss: 1.186989 \tValidation Loss: 1.389960 \t time: 0.3\n",
      "Epoch: 461 \tTraining Loss: 1.186921 \tValidation Loss: 1.390238 \t time: 0.3\n",
      "Epoch: 462 \tTraining Loss: 1.186857 \tValidation Loss: 1.390389 \t time: 0.3\n",
      "Epoch: 463 \tTraining Loss: 1.186817 \tValidation Loss: 1.390380 \t time: 0.3\n",
      "Epoch: 464 \tTraining Loss: 1.186775 \tValidation Loss: 1.390591 \t time: 0.3\n",
      "Epoch: 465 \tTraining Loss: 1.186674 \tValidation Loss: 1.390800 \t time: 0.3\n",
      "Epoch: 466 \tTraining Loss: 1.186585 \tValidation Loss: 1.390876 \t time: 0.3\n",
      "Epoch: 467 \tTraining Loss: 1.186546 \tValidation Loss: 1.390899 \t time: 0.3\n",
      "Epoch: 468 \tTraining Loss: 1.186489 \tValidation Loss: 1.390941 \t time: 0.3\n",
      "Epoch: 469 \tTraining Loss: 1.186432 \tValidation Loss: 1.391063 \t time: 0.3\n",
      "Epoch: 470 \tTraining Loss: 1.186381 \tValidation Loss: 1.391057 \t time: 0.3\n",
      "Epoch: 471 \tTraining Loss: 1.186350 \tValidation Loss: 1.390972 \t time: 0.2\n",
      "Epoch: 472 \tTraining Loss: 1.186317 \tValidation Loss: 1.390818 \t time: 0.3\n",
      "Epoch: 473 \tTraining Loss: 1.186256 \tValidation Loss: 1.390684 \t time: 0.3\n",
      "Epoch: 474 \tTraining Loss: 1.186191 \tValidation Loss: 1.390585 \t time: 0.3\n",
      "Epoch: 475 \tTraining Loss: 1.186130 \tValidation Loss: 1.390411 \t time: 0.3\n",
      "Epoch: 476 \tTraining Loss: 1.186078 \tValidation Loss: 1.390350 \t time: 0.3\n",
      "Epoch: 477 \tTraining Loss: 1.185859 \tValidation Loss: 1.389961 \t time: 0.3\n",
      "Epoch: 478 \tTraining Loss: 1.185718 \tValidation Loss: 1.389867 \t time: 0.3\n",
      "Epoch: 479 \tTraining Loss: 1.185718 \tValidation Loss: 1.389645 \t time: 0.3\n",
      "Epoch: 480 \tTraining Loss: 1.185627 \tValidation Loss: 1.389766 \t time: 0.3\n",
      "Epoch: 481 \tTraining Loss: 1.185570 \tValidation Loss: 1.389715 \t time: 0.3\n",
      "Epoch: 482 \tTraining Loss: 1.185531 \tValidation Loss: 1.389611 \t time: 0.3\n",
      "Epoch: 483 \tTraining Loss: 1.185495 \tValidation Loss: 1.389934 \t time: 0.3\n",
      "Epoch: 484 \tTraining Loss: 1.185475 \tValidation Loss: 1.390018 \t time: 0.3\n",
      "Epoch: 485 \tTraining Loss: 1.185458 \tValidation Loss: 1.389877 \t time: 0.3\n",
      "Epoch: 486 \tTraining Loss: 1.185426 \tValidation Loss: 1.390059 \t time: 0.2\n",
      "Epoch: 487 \tTraining Loss: 1.185407 \tValidation Loss: 1.389969 \t time: 0.3\n",
      "Epoch: 488 \tTraining Loss: 1.185371 \tValidation Loss: 1.389982 \t time: 0.3\n",
      "Epoch: 489 \tTraining Loss: 1.185337 \tValidation Loss: 1.390187 \t time: 0.2\n",
      "Epoch: 490 \tTraining Loss: 1.185305 \tValidation Loss: 1.389977 \t time: 0.3\n",
      "Epoch: 491 \tTraining Loss: 1.185250 \tValidation Loss: 1.389989 \t time: 0.3\n",
      "Epoch: 492 \tTraining Loss: 1.185207 \tValidation Loss: 1.390121 \t time: 0.2\n",
      "Epoch: 493 \tTraining Loss: 1.185159 \tValidation Loss: 1.390016 \t time: 0.3\n",
      "Epoch: 494 \tTraining Loss: 1.185070 \tValidation Loss: 1.390212 \t time: 0.3\n",
      "Epoch: 495 \tTraining Loss: 1.184948 \tValidation Loss: 1.390491 \t time: 0.3\n",
      "Epoch: 496 \tTraining Loss: 1.184828 \tValidation Loss: 1.390737 \t time: 0.3\n",
      "Epoch: 497 \tTraining Loss: 1.184792 \tValidation Loss: 1.391166 \t time: 0.3\n",
      "Epoch: 498 \tTraining Loss: 1.184764 \tValidation Loss: 1.391133 \t time: 0.2\n",
      "Epoch: 499 \tTraining Loss: 1.184691 \tValidation Loss: 1.391045 \t time: 0.3\n",
      "Epoch: 500 \tTraining Loss: 1.184545 \tValidation Loss: 1.391255 \t time: 0.3\n",
      "Epoch: 501 \tTraining Loss: 1.184521 \tValidation Loss: 1.391161 \t time: 0.2\n",
      "Epoch: 502 \tTraining Loss: 1.184474 \tValidation Loss: 1.390993 \t time: 0.3\n",
      "Epoch: 503 \tTraining Loss: 1.184446 \tValidation Loss: 1.391110 \t time: 0.3\n",
      "Epoch: 504 \tTraining Loss: 1.184384 \tValidation Loss: 1.391255 \t time: 0.3\n",
      "Epoch: 505 \tTraining Loss: 1.184275 \tValidation Loss: 1.391353 \t time: 0.3\n",
      "Epoch: 506 \tTraining Loss: 1.184013 \tValidation Loss: 1.391400 \t time: 0.3\n",
      "Epoch: 507 \tTraining Loss: 1.183939 \tValidation Loss: 1.391817 \t time: 0.3\n",
      "Epoch: 508 \tTraining Loss: 1.183908 \tValidation Loss: 1.392000 \t time: 0.3\n",
      "Epoch: 509 \tTraining Loss: 1.183868 \tValidation Loss: 1.391882 \t time: 0.3\n",
      "Epoch: 510 \tTraining Loss: 1.183834 \tValidation Loss: 1.392186 \t time: 0.3\n",
      "Epoch: 511 \tTraining Loss: 1.183795 \tValidation Loss: 1.392249 \t time: 0.3\n",
      "Epoch: 512 \tTraining Loss: 1.183752 \tValidation Loss: 1.392185 \t time: 0.3\n",
      "Epoch: 513 \tTraining Loss: 1.183710 \tValidation Loss: 1.392459 \t time: 0.3\n",
      "Epoch: 514 \tTraining Loss: 1.183669 \tValidation Loss: 1.392536 \t time: 0.3\n",
      "Epoch: 515 \tTraining Loss: 1.183632 \tValidation Loss: 1.392446 \t time: 0.3\n",
      "Epoch: 516 \tTraining Loss: 1.183600 \tValidation Loss: 1.392439 \t time: 0.3\n",
      "Epoch: 517 \tTraining Loss: 1.183579 \tValidation Loss: 1.392443 \t time: 0.3\n",
      "Epoch: 518 \tTraining Loss: 1.183555 \tValidation Loss: 1.392567 \t time: 0.4\n",
      "Epoch: 519 \tTraining Loss: 1.183535 \tValidation Loss: 1.392544 \t time: 0.3\n",
      "Epoch: 520 \tTraining Loss: 1.183513 \tValidation Loss: 1.392366 \t time: 0.3\n",
      "Epoch: 521 \tTraining Loss: 1.183487 \tValidation Loss: 1.392410 \t time: 0.3\n",
      "Epoch: 522 \tTraining Loss: 1.183451 \tValidation Loss: 1.392403 \t time: 0.2\n",
      "Epoch: 523 \tTraining Loss: 1.183422 \tValidation Loss: 1.392266 \t time: 0.3\n",
      "Epoch: 524 \tTraining Loss: 1.183394 \tValidation Loss: 1.392368 \t time: 0.3\n",
      "Epoch: 525 \tTraining Loss: 1.183357 \tValidation Loss: 1.392417 \t time: 0.3\n",
      "Epoch: 526 \tTraining Loss: 1.183298 \tValidation Loss: 1.392351 \t time: 0.3\n",
      "Epoch: 527 \tTraining Loss: 1.183170 \tValidation Loss: 1.392604 \t time: 0.3\n",
      "Epoch: 528 \tTraining Loss: 1.183135 \tValidation Loss: 1.392510 \t time: 0.3\n",
      "Epoch: 529 \tTraining Loss: 1.183043 \tValidation Loss: 1.392556 \t time: 0.3\n",
      "Epoch: 530 \tTraining Loss: 1.182932 \tValidation Loss: 1.392362 \t time: 0.3\n",
      "Epoch: 531 \tTraining Loss: 1.182781 \tValidation Loss: 1.392728 \t time: 0.2\n",
      "Epoch: 532 \tTraining Loss: 1.182607 \tValidation Loss: 1.392260 \t time: 0.2\n",
      "Epoch: 533 \tTraining Loss: 1.182528 \tValidation Loss: 1.392259 \t time: 0.3\n",
      "Epoch: 534 \tTraining Loss: 1.182435 \tValidation Loss: 1.392334 \t time: 0.3\n",
      "Epoch: 535 \tTraining Loss: 1.182361 \tValidation Loss: 1.392423 \t time: 0.2\n",
      "Epoch: 536 \tTraining Loss: 1.182314 \tValidation Loss: 1.392414 \t time: 0.2\n",
      "Epoch: 537 \tTraining Loss: 1.182265 \tValidation Loss: 1.392444 \t time: 0.3\n",
      "Epoch: 538 \tTraining Loss: 1.182155 \tValidation Loss: 1.392474 \t time: 0.2\n",
      "Epoch: 539 \tTraining Loss: 1.182057 \tValidation Loss: 1.392163 \t time: 0.3\n",
      "Epoch: 540 \tTraining Loss: 1.182019 \tValidation Loss: 1.392054 \t time: 0.3\n",
      "Epoch: 541 \tTraining Loss: 1.182015 \tValidation Loss: 1.392019 \t time: 0.2\n",
      "Epoch: 542 \tTraining Loss: 1.181994 \tValidation Loss: 1.391807 \t time: 0.3\n",
      "Epoch: 543 \tTraining Loss: 1.181898 \tValidation Loss: 1.392215 \t time: 0.3\n",
      "Epoch: 544 \tTraining Loss: 1.181828 \tValidation Loss: 1.392304 \t time: 0.3\n",
      "Epoch: 545 \tTraining Loss: 1.181802 \tValidation Loss: 1.391954 \t time: 0.3\n",
      "Epoch: 546 \tTraining Loss: 1.181785 \tValidation Loss: 1.392017 \t time: 0.3\n",
      "Epoch: 547 \tTraining Loss: 1.181823 \tValidation Loss: 1.391724 \t time: 0.2\n",
      "Epoch: 548 \tTraining Loss: 1.181731 \tValidation Loss: 1.391772 \t time: 0.3\n",
      "Epoch: 549 \tTraining Loss: 1.181662 \tValidation Loss: 1.391790 \t time: 0.3\n",
      "Epoch: 550 \tTraining Loss: 1.181650 \tValidation Loss: 1.391705 \t time: 0.3\n",
      "Epoch: 551 \tTraining Loss: 1.181609 \tValidation Loss: 1.391760 \t time: 0.3\n",
      "Epoch: 552 \tTraining Loss: 1.181584 \tValidation Loss: 1.391517 \t time: 0.3\n",
      "Epoch: 553 \tTraining Loss: 1.181525 \tValidation Loss: 1.391907 \t time: 0.2\n",
      "Epoch: 554 \tTraining Loss: 1.181446 \tValidation Loss: 1.392422 \t time: 0.3\n",
      "Epoch: 555 \tTraining Loss: 1.181387 \tValidation Loss: 1.391534 \t time: 0.3\n",
      "Epoch: 556 \tTraining Loss: 1.181341 \tValidation Loss: 1.391831 \t time: 0.2\n",
      "Epoch: 557 \tTraining Loss: 1.181279 \tValidation Loss: 1.392146 \t time: 0.3\n",
      "Epoch: 558 \tTraining Loss: 1.181274 \tValidation Loss: 1.391298 \t time: 0.3\n",
      "Epoch: 559 \tTraining Loss: 1.181239 \tValidation Loss: 1.391722 \t time: 0.3\n",
      "Epoch: 560 \tTraining Loss: 1.181203 \tValidation Loss: 1.391982 \t time: 0.3\n",
      "Epoch: 561 \tTraining Loss: 1.181180 \tValidation Loss: 1.391565 \t time: 0.3\n",
      "Epoch: 562 \tTraining Loss: 1.181166 \tValidation Loss: 1.392043 \t time: 0.3\n",
      "Epoch: 563 \tTraining Loss: 1.181141 \tValidation Loss: 1.392144 \t time: 0.3\n",
      "Epoch: 564 \tTraining Loss: 1.181077 \tValidation Loss: 1.392003 \t time: 0.3\n",
      "Epoch: 565 \tTraining Loss: 1.181038 \tValidation Loss: 1.392189 \t time: 0.3\n",
      "Epoch: 566 \tTraining Loss: 1.180991 \tValidation Loss: 1.391898 \t time: 0.3\n",
      "Epoch: 567 \tTraining Loss: 1.180905 \tValidation Loss: 1.392355 \t time: 0.3\n",
      "Epoch: 568 \tTraining Loss: 1.180851 \tValidation Loss: 1.392680 \t time: 0.3\n",
      "Epoch: 569 \tTraining Loss: 1.180821 \tValidation Loss: 1.392274 \t time: 0.3\n",
      "Epoch: 570 \tTraining Loss: 1.180773 \tValidation Loss: 1.392652 \t time: 0.3\n",
      "Epoch: 571 \tTraining Loss: 1.180694 \tValidation Loss: 1.392536 \t time: 0.3\n",
      "Epoch: 572 \tTraining Loss: 1.180576 \tValidation Loss: 1.392562 \t time: 0.3\n",
      "Epoch: 573 \tTraining Loss: 1.180547 \tValidation Loss: 1.392932 \t time: 0.3\n",
      "Epoch: 574 \tTraining Loss: 1.180512 \tValidation Loss: 1.392719 \t time: 0.3\n",
      "Epoch: 575 \tTraining Loss: 1.180419 \tValidation Loss: 1.392422 \t time: 0.3\n",
      "Epoch: 576 \tTraining Loss: 1.180338 \tValidation Loss: 1.392633 \t time: 0.3\n",
      "Epoch: 577 \tTraining Loss: 1.180294 \tValidation Loss: 1.392734 \t time: 0.2\n",
      "Epoch: 578 \tTraining Loss: 1.180255 \tValidation Loss: 1.392987 \t time: 0.3\n",
      "Epoch: 579 \tTraining Loss: 1.180198 \tValidation Loss: 1.393077 \t time: 0.3\n",
      "Epoch: 580 \tTraining Loss: 1.180063 \tValidation Loss: 1.393131 \t time: 0.3\n",
      "Epoch: 581 \tTraining Loss: 1.179997 \tValidation Loss: 1.392916 \t time: 0.3\n",
      "Epoch: 582 \tTraining Loss: 1.179970 \tValidation Loss: 1.392610 \t time: 0.3\n",
      "Epoch: 583 \tTraining Loss: 1.179905 \tValidation Loss: 1.392949 \t time: 0.2\n",
      "Epoch: 584 \tTraining Loss: 1.179855 \tValidation Loss: 1.392797 \t time: 0.3\n",
      "Epoch: 585 \tTraining Loss: 1.179767 \tValidation Loss: 1.392722 \t time: 0.3\n",
      "Epoch: 586 \tTraining Loss: 1.179623 \tValidation Loss: 1.393466 \t time: 0.3\n",
      "Epoch: 587 \tTraining Loss: 1.179557 \tValidation Loss: 1.393034 \t time: 0.2\n",
      "Epoch: 588 \tTraining Loss: 1.179512 \tValidation Loss: 1.393175 \t time: 0.2\n",
      "Epoch: 589 \tTraining Loss: 1.179454 \tValidation Loss: 1.393564 \t time: 0.3\n",
      "Epoch: 590 \tTraining Loss: 1.179388 \tValidation Loss: 1.393116 \t time: 0.3\n",
      "Epoch: 591 \tTraining Loss: 1.179350 \tValidation Loss: 1.393108 \t time: 0.3\n",
      "Epoch: 592 \tTraining Loss: 1.179309 \tValidation Loss: 1.392992 \t time: 0.3\n",
      "Epoch: 593 \tTraining Loss: 1.179245 \tValidation Loss: 1.392793 \t time: 0.3\n",
      "Epoch: 594 \tTraining Loss: 1.179185 \tValidation Loss: 1.392652 \t time: 0.3\n",
      "Epoch: 595 \tTraining Loss: 1.179138 \tValidation Loss: 1.392797 \t time: 0.3\n",
      "Epoch: 596 \tTraining Loss: 1.179078 \tValidation Loss: 1.393055 \t time: 0.3\n",
      "Epoch: 597 \tTraining Loss: 1.179031 \tValidation Loss: 1.392949 \t time: 0.3\n",
      "Epoch: 598 \tTraining Loss: 1.178995 \tValidation Loss: 1.393065 \t time: 0.3\n",
      "Epoch: 599 \tTraining Loss: 1.178968 \tValidation Loss: 1.393177 \t time: 0.2\n",
      "Epoch: 600 \tTraining Loss: 1.178941 \tValidation Loss: 1.392953 \t time: 0.3\n",
      "Epoch: 601 \tTraining Loss: 1.178859 \tValidation Loss: 1.393070 \t time: 0.3\n",
      "Epoch: 602 \tTraining Loss: 1.178856 \tValidation Loss: 1.393037 \t time: 0.3\n",
      "Epoch: 603 \tTraining Loss: 1.178827 \tValidation Loss: 1.393181 \t time: 0.3\n",
      "Epoch: 604 \tTraining Loss: 1.178773 \tValidation Loss: 1.393265 \t time: 0.3\n",
      "Epoch: 605 \tTraining Loss: 1.178708 \tValidation Loss: 1.393001 \t time: 0.3\n",
      "Epoch: 606 \tTraining Loss: 1.178684 \tValidation Loss: 1.393146 \t time: 0.3\n",
      "Epoch: 607 \tTraining Loss: 1.178667 \tValidation Loss: 1.393200 \t time: 0.3\n",
      "Epoch: 608 \tTraining Loss: 1.178618 \tValidation Loss: 1.393157 \t time: 0.3\n",
      "Epoch: 609 \tTraining Loss: 1.178570 \tValidation Loss: 1.393124 \t time: 0.3\n",
      "Epoch: 610 \tTraining Loss: 1.178554 \tValidation Loss: 1.393172 \t time: 0.3\n",
      "Epoch: 611 \tTraining Loss: 1.178560 \tValidation Loss: 1.393098 \t time: 0.2\n",
      "Epoch: 612 \tTraining Loss: 1.178582 \tValidation Loss: 1.392945 \t time: 0.3\n",
      "Epoch: 613 \tTraining Loss: 1.178555 \tValidation Loss: 1.392827 \t time: 0.3\n",
      "Epoch: 614 \tTraining Loss: 1.178491 \tValidation Loss: 1.393153 \t time: 0.3\n",
      "Epoch: 615 \tTraining Loss: 1.178461 \tValidation Loss: 1.393384 \t time: 0.3\n",
      "Epoch: 616 \tTraining Loss: 1.178421 \tValidation Loss: 1.393140 \t time: 0.3\n",
      "Epoch: 617 \tTraining Loss: 1.178420 \tValidation Loss: 1.393002 \t time: 0.2\n",
      "Epoch: 618 \tTraining Loss: 1.178382 \tValidation Loss: 1.393284 \t time: 0.3\n",
      "Epoch: 619 \tTraining Loss: 1.178350 \tValidation Loss: 1.393156 \t time: 0.3\n",
      "Epoch: 620 \tTraining Loss: 1.178309 \tValidation Loss: 1.392703 \t time: 0.3\n",
      "Epoch: 621 \tTraining Loss: 1.178296 \tValidation Loss: 1.392665 \t time: 0.3\n",
      "Epoch: 622 \tTraining Loss: 1.178270 \tValidation Loss: 1.392993 \t time: 0.3\n",
      "Epoch: 623 \tTraining Loss: 1.178270 \tValidation Loss: 1.392279 \t time: 0.2\n",
      "Epoch: 624 \tTraining Loss: 1.178241 \tValidation Loss: 1.392838 \t time: 0.3\n",
      "Epoch: 625 \tTraining Loss: 1.178201 \tValidation Loss: 1.393540 \t time: 0.3\n",
      "Epoch: 626 \tTraining Loss: 1.178181 \tValidation Loss: 1.392901 \t time: 0.2\n",
      "Epoch: 627 \tTraining Loss: 1.178040 \tValidation Loss: 1.393085 \t time: 0.3\n",
      "Epoch: 628 \tTraining Loss: 1.178047 \tValidation Loss: 1.393268 \t time: 0.3\n",
      "Epoch: 629 \tTraining Loss: 1.178205 \tValidation Loss: 1.393749 \t time: 0.3\n",
      "Epoch: 630 \tTraining Loss: 1.178159 \tValidation Loss: 1.392033 \t time: 0.3\n",
      "Epoch: 631 \tTraining Loss: 1.178131 \tValidation Loss: 1.392746 \t time: 0.3\n",
      "Epoch: 632 \tTraining Loss: 1.178041 \tValidation Loss: 1.393433 \t time: 0.3\n",
      "Epoch: 633 \tTraining Loss: 1.178012 \tValidation Loss: 1.391863 \t time: 0.2\n",
      "Epoch: 634 \tTraining Loss: 1.177926 \tValidation Loss: 1.392197 \t time: 0.3\n",
      "Epoch: 635 \tTraining Loss: 1.177939 \tValidation Loss: 1.392852 \t time: 0.3\n",
      "Epoch: 636 \tTraining Loss: 1.177973 \tValidation Loss: 1.392063 \t time: 0.3\n",
      "Epoch: 637 \tTraining Loss: 1.177852 \tValidation Loss: 1.391906 \t time: 0.3\n",
      "Epoch: 638 \tTraining Loss: 1.177789 \tValidation Loss: 1.392346 \t time: 0.3\n",
      "Epoch: 639 \tTraining Loss: 1.177750 \tValidation Loss: 1.392506 \t time: 0.3\n",
      "Epoch: 640 \tTraining Loss: 1.177661 \tValidation Loss: 1.392009 \t time: 0.3\n",
      "Epoch: 641 \tTraining Loss: 1.177637 \tValidation Loss: 1.391826 \t time: 0.3\n",
      "Epoch: 642 \tTraining Loss: 1.177562 \tValidation Loss: 1.392239 \t time: 0.3\n",
      "Epoch: 643 \tTraining Loss: 1.177592 \tValidation Loss: 1.391847 \t time: 0.3\n",
      "Epoch: 644 \tTraining Loss: 1.177615 \tValidation Loss: 1.391849 \t time: 0.3\n",
      "Epoch: 645 \tTraining Loss: 1.177532 \tValidation Loss: 1.391191 \t time: 0.2\n",
      "Epoch: 646 \tTraining Loss: 1.177504 \tValidation Loss: 1.391813 \t time: 0.3\n",
      "Epoch: 647 \tTraining Loss: 1.177463 \tValidation Loss: 1.392172 \t time: 0.3\n",
      "Epoch: 648 \tTraining Loss: 1.177424 \tValidation Loss: 1.391382 \t time: 0.2\n",
      "Epoch: 649 \tTraining Loss: 1.177404 \tValidation Loss: 1.391313 \t time: 0.2\n",
      "Epoch: 650 \tTraining Loss: 1.177387 \tValidation Loss: 1.391489 \t time: 0.3\n",
      "Epoch: 651 \tTraining Loss: 1.177286 \tValidation Loss: 1.391359 \t time: 0.3\n",
      "Epoch: 652 \tTraining Loss: 1.177250 \tValidation Loss: 1.391392 \t time: 0.2\n",
      "Epoch: 653 \tTraining Loss: 1.177220 \tValidation Loss: 1.391716 \t time: 0.3\n",
      "Epoch: 654 \tTraining Loss: 1.177200 \tValidation Loss: 1.392061 \t time: 0.2\n",
      "Epoch: 655 \tTraining Loss: 1.177162 \tValidation Loss: 1.391468 \t time: 0.2\n",
      "Epoch: 656 \tTraining Loss: 1.177088 \tValidation Loss: 1.391250 \t time: 0.3\n",
      "Epoch: 657 \tTraining Loss: 1.177057 \tValidation Loss: 1.391620 \t time: 0.2\n",
      "Epoch: 658 \tTraining Loss: 1.177033 \tValidation Loss: 1.391348 \t time: 0.2\n",
      "Epoch: 659 \tTraining Loss: 1.176963 \tValidation Loss: 1.391297 \t time: 0.3\n",
      "Epoch: 660 \tTraining Loss: 1.176911 \tValidation Loss: 1.391356 \t time: 0.2\n",
      "Epoch: 661 \tTraining Loss: 1.176825 \tValidation Loss: 1.391105 \t time: 0.2\n",
      "Epoch: 662 \tTraining Loss: 1.176680 \tValidation Loss: 1.391292 \t time: 0.3\n",
      "Epoch: 663 \tTraining Loss: 1.176583 \tValidation Loss: 1.391238 \t time: 0.3\n",
      "Epoch: 664 \tTraining Loss: 1.176526 \tValidation Loss: 1.391383 \t time: 0.3\n",
      "Epoch: 665 \tTraining Loss: 1.176566 \tValidation Loss: 1.391821 \t time: 0.3\n",
      "Epoch: 666 \tTraining Loss: 1.176430 \tValidation Loss: 1.391795 \t time: 0.3\n",
      "Epoch: 667 \tTraining Loss: 1.176438 \tValidation Loss: 1.391129 \t time: 0.3\n",
      "Epoch: 668 \tTraining Loss: 1.176370 \tValidation Loss: 1.391529 \t time: 0.3\n",
      "Epoch: 669 \tTraining Loss: 1.176293 \tValidation Loss: 1.392127 \t time: 0.3\n",
      "Epoch: 670 \tTraining Loss: 1.176425 \tValidation Loss: 1.391117 \t time: 0.2\n",
      "Epoch: 671 \tTraining Loss: 1.176760 \tValidation Loss: 1.392910 \t time: 0.3\n",
      "Epoch: 672 \tTraining Loss: 1.176980 \tValidation Loss: 1.391962 \t time: 0.3\n",
      "Epoch: 673 \tTraining Loss: 1.178383 \tValidation Loss: 1.403535 \t time: 0.2\n",
      "Epoch: 674 \tTraining Loss: 1.195226 \tValidation Loss: 1.536940 \t time: 0.3\n",
      "Epoch: 675 \tTraining Loss: 1.431069 \tValidation Loss: 1.508956 \t time: 0.3\n",
      "Epoch: 676 \tTraining Loss: 1.387573 \tValidation Loss: 1.429012 \t time: 0.3\n",
      "Epoch: 677 \tTraining Loss: 1.262725 \tValidation Loss: 1.466463 \t time: 0.3\n",
      "Epoch: 678 \tTraining Loss: 1.331719 \tValidation Loss: 1.442619 \t time: 0.3\n",
      "Epoch: 679 \tTraining Loss: 1.297713 \tValidation Loss: 1.417093 \t time: 0.3\n",
      "Epoch: 680 \tTraining Loss: 1.285144 \tValidation Loss: 1.415328 \t time: 0.3\n",
      "Epoch: 681 \tTraining Loss: 1.288122 \tValidation Loss: 1.405119 \t time: 0.3\n",
      "Epoch: 682 \tTraining Loss: 1.263005 \tValidation Loss: 1.414044 \t time: 0.2\n",
      "Epoch: 683 \tTraining Loss: 1.253946 \tValidation Loss: 1.413703 \t time: 0.3\n",
      "Epoch: 684 \tTraining Loss: 1.250839 \tValidation Loss: 1.407040 \t time: 0.3\n",
      "Epoch: 685 \tTraining Loss: 1.246326 \tValidation Loss: 1.403590 \t time: 0.3\n",
      "Epoch: 686 \tTraining Loss: 1.230368 \tValidation Loss: 1.404343 \t time: 0.3\n",
      "Epoch: 687 \tTraining Loss: 1.232071 \tValidation Loss: 1.398590 \t time: 0.3\n",
      "Epoch: 688 \tTraining Loss: 1.223096 \tValidation Loss: 1.399537 \t time: 0.2\n",
      "Epoch: 689 \tTraining Loss: 1.221476 \tValidation Loss: 1.399281 \t time: 0.3\n",
      "Epoch: 690 \tTraining Loss: 1.212716 \tValidation Loss: 1.402366 \t time: 0.3\n",
      "Epoch: 691 \tTraining Loss: 1.212693 \tValidation Loss: 1.398713 \t time: 0.3\n",
      "Epoch: 692 \tTraining Loss: 1.208099 \tValidation Loss: 1.396571 \t time: 0.3\n",
      "Epoch: 693 \tTraining Loss: 1.203291 \tValidation Loss: 1.394039 \t time: 0.3\n",
      "Epoch: 694 \tTraining Loss: 1.202057 \tValidation Loss: 1.392753 \t time: 0.3\n",
      "Epoch: 695 \tTraining Loss: 1.200609 \tValidation Loss: 1.392410 \t time: 0.2\n",
      "Epoch: 696 \tTraining Loss: 1.196600 \tValidation Loss: 1.391954 \t time: 0.3\n",
      "Epoch: 697 \tTraining Loss: 1.195032 \tValidation Loss: 1.388502 \t time: 0.2\n",
      "Epoch: 698 \tTraining Loss: 1.191607 \tValidation Loss: 1.388659 \t time: 0.3\n",
      "Epoch: 699 \tTraining Loss: 1.190363 \tValidation Loss: 1.388485 \t time: 0.3\n",
      "Epoch: 700 \tTraining Loss: 1.188532 \tValidation Loss: 1.388445 \t time: 0.2\n",
      "Epoch: 701 \tTraining Loss: 1.187308 \tValidation Loss: 1.388463 \t time: 0.3\n",
      "Epoch: 702 \tTraining Loss: 1.186236 \tValidation Loss: 1.388247 \t time: 0.3\n",
      "Epoch: 703 \tTraining Loss: 1.185118 \tValidation Loss: 1.389361 \t time: 0.3\n",
      "Epoch: 704 \tTraining Loss: 1.184027 \tValidation Loss: 1.389851 \t time: 0.2\n",
      "Epoch: 705 \tTraining Loss: 1.182801 \tValidation Loss: 1.389409 \t time: 0.3\n",
      "Epoch: 706 \tTraining Loss: 1.182218 \tValidation Loss: 1.390668 \t time: 0.3\n",
      "Epoch: 707 \tTraining Loss: 1.181512 \tValidation Loss: 1.390063 \t time: 0.3\n",
      "Epoch: 708 \tTraining Loss: 1.181067 \tValidation Loss: 1.390753 \t time: 0.3\n",
      "Epoch: 709 \tTraining Loss: 1.180541 \tValidation Loss: 1.389340 \t time: 0.3\n",
      "Epoch: 710 \tTraining Loss: 1.179807 \tValidation Loss: 1.390908 \t time: 0.3\n",
      "Epoch: 711 \tTraining Loss: 1.179232 \tValidation Loss: 1.390779 \t time: 0.3\n",
      "Epoch: 712 \tTraining Loss: 1.178732 \tValidation Loss: 1.389251 \t time: 0.3\n",
      "Epoch: 713 \tTraining Loss: 1.178452 \tValidation Loss: 1.391315 \t time: 0.2\n",
      "Epoch: 714 \tTraining Loss: 1.178172 \tValidation Loss: 1.389641 \t time: 0.3\n",
      "Epoch: 715 \tTraining Loss: 1.177762 \tValidation Loss: 1.390434 \t time: 0.2\n",
      "Epoch: 716 \tTraining Loss: 1.177105 \tValidation Loss: 1.390987 \t time: 0.2\n",
      "Epoch: 717 \tTraining Loss: 1.176536 \tValidation Loss: 1.391037 \t time: 0.3\n",
      "Epoch: 718 \tTraining Loss: 1.176270 \tValidation Loss: 1.392394 \t time: 0.3\n",
      "Epoch: 719 \tTraining Loss: 1.175998 \tValidation Loss: 1.391733 \t time: 0.2\n",
      "Epoch: 720 \tTraining Loss: 1.175661 \tValidation Loss: 1.391284 \t time: 0.3\n",
      "Epoch: 721 \tTraining Loss: 1.175344 \tValidation Loss: 1.392092 \t time: 0.3\n",
      "Epoch: 722 \tTraining Loss: 1.175126 \tValidation Loss: 1.390387 \t time: 0.2\n",
      "Epoch: 723 \tTraining Loss: 1.174752 \tValidation Loss: 1.390582 \t time: 0.3\n",
      "Epoch: 724 \tTraining Loss: 1.174461 \tValidation Loss: 1.390639 \t time: 0.3\n",
      "Epoch: 725 \tTraining Loss: 1.174350 \tValidation Loss: 1.389447 \t time: 0.2\n",
      "Epoch: 726 \tTraining Loss: 1.174088 \tValidation Loss: 1.390308 \t time: 0.3\n",
      "Epoch: 727 \tTraining Loss: 1.173762 \tValidation Loss: 1.391572 \t time: 0.3\n",
      "Epoch: 728 \tTraining Loss: 1.173618 \tValidation Loss: 1.390463 \t time: 0.2\n",
      "Epoch: 729 \tTraining Loss: 1.173454 \tValidation Loss: 1.390778 \t time: 0.2\n",
      "Epoch: 730 \tTraining Loss: 1.173211 \tValidation Loss: 1.390686 \t time: 0.2\n",
      "Epoch: 731 \tTraining Loss: 1.173007 \tValidation Loss: 1.390164 \t time: 0.2\n",
      "Epoch: 732 \tTraining Loss: 1.172859 \tValidation Loss: 1.390650 \t time: 0.3\n",
      "Epoch: 733 \tTraining Loss: 1.172678 \tValidation Loss: 1.390644 \t time: 0.3\n",
      "Epoch: 734 \tTraining Loss: 1.172520 \tValidation Loss: 1.390250 \t time: 0.2\n",
      "Epoch: 735 \tTraining Loss: 1.172291 \tValidation Loss: 1.390699 \t time: 0.3\n",
      "Epoch: 736 \tTraining Loss: 1.172169 \tValidation Loss: 1.390530 \t time: 0.3\n",
      "Epoch: 737 \tTraining Loss: 1.172056 \tValidation Loss: 1.389870 \t time: 0.2\n",
      "Epoch: 738 \tTraining Loss: 1.171983 \tValidation Loss: 1.390141 \t time: 0.2\n",
      "Epoch: 739 \tTraining Loss: 1.171792 \tValidation Loss: 1.390262 \t time: 0.3\n",
      "Epoch: 740 \tTraining Loss: 1.171735 \tValidation Loss: 1.389495 \t time: 0.2\n",
      "Epoch: 741 \tTraining Loss: 1.171593 \tValidation Loss: 1.389814 \t time: 0.3\n",
      "Epoch: 742 \tTraining Loss: 1.171452 \tValidation Loss: 1.390849 \t time: 0.3\n",
      "Epoch: 743 \tTraining Loss: 1.171293 \tValidation Loss: 1.390677 \t time: 0.2\n",
      "Epoch: 744 \tTraining Loss: 1.171150 \tValidation Loss: 1.390144 \t time: 0.2\n",
      "Epoch: 745 \tTraining Loss: 1.170997 \tValidation Loss: 1.390122 \t time: 0.3\n",
      "Epoch: 746 \tTraining Loss: 1.170866 \tValidation Loss: 1.390037 \t time: 0.3\n",
      "Epoch: 747 \tTraining Loss: 1.170645 \tValidation Loss: 1.390384 \t time: 0.3\n",
      "Epoch: 748 \tTraining Loss: 1.170486 \tValidation Loss: 1.389645 \t time: 0.3\n",
      "Epoch: 749 \tTraining Loss: 1.170397 \tValidation Loss: 1.388889 \t time: 0.3\n",
      "Epoch: 750 \tTraining Loss: 1.170341 \tValidation Loss: 1.389183 \t time: 0.3\n",
      "Epoch: 751 \tTraining Loss: 1.170243 \tValidation Loss: 1.389229 \t time: 0.3\n",
      "Epoch: 752 \tTraining Loss: 1.170171 \tValidation Loss: 1.388689 \t time: 0.3\n",
      "Epoch: 753 \tTraining Loss: 1.170096 \tValidation Loss: 1.388861 \t time: 0.2\n",
      "Epoch: 754 \tTraining Loss: 1.170045 \tValidation Loss: 1.389216 \t time: 0.2\n",
      "Epoch: 755 \tTraining Loss: 1.169989 \tValidation Loss: 1.388986 \t time: 0.3\n",
      "Epoch: 756 \tTraining Loss: 1.169927 \tValidation Loss: 1.388999 \t time: 0.2\n",
      "Epoch: 757 \tTraining Loss: 1.169879 \tValidation Loss: 1.389338 \t time: 0.3\n",
      "Epoch: 758 \tTraining Loss: 1.169837 \tValidation Loss: 1.389266 \t time: 0.2\n",
      "Epoch: 759 \tTraining Loss: 1.169784 \tValidation Loss: 1.389172 \t time: 0.2\n",
      "Epoch: 760 \tTraining Loss: 1.169739 \tValidation Loss: 1.389146 \t time: 0.2\n",
      "Epoch: 761 \tTraining Loss: 1.169682 \tValidation Loss: 1.388888 \t time: 0.2\n",
      "Epoch: 762 \tTraining Loss: 1.169617 \tValidation Loss: 1.388679 \t time: 0.3\n",
      "Epoch: 763 \tTraining Loss: 1.169559 \tValidation Loss: 1.388779 \t time: 0.3\n",
      "Epoch: 764 \tTraining Loss: 1.169512 \tValidation Loss: 1.388836 \t time: 0.3\n",
      "Epoch: 765 \tTraining Loss: 1.169466 \tValidation Loss: 1.388724 \t time: 0.3\n",
      "Epoch: 766 \tTraining Loss: 1.169411 \tValidation Loss: 1.388494 \t time: 0.3\n",
      "Epoch: 767 \tTraining Loss: 1.169345 \tValidation Loss: 1.388322 \t time: 0.3\n",
      "Epoch: 768 \tTraining Loss: 1.169286 \tValidation Loss: 1.388496 \t time: 0.3\n",
      "Epoch: 769 \tTraining Loss: 1.169234 \tValidation Loss: 1.388746 \t time: 0.3\n",
      "Epoch: 770 \tTraining Loss: 1.169172 \tValidation Loss: 1.388657 \t time: 0.3\n",
      "Epoch: 771 \tTraining Loss: 1.169114 \tValidation Loss: 1.388405 \t time: 0.2\n",
      "Epoch: 772 \tTraining Loss: 1.169080 \tValidation Loss: 1.388181 \t time: 0.3\n",
      "Epoch: 773 \tTraining Loss: 1.169052 \tValidation Loss: 1.387961 \t time: 0.3\n",
      "Epoch: 774 \tTraining Loss: 1.169026 \tValidation Loss: 1.387790 \t time: 0.2\n",
      "Epoch: 775 \tTraining Loss: 1.168997 \tValidation Loss: 1.387641 \t time: 0.2\n",
      "Epoch: 776 \tTraining Loss: 1.168963 \tValidation Loss: 1.387610 \t time: 0.3\n",
      "Epoch: 777 \tTraining Loss: 1.168931 \tValidation Loss: 1.387734 \t time: 0.2\n",
      "Epoch: 778 \tTraining Loss: 1.168883 \tValidation Loss: 1.387773 \t time: 0.3\n",
      "Epoch: 779 \tTraining Loss: 1.168813 \tValidation Loss: 1.387630 \t time: 0.3\n",
      "Epoch: 780 \tTraining Loss: 1.168782 \tValidation Loss: 1.387615 \t time: 0.3\n",
      "Epoch: 781 \tTraining Loss: 1.168760 \tValidation Loss: 1.387707 \t time: 0.3\n",
      "Epoch: 782 \tTraining Loss: 1.168731 \tValidation Loss: 1.387708 \t time: 0.3\n",
      "Epoch: 783 \tTraining Loss: 1.168685 \tValidation Loss: 1.387702 \t time: 0.3\n",
      "Epoch: 784 \tTraining Loss: 1.168623 \tValidation Loss: 1.387711 \t time: 0.2\n",
      "Epoch: 785 \tTraining Loss: 1.168557 \tValidation Loss: 1.387718 \t time: 0.3\n",
      "Epoch: 786 \tTraining Loss: 1.168504 \tValidation Loss: 1.387856 \t time: 0.3\n",
      "Epoch: 787 \tTraining Loss: 1.168420 \tValidation Loss: 1.388160 \t time: 0.3\n",
      "Epoch: 788 \tTraining Loss: 1.168380 \tValidation Loss: 1.388444 \t time: 0.3\n",
      "Epoch: 789 \tTraining Loss: 1.168347 \tValidation Loss: 1.388627 \t time: 0.3\n",
      "Epoch: 790 \tTraining Loss: 1.168286 \tValidation Loss: 1.388821 \t time: 0.3\n",
      "Epoch: 791 \tTraining Loss: 1.168218 \tValidation Loss: 1.388755 \t time: 0.3\n",
      "Epoch: 792 \tTraining Loss: 1.168144 \tValidation Loss: 1.388624 \t time: 0.3\n",
      "Epoch: 793 \tTraining Loss: 1.168090 \tValidation Loss: 1.388719 \t time: 0.3\n",
      "Epoch: 794 \tTraining Loss: 1.168042 \tValidation Loss: 1.388834 \t time: 0.3\n",
      "Epoch: 795 \tTraining Loss: 1.168018 \tValidation Loss: 1.388812 \t time: 0.3\n",
      "Epoch: 796 \tTraining Loss: 1.167984 \tValidation Loss: 1.388794 \t time: 0.3\n",
      "Epoch: 797 \tTraining Loss: 1.167926 \tValidation Loss: 1.388818 \t time: 0.3\n",
      "Epoch: 798 \tTraining Loss: 1.167826 \tValidation Loss: 1.388827 \t time: 0.3\n",
      "Epoch: 799 \tTraining Loss: 1.167788 \tValidation Loss: 1.388688 \t time: 0.3\n",
      "Epoch: 800 \tTraining Loss: 1.167760 \tValidation Loss: 1.388625 \t time: 0.3\n",
      "Epoch: 801 \tTraining Loss: 1.167730 \tValidation Loss: 1.388579 \t time: 0.3\n",
      "Epoch: 802 \tTraining Loss: 1.167700 \tValidation Loss: 1.388434 \t time: 0.3\n",
      "Epoch: 803 \tTraining Loss: 1.167667 \tValidation Loss: 1.388317 \t time: 0.3\n",
      "Epoch: 804 \tTraining Loss: 1.167618 \tValidation Loss: 1.388254 \t time: 0.3\n",
      "Epoch: 805 \tTraining Loss: 1.167496 \tValidation Loss: 1.388114 \t time: 0.3\n",
      "Epoch: 806 \tTraining Loss: 1.167429 \tValidation Loss: 1.388026 \t time: 0.3\n",
      "Epoch: 807 \tTraining Loss: 1.167386 \tValidation Loss: 1.388103 \t time: 0.3\n",
      "Epoch: 808 \tTraining Loss: 1.167346 \tValidation Loss: 1.388189 \t time: 0.3\n",
      "Epoch: 809 \tTraining Loss: 1.167317 \tValidation Loss: 1.388343 \t time: 0.3\n",
      "Epoch: 810 \tTraining Loss: 1.167291 \tValidation Loss: 1.388554 \t time: 0.3\n",
      "Epoch: 811 \tTraining Loss: 1.167269 \tValidation Loss: 1.388562 \t time: 0.3\n",
      "Epoch: 812 \tTraining Loss: 1.167248 \tValidation Loss: 1.388519 \t time: 0.2\n",
      "Epoch: 813 \tTraining Loss: 1.167228 \tValidation Loss: 1.388577 \t time: 0.2\n",
      "Epoch: 814 \tTraining Loss: 1.167208 \tValidation Loss: 1.388574 \t time: 0.3\n",
      "Epoch: 815 \tTraining Loss: 1.167188 \tValidation Loss: 1.388573 \t time: 0.3\n",
      "Epoch: 816 \tTraining Loss: 1.167170 \tValidation Loss: 1.388667 \t time: 0.3\n",
      "Epoch: 817 \tTraining Loss: 1.167152 \tValidation Loss: 1.388715 \t time: 0.3\n",
      "Epoch: 818 \tTraining Loss: 1.167136 \tValidation Loss: 1.388698 \t time: 0.3\n",
      "Epoch: 819 \tTraining Loss: 1.167119 \tValidation Loss: 1.388731 \t time: 0.3\n",
      "Epoch: 820 \tTraining Loss: 1.167096 \tValidation Loss: 1.388793 \t time: 0.3\n",
      "Epoch: 821 \tTraining Loss: 1.167030 \tValidation Loss: 1.388879 \t time: 0.3\n",
      "Epoch: 822 \tTraining Loss: 1.166987 \tValidation Loss: 1.388989 \t time: 0.3\n",
      "Epoch: 823 \tTraining Loss: 1.166946 \tValidation Loss: 1.389198 \t time: 0.3\n",
      "Epoch: 824 \tTraining Loss: 1.166881 \tValidation Loss: 1.389252 \t time: 0.3\n",
      "Epoch: 825 \tTraining Loss: 1.166865 \tValidation Loss: 1.389151 \t time: 0.2\n",
      "Epoch: 826 \tTraining Loss: 1.166850 \tValidation Loss: 1.389063 \t time: 0.2\n",
      "Epoch: 827 \tTraining Loss: 1.166834 \tValidation Loss: 1.389027 \t time: 0.3\n",
      "Epoch: 828 \tTraining Loss: 1.166823 \tValidation Loss: 1.388986 \t time: 0.3\n",
      "Epoch: 829 \tTraining Loss: 1.166808 \tValidation Loss: 1.389043 \t time: 0.3\n",
      "Epoch: 830 \tTraining Loss: 1.166789 \tValidation Loss: 1.389219 \t time: 0.3\n",
      "Epoch: 831 \tTraining Loss: 1.166770 \tValidation Loss: 1.389296 \t time: 0.2\n",
      "Epoch: 832 \tTraining Loss: 1.166749 \tValidation Loss: 1.389262 \t time: 0.3\n",
      "Epoch: 833 \tTraining Loss: 1.166726 \tValidation Loss: 1.389280 \t time: 0.3\n",
      "Epoch: 834 \tTraining Loss: 1.166696 \tValidation Loss: 1.389287 \t time: 0.3\n",
      "Epoch: 835 \tTraining Loss: 1.166641 \tValidation Loss: 1.389259 \t time: 0.3\n",
      "Epoch: 836 \tTraining Loss: 1.166617 \tValidation Loss: 1.389326 \t time: 0.3\n",
      "Epoch: 837 \tTraining Loss: 1.166604 \tValidation Loss: 1.389372 \t time: 0.2\n",
      "Epoch: 838 \tTraining Loss: 1.166595 \tValidation Loss: 1.389311 \t time: 0.3\n",
      "Epoch: 839 \tTraining Loss: 1.166586 \tValidation Loss: 1.389251 \t time: 0.3\n",
      "Epoch: 840 \tTraining Loss: 1.166574 \tValidation Loss: 1.389209 \t time: 0.2\n",
      "Epoch: 841 \tTraining Loss: 1.166560 \tValidation Loss: 1.389124 \t time: 0.3\n",
      "Epoch: 842 \tTraining Loss: 1.166539 \tValidation Loss: 1.389029 \t time: 0.3\n",
      "Epoch: 843 \tTraining Loss: 1.166475 \tValidation Loss: 1.389014 \t time: 0.2\n",
      "Epoch: 844 \tTraining Loss: 1.166457 \tValidation Loss: 1.388946 \t time: 0.2\n",
      "Epoch: 845 \tTraining Loss: 1.166442 \tValidation Loss: 1.388868 \t time: 0.3\n",
      "Epoch: 846 \tTraining Loss: 1.166426 \tValidation Loss: 1.388817 \t time: 0.3\n",
      "Epoch: 847 \tTraining Loss: 1.166385 \tValidation Loss: 1.388894 \t time: 0.3\n",
      "Epoch: 848 \tTraining Loss: 1.166304 \tValidation Loss: 1.388959 \t time: 0.3\n",
      "Epoch: 849 \tTraining Loss: 1.166276 \tValidation Loss: 1.389122 \t time: 0.3\n",
      "Epoch: 850 \tTraining Loss: 1.166264 \tValidation Loss: 1.389248 \t time: 0.3\n",
      "Epoch: 851 \tTraining Loss: 1.166252 \tValidation Loss: 1.389233 \t time: 0.3\n",
      "Epoch: 852 \tTraining Loss: 1.166226 \tValidation Loss: 1.389409 \t time: 0.3\n",
      "Epoch: 853 \tTraining Loss: 1.166227 \tValidation Loss: 1.389627 \t time: 0.3\n",
      "Epoch: 854 \tTraining Loss: 1.166204 \tValidation Loss: 1.389714 \t time: 0.3\n",
      "Epoch: 855 \tTraining Loss: 1.166206 \tValidation Loss: 1.389515 \t time: 0.2\n",
      "Epoch: 856 \tTraining Loss: 1.166238 \tValidation Loss: 1.389495 \t time: 0.3\n",
      "Epoch: 857 \tTraining Loss: 1.166361 \tValidation Loss: 1.389257 \t time: 0.3\n",
      "Epoch: 858 \tTraining Loss: 1.166383 \tValidation Loss: 1.389094 \t time: 0.3\n",
      "Epoch: 859 \tTraining Loss: 1.166382 \tValidation Loss: 1.389199 \t time: 0.3\n",
      "Epoch: 860 \tTraining Loss: 1.166362 \tValidation Loss: 1.389065 \t time: 0.3\n",
      "Epoch: 861 \tTraining Loss: 1.166351 \tValidation Loss: 1.388660 \t time: 0.2\n",
      "Epoch: 862 \tTraining Loss: 1.166305 \tValidation Loss: 1.388378 \t time: 0.2\n",
      "Epoch: 863 \tTraining Loss: 1.166277 \tValidation Loss: 1.388310 \t time: 0.3\n",
      "Epoch: 864 \tTraining Loss: 1.166237 \tValidation Loss: 1.388443 \t time: 0.3\n",
      "Epoch: 865 \tTraining Loss: 1.166220 \tValidation Loss: 1.388536 \t time: 0.3\n",
      "Epoch: 866 \tTraining Loss: 1.166192 \tValidation Loss: 1.388530 \t time: 0.3\n",
      "Epoch: 867 \tTraining Loss: 1.166159 \tValidation Loss: 1.388716 \t time: 0.3\n",
      "Epoch: 868 \tTraining Loss: 1.166065 \tValidation Loss: 1.388867 \t time: 0.3\n",
      "Epoch: 869 \tTraining Loss: 1.166091 \tValidation Loss: 1.388711 \t time: 0.3\n",
      "Epoch: 870 \tTraining Loss: 1.166088 \tValidation Loss: 1.388522 \t time: 0.3\n",
      "Epoch: 871 \tTraining Loss: 1.166043 \tValidation Loss: 1.388441 \t time: 0.3\n",
      "Epoch: 872 \tTraining Loss: 1.165979 \tValidation Loss: 1.388536 \t time: 0.3\n",
      "Epoch: 873 \tTraining Loss: 1.165896 \tValidation Loss: 1.388775 \t time: 0.2\n",
      "Epoch: 874 \tTraining Loss: 1.165836 \tValidation Loss: 1.388334 \t time: 0.2\n",
      "Epoch: 875 \tTraining Loss: 1.165874 \tValidation Loss: 1.388459 \t time: 0.3\n",
      "Epoch: 876 \tTraining Loss: 1.165812 \tValidation Loss: 1.387655 \t time: 0.3\n",
      "Epoch: 877 \tTraining Loss: 1.165742 \tValidation Loss: 1.387493 \t time: 0.2\n",
      "Epoch: 878 \tTraining Loss: 1.165718 \tValidation Loss: 1.387923 \t time: 0.3\n",
      "Epoch: 879 \tTraining Loss: 1.165684 \tValidation Loss: 1.387680 \t time: 0.3\n",
      "Epoch: 880 \tTraining Loss: 1.165639 \tValidation Loss: 1.387817 \t time: 0.2\n",
      "Epoch: 881 \tTraining Loss: 1.165612 \tValidation Loss: 1.388054 \t time: 0.3\n",
      "Epoch: 882 \tTraining Loss: 1.165579 \tValidation Loss: 1.387977 \t time: 0.3\n",
      "Epoch: 883 \tTraining Loss: 1.165546 \tValidation Loss: 1.388024 \t time: 0.2\n",
      "Epoch: 884 \tTraining Loss: 1.165528 \tValidation Loss: 1.388173 \t time: 0.3\n",
      "Epoch: 885 \tTraining Loss: 1.165489 \tValidation Loss: 1.388270 \t time: 0.2\n",
      "Epoch: 886 \tTraining Loss: 1.165480 \tValidation Loss: 1.388304 \t time: 0.2\n",
      "Epoch: 887 \tTraining Loss: 1.165470 \tValidation Loss: 1.388381 \t time: 0.3\n",
      "Epoch: 888 \tTraining Loss: 1.165444 \tValidation Loss: 1.388454 \t time: 0.2\n",
      "Epoch: 889 \tTraining Loss: 1.165431 \tValidation Loss: 1.388368 \t time: 0.2\n",
      "Epoch: 890 \tTraining Loss: 1.165415 \tValidation Loss: 1.388429 \t time: 0.2\n",
      "Epoch: 891 \tTraining Loss: 1.165397 \tValidation Loss: 1.388631 \t time: 0.3\n",
      "Epoch: 892 \tTraining Loss: 1.165388 \tValidation Loss: 1.388589 \t time: 0.3\n",
      "Epoch: 893 \tTraining Loss: 1.165367 \tValidation Loss: 1.388556 \t time: 0.3\n",
      "Epoch: 894 \tTraining Loss: 1.165332 \tValidation Loss: 1.388626 \t time: 0.3\n",
      "Epoch: 895 \tTraining Loss: 1.165311 \tValidation Loss: 1.388520 \t time: 0.2\n",
      "Epoch: 896 \tTraining Loss: 1.165292 \tValidation Loss: 1.388472 \t time: 0.2\n",
      "Epoch: 897 \tTraining Loss: 1.165265 \tValidation Loss: 1.388720 \t time: 0.3\n",
      "Epoch: 898 \tTraining Loss: 1.165170 \tValidation Loss: 1.389082 \t time: 0.3\n",
      "Epoch: 899 \tTraining Loss: 1.164925 \tValidation Loss: 1.388599 \t time: 0.3\n",
      "Epoch: 900 \tTraining Loss: 1.164936 \tValidation Loss: 1.388890 \t time: 0.3\n",
      "Epoch: 901 \tTraining Loss: 1.164857 \tValidation Loss: 1.388882 \t time: 0.3\n",
      "Epoch: 902 \tTraining Loss: 1.164880 \tValidation Loss: 1.387938 \t time: 0.3\n",
      "Epoch: 903 \tTraining Loss: 1.164882 \tValidation Loss: 1.388296 \t time: 0.3\n",
      "Epoch: 904 \tTraining Loss: 1.164807 \tValidation Loss: 1.388987 \t time: 0.2\n",
      "Epoch: 905 \tTraining Loss: 1.164858 \tValidation Loss: 1.388289 \t time: 0.3\n",
      "Epoch: 906 \tTraining Loss: 1.164808 \tValidation Loss: 1.388289 \t time: 0.3\n",
      "Epoch: 907 \tTraining Loss: 1.164796 \tValidation Loss: 1.388610 \t time: 0.2\n",
      "Epoch: 908 \tTraining Loss: 1.164777 \tValidation Loss: 1.387978 \t time: 0.3\n",
      "Epoch: 909 \tTraining Loss: 1.164762 \tValidation Loss: 1.387975 \t time: 0.3\n",
      "Epoch: 910 \tTraining Loss: 1.164750 \tValidation Loss: 1.388363 \t time: 0.3\n",
      "Epoch: 911 \tTraining Loss: 1.164707 \tValidation Loss: 1.388539 \t time: 0.2\n",
      "Epoch: 912 \tTraining Loss: 1.164715 \tValidation Loss: 1.388519 \t time: 0.3\n",
      "Epoch: 913 \tTraining Loss: 1.164649 \tValidation Loss: 1.388566 \t time: 0.3\n",
      "Epoch: 914 \tTraining Loss: 1.164647 \tValidation Loss: 1.388569 \t time: 0.2\n",
      "Epoch: 915 \tTraining Loss: 1.164621 \tValidation Loss: 1.388605 \t time: 0.3\n",
      "Epoch: 916 \tTraining Loss: 1.164599 \tValidation Loss: 1.388506 \t time: 0.3\n",
      "Epoch: 917 \tTraining Loss: 1.164585 \tValidation Loss: 1.388675 \t time: 0.2\n",
      "Epoch: 918 \tTraining Loss: 1.164557 \tValidation Loss: 1.388963 \t time: 0.3\n",
      "Epoch: 919 \tTraining Loss: 1.164546 \tValidation Loss: 1.388827 \t time: 0.3\n",
      "Epoch: 920 \tTraining Loss: 1.164511 \tValidation Loss: 1.388733 \t time: 0.2\n",
      "Epoch: 921 \tTraining Loss: 1.164482 \tValidation Loss: 1.388722 \t time: 0.3\n",
      "Epoch: 922 \tTraining Loss: 1.164475 \tValidation Loss: 1.388473 \t time: 0.3\n",
      "Epoch: 923 \tTraining Loss: 1.164462 \tValidation Loss: 1.388285 \t time: 0.3\n",
      "Epoch: 924 \tTraining Loss: 1.164461 \tValidation Loss: 1.388603 \t time: 0.3\n",
      "Epoch: 925 \tTraining Loss: 1.164448 \tValidation Loss: 1.388734 \t time: 0.3\n",
      "Epoch: 926 \tTraining Loss: 1.164444 \tValidation Loss: 1.388645 \t time: 0.3\n",
      "Epoch: 927 \tTraining Loss: 1.164448 \tValidation Loss: 1.388733 \t time: 0.3\n",
      "Epoch: 928 \tTraining Loss: 1.164421 \tValidation Loss: 1.388924 \t time: 0.3\n",
      "Epoch: 929 \tTraining Loss: 1.164416 \tValidation Loss: 1.388885 \t time: 0.3\n",
      "Epoch: 930 \tTraining Loss: 1.164406 \tValidation Loss: 1.388812 \t time: 0.3\n",
      "Epoch: 931 \tTraining Loss: 1.164394 \tValidation Loss: 1.388914 \t time: 0.3\n",
      "Epoch: 932 \tTraining Loss: 1.164379 \tValidation Loss: 1.388907 \t time: 0.2\n",
      "Epoch: 933 \tTraining Loss: 1.164381 \tValidation Loss: 1.388855 \t time: 0.3\n",
      "Epoch: 934 \tTraining Loss: 1.164412 \tValidation Loss: 1.389143 \t time: 0.3\n",
      "Epoch: 935 \tTraining Loss: 1.164529 \tValidation Loss: 1.388970 \t time: 0.2\n",
      "Epoch: 936 \tTraining Loss: 1.164537 \tValidation Loss: 1.388851 \t time: 0.3\n",
      "Epoch: 937 \tTraining Loss: 1.164529 \tValidation Loss: 1.388919 \t time: 0.3\n",
      "Epoch: 938 \tTraining Loss: 1.164523 \tValidation Loss: 1.388779 \t time: 0.3\n",
      "Epoch: 939 \tTraining Loss: 1.164523 \tValidation Loss: 1.388750 \t time: 0.3\n",
      "Epoch: 940 \tTraining Loss: 1.164496 \tValidation Loss: 1.388688 \t time: 0.3\n",
      "Epoch: 941 \tTraining Loss: 1.164477 \tValidation Loss: 1.388748 \t time: 0.3\n",
      "Epoch: 942 \tTraining Loss: 1.164464 \tValidation Loss: 1.388865 \t time: 0.3\n",
      "Epoch: 943 \tTraining Loss: 1.164447 \tValidation Loss: 1.388813 \t time: 0.3\n",
      "Epoch: 944 \tTraining Loss: 1.164430 \tValidation Loss: 1.388849 \t time: 0.3\n",
      "Epoch: 945 \tTraining Loss: 1.164405 \tValidation Loss: 1.388967 \t time: 0.3\n",
      "Epoch: 946 \tTraining Loss: 1.164349 \tValidation Loss: 1.389067 \t time: 0.3\n",
      "Epoch: 947 \tTraining Loss: 1.164295 \tValidation Loss: 1.388076 \t time: 0.2\n",
      "Epoch: 948 \tTraining Loss: 1.164267 \tValidation Loss: 1.389103 \t time: 0.3\n",
      "Epoch: 949 \tTraining Loss: 1.164211 \tValidation Loss: 1.388779 \t time: 0.3\n",
      "Epoch: 950 \tTraining Loss: 1.164187 \tValidation Loss: 1.388236 \t time: 0.2\n",
      "Epoch: 951 \tTraining Loss: 1.164226 \tValidation Loss: 1.388717 \t time: 0.3\n",
      "Epoch: 952 \tTraining Loss: 1.164135 \tValidation Loss: 1.388663 \t time: 0.3\n",
      "Epoch: 953 \tTraining Loss: 1.164092 \tValidation Loss: 1.388369 \t time: 0.3\n",
      "Epoch: 954 \tTraining Loss: 1.164112 \tValidation Loss: 1.388677 \t time: 0.3\n",
      "Epoch: 955 \tTraining Loss: 1.164071 \tValidation Loss: 1.388300 \t time: 0.3\n",
      "Epoch: 956 \tTraining Loss: 1.164034 \tValidation Loss: 1.387592 \t time: 0.3\n",
      "Epoch: 957 \tTraining Loss: 1.164037 \tValidation Loss: 1.387846 \t time: 0.3\n",
      "Epoch: 958 \tTraining Loss: 1.164014 \tValidation Loss: 1.388208 \t time: 0.2\n",
      "Epoch: 959 \tTraining Loss: 1.164024 \tValidation Loss: 1.387851 \t time: 0.2\n",
      "Epoch: 960 \tTraining Loss: 1.164012 \tValidation Loss: 1.388021 \t time: 0.3\n",
      "Epoch: 961 \tTraining Loss: 1.163992 \tValidation Loss: 1.388275 \t time: 0.3\n",
      "Epoch: 962 \tTraining Loss: 1.163981 \tValidation Loss: 1.387817 \t time: 0.2\n",
      "Epoch: 963 \tTraining Loss: 1.163982 \tValidation Loss: 1.387800 \t time: 0.3\n",
      "Epoch: 964 \tTraining Loss: 1.163960 \tValidation Loss: 1.388067 \t time: 0.2\n",
      "Epoch: 965 \tTraining Loss: 1.163952 \tValidation Loss: 1.388059 \t time: 0.2\n",
      "Epoch: 966 \tTraining Loss: 1.163948 \tValidation Loss: 1.388353 \t time: 0.3\n",
      "Epoch: 967 \tTraining Loss: 1.163927 \tValidation Loss: 1.388605 \t time: 0.2\n",
      "Epoch: 968 \tTraining Loss: 1.163922 \tValidation Loss: 1.388246 \t time: 0.3\n",
      "Epoch: 969 \tTraining Loss: 1.163899 \tValidation Loss: 1.388091 \t time: 0.3\n",
      "Epoch: 970 \tTraining Loss: 1.163892 \tValidation Loss: 1.388196 \t time: 0.3\n",
      "Epoch: 971 \tTraining Loss: 1.163891 \tValidation Loss: 1.388023 \t time: 0.3\n",
      "Epoch: 972 \tTraining Loss: 1.163869 \tValidation Loss: 1.388271 \t time: 0.3\n",
      "Epoch: 973 \tTraining Loss: 1.163861 \tValidation Loss: 1.388461 \t time: 0.3\n",
      "Epoch: 974 \tTraining Loss: 1.163857 \tValidation Loss: 1.388275 \t time: 0.3\n",
      "Epoch: 975 \tTraining Loss: 1.163850 \tValidation Loss: 1.388248 \t time: 0.3\n",
      "Epoch: 976 \tTraining Loss: 1.163837 \tValidation Loss: 1.388262 \t time: 0.3\n",
      "Epoch: 977 \tTraining Loss: 1.163833 \tValidation Loss: 1.388133 \t time: 0.3\n",
      "Epoch: 978 \tTraining Loss: 1.163831 \tValidation Loss: 1.388230 \t time: 0.3\n",
      "Epoch: 979 \tTraining Loss: 1.163817 \tValidation Loss: 1.388379 \t time: 0.3\n",
      "Epoch: 980 \tTraining Loss: 1.163817 \tValidation Loss: 1.388284 \t time: 0.3\n",
      "Epoch: 981 \tTraining Loss: 1.163812 \tValidation Loss: 1.388311 \t time: 0.3\n",
      "Epoch: 982 \tTraining Loss: 1.163802 \tValidation Loss: 1.388360 \t time: 0.3\n",
      "Epoch: 983 \tTraining Loss: 1.163803 \tValidation Loss: 1.388139 \t time: 0.2\n",
      "Epoch: 984 \tTraining Loss: 1.163793 \tValidation Loss: 1.388235 \t time: 0.3\n",
      "Epoch: 985 \tTraining Loss: 1.163784 \tValidation Loss: 1.388337 \t time: 0.3\n",
      "Epoch: 986 \tTraining Loss: 1.163775 \tValidation Loss: 1.388147 \t time: 0.3\n",
      "Epoch: 987 \tTraining Loss: 1.163759 \tValidation Loss: 1.388399 \t time: 0.3\n",
      "Epoch: 988 \tTraining Loss: 1.163715 \tValidation Loss: 1.388604 \t time: 0.3\n",
      "Epoch: 989 \tTraining Loss: 1.163606 \tValidation Loss: 1.388234 \t time: 0.2\n",
      "Epoch: 990 \tTraining Loss: 1.163599 \tValidation Loss: 1.388319 \t time: 0.3\n",
      "Epoch: 991 \tTraining Loss: 1.163588 \tValidation Loss: 1.388244 \t time: 0.3\n",
      "Epoch: 992 \tTraining Loss: 1.163578 \tValidation Loss: 1.387974 \t time: 0.2\n",
      "Epoch: 993 \tTraining Loss: 1.163620 \tValidation Loss: 1.388270 \t time: 0.3\n",
      "Epoch: 994 \tTraining Loss: 1.163545 \tValidation Loss: 1.388335 \t time: 0.3\n",
      "Epoch: 995 \tTraining Loss: 1.163543 \tValidation Loss: 1.388032 \t time: 0.2\n",
      "Epoch: 996 \tTraining Loss: 1.163541 \tValidation Loss: 1.388071 \t time: 0.3\n",
      "Epoch: 997 \tTraining Loss: 1.163526 \tValidation Loss: 1.388012 \t time: 0.3\n",
      "Epoch: 998 \tTraining Loss: 1.163524 \tValidation Loss: 1.387589 \t time: 0.2\n",
      "Epoch: 999 \tTraining Loss: 1.163512 \tValidation Loss: 1.387930 \t time: 0.3\n",
      "Epoch: 1000 \tTraining Loss: 1.163494 \tValidation Loss: 1.388036 \t time: 0.3\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf \n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "\n",
    "#             data = data.type((torch.FloatTensor))\n",
    "\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update accumulated training loss\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "            \n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
    "            \n",
    "#             data = data.type((torch.FloatTensor))\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update accumulated validation loss \n",
    "            valid_loss += loss.item()*data.size(0)\n",
    "            \n",
    "\n",
    "        train_loss = train_loss/len(loaders['train'].dataset)\n",
    "        valid_loss = valid_loss/len(loaders['valid'].dataset)\n",
    "        \n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\t time: {:.1f}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss,\n",
    "            time.time() - start\n",
    "            ))\n",
    "        \n",
    "        ## TODO: save the model if validation loss has decreased\n",
    "        if valid_loss < valid_loss_min:\n",
    "            print('Validation loss decreased from {:.6f} to {:.6f}. Model was saved'.format(\n",
    "                valid_loss_min,\n",
    "                valid_loss\n",
    "            ))\n",
    "            \n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            valid_loss_min = valid_loss\n",
    "    \n",
    "    # return trained model\n",
    "    return model\n",
    "\n",
    "\n",
    "# train the model\n",
    "model = train(n_epochs, loaders, model, optimizer, \n",
    "                      criterion, use_cuda, 'model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch: 215 \tTraining Loss: 1.329372 \tValidation Loss: 1.419149 \t time: 0.4\n",
    "Epoch: 216 \tTraining Loss: 1.328865 \tValidation Loss: 1.416587 \t time: 0.4\n",
    "Validation loss decreased from 1.417114 to 1.416587. Model was saved\n",
    "Kaggle score = 0.54438 (0.0006 IMPROVEMENT)\n",
    "\n",
    "Epoch: 216 \tTraining Loss: 1.326797 \tValidation Loss: 1.396590 \t time: 0.7\n",
    "Epoch: 217 \tTraining Loss: 1.326093 \tValidation Loss: 1.396514 \t time: 0.7\n",
    "Epoch: 218 \tTraining Loss: 1.325473 \tValidation Loss: 1.394434 \t time: 0.6\n",
    "Validation loss decreased from 1.396026 to 1.394434. Model was saved\n",
    "Kaggle score = 55245 (0.011 IMPROVEMENT)\n",
    "\n",
    "Epoch: 364 \tTraining Loss: 1.229904 \tValidation Loss: 1.381198 \t time: 0.3\n",
    "Validation loss decreased from 1.384373 to 1.381198. Model was saved\n",
    "Epoch: 365 \tTraining Loss: 1.231026 \tValidation Loss: 1.394960 \t time: 0.3\n",
    "Epoch: 366 \tTraining Loss: 1.231521 \tValidation Loss: 1.379200 \t time: 0.3\n",
    "Validation loss decreased from 1.381198 to 1.379200. Model was saved\n",
    "Epoch: 367 \tTraining Loss: 1.232151 \tValidation Loss: 1.395807 \t time: 0.3\n",
    "Epoch: 368 \tTraining Loss: 1.230561 \tValidation Loss: 1.391226 \t time: 0.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.456303\n",
      "\n",
      "\n",
      "Test Accuracy: 58% (17/29)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.6/site-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "def test(loaders, model, criterion, use_cuda):\n",
    "\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "        \n",
    "#         data = data.type((torch.FloatTensor))\n",
    "        # move to GPU\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average test loss \n",
    "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "        # convert output probabilities to predicted class\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        # compare predictions to true label\n",
    "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "        total += data.size(0)\n",
    "            \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "        100. * correct / total, correct, total))\n",
    "\n",
    "# load the model that got the best validation accuracy\n",
    "model.load_state_dict(torch.load('model.pt'))\n",
    "# call test function    \n",
    "test(loaders, model, criterion, use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.6/site-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4, 0, 4, 3, 0, 2, 4, 5, 2, 1, 4, 0, 3, 0, 2, 2, 5, 1, 0, 1, 5, 5,\n",
       "       5, 0, 5, 2, 4, 2, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# i,l = next(iter(loaders['test']))\n",
    "# if use_cuda:\n",
    "#     i, l = i.cuda(), l.cuda()\n",
    "\n",
    "# output = model(i)\n",
    "\n",
    "# result = output.cpu().data.max(1, keepdim=True)[1].numpy()\n",
    "# result[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([6., 0., 4., 0., 6., 0., 2., 0., 5., 6.]),\n",
       " array([0. , 0.5, 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = result[:,0]\n",
    "plt.hist(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5., 0., 5., 0., 5., 0., 4., 0., 5., 5.]),\n",
       " array([0. , 0.5, 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(l.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.6/site-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 2, 4, ..., 0, 2, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_test_data = torch.tensor(features_test.values).type((torch.FloatTensor))\n",
    "if use_cuda:\n",
    "    features_test_data = features_test_data.cuda()\n",
    "predicted_class = model(features_test_data)\n",
    "# We will look at the predicted prices to ensure we have something sensible.\n",
    "predicted_class = predicted_class.data.cpu().max(1, keepdim=True)[1].numpy()[:,0]\n",
    "predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = pd.DataFrame({\"ID\":id, \"class\":predicted_class})\n",
    "solution.to_csv(\"pokemon_sol.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
