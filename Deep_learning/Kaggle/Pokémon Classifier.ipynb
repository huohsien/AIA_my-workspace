{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "all_data = pd.concat((train.loc[:,'appearedTimeOfDay':'cooc_151'],\n",
    "                      test.loc[:,'appearedTimeOfDay':'cooc_151']))\n",
    "id = test['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>appearedTimeOfDay</th>\n",
       "      <th>appearedHour</th>\n",
       "      <th>appearedMinute</th>\n",
       "      <th>terrainType</th>\n",
       "      <th>closeToWater</th>\n",
       "      <th>city</th>\n",
       "      <th>continent</th>\n",
       "      <th>weather</th>\n",
       "      <th>temperature</th>\n",
       "      <th>windSpeed</th>\n",
       "      <th>...</th>\n",
       "      <th>cooc_142</th>\n",
       "      <th>cooc_143</th>\n",
       "      <th>cooc_144</th>\n",
       "      <th>cooc_145</th>\n",
       "      <th>cooc_146</th>\n",
       "      <th>cooc_147</th>\n",
       "      <th>cooc_148</th>\n",
       "      <th>cooc_149</th>\n",
       "      <th>cooc_150</th>\n",
       "      <th>cooc_151</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>evening</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>Bangkok</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Clear</td>\n",
       "      <td>27.8</td>\n",
       "      <td>9.00</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>night</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>13</td>\n",
       "      <td>True</td>\n",
       "      <td>New_York</td>\n",
       "      <td>America</td>\n",
       "      <td>Clear</td>\n",
       "      <td>26.1</td>\n",
       "      <td>8.70</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>evening</td>\n",
       "      <td>19</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>New_York</td>\n",
       "      <td>America</td>\n",
       "      <td>Clear</td>\n",
       "      <td>24.7</td>\n",
       "      <td>16.82</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>morning</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>Hobart</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Clear</td>\n",
       "      <td>12.7</td>\n",
       "      <td>13.25</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>evening</td>\n",
       "      <td>18</td>\n",
       "      <td>32</td>\n",
       "      <td>13</td>\n",
       "      <td>True</td>\n",
       "      <td>Los_Angeles</td>\n",
       "      <td>America</td>\n",
       "      <td>PartlyCloudy</td>\n",
       "      <td>19.1</td>\n",
       "      <td>5.78</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 182 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  appearedTimeOfDay  appearedHour  appearedMinute  terrainType  closeToWater  \\\n",
       "0           evening            19              10           13         False   \n",
       "1             night             5              19           13          True   \n",
       "2           evening            19              46            0          True   \n",
       "3           morning            11              10            0          True   \n",
       "4           evening            18              32           13          True   \n",
       "\n",
       "          city  continent       weather  temperature  windSpeed  ...  \\\n",
       "0      Bangkok       Asia         Clear         27.8       9.00  ...   \n",
       "1     New_York    America         Clear         26.1       8.70  ...   \n",
       "2     New_York    America         Clear         24.7      16.82  ...   \n",
       "3       Hobart  Australia         Clear         12.7      13.25  ...   \n",
       "4  Los_Angeles    America  PartlyCloudy         19.1       5.78  ...   \n",
       "\n",
       "   cooc_142 cooc_143  cooc_144  cooc_145  cooc_146  cooc_147  cooc_148  \\\n",
       "0     False    False     False     False     False     False     False   \n",
       "1     False    False     False     False     False     False     False   \n",
       "2     False    False     False     False     False     False     False   \n",
       "3     False    False     False     False     False     False     False   \n",
       "4     False    False     False     False     False     False     False   \n",
       "\n",
       "   cooc_149  cooc_150  cooc_151  \n",
       "0     False     False     False  \n",
       "1     False     False     False  \n",
       "2     False     False     False  \n",
       "3     False     False     False  \n",
       "4     False     False     False  \n",
       "\n",
       "[5 rows x 182 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data.applymap(lambda x: 1.0 if x == True else x)\n",
    "all_data = all_data.applymap(lambda x: 0.0 if x == False else x)\n",
    "all_data = pd.get_dummies(all_data)\n",
    "# numeric_feats = df.dtypes[df.dtypes != \"object\"].index\n",
    "# df = df[numeric_feats]\n",
    "apearedHour = all_data['appearedHour']\n",
    "appearedMinute = all_data['appearedMinute']\n",
    "appearedTimeDayCycle = apearedHour * 60 + appearedMinute\n",
    "appearedTimeDayCycle = np.sin(appearedTimeDayCycle / (24 * 60) * 2 * np.pi)\n",
    "# print('appearedTimeDayCycle= ',appearedTimeDayCycle)\n",
    "all_data = all_data.drop(['appearedHour'], axis=1)\n",
    "all_data = all_data.drop(['appearedMinute'], axis=1)\n",
    "all_data['appearedTimeDayCycle'] = appearedTimeDayCycle\n",
    "\n",
    "# df = df.drop(['temperature'], axis=1)\n",
    "# df = df.drop(['windSpeed'], axis=1)\n",
    "# df = df.drop(['pressure'], axis=1)\n",
    "# df = df.drop(['gymIn100m'], axis=1)\n",
    "# df = df.drop(['gymIn250m'], axis=1)\n",
    "# df = df.drop(['gymIn500m'], axis=1)\n",
    "# df = df.drop(['gymIn1000m'], axis=1)\n",
    "# df = df.drop(['gymIn2500m'], axis=1)\n",
    "# df = df.drop(['gymIn5000m'], axis=1)\n",
    "# df = df.drop(['rural'], axis=1)\n",
    "# df = df.drop(['midurban'], axis=1)\n",
    "# df = df.drop(['suburban'], axis=1)\n",
    "# df = df.drop(['urban'], axis=1)\n",
    "# df = df.drop(['pokestopIn100m'], axis=1)\n",
    "# df = df.drop(['pokestopIn250m'], axis=1)\n",
    "# df = df.drop(['pokestopIn500m'], axis=1)\n",
    "# df = df.drop(['pokestopIn1000m'], axis=1)\n",
    "# df = df.drop(['pokestopIn2500m'], axis=1)\n",
    "# df = df.drop(['pokestopIn5000m'], axis=1)\n",
    "# df = df.drop(['terrainType'], axis=1)\n",
    "# df = df.drop(['closeToWater'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>terrainType</th>\n",
       "      <th>closeToWater</th>\n",
       "      <th>temperature</th>\n",
       "      <th>windSpeed</th>\n",
       "      <th>pressure</th>\n",
       "      <th>population_density</th>\n",
       "      <th>urban</th>\n",
       "      <th>suburban</th>\n",
       "      <th>midurban</th>\n",
       "      <th>rural</th>\n",
       "      <th>...</th>\n",
       "      <th>weather_WindyandPartlyCloudy</th>\n",
       "      <th>weatherIcon_clear-day</th>\n",
       "      <th>weatherIcon_clear-night</th>\n",
       "      <th>weatherIcon_cloudy</th>\n",
       "      <th>weatherIcon_fog</th>\n",
       "      <th>weatherIcon_partly-cloudy-day</th>\n",
       "      <th>weatherIcon_partly-cloudy-night</th>\n",
       "      <th>weatherIcon_rain</th>\n",
       "      <th>weatherIcon_wind</th>\n",
       "      <th>appearedTimeDayCycle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.8</td>\n",
       "      <td>9.00</td>\n",
       "      <td>1008.96</td>\n",
       "      <td>6019.04440</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.953717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.1</td>\n",
       "      <td>8.70</td>\n",
       "      <td>1018.96</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.984041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.7</td>\n",
       "      <td>16.82</td>\n",
       "      <td>1023.22</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.894934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.7</td>\n",
       "      <td>13.25</td>\n",
       "      <td>1014.19</td>\n",
       "      <td>128.89505</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.216440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.1</td>\n",
       "      <td>5.78</td>\n",
       "      <td>1011.36</td>\n",
       "      <td>4188.39100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.990268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 297 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   terrainType  closeToWater  temperature  windSpeed  pressure  \\\n",
       "0         13.0           0.0         27.8       9.00   1008.96   \n",
       "1         13.0           1.0         26.1       8.70   1018.96   \n",
       "2          0.0           1.0         24.7      16.82   1023.22   \n",
       "3          0.0           1.0         12.7      13.25   1014.19   \n",
       "4         13.0           1.0         19.1       5.78   1011.36   \n",
       "\n",
       "   population_density  urban  suburban  midurban  rural  ...  \\\n",
       "0          6019.04440    1.0       1.0       1.0    0.0  ...   \n",
       "1             0.00000    0.0       0.0       0.0    1.0  ...   \n",
       "2             0.00000    0.0       0.0       0.0    1.0  ...   \n",
       "3           128.89505    0.0       0.0       0.0    1.0  ...   \n",
       "4          4188.39100    1.0       1.0       1.0    0.0  ...   \n",
       "\n",
       "   weather_WindyandPartlyCloudy  weatherIcon_clear-day  \\\n",
       "0                             0                      1   \n",
       "1                             0                      0   \n",
       "2                             0                      1   \n",
       "3                             0                      0   \n",
       "4                             0                      0   \n",
       "\n",
       "   weatherIcon_clear-night  weatherIcon_cloudy  weatherIcon_fog  \\\n",
       "0                        0                   0                0   \n",
       "1                        1                   0                0   \n",
       "2                        0                   0                0   \n",
       "3                        1                   0                0   \n",
       "4                        0                   0                0   \n",
       "\n",
       "   weatherIcon_partly-cloudy-day  weatherIcon_partly-cloudy-night  \\\n",
       "0                              0                                0   \n",
       "1                              0                                0   \n",
       "2                              0                                0   \n",
       "3                              0                                0   \n",
       "4                              1                                0   \n",
       "\n",
       "   weatherIcon_rain  weatherIcon_wind  appearedTimeDayCycle  \n",
       "0                 0                 0             -0.953717  \n",
       "1                 0                 0              0.984041  \n",
       "2                 0                 0             -0.894934  \n",
       "3                 0                 0              0.216440  \n",
       "4                 0                 0             -0.990268  \n",
       "\n",
       "[5 rows x 297 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(all_data.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's look at the data :\n",
    "# matplotlib.rcParams['figure.figsize'] = (6.0, 6.0)\n",
    "\n",
    "# data = pd.DataFrame({\"x\":df['population_density'], \"y\":targets})\n",
    "\n",
    "# data.plot(x = \"x\", y = \"y\",kind = \"scatter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normailize to 0-1\n",
    "for k in all_data.columns.values:\n",
    "    if (all_data[k].max() - all_data[k].min()) > 0:\n",
    "        all_data[k] = (all_data[k] - all_data[k].min())/(all_data[k].max() - all_data[k].min())\n",
    "    else:\n",
    "        all_data[k] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>terrainType</th>\n",
       "      <th>closeToWater</th>\n",
       "      <th>temperature</th>\n",
       "      <th>windSpeed</th>\n",
       "      <th>pressure</th>\n",
       "      <th>population_density</th>\n",
       "      <th>urban</th>\n",
       "      <th>suburban</th>\n",
       "      <th>midurban</th>\n",
       "      <th>rural</th>\n",
       "      <th>...</th>\n",
       "      <th>weather_WindyandPartlyCloudy</th>\n",
       "      <th>weatherIcon_clear-day</th>\n",
       "      <th>weatherIcon_clear-night</th>\n",
       "      <th>weatherIcon_cloudy</th>\n",
       "      <th>weatherIcon_fog</th>\n",
       "      <th>weatherIcon_partly-cloudy-day</th>\n",
       "      <th>weatherIcon_partly-cloudy-night</th>\n",
       "      <th>weatherIcon_rain</th>\n",
       "      <th>weatherIcon_wind</th>\n",
       "      <th>appearedTimeDayCycle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.631868</td>\n",
       "      <td>0.160342</td>\n",
       "      <td>0.333774</td>\n",
       "      <td>0.601904</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.8125</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.585165</td>\n",
       "      <td>0.154997</td>\n",
       "      <td>0.598044</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.992020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.546703</td>\n",
       "      <td>0.299662</td>\n",
       "      <td>0.710624</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.217033</td>\n",
       "      <td>0.236059</td>\n",
       "      <td>0.471987</td>\n",
       "      <td>0.012890</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.608220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.8125</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.102975</td>\n",
       "      <td>0.397199</td>\n",
       "      <td>0.418839</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 297 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   terrainType  closeToWater  temperature  windSpeed  pressure  \\\n",
       "0       0.8125           0.0     0.631868   0.160342  0.333774   \n",
       "1       0.8125           1.0     0.585165   0.154997  0.598044   \n",
       "2       0.0000           1.0     0.546703   0.299662  0.710624   \n",
       "3       0.0000           1.0     0.217033   0.236059  0.471987   \n",
       "4       0.8125           1.0     0.392857   0.102975  0.397199   \n",
       "\n",
       "   population_density  urban  suburban  midurban  rural  ...  \\\n",
       "0            0.601904    1.0       1.0       1.0    0.0  ...   \n",
       "1            0.000000    0.0       0.0       0.0    1.0  ...   \n",
       "2            0.000000    0.0       0.0       0.0    1.0  ...   \n",
       "3            0.012890    0.0       0.0       0.0    1.0  ...   \n",
       "4            0.418839    1.0       1.0       1.0    0.0  ...   \n",
       "\n",
       "   weather_WindyandPartlyCloudy  weatherIcon_clear-day  \\\n",
       "0                           0.0                    1.0   \n",
       "1                           0.0                    0.0   \n",
       "2                           0.0                    1.0   \n",
       "3                           0.0                    0.0   \n",
       "4                           0.0                    0.0   \n",
       "\n",
       "   weatherIcon_clear-night  weatherIcon_cloudy  weatherIcon_fog  \\\n",
       "0                      0.0                 0.0              0.0   \n",
       "1                      1.0                 0.0              0.0   \n",
       "2                      0.0                 0.0              0.0   \n",
       "3                      1.0                 0.0              0.0   \n",
       "4                      0.0                 0.0              0.0   \n",
       "\n",
       "   weatherIcon_partly-cloudy-day  weatherIcon_partly-cloudy-night  \\\n",
       "0                            0.0                              0.0   \n",
       "1                            0.0                              0.0   \n",
       "2                            0.0                              0.0   \n",
       "3                            0.0                              0.0   \n",
       "4                            1.0                              0.0   \n",
       "\n",
       "   weatherIcon_rain  weatherIcon_wind  appearedTimeDayCycle  \n",
       "0               0.0               0.0              0.023142  \n",
       "1               0.0               0.0              0.992020  \n",
       "2               0.0               0.0              0.052533  \n",
       "3               0.0               0.0              0.608220  \n",
       "4               0.0               0.0              0.004866  \n",
       "\n",
       "[5 rows x 297 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7160 entries, 0 to 7159\n",
      "Columns: 297 entries, terrainType to appearedTimeDayCycle\n",
      "dtypes: float64(272), int64(25)\n",
      "memory usage: 16.3 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7160, 297)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = all_data[:train.shape[0]]\n",
    "features_test = all_data[train.shape[0]:]\n",
    "targets = train['class']\n",
    "\n",
    "features.info()\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6444, 297), (572, 297), (144, 297))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_ratio = 0.9\n",
    "test_ratio = 0.2\n",
    "# split the data into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(features.values, targets.values, test_size = 1 - train_ratio, stratify=targets, random_state=0)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_valid, y_valid, test_size = test_ratio, stratify=y_valid, random_state=0)\n",
    "X_train.shape,X_valid.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_ratio = 0.7\n",
    "test_ratio = 0.2\n",
    "\n",
    "train_ratio = 0.9\n",
    "test_ratio = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data_utils\n",
    "\n",
    "data_train = data_utils.TensorDataset(torch.from_numpy(X_train).type((torch.FloatTensor)), torch.from_numpy(y_train).type((torch.LongTensor)))\n",
    "data_valid = data_utils.TensorDataset(torch.from_numpy(X_valid).type((torch.FloatTensor)), torch.from_numpy(y_valid).type((torch.LongTensor)))\n",
    "data_test = data_utils.TensorDataset(torch.from_numpy(X_test).type((torch.FloatTensor)), torch.from_numpy(y_test).type((torch.LongTensor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many data per batch to load\n",
    "batch_size = 10000\n",
    "\n",
    "# convert data to torch.FloatTensor\n",
    "\n",
    "loaders = {}\n",
    "loaders['train'] = torch.utils.data.DataLoader(data_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=1)\n",
    "\n",
    "loaders['valid'] = torch.utils.data.DataLoader(data_valid,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=1)\n",
    "loaders['test'] = torch.utils.data.DataLoader(data_test,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "#     print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the CNN architecture\n",
    "class Net(nn.Module):\n",
    "    ### TODO: choose an architecture, and complete the class\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(297, 128)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.fc3 = nn.Linear(32, 6)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.sigmoid(self.fc3(x))\n",
    "\n",
    "\n",
    "        return x\n",
    "\n",
    "#-#-# You do NOT have to modify the code below this line. #-#-#\n",
    "\n",
    "# instantiate the CNN\n",
    "model = Net()\n",
    "def init_weights(m):\n",
    "        print(m)\n",
    "        if type(m) == nn.Linear:\n",
    "            m.weight.data.fill_(1.0)\n",
    "            print(m.weight)\n",
    "            \n",
    "def init_ortho(m):\n",
    "    print()\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.orthogonal_(m.weight)\n",
    "        print(m.weight)\n",
    "\n",
    "# use the modules apply function to recursively apply the initialization\n",
    "# model.apply(init_ortho)\n",
    "\n",
    "# move tensors to GPU if CUDA is available\n",
    "if use_cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "### TODO: select loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "### TODO: select optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.05)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01,weight_decay= 1e-6, momentum = 0.9, nesterov = True)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huohsien/anaconda2/envs/ai/lib/python3.7/site-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.796277 \tValidation Loss: 1.761073 \t time: 0.3\n",
      "Validation loss decreased from inf to 1.761073. Model was saved\n",
      "Epoch: 2 \tTraining Loss: 1.761498 \tValidation Loss: 1.760908 \t time: 0.3\n",
      "Validation loss decreased from 1.761073 to 1.760908. Model was saved\n",
      "Epoch: 3 \tTraining Loss: 1.762357 \tValidation Loss: 1.761419 \t time: 0.3\n",
      "Epoch: 4 \tTraining Loss: 1.762975 \tValidation Loss: 1.761588 \t time: 0.3\n",
      "Epoch: 5 \tTraining Loss: 1.762194 \tValidation Loss: 1.760134 \t time: 0.3\n",
      "Validation loss decreased from 1.760908 to 1.760134. Model was saved\n",
      "Epoch: 6 \tTraining Loss: 1.761766 \tValidation Loss: 1.758744 \t time: 0.3\n",
      "Validation loss decreased from 1.760134 to 1.758744. Model was saved\n",
      "Epoch: 7 \tTraining Loss: 1.759063 \tValidation Loss: 1.758578 \t time: 0.3\n",
      "Validation loss decreased from 1.758744 to 1.758578. Model was saved\n",
      "Epoch: 8 \tTraining Loss: 1.758763 \tValidation Loss: 1.757950 \t time: 0.3\n",
      "Validation loss decreased from 1.758578 to 1.757950. Model was saved\n",
      "Epoch: 9 \tTraining Loss: 1.759756 \tValidation Loss: 1.756127 \t time: 0.3\n",
      "Validation loss decreased from 1.757950 to 1.756127. Model was saved\n",
      "Epoch: 10 \tTraining Loss: 1.756943 \tValidation Loss: 1.753726 \t time: 0.2\n",
      "Validation loss decreased from 1.756127 to 1.753726. Model was saved\n",
      "Epoch: 11 \tTraining Loss: 1.753798 \tValidation Loss: 1.750885 \t time: 0.4\n",
      "Validation loss decreased from 1.753726 to 1.750885. Model was saved\n",
      "Epoch: 12 \tTraining Loss: 1.753313 \tValidation Loss: 1.746995 \t time: 0.2\n",
      "Validation loss decreased from 1.750885 to 1.746995. Model was saved\n",
      "Epoch: 13 \tTraining Loss: 1.748031 \tValidation Loss: 1.742141 \t time: 0.2\n",
      "Validation loss decreased from 1.746995 to 1.742141. Model was saved\n",
      "Epoch: 14 \tTraining Loss: 1.743851 \tValidation Loss: 1.737092 \t time: 0.3\n",
      "Validation loss decreased from 1.742141 to 1.737092. Model was saved\n",
      "Epoch: 15 \tTraining Loss: 1.740450 \tValidation Loss: 1.731384 \t time: 0.3\n",
      "Validation loss decreased from 1.737092 to 1.731384. Model was saved\n",
      "Epoch: 16 \tTraining Loss: 1.734820 \tValidation Loss: 1.725668 \t time: 0.3\n",
      "Validation loss decreased from 1.731384 to 1.725668. Model was saved\n",
      "Epoch: 17 \tTraining Loss: 1.729888 \tValidation Loss: 1.720027 \t time: 0.3\n",
      "Validation loss decreased from 1.725668 to 1.720027. Model was saved\n",
      "Epoch: 18 \tTraining Loss: 1.722785 \tValidation Loss: 1.710539 \t time: 0.3\n",
      "Validation loss decreased from 1.720027 to 1.710539. Model was saved\n",
      "Epoch: 19 \tTraining Loss: 1.716855 \tValidation Loss: 1.698631 \t time: 0.3\n",
      "Validation loss decreased from 1.710539 to 1.698631. Model was saved\n",
      "Epoch: 20 \tTraining Loss: 1.708693 \tValidation Loss: 1.690063 \t time: 0.2\n",
      "Validation loss decreased from 1.698631 to 1.690063. Model was saved\n",
      "Epoch: 21 \tTraining Loss: 1.702799 \tValidation Loss: 1.686166 \t time: 0.3\n",
      "Validation loss decreased from 1.690063 to 1.686166. Model was saved\n",
      "Epoch: 22 \tTraining Loss: 1.696772 \tValidation Loss: 1.676077 \t time: 0.3\n",
      "Validation loss decreased from 1.686166 to 1.676077. Model was saved\n",
      "Epoch: 23 \tTraining Loss: 1.691020 \tValidation Loss: 1.663620 \t time: 0.3\n",
      "Validation loss decreased from 1.676077 to 1.663620. Model was saved\n",
      "Epoch: 24 \tTraining Loss: 1.684805 \tValidation Loss: 1.655357 \t time: 0.3\n",
      "Validation loss decreased from 1.663620 to 1.655357. Model was saved\n",
      "Epoch: 25 \tTraining Loss: 1.676887 \tValidation Loss: 1.649312 \t time: 0.3\n",
      "Validation loss decreased from 1.655357 to 1.649312. Model was saved\n",
      "Epoch: 26 \tTraining Loss: 1.670668 \tValidation Loss: 1.640987 \t time: 0.3\n",
      "Validation loss decreased from 1.649312 to 1.640987. Model was saved\n",
      "Epoch: 27 \tTraining Loss: 1.663708 \tValidation Loss: 1.631824 \t time: 0.3\n",
      "Validation loss decreased from 1.640987 to 1.631824. Model was saved\n",
      "Epoch: 28 \tTraining Loss: 1.657003 \tValidation Loss: 1.624490 \t time: 0.3\n",
      "Validation loss decreased from 1.631824 to 1.624490. Model was saved\n",
      "Epoch: 29 \tTraining Loss: 1.651774 \tValidation Loss: 1.617875 \t time: 0.3\n",
      "Validation loss decreased from 1.624490 to 1.617875. Model was saved\n",
      "Epoch: 30 \tTraining Loss: 1.642970 \tValidation Loss: 1.610168 \t time: 0.3\n",
      "Validation loss decreased from 1.617875 to 1.610168. Model was saved\n",
      "Epoch: 31 \tTraining Loss: 1.637802 \tValidation Loss: 1.604042 \t time: 0.2\n",
      "Validation loss decreased from 1.610168 to 1.604042. Model was saved\n",
      "Epoch: 32 \tTraining Loss: 1.636127 \tValidation Loss: 1.598944 \t time: 0.3\n",
      "Validation loss decreased from 1.604042 to 1.598944. Model was saved\n",
      "Epoch: 33 \tTraining Loss: 1.629210 \tValidation Loss: 1.594275 \t time: 0.3\n",
      "Validation loss decreased from 1.598944 to 1.594275. Model was saved\n",
      "Epoch: 34 \tTraining Loss: 1.623733 \tValidation Loss: 1.590387 \t time: 0.3\n",
      "Validation loss decreased from 1.594275 to 1.590387. Model was saved\n",
      "Epoch: 35 \tTraining Loss: 1.621127 \tValidation Loss: 1.586077 \t time: 0.3\n",
      "Validation loss decreased from 1.590387 to 1.586077. Model was saved\n",
      "Epoch: 36 \tTraining Loss: 1.618369 \tValidation Loss: 1.583712 \t time: 0.3\n",
      "Validation loss decreased from 1.586077 to 1.583712. Model was saved\n",
      "Epoch: 37 \tTraining Loss: 1.611836 \tValidation Loss: 1.580402 \t time: 0.3\n",
      "Validation loss decreased from 1.583712 to 1.580402. Model was saved\n",
      "Epoch: 38 \tTraining Loss: 1.609334 \tValidation Loss: 1.577538 \t time: 0.3\n",
      "Validation loss decreased from 1.580402 to 1.577538. Model was saved\n",
      "Epoch: 39 \tTraining Loss: 1.608598 \tValidation Loss: 1.576320 \t time: 0.2\n",
      "Validation loss decreased from 1.577538 to 1.576320. Model was saved\n",
      "Epoch: 40 \tTraining Loss: 1.602880 \tValidation Loss: 1.574638 \t time: 0.2\n",
      "Validation loss decreased from 1.576320 to 1.574638. Model was saved\n",
      "Epoch: 41 \tTraining Loss: 1.599134 \tValidation Loss: 1.576340 \t time: 0.4\n",
      "Epoch: 42 \tTraining Loss: 1.598210 \tValidation Loss: 1.568997 \t time: 0.2\n",
      "Validation loss decreased from 1.574638 to 1.568997. Model was saved\n",
      "Epoch: 43 \tTraining Loss: 1.596141 \tValidation Loss: 1.565000 \t time: 0.3\n",
      "Validation loss decreased from 1.568997 to 1.565000. Model was saved\n",
      "Epoch: 44 \tTraining Loss: 1.592276 \tValidation Loss: 1.562205 \t time: 0.3\n",
      "Validation loss decreased from 1.565000 to 1.562205. Model was saved\n",
      "Epoch: 45 \tTraining Loss: 1.593060 \tValidation Loss: 1.559078 \t time: 0.2\n",
      "Validation loss decreased from 1.562205 to 1.559078. Model was saved\n",
      "Epoch: 46 \tTraining Loss: 1.588704 \tValidation Loss: 1.557381 \t time: 0.3\n",
      "Validation loss decreased from 1.559078 to 1.557381. Model was saved\n",
      "Epoch: 47 \tTraining Loss: 1.588429 \tValidation Loss: 1.556436 \t time: 0.2\n",
      "Validation loss decreased from 1.557381 to 1.556436. Model was saved\n",
      "Epoch: 48 \tTraining Loss: 1.585724 \tValidation Loss: 1.553688 \t time: 0.3\n",
      "Validation loss decreased from 1.556436 to 1.553688. Model was saved\n",
      "Epoch: 49 \tTraining Loss: 1.587168 \tValidation Loss: 1.551746 \t time: 0.3\n",
      "Validation loss decreased from 1.553688 to 1.551746. Model was saved\n",
      "Epoch: 50 \tTraining Loss: 1.582316 \tValidation Loss: 1.551200 \t time: 0.2\n",
      "Validation loss decreased from 1.551746 to 1.551200. Model was saved\n",
      "Epoch: 51 \tTraining Loss: 1.582656 \tValidation Loss: 1.548073 \t time: 0.3\n",
      "Validation loss decreased from 1.551200 to 1.548073. Model was saved\n",
      "Epoch: 52 \tTraining Loss: 1.579113 \tValidation Loss: 1.549210 \t time: 0.4\n",
      "Epoch: 53 \tTraining Loss: 1.577119 \tValidation Loss: 1.545112 \t time: 0.2\n",
      "Validation loss decreased from 1.548073 to 1.545112. Model was saved\n",
      "Epoch: 54 \tTraining Loss: 1.574624 \tValidation Loss: 1.542572 \t time: 0.2\n",
      "Validation loss decreased from 1.545112 to 1.542572. Model was saved\n",
      "Epoch: 55 \tTraining Loss: 1.573706 \tValidation Loss: 1.541831 \t time: 0.3\n",
      "Validation loss decreased from 1.542572 to 1.541831. Model was saved\n",
      "Epoch: 56 \tTraining Loss: 1.569683 \tValidation Loss: 1.537117 \t time: 0.3\n",
      "Validation loss decreased from 1.541831 to 1.537117. Model was saved\n",
      "Epoch: 57 \tTraining Loss: 1.567241 \tValidation Loss: 1.535768 \t time: 0.3\n",
      "Validation loss decreased from 1.537117 to 1.535768. Model was saved\n",
      "Epoch: 58 \tTraining Loss: 1.568078 \tValidation Loss: 1.533748 \t time: 0.3\n",
      "Validation loss decreased from 1.535768 to 1.533748. Model was saved\n",
      "Epoch: 59 \tTraining Loss: 1.566714 \tValidation Loss: 1.531715 \t time: 0.3\n",
      "Validation loss decreased from 1.533748 to 1.531715. Model was saved\n",
      "Epoch: 60 \tTraining Loss: 1.562159 \tValidation Loss: 1.531402 \t time: 0.3\n",
      "Validation loss decreased from 1.531715 to 1.531402. Model was saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61 \tTraining Loss: 1.563360 \tValidation Loss: 1.529135 \t time: 0.2\n",
      "Validation loss decreased from 1.531402 to 1.529135. Model was saved\n",
      "Epoch: 62 \tTraining Loss: 1.558092 \tValidation Loss: 1.531357 \t time: 0.3\n",
      "Epoch: 63 \tTraining Loss: 1.557034 \tValidation Loss: 1.523904 \t time: 0.3\n",
      "Validation loss decreased from 1.529135 to 1.523904. Model was saved\n",
      "Epoch: 64 \tTraining Loss: 1.555791 \tValidation Loss: 1.521868 \t time: 0.3\n",
      "Validation loss decreased from 1.523904 to 1.521868. Model was saved\n",
      "Epoch: 65 \tTraining Loss: 1.551780 \tValidation Loss: 1.522181 \t time: 0.3\n",
      "Epoch: 66 \tTraining Loss: 1.550372 \tValidation Loss: 1.525931 \t time: 0.3\n",
      "Epoch: 67 \tTraining Loss: 1.549508 \tValidation Loss: 1.523970 \t time: 0.3\n",
      "Epoch: 68 \tTraining Loss: 1.548549 \tValidation Loss: 1.521828 \t time: 0.3\n",
      "Validation loss decreased from 1.521868 to 1.521828. Model was saved\n",
      "Epoch: 69 \tTraining Loss: 1.547018 \tValidation Loss: 1.520185 \t time: 0.3\n",
      "Validation loss decreased from 1.521828 to 1.520185. Model was saved\n",
      "Epoch: 70 \tTraining Loss: 1.543453 \tValidation Loss: 1.519222 \t time: 0.3\n",
      "Validation loss decreased from 1.520185 to 1.519222. Model was saved\n",
      "Epoch: 71 \tTraining Loss: 1.541021 \tValidation Loss: 1.519145 \t time: 0.3\n",
      "Validation loss decreased from 1.519222 to 1.519145. Model was saved\n",
      "Epoch: 72 \tTraining Loss: 1.538662 \tValidation Loss: 1.520712 \t time: 0.3\n",
      "Epoch: 73 \tTraining Loss: 1.537758 \tValidation Loss: 1.520000 \t time: 0.3\n",
      "Epoch: 74 \tTraining Loss: 1.533179 \tValidation Loss: 1.516378 \t time: 0.3\n",
      "Validation loss decreased from 1.519145 to 1.516378. Model was saved\n",
      "Epoch: 75 \tTraining Loss: 1.535975 \tValidation Loss: 1.514709 \t time: 0.3\n",
      "Validation loss decreased from 1.516378 to 1.514709. Model was saved\n",
      "Epoch: 76 \tTraining Loss: 1.530445 \tValidation Loss: 1.515210 \t time: 0.3\n",
      "Epoch: 77 \tTraining Loss: 1.529206 \tValidation Loss: 1.518528 \t time: 0.3\n",
      "Epoch: 78 \tTraining Loss: 1.525387 \tValidation Loss: 1.520976 \t time: 0.3\n",
      "Epoch: 79 \tTraining Loss: 1.528175 \tValidation Loss: 1.518282 \t time: 0.3\n",
      "Epoch: 80 \tTraining Loss: 1.523949 \tValidation Loss: 1.514593 \t time: 0.4\n",
      "Validation loss decreased from 1.514709 to 1.514593. Model was saved\n",
      "Epoch: 81 \tTraining Loss: 1.522189 \tValidation Loss: 1.510551 \t time: 0.3\n",
      "Validation loss decreased from 1.514593 to 1.510551. Model was saved\n",
      "Epoch: 82 \tTraining Loss: 1.517465 \tValidation Loss: 1.510948 \t time: 0.3\n",
      "Epoch: 83 \tTraining Loss: 1.519707 \tValidation Loss: 1.513318 \t time: 0.3\n",
      "Epoch: 84 \tTraining Loss: 1.516707 \tValidation Loss: 1.513494 \t time: 0.3\n",
      "Epoch: 85 \tTraining Loss: 1.516463 \tValidation Loss: 1.509901 \t time: 0.3\n",
      "Validation loss decreased from 1.510551 to 1.509901. Model was saved\n",
      "Epoch: 86 \tTraining Loss: 1.513060 \tValidation Loss: 1.507522 \t time: 0.3\n",
      "Validation loss decreased from 1.509901 to 1.507522. Model was saved\n",
      "Epoch: 87 \tTraining Loss: 1.514804 \tValidation Loss: 1.507925 \t time: 0.3\n",
      "Epoch: 88 \tTraining Loss: 1.511883 \tValidation Loss: 1.505432 \t time: 0.3\n",
      "Validation loss decreased from 1.507522 to 1.505432. Model was saved\n",
      "Epoch: 89 \tTraining Loss: 1.508723 \tValidation Loss: 1.502469 \t time: 0.3\n",
      "Validation loss decreased from 1.505432 to 1.502469. Model was saved\n",
      "Epoch: 90 \tTraining Loss: 1.510106 \tValidation Loss: 1.502547 \t time: 0.3\n",
      "Epoch: 91 \tTraining Loss: 1.505897 \tValidation Loss: 1.509785 \t time: 0.3\n",
      "Epoch: 92 \tTraining Loss: 1.506288 \tValidation Loss: 1.508982 \t time: 0.3\n",
      "Epoch: 93 \tTraining Loss: 1.503039 \tValidation Loss: 1.506945 \t time: 0.3\n",
      "Epoch: 94 \tTraining Loss: 1.502579 \tValidation Loss: 1.507237 \t time: 0.3\n",
      "Epoch: 95 \tTraining Loss: 1.495098 \tValidation Loss: 1.508995 \t time: 0.3\n",
      "Epoch: 96 \tTraining Loss: 1.499820 \tValidation Loss: 1.505942 \t time: 0.2\n",
      "Epoch: 97 \tTraining Loss: 1.494583 \tValidation Loss: 1.502165 \t time: 0.3\n",
      "Validation loss decreased from 1.502469 to 1.502165. Model was saved\n",
      "Epoch: 98 \tTraining Loss: 1.494838 \tValidation Loss: 1.499280 \t time: 0.3\n",
      "Validation loss decreased from 1.502165 to 1.499280. Model was saved\n",
      "Epoch: 99 \tTraining Loss: 1.493667 \tValidation Loss: 1.497090 \t time: 0.3\n",
      "Validation loss decreased from 1.499280 to 1.497090. Model was saved\n",
      "Epoch: 100 \tTraining Loss: 1.489324 \tValidation Loss: 1.495634 \t time: 0.3\n",
      "Validation loss decreased from 1.497090 to 1.495634. Model was saved\n",
      "Epoch: 101 \tTraining Loss: 1.488907 \tValidation Loss: 1.494448 \t time: 0.3\n",
      "Validation loss decreased from 1.495634 to 1.494448. Model was saved\n",
      "Epoch: 102 \tTraining Loss: 1.489866 \tValidation Loss: 1.493212 \t time: 0.3\n",
      "Validation loss decreased from 1.494448 to 1.493212. Model was saved\n",
      "Epoch: 103 \tTraining Loss: 1.489540 \tValidation Loss: 1.491560 \t time: 0.3\n",
      "Validation loss decreased from 1.493212 to 1.491560. Model was saved\n",
      "Epoch: 104 \tTraining Loss: 1.485875 \tValidation Loss: 1.490236 \t time: 0.3\n",
      "Validation loss decreased from 1.491560 to 1.490236. Model was saved\n",
      "Epoch: 105 \tTraining Loss: 1.483296 \tValidation Loss: 1.490110 \t time: 0.3\n",
      "Validation loss decreased from 1.490236 to 1.490110. Model was saved\n",
      "Epoch: 106 \tTraining Loss: 1.479831 \tValidation Loss: 1.489590 \t time: 0.3\n",
      "Validation loss decreased from 1.490110 to 1.489590. Model was saved\n",
      "Epoch: 107 \tTraining Loss: 1.479912 \tValidation Loss: 1.489850 \t time: 0.3\n",
      "Epoch: 108 \tTraining Loss: 1.477929 \tValidation Loss: 1.489763 \t time: 0.2\n",
      "Epoch: 109 \tTraining Loss: 1.475198 \tValidation Loss: 1.488211 \t time: 0.3\n",
      "Validation loss decreased from 1.489590 to 1.488211. Model was saved\n",
      "Epoch: 110 \tTraining Loss: 1.477448 \tValidation Loss: 1.484686 \t time: 0.3\n",
      "Validation loss decreased from 1.488211 to 1.484686. Model was saved\n",
      "Epoch: 111 \tTraining Loss: 1.472539 \tValidation Loss: 1.480557 \t time: 0.3\n",
      "Validation loss decreased from 1.484686 to 1.480557. Model was saved\n",
      "Epoch: 112 \tTraining Loss: 1.470221 \tValidation Loss: 1.477324 \t time: 0.3\n",
      "Validation loss decreased from 1.480557 to 1.477324. Model was saved\n",
      "Epoch: 113 \tTraining Loss: 1.467688 \tValidation Loss: 1.475036 \t time: 0.3\n",
      "Validation loss decreased from 1.477324 to 1.475036. Model was saved\n",
      "Epoch: 114 \tTraining Loss: 1.464982 \tValidation Loss: 1.473412 \t time: 0.3\n",
      "Validation loss decreased from 1.475036 to 1.473412. Model was saved\n",
      "Epoch: 115 \tTraining Loss: 1.464988 \tValidation Loss: 1.470636 \t time: 0.3\n",
      "Validation loss decreased from 1.473412 to 1.470636. Model was saved\n",
      "Epoch: 116 \tTraining Loss: 1.464534 \tValidation Loss: 1.470583 \t time: 0.3\n",
      "Validation loss decreased from 1.470636 to 1.470583. Model was saved\n",
      "Epoch: 117 \tTraining Loss: 1.462633 \tValidation Loss: 1.480078 \t time: 0.3\n",
      "Epoch: 118 \tTraining Loss: 1.462317 \tValidation Loss: 1.480729 \t time: 0.3\n",
      "Epoch: 119 \tTraining Loss: 1.459519 \tValidation Loss: 1.476173 \t time: 0.3\n",
      "Epoch: 120 \tTraining Loss: 1.458302 \tValidation Loss: 1.477358 \t time: 0.3\n",
      "Epoch: 121 \tTraining Loss: 1.455512 \tValidation Loss: 1.482479 \t time: 0.3\n",
      "Epoch: 122 \tTraining Loss: 1.454734 \tValidation Loss: 1.479640 \t time: 0.3\n",
      "Epoch: 123 \tTraining Loss: 1.452079 \tValidation Loss: 1.475217 \t time: 0.3\n",
      "Epoch: 124 \tTraining Loss: 1.450428 \tValidation Loss: 1.473859 \t time: 0.3\n",
      "Epoch: 125 \tTraining Loss: 1.452162 \tValidation Loss: 1.473899 \t time: 0.3\n",
      "Epoch: 126 \tTraining Loss: 1.448704 \tValidation Loss: 1.474879 \t time: 0.3\n",
      "Epoch: 127 \tTraining Loss: 1.444070 \tValidation Loss: 1.473014 \t time: 0.3\n",
      "Epoch: 128 \tTraining Loss: 1.446543 \tValidation Loss: 1.469627 \t time: 0.3\n",
      "Validation loss decreased from 1.470583 to 1.469627. Model was saved\n",
      "Epoch: 129 \tTraining Loss: 1.441819 \tValidation Loss: 1.469967 \t time: 0.3\n",
      "Epoch: 130 \tTraining Loss: 1.442315 \tValidation Loss: 1.469852 \t time: 0.3\n",
      "Epoch: 131 \tTraining Loss: 1.443194 \tValidation Loss: 1.472048 \t time: 0.3\n",
      "Epoch: 132 \tTraining Loss: 1.437973 \tValidation Loss: 1.468473 \t time: 0.3\n",
      "Validation loss decreased from 1.469627 to 1.468473. Model was saved\n",
      "Epoch: 133 \tTraining Loss: 1.438271 \tValidation Loss: 1.464218 \t time: 0.3\n",
      "Validation loss decreased from 1.468473 to 1.464218. Model was saved\n",
      "Epoch: 134 \tTraining Loss: 1.435982 \tValidation Loss: 1.463722 \t time: 0.3\n",
      "Validation loss decreased from 1.464218 to 1.463722. Model was saved\n",
      "Epoch: 135 \tTraining Loss: 1.433134 \tValidation Loss: 1.462349 \t time: 0.3\n",
      "Validation loss decreased from 1.463722 to 1.462349. Model was saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 136 \tTraining Loss: 1.435920 \tValidation Loss: 1.459058 \t time: 0.3\n",
      "Validation loss decreased from 1.462349 to 1.459058. Model was saved\n",
      "Epoch: 137 \tTraining Loss: 1.434339 \tValidation Loss: 1.460496 \t time: 0.3\n",
      "Epoch: 138 \tTraining Loss: 1.432657 \tValidation Loss: 1.461830 \t time: 0.3\n",
      "Epoch: 139 \tTraining Loss: 1.427479 \tValidation Loss: 1.463292 \t time: 0.4\n",
      "Epoch: 140 \tTraining Loss: 1.431768 \tValidation Loss: 1.464918 \t time: 0.3\n",
      "Epoch: 141 \tTraining Loss: 1.426832 \tValidation Loss: 1.465833 \t time: 0.3\n",
      "Epoch: 142 \tTraining Loss: 1.426358 \tValidation Loss: 1.463760 \t time: 0.3\n",
      "Epoch: 143 \tTraining Loss: 1.430139 \tValidation Loss: 1.458084 \t time: 0.3\n",
      "Validation loss decreased from 1.459058 to 1.458084. Model was saved\n",
      "Epoch: 144 \tTraining Loss: 1.425454 \tValidation Loss: 1.453243 \t time: 0.3\n",
      "Validation loss decreased from 1.458084 to 1.453243. Model was saved\n",
      "Epoch: 145 \tTraining Loss: 1.427366 \tValidation Loss: 1.451579 \t time: 0.3\n",
      "Validation loss decreased from 1.453243 to 1.451579. Model was saved\n",
      "Epoch: 146 \tTraining Loss: 1.424790 \tValidation Loss: 1.457696 \t time: 0.3\n",
      "Epoch: 147 \tTraining Loss: 1.421180 \tValidation Loss: 1.460979 \t time: 0.3\n",
      "Epoch: 148 \tTraining Loss: 1.422388 \tValidation Loss: 1.456182 \t time: 0.3\n",
      "Epoch: 149 \tTraining Loss: 1.418694 \tValidation Loss: 1.455463 \t time: 0.3\n",
      "Epoch: 150 \tTraining Loss: 1.424060 \tValidation Loss: 1.459269 \t time: 0.3\n",
      "Epoch: 151 \tTraining Loss: 1.417160 \tValidation Loss: 1.459313 \t time: 0.3\n",
      "Epoch: 152 \tTraining Loss: 1.418679 \tValidation Loss: 1.455612 \t time: 0.3\n",
      "Epoch: 153 \tTraining Loss: 1.415557 \tValidation Loss: 1.453348 \t time: 0.3\n",
      "Epoch: 154 \tTraining Loss: 1.415814 \tValidation Loss: 1.454496 \t time: 0.3\n",
      "Epoch: 155 \tTraining Loss: 1.411790 \tValidation Loss: 1.455371 \t time: 0.3\n",
      "Epoch: 156 \tTraining Loss: 1.411384 \tValidation Loss: 1.451732 \t time: 0.3\n",
      "Epoch: 157 \tTraining Loss: 1.413806 \tValidation Loss: 1.445682 \t time: 0.3\n",
      "Validation loss decreased from 1.451579 to 1.445682. Model was saved\n",
      "Epoch: 158 \tTraining Loss: 1.411256 \tValidation Loss: 1.444391 \t time: 0.3\n",
      "Validation loss decreased from 1.445682 to 1.444391. Model was saved\n",
      "Epoch: 159 \tTraining Loss: 1.410697 \tValidation Loss: 1.443713 \t time: 0.3\n",
      "Validation loss decreased from 1.444391 to 1.443713. Model was saved\n",
      "Epoch: 160 \tTraining Loss: 1.405618 \tValidation Loss: 1.439337 \t time: 0.3\n",
      "Validation loss decreased from 1.443713 to 1.439337. Model was saved\n",
      "Epoch: 161 \tTraining Loss: 1.407457 \tValidation Loss: 1.435690 \t time: 0.2\n",
      "Validation loss decreased from 1.439337 to 1.435690. Model was saved\n",
      "Epoch: 162 \tTraining Loss: 1.405447 \tValidation Loss: 1.434344 \t time: 0.3\n",
      "Validation loss decreased from 1.435690 to 1.434344. Model was saved\n",
      "Epoch: 163 \tTraining Loss: 1.406188 \tValidation Loss: 1.436633 \t time: 0.3\n",
      "Epoch: 164 \tTraining Loss: 1.405186 \tValidation Loss: 1.441138 \t time: 0.3\n",
      "Epoch: 165 \tTraining Loss: 1.402214 \tValidation Loss: 1.440354 \t time: 0.3\n",
      "Epoch: 166 \tTraining Loss: 1.406462 \tValidation Loss: 1.438398 \t time: 0.3\n",
      "Epoch: 167 \tTraining Loss: 1.401009 \tValidation Loss: 1.438848 \t time: 0.3\n",
      "Epoch: 168 \tTraining Loss: 1.399910 \tValidation Loss: 1.440113 \t time: 0.3\n",
      "Epoch: 169 \tTraining Loss: 1.398674 \tValidation Loss: 1.440031 \t time: 0.3\n",
      "Epoch: 170 \tTraining Loss: 1.394138 \tValidation Loss: 1.435417 \t time: 0.3\n",
      "Epoch: 171 \tTraining Loss: 1.394114 \tValidation Loss: 1.430542 \t time: 0.3\n",
      "Validation loss decreased from 1.434344 to 1.430542. Model was saved\n",
      "Epoch: 172 \tTraining Loss: 1.394163 \tValidation Loss: 1.427726 \t time: 0.3\n",
      "Validation loss decreased from 1.430542 to 1.427726. Model was saved\n",
      "Epoch: 173 \tTraining Loss: 1.395164 \tValidation Loss: 1.430936 \t time: 0.3\n",
      "Epoch: 174 \tTraining Loss: 1.393772 \tValidation Loss: 1.432252 \t time: 0.3\n",
      "Epoch: 175 \tTraining Loss: 1.391600 \tValidation Loss: 1.425192 \t time: 0.3\n",
      "Validation loss decreased from 1.427726 to 1.425192. Model was saved\n",
      "Epoch: 176 \tTraining Loss: 1.389943 \tValidation Loss: 1.425799 \t time: 0.3\n",
      "Epoch: 177 \tTraining Loss: 1.393275 \tValidation Loss: 1.430201 \t time: 0.3\n",
      "Epoch: 178 \tTraining Loss: 1.387945 \tValidation Loss: 1.433767 \t time: 0.3\n",
      "Epoch: 179 \tTraining Loss: 1.388605 \tValidation Loss: 1.433368 \t time: 0.3\n",
      "Epoch: 180 \tTraining Loss: 1.386252 \tValidation Loss: 1.432027 \t time: 0.3\n",
      "Epoch: 181 \tTraining Loss: 1.383631 \tValidation Loss: 1.431728 \t time: 0.3\n",
      "Epoch: 182 \tTraining Loss: 1.385352 \tValidation Loss: 1.433204 \t time: 0.3\n",
      "Epoch: 183 \tTraining Loss: 1.384892 \tValidation Loss: 1.438278 \t time: 0.5\n",
      "Epoch: 184 \tTraining Loss: 1.382406 \tValidation Loss: 1.437811 \t time: 0.3\n",
      "Epoch: 185 \tTraining Loss: 1.381900 \tValidation Loss: 1.433109 \t time: 0.3\n",
      "Epoch: 186 \tTraining Loss: 1.383823 \tValidation Loss: 1.429036 \t time: 0.3\n",
      "Epoch: 187 \tTraining Loss: 1.378722 \tValidation Loss: 1.427902 \t time: 0.3\n",
      "Epoch: 188 \tTraining Loss: 1.379886 \tValidation Loss: 1.427147 \t time: 0.3\n",
      "Epoch: 189 \tTraining Loss: 1.376753 \tValidation Loss: 1.424885 \t time: 0.3\n",
      "Validation loss decreased from 1.425192 to 1.424885. Model was saved\n",
      "Epoch: 190 \tTraining Loss: 1.376180 \tValidation Loss: 1.422756 \t time: 0.3\n",
      "Validation loss decreased from 1.424885 to 1.422756. Model was saved\n",
      "Epoch: 191 \tTraining Loss: 1.377325 \tValidation Loss: 1.424771 \t time: 0.3\n",
      "Epoch: 192 \tTraining Loss: 1.378009 \tValidation Loss: 1.429946 \t time: 0.3\n",
      "Epoch: 193 \tTraining Loss: 1.372779 \tValidation Loss: 1.435530 \t time: 0.3\n",
      "Epoch: 194 \tTraining Loss: 1.378100 \tValidation Loss: 1.434951 \t time: 0.3\n",
      "Epoch: 195 \tTraining Loss: 1.371129 \tValidation Loss: 1.431704 \t time: 0.3\n",
      "Epoch: 196 \tTraining Loss: 1.377106 \tValidation Loss: 1.432170 \t time: 0.3\n",
      "Epoch: 197 \tTraining Loss: 1.372187 \tValidation Loss: 1.436936 \t time: 0.3\n",
      "Epoch: 198 \tTraining Loss: 1.372494 \tValidation Loss: 1.438612 \t time: 0.3\n",
      "Epoch: 199 \tTraining Loss: 1.375237 \tValidation Loss: 1.437831 \t time: 0.4\n",
      "Epoch: 200 \tTraining Loss: 1.371618 \tValidation Loss: 1.436206 \t time: 0.3\n",
      "Epoch: 201 \tTraining Loss: 1.369058 \tValidation Loss: 1.435860 \t time: 0.3\n",
      "Epoch: 202 \tTraining Loss: 1.368768 \tValidation Loss: 1.437778 \t time: 0.3\n",
      "Epoch: 203 \tTraining Loss: 1.369310 \tValidation Loss: 1.436363 \t time: 0.3\n",
      "Epoch: 204 \tTraining Loss: 1.366149 \tValidation Loss: 1.426252 \t time: 0.3\n",
      "Epoch: 205 \tTraining Loss: 1.365025 \tValidation Loss: 1.423670 \t time: 0.4\n",
      "Epoch: 206 \tTraining Loss: 1.367760 \tValidation Loss: 1.424714 \t time: 0.4\n",
      "Epoch: 207 \tTraining Loss: 1.364886 \tValidation Loss: 1.428724 \t time: 0.3\n",
      "Epoch: 208 \tTraining Loss: 1.364428 \tValidation Loss: 1.431299 \t time: 0.4\n",
      "Epoch: 209 \tTraining Loss: 1.363177 \tValidation Loss: 1.431535 \t time: 0.5\n",
      "Epoch: 210 \tTraining Loss: 1.363951 \tValidation Loss: 1.432685 \t time: 0.4\n",
      "Epoch: 211 \tTraining Loss: 1.362801 \tValidation Loss: 1.435287 \t time: 0.3\n",
      "Epoch: 212 \tTraining Loss: 1.362879 \tValidation Loss: 1.437297 \t time: 0.3\n",
      "Epoch: 213 \tTraining Loss: 1.356550 \tValidation Loss: 1.439443 \t time: 0.3\n",
      "Epoch: 214 \tTraining Loss: 1.359948 \tValidation Loss: 1.435090 \t time: 0.3\n",
      "Epoch: 215 \tTraining Loss: 1.360255 \tValidation Loss: 1.430136 \t time: 0.3\n",
      "Epoch: 216 \tTraining Loss: 1.357857 \tValidation Loss: 1.425947 \t time: 0.3\n",
      "Epoch: 217 \tTraining Loss: 1.361009 \tValidation Loss: 1.423546 \t time: 0.3\n",
      "Epoch: 218 \tTraining Loss: 1.357988 \tValidation Loss: 1.425960 \t time: 0.3\n",
      "Epoch: 219 \tTraining Loss: 1.358390 \tValidation Loss: 1.431326 \t time: 0.3\n",
      "Epoch: 220 \tTraining Loss: 1.358656 \tValidation Loss: 1.431231 \t time: 0.4\n",
      "Epoch: 221 \tTraining Loss: 1.354176 \tValidation Loss: 1.427849 \t time: 0.3\n",
      "Epoch: 222 \tTraining Loss: 1.356368 \tValidation Loss: 1.426749 \t time: 0.3\n",
      "Epoch: 223 \tTraining Loss: 1.351068 \tValidation Loss: 1.428196 \t time: 0.4\n",
      "Epoch: 224 \tTraining Loss: 1.351359 \tValidation Loss: 1.430258 \t time: 0.3\n",
      "Epoch: 225 \tTraining Loss: 1.354633 \tValidation Loss: 1.431134 \t time: 0.3\n",
      "Epoch: 226 \tTraining Loss: 1.353507 \tValidation Loss: 1.431540 \t time: 0.3\n",
      "Epoch: 227 \tTraining Loss: 1.351585 \tValidation Loss: 1.431252 \t time: 0.3\n",
      "Epoch: 228 \tTraining Loss: 1.348165 \tValidation Loss: 1.431157 \t time: 0.3\n",
      "Epoch: 229 \tTraining Loss: 1.345934 \tValidation Loss: 1.429758 \t time: 0.3\n",
      "Epoch: 230 \tTraining Loss: 1.344837 \tValidation Loss: 1.427215 \t time: 0.3\n",
      "Epoch: 231 \tTraining Loss: 1.342778 \tValidation Loss: 1.423407 \t time: 0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 232 \tTraining Loss: 1.344076 \tValidation Loss: 1.418771 \t time: 0.3\n",
      "Validation loss decreased from 1.422756 to 1.418771. Model was saved\n",
      "Epoch: 233 \tTraining Loss: 1.340498 \tValidation Loss: 1.413967 \t time: 0.3\n",
      "Validation loss decreased from 1.418771 to 1.413967. Model was saved\n",
      "Epoch: 234 \tTraining Loss: 1.341094 \tValidation Loss: 1.408319 \t time: 0.3\n",
      "Validation loss decreased from 1.413967 to 1.408319. Model was saved\n",
      "Epoch: 235 \tTraining Loss: 1.339131 \tValidation Loss: 1.400042 \t time: 0.3\n",
      "Validation loss decreased from 1.408319 to 1.400042. Model was saved\n",
      "Epoch: 236 \tTraining Loss: 1.333654 \tValidation Loss: 1.395380 \t time: 0.4\n",
      "Validation loss decreased from 1.400042 to 1.395380. Model was saved\n",
      "Epoch: 237 \tTraining Loss: 1.335183 \tValidation Loss: 1.395521 \t time: 0.3\n",
      "Epoch: 238 \tTraining Loss: 1.333640 \tValidation Loss: 1.397115 \t time: 0.3\n",
      "Epoch: 239 \tTraining Loss: 1.334949 \tValidation Loss: 1.397981 \t time: 0.3\n",
      "Epoch: 240 \tTraining Loss: 1.329552 \tValidation Loss: 1.398976 \t time: 0.3\n",
      "Epoch: 241 \tTraining Loss: 1.328643 \tValidation Loss: 1.398470 \t time: 0.3\n",
      "Epoch: 242 \tTraining Loss: 1.331045 \tValidation Loss: 1.397184 \t time: 0.4\n",
      "Epoch: 243 \tTraining Loss: 1.330071 \tValidation Loss: 1.396496 \t time: 0.3\n",
      "Epoch: 244 \tTraining Loss: 1.324610 \tValidation Loss: 1.394760 \t time: 0.3\n",
      "Validation loss decreased from 1.395380 to 1.394760. Model was saved\n",
      "Epoch: 245 \tTraining Loss: 1.327358 \tValidation Loss: 1.395477 \t time: 0.3\n",
      "Epoch: 246 \tTraining Loss: 1.324835 \tValidation Loss: 1.399257 \t time: 0.3\n",
      "Epoch: 247 \tTraining Loss: 1.324280 \tValidation Loss: 1.400561 \t time: 0.3\n",
      "Epoch: 248 \tTraining Loss: 1.319464 \tValidation Loss: 1.400972 \t time: 0.4\n",
      "Epoch: 249 \tTraining Loss: 1.321522 \tValidation Loss: 1.398601 \t time: 0.3\n",
      "Epoch: 250 \tTraining Loss: 1.323371 \tValidation Loss: 1.397995 \t time: 0.3\n",
      "Epoch: 251 \tTraining Loss: 1.318971 \tValidation Loss: 1.396085 \t time: 0.4\n",
      "Epoch: 252 \tTraining Loss: 1.320757 \tValidation Loss: 1.393163 \t time: 0.3\n",
      "Validation loss decreased from 1.394760 to 1.393163. Model was saved\n",
      "Epoch: 253 \tTraining Loss: 1.324123 \tValidation Loss: 1.385401 \t time: 0.3\n",
      "Validation loss decreased from 1.393163 to 1.385401. Model was saved\n",
      "Epoch: 254 \tTraining Loss: 1.322631 \tValidation Loss: 1.378531 \t time: 0.3\n",
      "Validation loss decreased from 1.385401 to 1.378531. Model was saved\n",
      "Epoch: 255 \tTraining Loss: 1.319758 \tValidation Loss: 1.376815 \t time: 0.3\n",
      "Validation loss decreased from 1.378531 to 1.376815. Model was saved\n",
      "Epoch: 256 \tTraining Loss: 1.316401 \tValidation Loss: 1.380714 \t time: 0.3\n",
      "Epoch: 257 \tTraining Loss: 1.318533 \tValidation Loss: 1.389196 \t time: 0.3\n",
      "Epoch: 258 \tTraining Loss: 1.315718 \tValidation Loss: 1.394197 \t time: 0.3\n",
      "Epoch: 259 \tTraining Loss: 1.319373 \tValidation Loss: 1.400973 \t time: 0.3\n",
      "Epoch: 260 \tTraining Loss: 1.317382 \tValidation Loss: 1.406564 \t time: 0.3\n",
      "Epoch: 261 \tTraining Loss: 1.314729 \tValidation Loss: 1.407482 \t time: 0.4\n",
      "Epoch: 262 \tTraining Loss: 1.315614 \tValidation Loss: 1.414434 \t time: 0.3\n",
      "Epoch: 263 \tTraining Loss: 1.314593 \tValidation Loss: 1.413393 \t time: 0.3\n",
      "Epoch: 264 \tTraining Loss: 1.314500 \tValidation Loss: 1.413089 \t time: 0.3\n",
      "Epoch: 265 \tTraining Loss: 1.314480 \tValidation Loss: 1.408923 \t time: 0.3\n",
      "Epoch: 266 \tTraining Loss: 1.311859 \tValidation Loss: 1.400098 \t time: 0.3\n",
      "Epoch: 267 \tTraining Loss: 1.310422 \tValidation Loss: 1.393122 \t time: 0.2\n",
      "Epoch: 268 \tTraining Loss: 1.313780 \tValidation Loss: 1.391067 \t time: 0.3\n",
      "Epoch: 269 \tTraining Loss: 1.315409 \tValidation Loss: 1.392205 \t time: 0.3\n",
      "Epoch: 270 \tTraining Loss: 1.310559 \tValidation Loss: 1.397236 \t time: 0.3\n",
      "Epoch: 271 \tTraining Loss: 1.312202 \tValidation Loss: 1.400775 \t time: 0.3\n",
      "Epoch: 272 \tTraining Loss: 1.311708 \tValidation Loss: 1.400827 \t time: 0.3\n",
      "Epoch: 273 \tTraining Loss: 1.310499 \tValidation Loss: 1.401639 \t time: 0.3\n",
      "Epoch: 274 \tTraining Loss: 1.309316 \tValidation Loss: 1.400373 \t time: 0.3\n",
      "Epoch: 275 \tTraining Loss: 1.310230 \tValidation Loss: 1.397996 \t time: 0.3\n",
      "Epoch: 276 \tTraining Loss: 1.309115 \tValidation Loss: 1.404311 \t time: 0.3\n",
      "Epoch: 277 \tTraining Loss: 1.308023 \tValidation Loss: 1.411270 \t time: 0.3\n",
      "Epoch: 278 \tTraining Loss: 1.305351 \tValidation Loss: 1.416061 \t time: 0.3\n",
      "Epoch: 279 \tTraining Loss: 1.309156 \tValidation Loss: 1.415762 \t time: 0.3\n",
      "Epoch: 280 \tTraining Loss: 1.304356 \tValidation Loss: 1.410058 \t time: 0.3\n",
      "Epoch: 281 \tTraining Loss: 1.306651 \tValidation Loss: 1.397796 \t time: 0.3\n",
      "Epoch: 282 \tTraining Loss: 1.303293 \tValidation Loss: 1.396289 \t time: 0.3\n",
      "Epoch: 283 \tTraining Loss: 1.302864 \tValidation Loss: 1.396321 \t time: 0.3\n",
      "Epoch: 284 \tTraining Loss: 1.304686 \tValidation Loss: 1.397370 \t time: 0.4\n",
      "Epoch: 285 \tTraining Loss: 1.305263 \tValidation Loss: 1.398394 \t time: 0.4\n",
      "Epoch: 286 \tTraining Loss: 1.305412 \tValidation Loss: 1.397771 \t time: 0.4\n",
      "Epoch: 287 \tTraining Loss: 1.300919 \tValidation Loss: 1.396553 \t time: 0.3\n",
      "Epoch: 288 \tTraining Loss: 1.301590 \tValidation Loss: 1.395609 \t time: 0.3\n",
      "Epoch: 289 \tTraining Loss: 1.301088 \tValidation Loss: 1.402205 \t time: 0.3\n",
      "Epoch: 290 \tTraining Loss: 1.306001 \tValidation Loss: 1.410998 \t time: 0.3\n",
      "Epoch: 291 \tTraining Loss: 1.302677 \tValidation Loss: 1.416628 \t time: 0.4\n",
      "Epoch: 292 \tTraining Loss: 1.300608 \tValidation Loss: 1.416355 \t time: 0.3\n",
      "Epoch: 293 \tTraining Loss: 1.302647 \tValidation Loss: 1.412936 \t time: 0.3\n",
      "Epoch: 294 \tTraining Loss: 1.301636 \tValidation Loss: 1.410401 \t time: 0.3\n",
      "Epoch: 295 \tTraining Loss: 1.299470 \tValidation Loss: 1.411602 \t time: 0.3\n",
      "Epoch: 296 \tTraining Loss: 1.300962 \tValidation Loss: 1.412094 \t time: 0.3\n",
      "Epoch: 297 \tTraining Loss: 1.298748 \tValidation Loss: 1.411080 \t time: 0.3\n",
      "Epoch: 298 \tTraining Loss: 1.300144 \tValidation Loss: 1.407060 \t time: 0.3\n",
      "Epoch: 299 \tTraining Loss: 1.299482 \tValidation Loss: 1.403683 \t time: 0.3\n",
      "Epoch: 300 \tTraining Loss: 1.297626 \tValidation Loss: 1.401547 \t time: 0.3\n",
      "Epoch: 301 \tTraining Loss: 1.303094 \tValidation Loss: 1.401084 \t time: 0.3\n",
      "Epoch: 302 \tTraining Loss: 1.299843 \tValidation Loss: 1.401689 \t time: 0.3\n",
      "Epoch: 303 \tTraining Loss: 1.296000 \tValidation Loss: 1.400727 \t time: 0.3\n",
      "Epoch: 304 \tTraining Loss: 1.296043 \tValidation Loss: 1.401783 \t time: 0.3\n",
      "Epoch: 305 \tTraining Loss: 1.300304 \tValidation Loss: 1.402262 \t time: 0.3\n",
      "Epoch: 306 \tTraining Loss: 1.294375 \tValidation Loss: 1.403390 \t time: 0.3\n",
      "Epoch: 307 \tTraining Loss: 1.297472 \tValidation Loss: 1.402154 \t time: 0.3\n",
      "Epoch: 308 \tTraining Loss: 1.291758 \tValidation Loss: 1.399007 \t time: 0.3\n",
      "Epoch: 309 \tTraining Loss: 1.296934 \tValidation Loss: 1.399347 \t time: 0.3\n",
      "Epoch: 310 \tTraining Loss: 1.292666 \tValidation Loss: 1.398485 \t time: 0.3\n",
      "Epoch: 311 \tTraining Loss: 1.294828 \tValidation Loss: 1.396001 \t time: 0.3\n",
      "Epoch: 312 \tTraining Loss: 1.296786 \tValidation Loss: 1.395782 \t time: 0.3\n",
      "Epoch: 313 \tTraining Loss: 1.294611 \tValidation Loss: 1.400285 \t time: 0.3\n",
      "Epoch: 314 \tTraining Loss: 1.294197 \tValidation Loss: 1.407049 \t time: 0.3\n",
      "Epoch: 315 \tTraining Loss: 1.294295 \tValidation Loss: 1.410005 \t time: 0.3\n",
      "Epoch: 316 \tTraining Loss: 1.292849 \tValidation Loss: 1.407222 \t time: 0.3\n",
      "Epoch: 317 \tTraining Loss: 1.295124 \tValidation Loss: 1.400863 \t time: 0.3\n",
      "Epoch: 318 \tTraining Loss: 1.294520 \tValidation Loss: 1.398394 \t time: 0.3\n",
      "Epoch: 319 \tTraining Loss: 1.292099 \tValidation Loss: 1.397323 \t time: 0.3\n",
      "Epoch: 320 \tTraining Loss: 1.293311 \tValidation Loss: 1.398274 \t time: 0.3\n",
      "Epoch: 321 \tTraining Loss: 1.293195 \tValidation Loss: 1.400326 \t time: 0.4\n",
      "Epoch: 322 \tTraining Loss: 1.296090 \tValidation Loss: 1.404835 \t time: 0.3\n",
      "Epoch: 323 \tTraining Loss: 1.291370 \tValidation Loss: 1.404485 \t time: 0.3\n",
      "Epoch: 324 \tTraining Loss: 1.293184 \tValidation Loss: 1.401609 \t time: 0.3\n",
      "Epoch: 325 \tTraining Loss: 1.294001 \tValidation Loss: 1.401459 \t time: 0.3\n",
      "Epoch: 326 \tTraining Loss: 1.289986 \tValidation Loss: 1.403763 \t time: 0.3\n",
      "Epoch: 327 \tTraining Loss: 1.293193 \tValidation Loss: 1.406540 \t time: 0.3\n",
      "Epoch: 328 \tTraining Loss: 1.293682 \tValidation Loss: 1.408566 \t time: 0.3\n",
      "Epoch: 329 \tTraining Loss: 1.289917 \tValidation Loss: 1.406415 \t time: 0.3\n",
      "Epoch: 330 \tTraining Loss: 1.291095 \tValidation Loss: 1.401455 \t time: 0.3\n",
      "Epoch: 331 \tTraining Loss: 1.289821 \tValidation Loss: 1.397552 \t time: 0.3\n",
      "Epoch: 332 \tTraining Loss: 1.286897 \tValidation Loss: 1.398007 \t time: 0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 333 \tTraining Loss: 1.288602 \tValidation Loss: 1.402283 \t time: 0.3\n",
      "Epoch: 334 \tTraining Loss: 1.289381 \tValidation Loss: 1.405836 \t time: 0.3\n",
      "Epoch: 335 \tTraining Loss: 1.290182 \tValidation Loss: 1.408099 \t time: 0.3\n",
      "Epoch: 336 \tTraining Loss: 1.288280 \tValidation Loss: 1.408612 \t time: 0.3\n",
      "Epoch: 337 \tTraining Loss: 1.287483 \tValidation Loss: 1.407927 \t time: 0.3\n",
      "Epoch: 338 \tTraining Loss: 1.288148 \tValidation Loss: 1.408003 \t time: 0.3\n",
      "Epoch: 339 \tTraining Loss: 1.285565 \tValidation Loss: 1.408566 \t time: 0.3\n",
      "Epoch: 340 \tTraining Loss: 1.291474 \tValidation Loss: 1.405564 \t time: 0.3\n",
      "Epoch: 341 \tTraining Loss: 1.284127 \tValidation Loss: 1.403214 \t time: 0.3\n",
      "Epoch: 342 \tTraining Loss: 1.287227 \tValidation Loss: 1.400912 \t time: 0.3\n",
      "Epoch: 343 \tTraining Loss: 1.285817 \tValidation Loss: 1.398426 \t time: 0.3\n",
      "Epoch: 344 \tTraining Loss: 1.285474 \tValidation Loss: 1.396254 \t time: 0.2\n",
      "Epoch: 345 \tTraining Loss: 1.286574 \tValidation Loss: 1.398473 \t time: 0.3\n",
      "Epoch: 346 \tTraining Loss: 1.287717 \tValidation Loss: 1.403062 \t time: 0.3\n",
      "Epoch: 347 \tTraining Loss: 1.288914 \tValidation Loss: 1.405941 \t time: 0.3\n",
      "Epoch: 348 \tTraining Loss: 1.285723 \tValidation Loss: 1.408131 \t time: 0.3\n",
      "Epoch: 349 \tTraining Loss: 1.284373 \tValidation Loss: 1.408646 \t time: 0.3\n",
      "Epoch: 350 \tTraining Loss: 1.287917 \tValidation Loss: 1.410225 \t time: 0.3\n",
      "Epoch: 351 \tTraining Loss: 1.286070 \tValidation Loss: 1.410327 \t time: 0.3\n",
      "Epoch: 352 \tTraining Loss: 1.284940 \tValidation Loss: 1.409251 \t time: 0.4\n",
      "Epoch: 353 \tTraining Loss: 1.285055 \tValidation Loss: 1.407547 \t time: 0.3\n",
      "Epoch: 354 \tTraining Loss: 1.285445 \tValidation Loss: 1.404445 \t time: 0.3\n",
      "Epoch: 355 \tTraining Loss: 1.284593 \tValidation Loss: 1.403111 \t time: 0.3\n",
      "Epoch: 356 \tTraining Loss: 1.280176 \tValidation Loss: 1.405333 \t time: 0.3\n",
      "Epoch: 357 \tTraining Loss: 1.284120 \tValidation Loss: 1.403299 \t time: 0.3\n",
      "Epoch: 358 \tTraining Loss: 1.284750 \tValidation Loss: 1.404588 \t time: 0.3\n",
      "Epoch: 359 \tTraining Loss: 1.286343 \tValidation Loss: 1.408858 \t time: 0.3\n",
      "Epoch: 360 \tTraining Loss: 1.282242 \tValidation Loss: 1.410110 \t time: 0.3\n",
      "Epoch: 361 \tTraining Loss: 1.283762 \tValidation Loss: 1.409509 \t time: 0.3\n",
      "Epoch: 362 \tTraining Loss: 1.283206 \tValidation Loss: 1.409078 \t time: 0.3\n",
      "Epoch: 363 \tTraining Loss: 1.281735 \tValidation Loss: 1.410545 \t time: 0.3\n",
      "Epoch: 364 \tTraining Loss: 1.283631 \tValidation Loss: 1.409387 \t time: 0.3\n",
      "Epoch: 365 \tTraining Loss: 1.280300 \tValidation Loss: 1.405810 \t time: 0.3\n",
      "Epoch: 366 \tTraining Loss: 1.281808 \tValidation Loss: 1.406624 \t time: 0.3\n",
      "Epoch: 367 \tTraining Loss: 1.282542 \tValidation Loss: 1.408433 \t time: 0.3\n",
      "Epoch: 368 \tTraining Loss: 1.280531 \tValidation Loss: 1.412394 \t time: 0.3\n",
      "Epoch: 369 \tTraining Loss: 1.283808 \tValidation Loss: 1.413877 \t time: 0.3\n",
      "Epoch: 370 \tTraining Loss: 1.281151 \tValidation Loss: 1.414343 \t time: 0.3\n",
      "Epoch: 371 \tTraining Loss: 1.282056 \tValidation Loss: 1.411907 \t time: 0.3\n",
      "Epoch: 372 \tTraining Loss: 1.278372 \tValidation Loss: 1.408327 \t time: 0.3\n",
      "Epoch: 373 \tTraining Loss: 1.283733 \tValidation Loss: 1.406088 \t time: 0.3\n",
      "Epoch: 374 \tTraining Loss: 1.283254 \tValidation Loss: 1.403456 \t time: 0.3\n",
      "Epoch: 375 \tTraining Loss: 1.282216 \tValidation Loss: 1.399628 \t time: 0.3\n",
      "Epoch: 376 \tTraining Loss: 1.279764 \tValidation Loss: 1.400687 \t time: 0.3\n",
      "Epoch: 377 \tTraining Loss: 1.281449 \tValidation Loss: 1.402229 \t time: 0.3\n",
      "Epoch: 378 \tTraining Loss: 1.281364 \tValidation Loss: 1.404300 \t time: 0.3\n",
      "Epoch: 379 \tTraining Loss: 1.280044 \tValidation Loss: 1.409773 \t time: 0.3\n",
      "Epoch: 380 \tTraining Loss: 1.278153 \tValidation Loss: 1.411307 \t time: 0.3\n",
      "Epoch: 381 \tTraining Loss: 1.277837 \tValidation Loss: 1.409103 \t time: 0.3\n",
      "Epoch: 382 \tTraining Loss: 1.278981 \tValidation Loss: 1.408547 \t time: 0.3\n",
      "Epoch: 383 \tTraining Loss: 1.277505 \tValidation Loss: 1.409473 \t time: 0.3\n",
      "Epoch: 384 \tTraining Loss: 1.278932 \tValidation Loss: 1.413940 \t time: 0.3\n",
      "Epoch: 385 \tTraining Loss: 1.279850 \tValidation Loss: 1.416611 \t time: 0.3\n",
      "Epoch: 386 \tTraining Loss: 1.278835 \tValidation Loss: 1.410410 \t time: 0.3\n",
      "Epoch: 387 \tTraining Loss: 1.278422 \tValidation Loss: 1.406621 \t time: 0.3\n",
      "Epoch: 388 \tTraining Loss: 1.277331 \tValidation Loss: 1.404212 \t time: 0.3\n",
      "Epoch: 389 \tTraining Loss: 1.277246 \tValidation Loss: 1.401154 \t time: 0.3\n",
      "Epoch: 390 \tTraining Loss: 1.279540 \tValidation Loss: 1.400822 \t time: 0.3\n",
      "Epoch: 391 \tTraining Loss: 1.276537 \tValidation Loss: 1.402325 \t time: 0.3\n",
      "Epoch: 392 \tTraining Loss: 1.274010 \tValidation Loss: 1.403478 \t time: 0.3\n",
      "Epoch: 393 \tTraining Loss: 1.277900 \tValidation Loss: 1.402728 \t time: 0.3\n",
      "Epoch: 394 \tTraining Loss: 1.276272 \tValidation Loss: 1.400046 \t time: 0.4\n",
      "Epoch: 395 \tTraining Loss: 1.280026 \tValidation Loss: 1.401093 \t time: 0.3\n",
      "Epoch: 396 \tTraining Loss: 1.276115 \tValidation Loss: 1.404961 \t time: 0.3\n",
      "Epoch: 397 \tTraining Loss: 1.277133 \tValidation Loss: 1.407219 \t time: 0.3\n",
      "Epoch: 398 \tTraining Loss: 1.275581 \tValidation Loss: 1.411202 \t time: 0.3\n",
      "Epoch: 399 \tTraining Loss: 1.278202 \tValidation Loss: 1.412287 \t time: 0.3\n",
      "Epoch: 400 \tTraining Loss: 1.276007 \tValidation Loss: 1.409841 \t time: 0.3\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf \n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "\n",
    "#             data = data.type((torch.FloatTensor))\n",
    "\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update accumulated training loss\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "            \n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
    "            \n",
    "#             data = data.type((torch.FloatTensor))\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update accumulated validation loss \n",
    "            valid_loss += loss.item()*data.size(0)\n",
    "            \n",
    "\n",
    "        train_loss = train_loss/len(loaders['train'].dataset)\n",
    "        valid_loss = valid_loss/len(loaders['valid'].dataset)\n",
    "        \n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\t time: {:.1f}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss,\n",
    "            time.time() - start\n",
    "            ))\n",
    "        \n",
    "        ## TODO: save the model if validation loss has decreased\n",
    "        if valid_loss < valid_loss_min:\n",
    "            print('Validation loss decreased from {:.6f} to {:.6f}. Model was saved'.format(\n",
    "                valid_loss_min,\n",
    "                valid_loss\n",
    "            ))\n",
    "            \n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            valid_loss_min = valid_loss\n",
    "    \n",
    "    # return trained model\n",
    "    return model\n",
    "\n",
    "\n",
    "# train the model\n",
    "model = train(400, loaders, model, optimizer, \n",
    "                      criterion, use_cuda, 'model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch: 215 \tTraining Loss: 1.329372 \tValidation Loss: 1.419149 \t time: 0.4\n",
    "Epoch: 216 \tTraining Loss: 1.328865 \tValidation Loss: 1.416587 \t time: 0.4\n",
    "Validation loss decreased from 1.417114 to 1.416587. Model was saved\n",
    "Kaggle score = 0.54438 (0.0006 IMPROVEMENT)\n",
    "\n",
    "Epoch: 216 \tTraining Loss: 1.326797 \tValidation Loss: 1.396590 \t time: 0.7\n",
    "Epoch: 217 \tTraining Loss: 1.326093 \tValidation Loss: 1.396514 \t time: 0.7\n",
    "Epoch: 218 \tTraining Loss: 1.325473 \tValidation Loss: 1.394434 \t time: 0.6\n",
    "Validation loss decreased from 1.396026 to 1.394434. Model was saved\n",
    "Kaggle score = 55245 (0.011 IMPROVEMENT)\n",
    "\n",
    "Epoch: 364 \tTraining Loss: 1.229904 \tValidation Loss: 1.381198 \t time: 0.3\n",
    "Validation loss decreased from 1.384373 to 1.381198. Model was saved\n",
    "Epoch: 365 \tTraining Loss: 1.231026 \tValidation Loss: 1.394960 \t time: 0.3\n",
    "Epoch: 366 \tTraining Loss: 1.231521 \tValidation Loss: 1.379200 \t time: 0.3\n",
    "Validation loss decreased from 1.381198 to 1.379200. Model was saved\n",
    "Epoch: 367 \tTraining Loss: 1.232151 \tValidation Loss: 1.395807 \t time: 0.3\n",
    "Epoch: 368 \tTraining Loss: 1.230561 \tValidation Loss: 1.391226 \t time: 0.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.636884\n",
      "\n",
      "\n",
      "Test Accuracy: 40% ( 6/15)\n"
     ]
    }
   ],
   "source": [
    "def test(loaders, model, criterion, use_cuda):\n",
    "\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "        \n",
    "#         data = data.type((torch.FloatTensor))\n",
    "        # move to GPU\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average test loss \n",
    "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "        # convert output probabilities to predicted class\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        # compare predictions to true label\n",
    "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "        total += data.size(0)\n",
    "            \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "        100. * correct / total, correct, total))\n",
    "\n",
    "# load the model that got the best validation accuracy\n",
    "model.load_state_dict(torch.load('model.pt'))\n",
    "# call test function    \n",
    "test(loaders, model, criterion, use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 5, 2, 0, 0, 1, 5, 0, 3, 5, 1, 5, 2, 5])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i,l = next(iter(loaders['test']))\n",
    "output = model(i)\n",
    "result = output.data.max(1, keepdim=True)[1].numpy()\n",
    "result[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4., 0., 2., 0., 3., 0., 1., 0., 0., 5.]),\n",
       " array([0. , 0.5, 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = result[:,0]\n",
    "plt.hist(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3., 0., 2., 0., 3., 0., 2., 0., 2., 3.]),\n",
       " array([0. , 0.5, 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(l.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huohsien/anaconda2/envs/ai/lib/python3.7/site-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 5,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 5,\n",
       " 3,\n",
       " 0,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 0,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 5,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class = model(torch.tensor(features_test.values).type((torch.FloatTensor)))\n",
    "# We will look at the predicted prices to ensure we have something sensible.\n",
    "predicted_class = predicted_class.data.cpu().max(1, keepdim=True)[1].numpy()[:,0]\n",
    "list(predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = pd.DataFrame({\"ID\":id, \"class\":predicted_class})\n",
    "solution.to_csv(\"pokemon_sol.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
