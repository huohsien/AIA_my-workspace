{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_class = 6\n",
    "# how many data per batch to load\n",
    "batch_size = 10000\n",
    "# data split ratio\n",
    "train_ratio = 0.95\n",
    "test_ratio = 0.1\n",
    "\n",
    "n_epochs = 1000\n",
    "\n",
    "lr=0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "Counter({'class_0': 1611, 'class_2': 1478, 'class_5': 1411, 'class_1': 920, 'class_3': 889, 'class_4': 851})\n",
      "After:\n",
      "Counter({'class_0': 1611, 0: 1611, 1: 1611, 2: 1611, 3: 1611, 4: 1611, 5: 1611, 'class_2': 1478, 'class_5': 1411, 'class_1': 920, 'class_3': 889, 'class_4': 851})\n"
     ]
    }
   ],
   "source": [
    "# make number of data for each class equal\n",
    "#\n",
    "from collections import Counter\n",
    "\n",
    "class_counter = Counter()\n",
    "\n",
    "class_names =['class_' + str(i) for i in range(number_of_class)]\n",
    "for class_name in class_names:\n",
    "    class_counter[class_name] = 0\n",
    "for i in train['class']:\n",
    "    class_counter['class_' + str(i)] += 1\n",
    "\n",
    "print('Before:')\n",
    "print(class_counter)\n",
    "\n",
    "max_count = -np.Inf\n",
    "for i in range(number_of_class):\n",
    "    if class_counter['class_' + str(i)] > max_count:\n",
    "        max_count = class_counter['class_' + str(i)]\n",
    "\n",
    "train_classified = [train[train['class'] == i] for i in range(number_of_class)]\n",
    "\n",
    "for i in range(number_of_class):\n",
    "    num_need_resample = max_count - class_counter['class_' + str(i)]\n",
    "    num_resample_batch = num_need_resample // class_counter['class_' + str(i)]\n",
    "    num_resample_leftover = num_need_resample % class_counter['class_' + str(i)]\n",
    "    for j in range(num_resample_batch):\n",
    "        add_df = train_classified[i]\n",
    "        train =  pd.concat([train, add_df[0:dist_class[i][1]]], ignore_index=True)\n",
    "        train =  train.append(df_to_be_added)\n",
    "        \n",
    "    df_to_be_added = train_classified[i][:num_resample_leftover]\n",
    "    train =  train.append(df_to_be_added)\n",
    "\n",
    "class_names =[i for i in range(number_of_class)]\n",
    "for class_name in class_names:\n",
    "    class_counter[class_name] = 0\n",
    "for i in train['class']:\n",
    "    class_counter[i] += 1\n",
    "\n",
    "print('After:')\n",
    "print(class_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat((train.loc[:,'appearedTimeOfDay':'cooc_151'],\n",
    "                      test.loc[:,'appearedTimeOfDay':'cooc_151']))\n",
    "id = test['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data.applymap(lambda x: 1.0 if x == True else x)\n",
    "all_data = all_data.applymap(lambda x: 0.0 if x == False else x)\n",
    "all_data = pd.get_dummies(all_data)\n",
    "# numeric_feats = df.dtypes[df.dtypes != \"object\"].index\n",
    "# df = df[numeric_feats]\n",
    "apearedHour = all_data['appearedHour']\n",
    "appearedMinute = all_data['appearedMinute']\n",
    "appearedTimeDayCycle = apearedHour * 60 + appearedMinute\n",
    "appearedTimeDayCycle = np.sin(appearedTimeDayCycle / (24 * 60) * 2 * np.pi)\n",
    "# print('appearedTimeDayCycle= ',appearedTimeDayCycle)\n",
    "all_data = all_data.drop(['appearedHour'], axis=1)\n",
    "all_data = all_data.drop(['appearedMinute'], axis=1)\n",
    "all_data['appearedTimeDayCycle'] = appearedTimeDayCycle\n",
    "\n",
    "# df = df.drop(['temperature'], axis=1)\n",
    "# df = df.drop(['windSpeed'], axis=1)\n",
    "# df = df.drop(['pressure'], axis=1)\n",
    "# df = df.drop(['gymIn100m'], axis=1)\n",
    "# df = df.drop(['gymIn250m'], axis=1)\n",
    "# df = df.drop(['gymIn500m'], axis=1)\n",
    "# df = df.drop(['gymIn1000m'], axis=1)\n",
    "# df = df.drop(['gymIn2500m'], axis=1)\n",
    "# df = df.drop(['gymIn5000m'], axis=1)\n",
    "# df = df.drop(['rural'], axis=1)\n",
    "# df = df.drop(['midurban'], axis=1)\n",
    "# df = df.drop(['suburban'], axis=1)\n",
    "# df = df.drop(['urban'], axis=1)\n",
    "# df = df.drop(['pokestopIn100m'], axis=1)\n",
    "# df = df.drop(['pokestopIn250m'], axis=1)\n",
    "# df = df.drop(['pokestopIn500m'], axis=1)\n",
    "# df = df.drop(['pokestopIn1000m'], axis=1)\n",
    "# df = df.drop(['pokestopIn2500m'], axis=1)\n",
    "# df = df.drop(['pokestopIn5000m'], axis=1)\n",
    "# df = df.drop(['terrainType'], axis=1)\n",
    "# df = df.drop(['closeToWater'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(all_data.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's look at the data :\n",
    "# matplotlib.rcParams['figure.figsize'] = (6.0, 6.0)\n",
    "\n",
    "# data = pd.DataFrame({\"x\":df['population_density'], \"y\":targets})\n",
    "\n",
    "# data.plot(x = \"x\", y = \"y\",kind = \"scatter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normailize to 0-1\n",
    "for k in all_data.columns.values:\n",
    "    if (all_data[k].max() - all_data[k].min()) > 0:\n",
    "        all_data[k] = (all_data[k] - all_data[k].min())/(all_data[k].max() - all_data[k].min())\n",
    "    else:\n",
    "        all_data[k] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9666 entries, 0 to 1088\n",
      "Columns: 297 entries, terrainType to appearedTimeDayCycle\n",
      "dtypes: float64(272), int64(25)\n",
      "memory usage: 22.0 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9666, 297)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = all_data[:train.shape[0]]\n",
    "features_test = all_data[train.shape[0]:]\n",
    "targets = train['class']\n",
    "\n",
    "features.info()\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = plt.hist(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9182, 297), (435, 297), (49, 297))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# split the data into training and validation sets\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(features.values, targets.values, test_size = 1 - train_ratio, stratify=targets.values, random_state=0)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_valid, y_valid, test_size = test_ratio, stratify=y_valid, random_state=0)\n",
    "X_train.shape,X_valid.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_ratio = 0.98\n",
    "\n",
    "test_ratio = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.hist(y_test)\n",
    "# plt.hist(y_valid)\n",
    "a =plt.hist(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data_utils\n",
    "\n",
    "data_train = data_utils.TensorDataset(torch.from_numpy(X_train).type((torch.FloatTensor)), torch.from_numpy(y_train).type((torch.LongTensor)))\n",
    "data_valid = data_utils.TensorDataset(torch.from_numpy(X_valid).type((torch.FloatTensor)), torch.from_numpy(y_valid).type((torch.LongTensor)))\n",
    "data_test = data_utils.TensorDataset(torch.from_numpy(X_test).type((torch.FloatTensor)), torch.from_numpy(y_test).type((torch.LongTensor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# convert data to torch.FloatTensor\n",
    "\n",
    "loaders = {}\n",
    "loaders['train'] = torch.utils.data.DataLoader(data_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=1)\n",
    "\n",
    "loaders['valid'] = torch.utils.data.DataLoader(data_valid,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=1)\n",
    "loaders['test'] = torch.utils.data.DataLoader(data_test,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "#     print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the CNN architecture\n",
    "class Net(nn.Module):\n",
    "    ### TODO: choose an architecture, and complete the class\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(297, 128)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.fc3 = nn.Linear(32, 6)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "#         x = self.dropout(x)\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "#         x = self.dropout(x)\n",
    "        x = F.sigmoid(self.fc3(x))\n",
    "\n",
    "\n",
    "        return x\n",
    "\n",
    "#-#-# You do NOT have to modify the code below this line. #-#-#\n",
    "\n",
    "# instantiate the CNN\n",
    "model = Net()\n",
    "def init_weights(m):\n",
    "        print(m)\n",
    "        if type(m) == nn.Linear:\n",
    "            m.weight.data.fill_(1.0)\n",
    "            print(m.weight)\n",
    "            \n",
    "def init_ortho(m):\n",
    "    print()\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.orthogonal_(m.weight)\n",
    "        print(m.weight)\n",
    "\n",
    "# use the modules apply function to recursively apply the initialization\n",
    "# model.apply(init_ortho)\n",
    "\n",
    "# move tensors to GPU if CUDA is available\n",
    "if use_cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "### TODO: select loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "### TODO: select optimizer\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "# optimizer = optim.Adamax(model.parameters(), lr=0.01)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01,weight_decay= 1e-6, momentum = 0.9, nesterov = True)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.6/site-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.795349 \tValidation Loss: 1.806525 \t time: 0.3\n",
      "Validation loss decreased from inf to 1.806525. Model was saved\n",
      "Epoch: 2 \tTraining Loss: 1.806869 \tValidation Loss: 1.796872 \t time: 0.3\n",
      "Validation loss decreased from 1.806525 to 1.796872. Model was saved\n",
      "Epoch: 3 \tTraining Loss: 1.797095 \tValidation Loss: 1.791481 \t time: 0.3\n",
      "Validation loss decreased from 1.796872 to 1.791481. Model was saved\n",
      "Epoch: 4 \tTraining Loss: 1.791602 \tValidation Loss: 1.789828 \t time: 0.3\n",
      "Validation loss decreased from 1.791481 to 1.789828. Model was saved\n",
      "Epoch: 5 \tTraining Loss: 1.789783 \tValidation Loss: 1.786756 \t time: 0.3\n",
      "Validation loss decreased from 1.789828 to 1.786756. Model was saved\n",
      "Epoch: 6 \tTraining Loss: 1.786633 \tValidation Loss: 1.779005 \t time: 0.3\n",
      "Validation loss decreased from 1.786756 to 1.779005. Model was saved\n",
      "Epoch: 7 \tTraining Loss: 1.779274 \tValidation Loss: 1.765309 \t time: 0.3\n",
      "Validation loss decreased from 1.779005 to 1.765309. Model was saved\n",
      "Epoch: 8 \tTraining Loss: 1.766636 \tValidation Loss: 1.747704 \t time: 0.3\n",
      "Validation loss decreased from 1.765309 to 1.747704. Model was saved\n",
      "Epoch: 9 \tTraining Loss: 1.749687 \tValidation Loss: 1.729520 \t time: 0.3\n",
      "Validation loss decreased from 1.747704 to 1.729520. Model was saved\n",
      "Epoch: 10 \tTraining Loss: 1.731132 \tValidation Loss: 1.710269 \t time: 0.3\n",
      "Validation loss decreased from 1.729520 to 1.710269. Model was saved\n",
      "Epoch: 11 \tTraining Loss: 1.711508 \tValidation Loss: 1.692464 \t time: 0.3\n",
      "Validation loss decreased from 1.710269 to 1.692464. Model was saved\n",
      "Epoch: 12 \tTraining Loss: 1.691884 \tValidation Loss: 1.676097 \t time: 0.3\n",
      "Validation loss decreased from 1.692464 to 1.676097. Model was saved\n",
      "Epoch: 13 \tTraining Loss: 1.671601 \tValidation Loss: 1.660144 \t time: 0.3\n",
      "Validation loss decreased from 1.676097 to 1.660144. Model was saved\n",
      "Epoch: 14 \tTraining Loss: 1.650877 \tValidation Loss: 1.644374 \t time: 0.3\n",
      "Validation loss decreased from 1.660144 to 1.644374. Model was saved\n",
      "Epoch: 15 \tTraining Loss: 1.630630 \tValidation Loss: 1.631335 \t time: 0.3\n",
      "Validation loss decreased from 1.644374 to 1.631335. Model was saved\n",
      "Epoch: 16 \tTraining Loss: 1.612053 \tValidation Loss: 1.620370 \t time: 0.3\n",
      "Validation loss decreased from 1.631335 to 1.620370. Model was saved\n",
      "Epoch: 17 \tTraining Loss: 1.595461 \tValidation Loss: 1.609657 \t time: 0.3\n",
      "Validation loss decreased from 1.620370 to 1.609657. Model was saved\n",
      "Epoch: 18 \tTraining Loss: 1.580688 \tValidation Loss: 1.598939 \t time: 0.3\n",
      "Validation loss decreased from 1.609657 to 1.598939. Model was saved\n",
      "Epoch: 19 \tTraining Loss: 1.566612 \tValidation Loss: 1.585871 \t time: 0.3\n",
      "Validation loss decreased from 1.598939 to 1.585871. Model was saved\n",
      "Epoch: 20 \tTraining Loss: 1.553558 \tValidation Loss: 1.573429 \t time: 0.3\n",
      "Validation loss decreased from 1.585871 to 1.573429. Model was saved\n",
      "Epoch: 21 \tTraining Loss: 1.541737 \tValidation Loss: 1.562586 \t time: 0.3\n",
      "Validation loss decreased from 1.573429 to 1.562586. Model was saved\n",
      "Epoch: 22 \tTraining Loss: 1.531984 \tValidation Loss: 1.552024 \t time: 0.3\n",
      "Validation loss decreased from 1.562586 to 1.552024. Model was saved\n",
      "Epoch: 23 \tTraining Loss: 1.523277 \tValidation Loss: 1.545072 \t time: 0.3\n",
      "Validation loss decreased from 1.552024 to 1.545072. Model was saved\n",
      "Epoch: 24 \tTraining Loss: 1.515354 \tValidation Loss: 1.539242 \t time: 0.3\n",
      "Validation loss decreased from 1.545072 to 1.539242. Model was saved\n",
      "Epoch: 25 \tTraining Loss: 1.508593 \tValidation Loss: 1.535024 \t time: 0.3\n",
      "Validation loss decreased from 1.539242 to 1.535024. Model was saved\n",
      "Epoch: 26 \tTraining Loss: 1.502637 \tValidation Loss: 1.534576 \t time: 0.3\n",
      "Validation loss decreased from 1.535024 to 1.534576. Model was saved\n",
      "Epoch: 27 \tTraining Loss: 1.497320 \tValidation Loss: 1.532314 \t time: 0.3\n",
      "Validation loss decreased from 1.534576 to 1.532314. Model was saved\n",
      "Epoch: 28 \tTraining Loss: 1.492781 \tValidation Loss: 1.530746 \t time: 0.3\n",
      "Validation loss decreased from 1.532314 to 1.530746. Model was saved\n",
      "Epoch: 29 \tTraining Loss: 1.488058 \tValidation Loss: 1.529772 \t time: 0.3\n",
      "Validation loss decreased from 1.530746 to 1.529772. Model was saved\n",
      "Epoch: 30 \tTraining Loss: 1.482162 \tValidation Loss: 1.529768 \t time: 0.3\n",
      "Validation loss decreased from 1.529772 to 1.529768. Model was saved\n",
      "Epoch: 31 \tTraining Loss: 1.479156 \tValidation Loss: 1.529105 \t time: 0.3\n",
      "Validation loss decreased from 1.529768 to 1.529105. Model was saved\n",
      "Epoch: 32 \tTraining Loss: 1.474542 \tValidation Loss: 1.526971 \t time: 0.3\n",
      "Validation loss decreased from 1.529105 to 1.526971. Model was saved\n",
      "Epoch: 33 \tTraining Loss: 1.470270 \tValidation Loss: 1.526180 \t time: 0.3\n",
      "Validation loss decreased from 1.526971 to 1.526180. Model was saved\n",
      "Epoch: 34 \tTraining Loss: 1.468179 \tValidation Loss: 1.527227 \t time: 0.3\n",
      "Epoch: 35 \tTraining Loss: 1.464385 \tValidation Loss: 1.521838 \t time: 0.3\n",
      "Validation loss decreased from 1.526180 to 1.521838. Model was saved\n",
      "Epoch: 36 \tTraining Loss: 1.461535 \tValidation Loss: 1.522640 \t time: 0.3\n",
      "Epoch: 37 \tTraining Loss: 1.458223 \tValidation Loss: 1.521228 \t time: 0.3\n",
      "Validation loss decreased from 1.521838 to 1.521228. Model was saved\n",
      "Epoch: 38 \tTraining Loss: 1.454871 \tValidation Loss: 1.515730 \t time: 0.3\n",
      "Validation loss decreased from 1.521228 to 1.515730. Model was saved\n",
      "Epoch: 39 \tTraining Loss: 1.452718 \tValidation Loss: 1.516485 \t time: 0.3\n",
      "Epoch: 40 \tTraining Loss: 1.449331 \tValidation Loss: 1.513019 \t time: 0.3\n",
      "Validation loss decreased from 1.515730 to 1.513019. Model was saved\n",
      "Epoch: 41 \tTraining Loss: 1.446373 \tValidation Loss: 1.506787 \t time: 0.3\n",
      "Validation loss decreased from 1.513019 to 1.506787. Model was saved\n",
      "Epoch: 42 \tTraining Loss: 1.444613 \tValidation Loss: 1.509003 \t time: 0.3\n",
      "Epoch: 43 \tTraining Loss: 1.441997 \tValidation Loss: 1.504961 \t time: 0.3\n",
      "Validation loss decreased from 1.506787 to 1.504961. Model was saved\n",
      "Epoch: 44 \tTraining Loss: 1.439496 \tValidation Loss: 1.502411 \t time: 0.3\n",
      "Validation loss decreased from 1.504961 to 1.502411. Model was saved\n",
      "Epoch: 45 \tTraining Loss: 1.437597 \tValidation Loss: 1.502223 \t time: 0.3\n",
      "Validation loss decreased from 1.502411 to 1.502223. Model was saved\n",
      "Epoch: 46 \tTraining Loss: 1.434334 \tValidation Loss: 1.501715 \t time: 0.3\n",
      "Validation loss decreased from 1.502223 to 1.501715. Model was saved\n",
      "Epoch: 47 \tTraining Loss: 1.431752 \tValidation Loss: 1.496434 \t time: 0.3\n",
      "Validation loss decreased from 1.501715 to 1.496434. Model was saved\n",
      "Epoch: 48 \tTraining Loss: 1.429728 \tValidation Loss: 1.498845 \t time: 0.3\n",
      "Epoch: 49 \tTraining Loss: 1.427124 \tValidation Loss: 1.495793 \t time: 0.3\n",
      "Validation loss decreased from 1.496434 to 1.495793. Model was saved\n",
      "Epoch: 50 \tTraining Loss: 1.424893 \tValidation Loss: 1.493784 \t time: 0.3\n",
      "Validation loss decreased from 1.495793 to 1.493784. Model was saved\n",
      "Epoch: 51 \tTraining Loss: 1.424237 \tValidation Loss: 1.493161 \t time: 0.3\n",
      "Validation loss decreased from 1.493784 to 1.493161. Model was saved\n",
      "Epoch: 52 \tTraining Loss: 1.422797 \tValidation Loss: 1.491284 \t time: 0.3\n",
      "Validation loss decreased from 1.493161 to 1.491284. Model was saved\n",
      "Epoch: 53 \tTraining Loss: 1.421498 \tValidation Loss: 1.483375 \t time: 0.3\n",
      "Validation loss decreased from 1.491284 to 1.483375. Model was saved\n",
      "Epoch: 54 \tTraining Loss: 1.415721 \tValidation Loss: 1.482293 \t time: 0.3\n",
      "Validation loss decreased from 1.483375 to 1.482293. Model was saved\n",
      "Epoch: 55 \tTraining Loss: 1.410921 \tValidation Loss: 1.480374 \t time: 0.3\n",
      "Validation loss decreased from 1.482293 to 1.480374. Model was saved\n",
      "Epoch: 56 \tTraining Loss: 1.410735 \tValidation Loss: 1.474168 \t time: 0.3\n",
      "Validation loss decreased from 1.480374 to 1.474168. Model was saved\n",
      "Epoch: 57 \tTraining Loss: 1.405481 \tValidation Loss: 1.471260 \t time: 0.3\n",
      "Validation loss decreased from 1.474168 to 1.471260. Model was saved\n",
      "Epoch: 58 \tTraining Loss: 1.400928 \tValidation Loss: 1.473164 \t time: 0.3\n",
      "Epoch: 59 \tTraining Loss: 1.401220 \tValidation Loss: 1.469311 \t time: 0.3\n",
      "Validation loss decreased from 1.471260 to 1.469311. Model was saved\n",
      "Epoch: 60 \tTraining Loss: 1.397167 \tValidation Loss: 1.466812 \t time: 0.3\n",
      "Validation loss decreased from 1.469311 to 1.466812. Model was saved\n",
      "Epoch: 61 \tTraining Loss: 1.394775 \tValidation Loss: 1.468252 \t time: 0.3\n",
      "Epoch: 62 \tTraining Loss: 1.392807 \tValidation Loss: 1.468090 \t time: 0.3\n",
      "Epoch: 63 \tTraining Loss: 1.391905 \tValidation Loss: 1.464680 \t time: 0.3\n",
      "Validation loss decreased from 1.466812 to 1.464680. Model was saved\n",
      "Epoch: 64 \tTraining Loss: 1.388646 \tValidation Loss: 1.463602 \t time: 0.3\n",
      "Validation loss decreased from 1.464680 to 1.463602. Model was saved\n",
      "Epoch: 65 \tTraining Loss: 1.386745 \tValidation Loss: 1.464843 \t time: 0.3\n",
      "Epoch: 66 \tTraining Loss: 1.385740 \tValidation Loss: 1.464838 \t time: 0.3\n",
      "Epoch: 67 \tTraining Loss: 1.383401 \tValidation Loss: 1.461590 \t time: 0.3\n",
      "Validation loss decreased from 1.463602 to 1.461590. Model was saved\n",
      "Epoch: 68 \tTraining Loss: 1.381865 \tValidation Loss: 1.462487 \t time: 0.3\n",
      "Epoch: 69 \tTraining Loss: 1.379313 \tValidation Loss: 1.465170 \t time: 0.3\n",
      "Epoch: 70 \tTraining Loss: 1.378982 \tValidation Loss: 1.459878 \t time: 0.3\n",
      "Validation loss decreased from 1.461590 to 1.459878. Model was saved\n",
      "Epoch: 71 \tTraining Loss: 1.377298 \tValidation Loss: 1.461830 \t time: 0.3\n",
      "Epoch: 72 \tTraining Loss: 1.375952 \tValidation Loss: 1.459999 \t time: 0.3\n",
      "Epoch: 73 \tTraining Loss: 1.373276 \tValidation Loss: 1.458692 \t time: 0.3\n",
      "Validation loss decreased from 1.459878 to 1.458692. Model was saved\n",
      "Epoch: 74 \tTraining Loss: 1.371244 \tValidation Loss: 1.455581 \t time: 0.3\n",
      "Validation loss decreased from 1.458692 to 1.455581. Model was saved\n",
      "Epoch: 75 \tTraining Loss: 1.370077 \tValidation Loss: 1.453800 \t time: 0.3\n",
      "Validation loss decreased from 1.455581 to 1.453800. Model was saved\n",
      "Epoch: 76 \tTraining Loss: 1.368518 \tValidation Loss: 1.456803 \t time: 0.3\n",
      "Epoch: 77 \tTraining Loss: 1.368222 \tValidation Loss: 1.451525 \t time: 0.3\n",
      "Validation loss decreased from 1.453800 to 1.451525. Model was saved\n",
      "Epoch: 78 \tTraining Loss: 1.368131 \tValidation Loss: 1.459781 \t time: 0.3\n",
      "Epoch: 79 \tTraining Loss: 1.369605 \tValidation Loss: 1.452148 \t time: 0.3\n",
      "Epoch: 80 \tTraining Loss: 1.367134 \tValidation Loss: 1.452798 \t time: 0.3\n",
      "Epoch: 81 \tTraining Loss: 1.362131 \tValidation Loss: 1.453269 \t time: 0.3\n",
      "Epoch: 82 \tTraining Loss: 1.360907 \tValidation Loss: 1.450587 \t time: 0.3\n",
      "Validation loss decreased from 1.451525 to 1.450587. Model was saved\n",
      "Epoch: 83 \tTraining Loss: 1.361776 \tValidation Loss: 1.456792 \t time: 0.3\n",
      "Epoch: 84 \tTraining Loss: 1.358645 \tValidation Loss: 1.454785 \t time: 0.3\n",
      "Epoch: 85 \tTraining Loss: 1.355896 \tValidation Loss: 1.452339 \t time: 0.3\n",
      "Epoch: 86 \tTraining Loss: 1.355652 \tValidation Loss: 1.457074 \t time: 0.3\n",
      "Epoch: 87 \tTraining Loss: 1.355291 \tValidation Loss: 1.452327 \t time: 0.3\n",
      "Epoch: 88 \tTraining Loss: 1.352923 \tValidation Loss: 1.453961 \t time: 0.3\n",
      "Epoch: 89 \tTraining Loss: 1.350375 \tValidation Loss: 1.454272 \t time: 0.3\n",
      "Epoch: 90 \tTraining Loss: 1.349044 \tValidation Loss: 1.451772 \t time: 0.3\n",
      "Epoch: 91 \tTraining Loss: 1.348902 \tValidation Loss: 1.458235 \t time: 0.3\n",
      "Epoch: 92 \tTraining Loss: 1.349750 \tValidation Loss: 1.450314 \t time: 0.3\n",
      "Validation loss decreased from 1.450587 to 1.450314. Model was saved\n",
      "Epoch: 93 \tTraining Loss: 1.350055 \tValidation Loss: 1.455357 \t time: 0.3\n",
      "Epoch: 94 \tTraining Loss: 1.347859 \tValidation Loss: 1.448610 \t time: 0.3\n",
      "Validation loss decreased from 1.450314 to 1.448610. Model was saved\n",
      "Epoch: 95 \tTraining Loss: 1.343534 \tValidation Loss: 1.449041 \t time: 0.3\n",
      "Epoch: 96 \tTraining Loss: 1.344239 \tValidation Loss: 1.456225 \t time: 0.3\n",
      "Epoch: 97 \tTraining Loss: 1.345677 \tValidation Loss: 1.446800 \t time: 0.3\n",
      "Validation loss decreased from 1.448610 to 1.446800. Model was saved\n",
      "Epoch: 98 \tTraining Loss: 1.342940 \tValidation Loss: 1.449638 \t time: 0.3\n",
      "Epoch: 99 \tTraining Loss: 1.339084 \tValidation Loss: 1.451931 \t time: 0.3\n",
      "Epoch: 100 \tTraining Loss: 1.340591 \tValidation Loss: 1.447632 \t time: 0.3\n",
      "Epoch: 101 \tTraining Loss: 1.340719 \tValidation Loss: 1.448954 \t time: 0.3\n",
      "Epoch: 102 \tTraining Loss: 1.336874 \tValidation Loss: 1.451406 \t time: 0.3\n",
      "Epoch: 103 \tTraining Loss: 1.335984 \tValidation Loss: 1.449120 \t time: 0.3\n",
      "Epoch: 104 \tTraining Loss: 1.336381 \tValidation Loss: 1.452381 \t time: 0.3\n",
      "Epoch: 105 \tTraining Loss: 1.338755 \tValidation Loss: 1.445316 \t time: 0.3\n",
      "Validation loss decreased from 1.446800 to 1.445316. Model was saved\n",
      "Epoch: 106 \tTraining Loss: 1.332152 \tValidation Loss: 1.449150 \t time: 0.3\n",
      "Epoch: 107 \tTraining Loss: 1.336855 \tValidation Loss: 1.452476 \t time: 0.3\n",
      "Epoch: 108 \tTraining Loss: 1.340005 \tValidation Loss: 1.443776 \t time: 0.3\n",
      "Validation loss decreased from 1.445316 to 1.443776. Model was saved\n",
      "Epoch: 109 \tTraining Loss: 1.338223 \tValidation Loss: 1.442376 \t time: 0.3\n",
      "Validation loss decreased from 1.443776 to 1.442376. Model was saved\n",
      "Epoch: 110 \tTraining Loss: 1.330706 \tValidation Loss: 1.449174 \t time: 0.3\n",
      "Epoch: 111 \tTraining Loss: 1.339383 \tValidation Loss: 1.442096 \t time: 0.3\n",
      "Validation loss decreased from 1.442376 to 1.442096. Model was saved\n",
      "Epoch: 112 \tTraining Loss: 1.328068 \tValidation Loss: 1.443099 \t time: 0.3\n",
      "Epoch: 113 \tTraining Loss: 1.333398 \tValidation Loss: 1.444707 \t time: 0.3\n",
      "Epoch: 114 \tTraining Loss: 1.328509 \tValidation Loss: 1.443316 \t time: 0.3\n",
      "Epoch: 115 \tTraining Loss: 1.326812 \tValidation Loss: 1.440676 \t time: 0.3\n",
      "Validation loss decreased from 1.442096 to 1.440676. Model was saved\n",
      "Epoch: 116 \tTraining Loss: 1.328396 \tValidation Loss: 1.445219 \t time: 0.3\n",
      "Epoch: 117 \tTraining Loss: 1.328964 \tValidation Loss: 1.442004 \t time: 0.3\n",
      "Epoch: 118 \tTraining Loss: 1.326801 \tValidation Loss: 1.434231 \t time: 0.3\n",
      "Validation loss decreased from 1.440676 to 1.434231. Model was saved\n",
      "Epoch: 119 \tTraining Loss: 1.323700 \tValidation Loss: 1.439487 \t time: 0.3\n",
      "Epoch: 120 \tTraining Loss: 1.325093 \tValidation Loss: 1.438504 \t time: 0.3\n",
      "Epoch: 121 \tTraining Loss: 1.320327 \tValidation Loss: 1.439592 \t time: 0.3\n",
      "Epoch: 122 \tTraining Loss: 1.323045 \tValidation Loss: 1.441740 \t time: 0.3\n",
      "Epoch: 123 \tTraining Loss: 1.318761 \tValidation Loss: 1.442601 \t time: 0.3\n",
      "Epoch: 124 \tTraining Loss: 1.319180 \tValidation Loss: 1.438557 \t time: 0.3\n",
      "Epoch: 125 \tTraining Loss: 1.318975 \tValidation Loss: 1.439570 \t time: 0.3\n",
      "Epoch: 126 \tTraining Loss: 1.317250 \tValidation Loss: 1.439830 \t time: 0.3\n",
      "Epoch: 127 \tTraining Loss: 1.317649 \tValidation Loss: 1.433028 \t time: 0.3\n",
      "Validation loss decreased from 1.434231 to 1.433028. Model was saved\n",
      "Epoch: 128 \tTraining Loss: 1.314145 \tValidation Loss: 1.434511 \t time: 0.3\n",
      "Epoch: 129 \tTraining Loss: 1.315678 \tValidation Loss: 1.434307 \t time: 0.3\n",
      "Epoch: 130 \tTraining Loss: 1.312132 \tValidation Loss: 1.436266 \t time: 0.3\n",
      "Epoch: 131 \tTraining Loss: 1.313815 \tValidation Loss: 1.435691 \t time: 0.3\n",
      "Epoch: 132 \tTraining Loss: 1.310700 \tValidation Loss: 1.433442 \t time: 0.3\n",
      "Epoch: 133 \tTraining Loss: 1.311079 \tValidation Loss: 1.430828 \t time: 0.3\n",
      "Validation loss decreased from 1.433028 to 1.430828. Model was saved\n",
      "Epoch: 134 \tTraining Loss: 1.309350 \tValidation Loss: 1.431843 \t time: 0.3\n",
      "Epoch: 135 \tTraining Loss: 1.308869 \tValidation Loss: 1.430479 \t time: 0.3\n",
      "Validation loss decreased from 1.430828 to 1.430479. Model was saved\n",
      "Epoch: 136 \tTraining Loss: 1.307758 \tValidation Loss: 1.429436 \t time: 0.3\n",
      "Validation loss decreased from 1.430479 to 1.429436. Model was saved\n",
      "Epoch: 137 \tTraining Loss: 1.307073 \tValidation Loss: 1.428680 \t time: 0.3\n",
      "Validation loss decreased from 1.429436 to 1.428680. Model was saved\n",
      "Epoch: 138 \tTraining Loss: 1.306461 \tValidation Loss: 1.429313 \t time: 0.3\n",
      "Epoch: 139 \tTraining Loss: 1.305236 \tValidation Loss: 1.429711 \t time: 0.3\n",
      "Epoch: 140 \tTraining Loss: 1.304108 \tValidation Loss: 1.427952 \t time: 0.3\n",
      "Validation loss decreased from 1.428680 to 1.427952. Model was saved\n",
      "Epoch: 141 \tTraining Loss: 1.303180 \tValidation Loss: 1.427311 \t time: 0.3\n",
      "Validation loss decreased from 1.427952 to 1.427311. Model was saved\n",
      "Epoch: 142 \tTraining Loss: 1.303458 \tValidation Loss: 1.429080 \t time: 0.3\n",
      "Epoch: 143 \tTraining Loss: 1.303409 \tValidation Loss: 1.427964 \t time: 0.3\n",
      "Epoch: 144 \tTraining Loss: 1.302684 \tValidation Loss: 1.424731 \t time: 0.3\n",
      "Validation loss decreased from 1.427311 to 1.424731. Model was saved\n",
      "Epoch: 145 \tTraining Loss: 1.300366 \tValidation Loss: 1.424169 \t time: 0.3\n",
      "Validation loss decreased from 1.424731 to 1.424169. Model was saved\n",
      "Epoch: 146 \tTraining Loss: 1.299741 \tValidation Loss: 1.425240 \t time: 0.3\n",
      "Epoch: 147 \tTraining Loss: 1.298904 \tValidation Loss: 1.425520 \t time: 0.3\n",
      "Epoch: 148 \tTraining Loss: 1.298546 \tValidation Loss: 1.423631 \t time: 0.3\n",
      "Validation loss decreased from 1.424169 to 1.423631. Model was saved\n",
      "Epoch: 149 \tTraining Loss: 1.296885 \tValidation Loss: 1.423438 \t time: 0.3\n",
      "Validation loss decreased from 1.423631 to 1.423438. Model was saved\n",
      "Epoch: 150 \tTraining Loss: 1.296162 \tValidation Loss: 1.424019 \t time: 0.3\n",
      "Epoch: 151 \tTraining Loss: 1.294979 \tValidation Loss: 1.423557 \t time: 0.3\n",
      "Epoch: 152 \tTraining Loss: 1.294491 \tValidation Loss: 1.422045 \t time: 0.3\n",
      "Validation loss decreased from 1.423438 to 1.422045. Model was saved\n",
      "Epoch: 153 \tTraining Loss: 1.293767 \tValidation Loss: 1.421844 \t time: 0.3\n",
      "Validation loss decreased from 1.422045 to 1.421844. Model was saved\n",
      "Epoch: 154 \tTraining Loss: 1.293496 \tValidation Loss: 1.424266 \t time: 0.3\n",
      "Epoch: 155 \tTraining Loss: 1.293511 \tValidation Loss: 1.422668 \t time: 0.3\n",
      "Epoch: 156 \tTraining Loss: 1.293604 \tValidation Loss: 1.422588 \t time: 0.3\n",
      "Epoch: 157 \tTraining Loss: 1.292858 \tValidation Loss: 1.421380 \t time: 0.3\n",
      "Validation loss decreased from 1.421844 to 1.421380. Model was saved\n",
      "Epoch: 158 \tTraining Loss: 1.292126 \tValidation Loss: 1.422600 \t time: 0.3\n",
      "Epoch: 159 \tTraining Loss: 1.290925 \tValidation Loss: 1.420628 \t time: 0.3\n",
      "Validation loss decreased from 1.421380 to 1.420628. Model was saved\n",
      "Epoch: 160 \tTraining Loss: 1.288477 \tValidation Loss: 1.420049 \t time: 0.3\n",
      "Validation loss decreased from 1.420628 to 1.420049. Model was saved\n",
      "Epoch: 161 \tTraining Loss: 1.287399 \tValidation Loss: 1.421737 \t time: 0.3\n",
      "Epoch: 162 \tTraining Loss: 1.287755 \tValidation Loss: 1.423343 \t time: 0.3\n",
      "Epoch: 163 \tTraining Loss: 1.289205 \tValidation Loss: 1.423683 \t time: 0.3\n",
      "Epoch: 164 \tTraining Loss: 1.288744 \tValidation Loss: 1.420928 \t time: 0.3\n",
      "Epoch: 165 \tTraining Loss: 1.287456 \tValidation Loss: 1.419557 \t time: 0.3\n",
      "Validation loss decreased from 1.420049 to 1.419557. Model was saved\n",
      "Epoch: 166 \tTraining Loss: 1.284606 \tValidation Loss: 1.419643 \t time: 0.3\n",
      "Epoch: 167 \tTraining Loss: 1.283442 \tValidation Loss: 1.420610 \t time: 0.3\n",
      "Epoch: 168 \tTraining Loss: 1.284041 \tValidation Loss: 1.419629 \t time: 0.3\n",
      "Epoch: 169 \tTraining Loss: 1.283791 \tValidation Loss: 1.420257 \t time: 0.3\n",
      "Epoch: 170 \tTraining Loss: 1.282231 \tValidation Loss: 1.419839 \t time: 0.3\n",
      "Epoch: 171 \tTraining Loss: 1.280773 \tValidation Loss: 1.419043 \t time: 0.3\n",
      "Validation loss decreased from 1.419557 to 1.419043. Model was saved\n",
      "Epoch: 172 \tTraining Loss: 1.280561 \tValidation Loss: 1.419060 \t time: 0.3\n",
      "Epoch: 173 \tTraining Loss: 1.280777 \tValidation Loss: 1.417738 \t time: 0.3\n",
      "Validation loss decreased from 1.419043 to 1.417738. Model was saved\n",
      "Epoch: 174 \tTraining Loss: 1.280233 \tValidation Loss: 1.416799 \t time: 0.3\n",
      "Validation loss decreased from 1.417738 to 1.416799. Model was saved\n",
      "Epoch: 175 \tTraining Loss: 1.278938 \tValidation Loss: 1.414698 \t time: 0.3\n",
      "Validation loss decreased from 1.416799 to 1.414698. Model was saved\n",
      "Epoch: 176 \tTraining Loss: 1.277703 \tValidation Loss: 1.414293 \t time: 0.3\n",
      "Validation loss decreased from 1.414698 to 1.414293. Model was saved\n",
      "Epoch: 177 \tTraining Loss: 1.277369 \tValidation Loss: 1.414458 \t time: 0.3\n",
      "Epoch: 178 \tTraining Loss: 1.277419 \tValidation Loss: 1.414849 \t time: 0.3\n",
      "Epoch: 179 \tTraining Loss: 1.276894 \tValidation Loss: 1.414373 \t time: 0.3\n",
      "Epoch: 180 \tTraining Loss: 1.275956 \tValidation Loss: 1.414938 \t time: 0.3\n",
      "Epoch: 181 \tTraining Loss: 1.275181 \tValidation Loss: 1.414877 \t time: 0.3\n",
      "Epoch: 182 \tTraining Loss: 1.274724 \tValidation Loss: 1.413902 \t time: 0.3\n",
      "Validation loss decreased from 1.414293 to 1.413902. Model was saved\n",
      "Epoch: 183 \tTraining Loss: 1.274496 \tValidation Loss: 1.413693 \t time: 0.3\n",
      "Validation loss decreased from 1.413902 to 1.413693. Model was saved\n",
      "Epoch: 184 \tTraining Loss: 1.274375 \tValidation Loss: 1.413358 \t time: 0.3\n",
      "Validation loss decreased from 1.413693 to 1.413358. Model was saved\n",
      "Epoch: 185 \tTraining Loss: 1.274203 \tValidation Loss: 1.413520 \t time: 0.3\n",
      "Epoch: 186 \tTraining Loss: 1.274050 \tValidation Loss: 1.412426 \t time: 0.3\n",
      "Validation loss decreased from 1.413358 to 1.412426. Model was saved\n",
      "Epoch: 187 \tTraining Loss: 1.273673 \tValidation Loss: 1.412268 \t time: 0.3\n",
      "Validation loss decreased from 1.412426 to 1.412268. Model was saved\n",
      "Epoch: 188 \tTraining Loss: 1.273276 \tValidation Loss: 1.412056 \t time: 0.3\n",
      "Validation loss decreased from 1.412268 to 1.412056. Model was saved\n",
      "Epoch: 189 \tTraining Loss: 1.272684 \tValidation Loss: 1.411304 \t time: 0.3\n",
      "Validation loss decreased from 1.412056 to 1.411304. Model was saved\n",
      "Epoch: 190 \tTraining Loss: 1.271888 \tValidation Loss: 1.410220 \t time: 0.3\n",
      "Validation loss decreased from 1.411304 to 1.410220. Model was saved\n",
      "Epoch: 191 \tTraining Loss: 1.270926 \tValidation Loss: 1.410088 \t time: 0.3\n",
      "Validation loss decreased from 1.410220 to 1.410088. Model was saved\n",
      "Epoch: 192 \tTraining Loss: 1.270136 \tValidation Loss: 1.410131 \t time: 0.3\n",
      "Epoch: 193 \tTraining Loss: 1.269682 \tValidation Loss: 1.409586 \t time: 0.3\n",
      "Validation loss decreased from 1.410088 to 1.409586. Model was saved\n",
      "Epoch: 194 \tTraining Loss: 1.269503 \tValidation Loss: 1.408436 \t time: 0.3\n",
      "Validation loss decreased from 1.409586 to 1.408436. Model was saved\n",
      "Epoch: 195 \tTraining Loss: 1.269567 \tValidation Loss: 1.409110 \t time: 0.3\n",
      "Epoch: 196 \tTraining Loss: 1.269993 \tValidation Loss: 1.408906 \t time: 0.3\n",
      "Epoch: 197 \tTraining Loss: 1.271653 \tValidation Loss: 1.411172 \t time: 0.3\n",
      "Epoch: 198 \tTraining Loss: 1.275034 \tValidation Loss: 1.412371 \t time: 0.3\n",
      "Epoch: 199 \tTraining Loss: 1.280581 \tValidation Loss: 1.414264 \t time: 0.3\n",
      "Epoch: 200 \tTraining Loss: 1.283620 \tValidation Loss: 1.404732 \t time: 0.3\n",
      "Validation loss decreased from 1.408436 to 1.404732. Model was saved\n",
      "Epoch: 201 \tTraining Loss: 1.272752 \tValidation Loss: 1.402672 \t time: 0.3\n",
      "Validation loss decreased from 1.404732 to 1.402672. Model was saved\n",
      "Epoch: 202 \tTraining Loss: 1.272256 \tValidation Loss: 1.409529 \t time: 0.3\n",
      "Epoch: 203 \tTraining Loss: 1.276058 \tValidation Loss: 1.406013 \t time: 0.3\n",
      "Epoch: 204 \tTraining Loss: 1.267535 \tValidation Loss: 1.407623 \t time: 0.3\n",
      "Epoch: 205 \tTraining Loss: 1.273300 \tValidation Loss: 1.409067 \t time: 0.3\n",
      "Epoch: 206 \tTraining Loss: 1.273191 \tValidation Loss: 1.403613 \t time: 0.3\n",
      "Epoch: 207 \tTraining Loss: 1.266242 \tValidation Loss: 1.406497 \t time: 0.3\n",
      "Epoch: 208 \tTraining Loss: 1.271467 \tValidation Loss: 1.404791 \t time: 0.3\n",
      "Epoch: 209 \tTraining Loss: 1.265790 \tValidation Loss: 1.405097 \t time: 0.3\n",
      "Epoch: 210 \tTraining Loss: 1.268376 \tValidation Loss: 1.405101 \t time: 0.3\n",
      "Epoch: 211 \tTraining Loss: 1.270253 \tValidation Loss: 1.404932 \t time: 0.3\n",
      "Epoch: 212 \tTraining Loss: 1.263855 \tValidation Loss: 1.405717 \t time: 0.3\n",
      "Epoch: 213 \tTraining Loss: 1.267609 \tValidation Loss: 1.404138 \t time: 0.3\n",
      "Epoch: 214 \tTraining Loss: 1.264676 \tValidation Loss: 1.404421 \t time: 0.3\n",
      "Epoch: 215 \tTraining Loss: 1.264550 \tValidation Loss: 1.406576 \t time: 0.3\n",
      "Epoch: 216 \tTraining Loss: 1.264747 \tValidation Loss: 1.406184 \t time: 0.3\n",
      "Epoch: 217 \tTraining Loss: 1.262023 \tValidation Loss: 1.403825 \t time: 0.3\n",
      "Epoch: 218 \tTraining Loss: 1.263550 \tValidation Loss: 1.402680 \t time: 0.3\n",
      "Epoch: 219 \tTraining Loss: 1.261088 \tValidation Loss: 1.402805 \t time: 0.3\n",
      "Epoch: 220 \tTraining Loss: 1.261411 \tValidation Loss: 1.404457 \t time: 0.3\n",
      "Epoch: 221 \tTraining Loss: 1.260725 \tValidation Loss: 1.402926 \t time: 0.3\n",
      "Epoch: 222 \tTraining Loss: 1.259960 \tValidation Loss: 1.403171 \t time: 0.3\n",
      "Epoch: 223 \tTraining Loss: 1.260516 \tValidation Loss: 1.403825 \t time: 0.3\n",
      "Epoch: 224 \tTraining Loss: 1.259287 \tValidation Loss: 1.404422 \t time: 0.3\n",
      "Epoch: 225 \tTraining Loss: 1.259248 \tValidation Loss: 1.403831 \t time: 0.3\n",
      "Epoch: 226 \tTraining Loss: 1.258662 \tValidation Loss: 1.402976 \t time: 0.3\n",
      "Epoch: 227 \tTraining Loss: 1.257795 \tValidation Loss: 1.403777 \t time: 0.3\n",
      "Epoch: 228 \tTraining Loss: 1.257619 \tValidation Loss: 1.403360 \t time: 0.3\n",
      "Epoch: 229 \tTraining Loss: 1.257369 \tValidation Loss: 1.402609 \t time: 0.3\n",
      "Validation loss decreased from 1.402672 to 1.402609. Model was saved\n",
      "Epoch: 230 \tTraining Loss: 1.256534 \tValidation Loss: 1.402308 \t time: 0.3\n",
      "Validation loss decreased from 1.402609 to 1.402308. Model was saved\n",
      "Epoch: 231 \tTraining Loss: 1.256877 \tValidation Loss: 1.402446 \t time: 0.3\n",
      "Epoch: 232 \tTraining Loss: 1.256005 \tValidation Loss: 1.402227 \t time: 0.3\n",
      "Validation loss decreased from 1.402308 to 1.402227. Model was saved\n",
      "Epoch: 233 \tTraining Loss: 1.255796 \tValidation Loss: 1.401459 \t time: 0.3\n",
      "Validation loss decreased from 1.402227 to 1.401459. Model was saved\n",
      "Epoch: 234 \tTraining Loss: 1.255607 \tValidation Loss: 1.400916 \t time: 0.3\n",
      "Validation loss decreased from 1.401459 to 1.400916. Model was saved\n",
      "Epoch: 235 \tTraining Loss: 1.255258 \tValidation Loss: 1.401237 \t time: 0.3\n",
      "Epoch: 236 \tTraining Loss: 1.254673 \tValidation Loss: 1.401515 \t time: 0.3\n",
      "Epoch: 237 \tTraining Loss: 1.254669 \tValidation Loss: 1.400265 \t time: 0.3\n",
      "Validation loss decreased from 1.400916 to 1.400265. Model was saved\n",
      "Epoch: 238 \tTraining Loss: 1.254104 \tValidation Loss: 1.399814 \t time: 0.3\n",
      "Validation loss decreased from 1.400265 to 1.399814. Model was saved\n",
      "Epoch: 239 \tTraining Loss: 1.253820 \tValidation Loss: 1.400478 \t time: 0.3\n",
      "Epoch: 240 \tTraining Loss: 1.253514 \tValidation Loss: 1.400873 \t time: 0.3\n",
      "Epoch: 241 \tTraining Loss: 1.253236 \tValidation Loss: 1.400185 \t time: 0.3\n",
      "Epoch: 242 \tTraining Loss: 1.252825 \tValidation Loss: 1.400201 \t time: 0.3\n",
      "Epoch: 243 \tTraining Loss: 1.252701 \tValidation Loss: 1.401558 \t time: 0.3\n",
      "Epoch: 244 \tTraining Loss: 1.252352 \tValidation Loss: 1.402260 \t time: 0.3\n",
      "Epoch: 245 \tTraining Loss: 1.252075 \tValidation Loss: 1.402017 \t time: 0.3\n",
      "Epoch: 246 \tTraining Loss: 1.251741 \tValidation Loss: 1.402154 \t time: 0.3\n",
      "Epoch: 247 \tTraining Loss: 1.251522 \tValidation Loss: 1.402244 \t time: 0.3\n",
      "Epoch: 248 \tTraining Loss: 1.251062 \tValidation Loss: 1.402323 \t time: 0.3\n",
      "Epoch: 249 \tTraining Loss: 1.250762 \tValidation Loss: 1.401728 \t time: 0.3\n",
      "Epoch: 250 \tTraining Loss: 1.250472 \tValidation Loss: 1.401121 \t time: 0.3\n",
      "Epoch: 251 \tTraining Loss: 1.250228 \tValidation Loss: 1.401346 \t time: 0.3\n",
      "Epoch: 252 \tTraining Loss: 1.249862 \tValidation Loss: 1.401279 \t time: 0.3\n",
      "Epoch: 253 \tTraining Loss: 1.249545 \tValidation Loss: 1.400751 \t time: 0.3\n",
      "Epoch: 254 \tTraining Loss: 1.249261 \tValidation Loss: 1.400876 \t time: 0.3\n",
      "Epoch: 255 \tTraining Loss: 1.248995 \tValidation Loss: 1.400974 \t time: 0.3\n",
      "Epoch: 256 \tTraining Loss: 1.248678 \tValidation Loss: 1.401412 \t time: 0.3\n",
      "Epoch: 257 \tTraining Loss: 1.248365 \tValidation Loss: 1.401706 \t time: 0.3\n",
      "Epoch: 258 \tTraining Loss: 1.248110 \tValidation Loss: 1.401560 \t time: 0.3\n",
      "Epoch: 259 \tTraining Loss: 1.247832 \tValidation Loss: 1.401969 \t time: 0.3\n",
      "Epoch: 260 \tTraining Loss: 1.247555 \tValidation Loss: 1.401928 \t time: 0.3\n",
      "Epoch: 261 \tTraining Loss: 1.247285 \tValidation Loss: 1.401735 \t time: 0.3\n",
      "Epoch: 262 \tTraining Loss: 1.247056 \tValidation Loss: 1.401995 \t time: 0.3\n",
      "Epoch: 263 \tTraining Loss: 1.246769 \tValidation Loss: 1.401881 \t time: 0.3\n",
      "Epoch: 264 \tTraining Loss: 1.246489 \tValidation Loss: 1.402336 \t time: 0.3\n",
      "Epoch: 265 \tTraining Loss: 1.246211 \tValidation Loss: 1.402327 \t time: 0.3\n",
      "Epoch: 266 \tTraining Loss: 1.245944 \tValidation Loss: 1.402201 \t time: 0.3\n",
      "Epoch: 267 \tTraining Loss: 1.245651 \tValidation Loss: 1.402321 \t time: 0.3\n",
      "Epoch: 268 \tTraining Loss: 1.245430 \tValidation Loss: 1.401953 \t time: 0.3\n",
      "Epoch: 269 \tTraining Loss: 1.245199 \tValidation Loss: 1.401634 \t time: 0.3\n",
      "Epoch: 270 \tTraining Loss: 1.244946 \tValidation Loss: 1.400988 \t time: 0.3\n",
      "Epoch: 271 \tTraining Loss: 1.244699 \tValidation Loss: 1.400503 \t time: 0.3\n",
      "Epoch: 272 \tTraining Loss: 1.244460 \tValidation Loss: 1.400449 \t time: 0.3\n",
      "Epoch: 273 \tTraining Loss: 1.244200 \tValidation Loss: 1.399801 \t time: 0.3\n",
      "Validation loss decreased from 1.399814 to 1.399801. Model was saved\n",
      "Epoch: 274 \tTraining Loss: 1.243944 \tValidation Loss: 1.399654 \t time: 0.3\n",
      "Validation loss decreased from 1.399801 to 1.399654. Model was saved\n",
      "Epoch: 275 \tTraining Loss: 1.243740 \tValidation Loss: 1.399336 \t time: 0.3\n",
      "Validation loss decreased from 1.399654 to 1.399336. Model was saved\n",
      "Epoch: 276 \tTraining Loss: 1.243545 \tValidation Loss: 1.399297 \t time: 0.3\n",
      "Validation loss decreased from 1.399336 to 1.399297. Model was saved\n",
      "Epoch: 277 \tTraining Loss: 1.243351 \tValidation Loss: 1.399346 \t time: 0.3\n",
      "Epoch: 278 \tTraining Loss: 1.243111 \tValidation Loss: 1.398993 \t time: 0.3\n",
      "Validation loss decreased from 1.399297 to 1.398993. Model was saved\n",
      "Epoch: 279 \tTraining Loss: 1.242845 \tValidation Loss: 1.399150 \t time: 0.3\n",
      "Epoch: 280 \tTraining Loss: 1.242583 \tValidation Loss: 1.398757 \t time: 0.3\n",
      "Validation loss decreased from 1.398993 to 1.398757. Model was saved\n",
      "Epoch: 281 \tTraining Loss: 1.242334 \tValidation Loss: 1.398833 \t time: 0.3\n",
      "Epoch: 282 \tTraining Loss: 1.242088 \tValidation Loss: 1.398448 \t time: 0.3\n",
      "Validation loss decreased from 1.398757 to 1.398448. Model was saved\n",
      "Epoch: 283 \tTraining Loss: 1.241848 \tValidation Loss: 1.398292 \t time: 0.3\n",
      "Validation loss decreased from 1.398448 to 1.398292. Model was saved\n",
      "Epoch: 284 \tTraining Loss: 1.241606 \tValidation Loss: 1.397523 \t time: 0.3\n",
      "Validation loss decreased from 1.398292 to 1.397523. Model was saved\n",
      "Epoch: 285 \tTraining Loss: 1.241345 \tValidation Loss: 1.397366 \t time: 0.3\n",
      "Validation loss decreased from 1.397523 to 1.397366. Model was saved\n",
      "Epoch: 286 \tTraining Loss: 1.241127 \tValidation Loss: 1.396942 \t time: 0.3\n",
      "Validation loss decreased from 1.397366 to 1.396942. Model was saved\n",
      "Epoch: 287 \tTraining Loss: 1.240901 \tValidation Loss: 1.396992 \t time: 0.3\n",
      "Epoch: 288 \tTraining Loss: 1.240691 \tValidation Loss: 1.395292 \t time: 0.3\n",
      "Validation loss decreased from 1.396942 to 1.395292. Model was saved\n",
      "Epoch: 289 \tTraining Loss: 1.240443 \tValidation Loss: 1.395029 \t time: 0.3\n",
      "Validation loss decreased from 1.395292 to 1.395029. Model was saved\n",
      "Epoch: 290 \tTraining Loss: 1.240231 \tValidation Loss: 1.393935 \t time: 0.3\n",
      "Validation loss decreased from 1.395029 to 1.393935. Model was saved\n",
      "Epoch: 291 \tTraining Loss: 1.240079 \tValidation Loss: 1.394413 \t time: 0.3\n",
      "Epoch: 292 \tTraining Loss: 1.239947 \tValidation Loss: 1.392454 \t time: 0.3\n",
      "Validation loss decreased from 1.393935 to 1.392454. Model was saved\n",
      "Epoch: 293 \tTraining Loss: 1.239876 \tValidation Loss: 1.394534 \t time: 0.3\n",
      "Epoch: 294 \tTraining Loss: 1.240057 \tValidation Loss: 1.391248 \t time: 0.3\n",
      "Validation loss decreased from 1.392454 to 1.391248. Model was saved\n",
      "Epoch: 295 \tTraining Loss: 1.240827 \tValidation Loss: 1.397521 \t time: 0.3\n",
      "Epoch: 296 \tTraining Loss: 1.243401 \tValidation Loss: 1.396102 \t time: 0.3\n",
      "Epoch: 297 \tTraining Loss: 1.249712 \tValidation Loss: 1.408431 \t time: 0.3\n",
      "Epoch: 298 \tTraining Loss: 1.264550 \tValidation Loss: 1.412395 \t time: 0.3\n",
      "Epoch: 299 \tTraining Loss: 1.275319 \tValidation Loss: 1.398398 \t time: 0.3\n",
      "Epoch: 300 \tTraining Loss: 1.253206 \tValidation Loss: 1.409423 \t time: 0.3\n",
      "Epoch: 301 \tTraining Loss: 1.266857 \tValidation Loss: 1.401926 \t time: 0.3\n",
      "Epoch: 302 \tTraining Loss: 1.266549 \tValidation Loss: 1.407441 \t time: 0.3\n",
      "Epoch: 303 \tTraining Loss: 1.267261 \tValidation Loss: 1.390136 \t time: 0.3\n",
      "Validation loss decreased from 1.391248 to 1.390136. Model was saved\n",
      "Epoch: 304 \tTraining Loss: 1.253624 \tValidation Loss: 1.415633 \t time: 0.3\n",
      "Epoch: 305 \tTraining Loss: 1.275795 \tValidation Loss: 1.405152 \t time: 0.3\n",
      "Epoch: 306 \tTraining Loss: 1.259699 \tValidation Loss: 1.414713 \t time: 0.3\n",
      "Epoch: 307 \tTraining Loss: 1.279093 \tValidation Loss: 1.393952 \t time: 0.3\n",
      "Epoch: 308 \tTraining Loss: 1.257962 \tValidation Loss: 1.418610 \t time: 0.3\n",
      "Epoch: 309 \tTraining Loss: 1.288372 \tValidation Loss: 1.389413 \t time: 0.3\n",
      "Validation loss decreased from 1.390136 to 1.389413. Model was saved\n",
      "Epoch: 310 \tTraining Loss: 1.251044 \tValidation Loss: 1.411274 \t time: 0.3\n",
      "Epoch: 311 \tTraining Loss: 1.274538 \tValidation Loss: 1.405873 \t time: 0.3\n",
      "Epoch: 312 \tTraining Loss: 1.262529 \tValidation Loss: 1.392637 \t time: 0.3\n",
      "Epoch: 313 \tTraining Loss: 1.250258 \tValidation Loss: 1.390246 \t time: 0.3\n",
      "Epoch: 314 \tTraining Loss: 1.259590 \tValidation Loss: 1.389365 \t time: 0.3\n",
      "Validation loss decreased from 1.389413 to 1.389365. Model was saved\n",
      "Epoch: 315 \tTraining Loss: 1.256601 \tValidation Loss: 1.394374 \t time: 0.3\n",
      "Epoch: 316 \tTraining Loss: 1.246694 \tValidation Loss: 1.401967 \t time: 0.3\n",
      "Epoch: 317 \tTraining Loss: 1.251789 \tValidation Loss: 1.402073 \t time: 0.3\n",
      "Epoch: 318 \tTraining Loss: 1.247183 \tValidation Loss: 1.396986 \t time: 0.3\n",
      "Epoch: 319 \tTraining Loss: 1.244714 \tValidation Loss: 1.392560 \t time: 0.3\n",
      "Epoch: 320 \tTraining Loss: 1.243824 \tValidation Loss: 1.388207 \t time: 0.3\n",
      "Validation loss decreased from 1.389365 to 1.388207. Model was saved\n",
      "Epoch: 321 \tTraining Loss: 1.241709 \tValidation Loss: 1.390211 \t time: 0.3\n",
      "Epoch: 322 \tTraining Loss: 1.241532 \tValidation Loss: 1.391886 \t time: 0.3\n",
      "Epoch: 323 \tTraining Loss: 1.240837 \tValidation Loss: 1.390879 \t time: 0.3\n",
      "Epoch: 324 \tTraining Loss: 1.239237 \tValidation Loss: 1.388953 \t time: 0.3\n",
      "Epoch: 325 \tTraining Loss: 1.238771 \tValidation Loss: 1.385666 \t time: 0.2\n",
      "Validation loss decreased from 1.388207 to 1.385666. Model was saved\n",
      "Epoch: 326 \tTraining Loss: 1.235577 \tValidation Loss: 1.386047 \t time: 0.3\n",
      "Epoch: 327 \tTraining Loss: 1.234987 \tValidation Loss: 1.386410 \t time: 0.3\n",
      "Epoch: 328 \tTraining Loss: 1.234604 \tValidation Loss: 1.384011 \t time: 0.3\n",
      "Validation loss decreased from 1.385666 to 1.384011. Model was saved\n",
      "Epoch: 329 \tTraining Loss: 1.233317 \tValidation Loss: 1.382045 \t time: 0.3\n",
      "Validation loss decreased from 1.384011 to 1.382045. Model was saved\n",
      "Epoch: 330 \tTraining Loss: 1.233074 \tValidation Loss: 1.380093 \t time: 0.2\n",
      "Validation loss decreased from 1.382045 to 1.380093. Model was saved\n",
      "Epoch: 331 \tTraining Loss: 1.231679 \tValidation Loss: 1.379092 \t time: 0.3\n",
      "Validation loss decreased from 1.380093 to 1.379092. Model was saved\n",
      "Epoch: 332 \tTraining Loss: 1.230938 \tValidation Loss: 1.378137 \t time: 0.3\n",
      "Validation loss decreased from 1.379092 to 1.378137. Model was saved\n",
      "Epoch: 333 \tTraining Loss: 1.230396 \tValidation Loss: 1.377975 \t time: 0.3\n",
      "Validation loss decreased from 1.378137 to 1.377975. Model was saved\n",
      "Epoch: 334 \tTraining Loss: 1.230194 \tValidation Loss: 1.377264 \t time: 0.3\n",
      "Validation loss decreased from 1.377975 to 1.377264. Model was saved\n",
      "Epoch: 335 \tTraining Loss: 1.229532 \tValidation Loss: 1.377277 \t time: 0.3\n",
      "Epoch: 336 \tTraining Loss: 1.229156 \tValidation Loss: 1.377201 \t time: 0.3\n",
      "Validation loss decreased from 1.377264 to 1.377201. Model was saved\n",
      "Epoch: 337 \tTraining Loss: 1.228718 \tValidation Loss: 1.377327 \t time: 0.3\n",
      "Epoch: 338 \tTraining Loss: 1.228257 \tValidation Loss: 1.378031 \t time: 0.3\n",
      "Epoch: 339 \tTraining Loss: 1.227679 \tValidation Loss: 1.379034 \t time: 0.3\n",
      "Epoch: 340 \tTraining Loss: 1.227201 \tValidation Loss: 1.379754 \t time: 0.3\n",
      "Epoch: 341 \tTraining Loss: 1.226700 \tValidation Loss: 1.379115 \t time: 0.3\n",
      "Epoch: 342 \tTraining Loss: 1.226350 \tValidation Loss: 1.377924 \t time: 0.3\n",
      "Epoch: 343 \tTraining Loss: 1.225604 \tValidation Loss: 1.376907 \t time: 0.3\n",
      "Validation loss decreased from 1.377201 to 1.376907. Model was saved\n",
      "Epoch: 344 \tTraining Loss: 1.224818 \tValidation Loss: 1.375340 \t time: 0.3\n",
      "Validation loss decreased from 1.376907 to 1.375340. Model was saved\n",
      "Epoch: 345 \tTraining Loss: 1.224382 \tValidation Loss: 1.374240 \t time: 0.3\n",
      "Validation loss decreased from 1.375340 to 1.374240. Model was saved\n",
      "Epoch: 346 \tTraining Loss: 1.224118 \tValidation Loss: 1.373943 \t time: 0.3\n",
      "Validation loss decreased from 1.374240 to 1.373943. Model was saved\n",
      "Epoch: 347 \tTraining Loss: 1.223761 \tValidation Loss: 1.374135 \t time: 0.3\n",
      "Epoch: 348 \tTraining Loss: 1.223415 \tValidation Loss: 1.374019 \t time: 0.3\n",
      "Epoch: 349 \tTraining Loss: 1.223021 \tValidation Loss: 1.373900 \t time: 0.3\n",
      "Validation loss decreased from 1.373943 to 1.373900. Model was saved\n",
      "Epoch: 350 \tTraining Loss: 1.222643 \tValidation Loss: 1.374315 \t time: 0.3\n",
      "Epoch: 351 \tTraining Loss: 1.222279 \tValidation Loss: 1.375027 \t time: 0.3\n",
      "Epoch: 352 \tTraining Loss: 1.221934 \tValidation Loss: 1.375154 \t time: 0.3\n",
      "Epoch: 353 \tTraining Loss: 1.221662 \tValidation Loss: 1.374743 \t time: 0.3\n",
      "Epoch: 354 \tTraining Loss: 1.221486 \tValidation Loss: 1.374449 \t time: 0.3\n",
      "Epoch: 355 \tTraining Loss: 1.221305 \tValidation Loss: 1.374350 \t time: 0.3\n",
      "Epoch: 356 \tTraining Loss: 1.221103 \tValidation Loss: 1.374105 \t time: 0.3\n",
      "Epoch: 357 \tTraining Loss: 1.220888 \tValidation Loss: 1.373853 \t time: 0.3\n",
      "Validation loss decreased from 1.373900 to 1.373853. Model was saved\n",
      "Epoch: 358 \tTraining Loss: 1.220688 \tValidation Loss: 1.373660 \t time: 0.3\n",
      "Validation loss decreased from 1.373853 to 1.373660. Model was saved\n",
      "Epoch: 359 \tTraining Loss: 1.220495 \tValidation Loss: 1.373328 \t time: 0.3\n",
      "Validation loss decreased from 1.373660 to 1.373328. Model was saved\n",
      "Epoch: 360 \tTraining Loss: 1.220301 \tValidation Loss: 1.372779 \t time: 0.3\n",
      "Validation loss decreased from 1.373328 to 1.372779. Model was saved\n",
      "Epoch: 361 \tTraining Loss: 1.220086 \tValidation Loss: 1.372507 \t time: 0.3\n",
      "Validation loss decreased from 1.372779 to 1.372507. Model was saved\n",
      "Epoch: 362 \tTraining Loss: 1.219890 \tValidation Loss: 1.372771 \t time: 0.3\n",
      "Epoch: 363 \tTraining Loss: 1.219705 \tValidation Loss: 1.372986 \t time: 0.3\n",
      "Epoch: 364 \tTraining Loss: 1.219555 \tValidation Loss: 1.372777 \t time: 0.3\n",
      "Epoch: 365 \tTraining Loss: 1.219404 \tValidation Loss: 1.372538 \t time: 0.3\n",
      "Epoch: 366 \tTraining Loss: 1.219236 \tValidation Loss: 1.372455 \t time: 0.3\n",
      "Validation loss decreased from 1.372507 to 1.372455. Model was saved\n",
      "Epoch: 367 \tTraining Loss: 1.219037 \tValidation Loss: 1.372186 \t time: 0.3\n",
      "Validation loss decreased from 1.372455 to 1.372186. Model was saved\n",
      "Epoch: 368 \tTraining Loss: 1.218868 \tValidation Loss: 1.371648 \t time: 0.3\n",
      "Validation loss decreased from 1.372186 to 1.371648. Model was saved\n",
      "Epoch: 369 \tTraining Loss: 1.218705 \tValidation Loss: 1.371305 \t time: 0.3\n",
      "Validation loss decreased from 1.371648 to 1.371305. Model was saved\n",
      "Epoch: 370 \tTraining Loss: 1.218549 \tValidation Loss: 1.371397 \t time: 0.3\n",
      "Epoch: 371 \tTraining Loss: 1.218399 \tValidation Loss: 1.371633 \t time: 0.3\n",
      "Epoch: 372 \tTraining Loss: 1.218255 \tValidation Loss: 1.371738 \t time: 0.3\n",
      "Epoch: 373 \tTraining Loss: 1.218105 \tValidation Loss: 1.371752 \t time: 0.3\n",
      "Epoch: 374 \tTraining Loss: 1.217947 \tValidation Loss: 1.371794 \t time: 0.3\n",
      "Epoch: 375 \tTraining Loss: 1.217817 \tValidation Loss: 1.371828 \t time: 0.3\n",
      "Epoch: 376 \tTraining Loss: 1.217708 \tValidation Loss: 1.371795 \t time: 0.3\n",
      "Epoch: 377 \tTraining Loss: 1.217604 \tValidation Loss: 1.371670 \t time: 0.3\n",
      "Epoch: 378 \tTraining Loss: 1.217498 \tValidation Loss: 1.371346 \t time: 0.3\n",
      "Epoch: 379 \tTraining Loss: 1.217393 \tValidation Loss: 1.370874 \t time: 0.3\n",
      "Validation loss decreased from 1.371305 to 1.370874. Model was saved\n",
      "Epoch: 380 \tTraining Loss: 1.217279 \tValidation Loss: 1.370570 \t time: 0.3\n",
      "Validation loss decreased from 1.370874 to 1.370570. Model was saved\n",
      "Epoch: 381 \tTraining Loss: 1.217154 \tValidation Loss: 1.370602 \t time: 0.3\n",
      "Epoch: 382 \tTraining Loss: 1.217015 \tValidation Loss: 1.370731 \t time: 0.3\n",
      "Epoch: 383 \tTraining Loss: 1.216895 \tValidation Loss: 1.370813 \t time: 0.3\n",
      "Epoch: 384 \tTraining Loss: 1.216763 \tValidation Loss: 1.371014 \t time: 0.3\n",
      "Epoch: 385 \tTraining Loss: 1.216644 \tValidation Loss: 1.371374 \t time: 0.3\n",
      "Epoch: 386 \tTraining Loss: 1.216540 \tValidation Loss: 1.371636 \t time: 0.3\n",
      "Epoch: 387 \tTraining Loss: 1.216430 \tValidation Loss: 1.371673 \t time: 0.3\n",
      "Epoch: 388 \tTraining Loss: 1.216317 \tValidation Loss: 1.371589 \t time: 0.3\n",
      "Epoch: 389 \tTraining Loss: 1.216199 \tValidation Loss: 1.371495 \t time: 0.3\n",
      "Epoch: 390 \tTraining Loss: 1.216086 \tValidation Loss: 1.371444 \t time: 0.3\n",
      "Epoch: 391 \tTraining Loss: 1.215966 \tValidation Loss: 1.371471 \t time: 0.3\n",
      "Epoch: 392 \tTraining Loss: 1.215850 \tValidation Loss: 1.371542 \t time: 0.3\n",
      "Epoch: 393 \tTraining Loss: 1.215718 \tValidation Loss: 1.371623 \t time: 0.3\n",
      "Epoch: 394 \tTraining Loss: 1.215571 \tValidation Loss: 1.371826 \t time: 0.3\n",
      "Epoch: 395 \tTraining Loss: 1.215437 \tValidation Loss: 1.372169 \t time: 0.3\n",
      "Epoch: 396 \tTraining Loss: 1.215292 \tValidation Loss: 1.372421 \t time: 0.3\n",
      "Epoch: 397 \tTraining Loss: 1.215163 \tValidation Loss: 1.372399 \t time: 0.3\n",
      "Epoch: 398 \tTraining Loss: 1.215051 \tValidation Loss: 1.372300 \t time: 0.3\n",
      "Epoch: 399 \tTraining Loss: 1.214942 \tValidation Loss: 1.372437 \t time: 0.3\n",
      "Epoch: 400 \tTraining Loss: 1.214832 \tValidation Loss: 1.372763 \t time: 0.3\n",
      "Epoch: 401 \tTraining Loss: 1.214725 \tValidation Loss: 1.372982 \t time: 0.3\n",
      "Epoch: 402 \tTraining Loss: 1.214603 \tValidation Loss: 1.373056 \t time: 0.3\n",
      "Epoch: 403 \tTraining Loss: 1.214441 \tValidation Loss: 1.373144 \t time: 0.3\n",
      "Epoch: 404 \tTraining Loss: 1.214319 \tValidation Loss: 1.373260 \t time: 0.3\n",
      "Epoch: 405 \tTraining Loss: 1.214222 \tValidation Loss: 1.373291 \t time: 0.3\n",
      "Epoch: 406 \tTraining Loss: 1.214117 \tValidation Loss: 1.373257 \t time: 0.3\n",
      "Epoch: 407 \tTraining Loss: 1.213998 \tValidation Loss: 1.373283 \t time: 0.3\n",
      "Epoch: 408 \tTraining Loss: 1.213870 \tValidation Loss: 1.373372 \t time: 0.3\n",
      "Epoch: 409 \tTraining Loss: 1.213727 \tValidation Loss: 1.373456 \t time: 0.3\n",
      "Epoch: 410 \tTraining Loss: 1.213571 \tValidation Loss: 1.373531 \t time: 0.3\n",
      "Epoch: 411 \tTraining Loss: 1.213411 \tValidation Loss: 1.373549 \t time: 0.3\n",
      "Epoch: 412 \tTraining Loss: 1.213248 \tValidation Loss: 1.373528 \t time: 0.3\n",
      "Epoch: 413 \tTraining Loss: 1.213092 \tValidation Loss: 1.373608 \t time: 0.3\n",
      "Epoch: 414 \tTraining Loss: 1.212951 \tValidation Loss: 1.373883 \t time: 0.3\n",
      "Epoch: 415 \tTraining Loss: 1.212830 \tValidation Loss: 1.374149 \t time: 0.3\n",
      "Epoch: 416 \tTraining Loss: 1.212708 \tValidation Loss: 1.374311 \t time: 0.3\n",
      "Epoch: 417 \tTraining Loss: 1.212590 \tValidation Loss: 1.374453 \t time: 0.3\n",
      "Epoch: 418 \tTraining Loss: 1.212491 \tValidation Loss: 1.374613 \t time: 0.3\n",
      "Epoch: 419 \tTraining Loss: 1.212394 \tValidation Loss: 1.374696 \t time: 0.3\n",
      "Epoch: 420 \tTraining Loss: 1.212292 \tValidation Loss: 1.374636 \t time: 0.3\n",
      "Epoch: 421 \tTraining Loss: 1.212192 \tValidation Loss: 1.374540 \t time: 0.3\n",
      "Epoch: 422 \tTraining Loss: 1.212082 \tValidation Loss: 1.374552 \t time: 0.3\n",
      "Epoch: 423 \tTraining Loss: 1.211976 \tValidation Loss: 1.374633 \t time: 0.3\n",
      "Epoch: 424 \tTraining Loss: 1.211854 \tValidation Loss: 1.374700 \t time: 0.3\n",
      "Epoch: 425 \tTraining Loss: 1.211729 \tValidation Loss: 1.374815 \t time: 0.3\n",
      "Epoch: 426 \tTraining Loss: 1.211623 \tValidation Loss: 1.374975 \t time: 0.3\n",
      "Epoch: 427 \tTraining Loss: 1.211527 \tValidation Loss: 1.375140 \t time: 0.3\n",
      "Epoch: 428 \tTraining Loss: 1.211406 \tValidation Loss: 1.375398 \t time: 0.3\n",
      "Epoch: 429 \tTraining Loss: 1.211254 \tValidation Loss: 1.375652 \t time: 0.3\n",
      "Epoch: 430 \tTraining Loss: 1.211154 \tValidation Loss: 1.375852 \t time: 0.3\n",
      "Epoch: 431 \tTraining Loss: 1.211071 \tValidation Loss: 1.376079 \t time: 0.3\n",
      "Epoch: 432 \tTraining Loss: 1.210978 \tValidation Loss: 1.376248 \t time: 0.3\n",
      "Epoch: 433 \tTraining Loss: 1.210861 \tValidation Loss: 1.376236 \t time: 0.3\n",
      "Epoch: 434 \tTraining Loss: 1.210734 \tValidation Loss: 1.376085 \t time: 0.3\n",
      "Epoch: 435 \tTraining Loss: 1.210609 \tValidation Loss: 1.375918 \t time: 0.3\n",
      "Epoch: 436 \tTraining Loss: 1.210427 \tValidation Loss: 1.375918 \t time: 0.3\n",
      "Epoch: 437 \tTraining Loss: 1.210306 \tValidation Loss: 1.375881 \t time: 0.3\n",
      "Epoch: 438 \tTraining Loss: 1.210176 \tValidation Loss: 1.375733 \t time: 0.3\n",
      "Epoch: 439 \tTraining Loss: 1.210061 \tValidation Loss: 1.375646 \t time: 0.3\n",
      "Epoch: 440 \tTraining Loss: 1.209939 \tValidation Loss: 1.375605 \t time: 0.3\n",
      "Epoch: 441 \tTraining Loss: 1.209748 \tValidation Loss: 1.375505 \t time: 0.3\n",
      "Epoch: 442 \tTraining Loss: 1.209652 \tValidation Loss: 1.375336 \t time: 0.3\n",
      "Epoch: 443 \tTraining Loss: 1.209579 \tValidation Loss: 1.375119 \t time: 0.3\n",
      "Epoch: 444 \tTraining Loss: 1.209475 \tValidation Loss: 1.375177 \t time: 0.3\n",
      "Epoch: 445 \tTraining Loss: 1.209349 \tValidation Loss: 1.375285 \t time: 0.3\n",
      "Epoch: 446 \tTraining Loss: 1.209254 \tValidation Loss: 1.375371 \t time: 0.3\n",
      "Epoch: 447 \tTraining Loss: 1.209152 \tValidation Loss: 1.375528 \t time: 0.3\n",
      "Epoch: 448 \tTraining Loss: 1.209000 \tValidation Loss: 1.375785 \t time: 0.3\n",
      "Epoch: 449 \tTraining Loss: 1.208844 \tValidation Loss: 1.376058 \t time: 0.3\n",
      "Epoch: 450 \tTraining Loss: 1.208751 \tValidation Loss: 1.376240 \t time: 0.3\n",
      "Epoch: 451 \tTraining Loss: 1.208669 \tValidation Loss: 1.376327 \t time: 0.3\n",
      "Epoch: 452 \tTraining Loss: 1.208569 \tValidation Loss: 1.376343 \t time: 0.3\n",
      "Epoch: 453 \tTraining Loss: 1.208458 \tValidation Loss: 1.376207 \t time: 0.3\n",
      "Epoch: 454 \tTraining Loss: 1.208363 \tValidation Loss: 1.375953 \t time: 0.3\n",
      "Epoch: 455 \tTraining Loss: 1.208287 \tValidation Loss: 1.375784 \t time: 0.3\n",
      "Epoch: 456 \tTraining Loss: 1.208208 \tValidation Loss: 1.375789 \t time: 0.3\n",
      "Epoch: 457 \tTraining Loss: 1.208131 \tValidation Loss: 1.375770 \t time: 0.3\n",
      "Epoch: 458 \tTraining Loss: 1.208056 \tValidation Loss: 1.375507 \t time: 0.3\n",
      "Epoch: 459 \tTraining Loss: 1.207982 \tValidation Loss: 1.375080 \t time: 0.3\n",
      "Epoch: 460 \tTraining Loss: 1.207907 \tValidation Loss: 1.374731 \t time: 0.3\n",
      "Epoch: 461 \tTraining Loss: 1.207834 \tValidation Loss: 1.374589 \t time: 0.3\n",
      "Epoch: 462 \tTraining Loss: 1.207768 \tValidation Loss: 1.374655 \t time: 0.3\n",
      "Epoch: 463 \tTraining Loss: 1.207686 \tValidation Loss: 1.374866 \t time: 0.3\n",
      "Epoch: 464 \tTraining Loss: 1.207605 \tValidation Loss: 1.375190 \t time: 0.3\n",
      "Epoch: 465 \tTraining Loss: 1.207536 \tValidation Loss: 1.375436 \t time: 0.3\n",
      "Epoch: 466 \tTraining Loss: 1.207471 \tValidation Loss: 1.375467 \t time: 0.3\n",
      "Epoch: 467 \tTraining Loss: 1.207404 \tValidation Loss: 1.375387 \t time: 0.3\n",
      "Epoch: 468 \tTraining Loss: 1.207332 \tValidation Loss: 1.375336 \t time: 0.3\n",
      "Epoch: 469 \tTraining Loss: 1.207250 \tValidation Loss: 1.375272 \t time: 0.3\n",
      "Epoch: 470 \tTraining Loss: 1.207162 \tValidation Loss: 1.375082 \t time: 0.3\n",
      "Epoch: 471 \tTraining Loss: 1.207069 \tValidation Loss: 1.374848 \t time: 0.3\n",
      "Epoch: 472 \tTraining Loss: 1.206978 \tValidation Loss: 1.374724 \t time: 0.3\n",
      "Epoch: 473 \tTraining Loss: 1.206891 \tValidation Loss: 1.374735 \t time: 0.3\n",
      "Epoch: 474 \tTraining Loss: 1.206789 \tValidation Loss: 1.374839 \t time: 0.3\n",
      "Epoch: 475 \tTraining Loss: 1.206703 \tValidation Loss: 1.374845 \t time: 0.3\n",
      "Epoch: 476 \tTraining Loss: 1.206617 \tValidation Loss: 1.374971 \t time: 0.3\n",
      "Epoch: 477 \tTraining Loss: 1.206520 \tValidation Loss: 1.375121 \t time: 0.3\n",
      "Epoch: 478 \tTraining Loss: 1.206419 \tValidation Loss: 1.375086 \t time: 0.3\n",
      "Epoch: 479 \tTraining Loss: 1.206281 \tValidation Loss: 1.375017 \t time: 0.3\n",
      "Epoch: 480 \tTraining Loss: 1.206113 \tValidation Loss: 1.375016 \t time: 0.3\n",
      "Epoch: 481 \tTraining Loss: 1.205933 \tValidation Loss: 1.375055 \t time: 0.3\n",
      "Epoch: 482 \tTraining Loss: 1.205760 \tValidation Loss: 1.375046 \t time: 0.3\n",
      "Epoch: 483 \tTraining Loss: 1.205626 \tValidation Loss: 1.374832 \t time: 0.3\n",
      "Epoch: 484 \tTraining Loss: 1.205518 \tValidation Loss: 1.374392 \t time: 0.3\n",
      "Epoch: 485 \tTraining Loss: 1.205415 \tValidation Loss: 1.374438 \t time: 0.3\n",
      "Epoch: 486 \tTraining Loss: 1.205343 \tValidation Loss: 1.374724 \t time: 0.3\n",
      "Epoch: 487 \tTraining Loss: 1.205271 \tValidation Loss: 1.374751 \t time: 0.3\n",
      "Epoch: 488 \tTraining Loss: 1.205168 \tValidation Loss: 1.374775 \t time: 0.3\n",
      "Epoch: 489 \tTraining Loss: 1.205068 \tValidation Loss: 1.374581 \t time: 0.3\n",
      "Epoch: 490 \tTraining Loss: 1.204977 \tValidation Loss: 1.374047 \t time: 0.3\n",
      "Epoch: 491 \tTraining Loss: 1.204881 \tValidation Loss: 1.373741 \t time: 0.3\n",
      "Epoch: 492 \tTraining Loss: 1.204795 \tValidation Loss: 1.373238 \t time: 0.3\n",
      "Epoch: 493 \tTraining Loss: 1.204659 \tValidation Loss: 1.372477 \t time: 0.3\n",
      "Epoch: 494 \tTraining Loss: 1.204558 \tValidation Loss: 1.372201 \t time: 0.3\n",
      "Epoch: 495 \tTraining Loss: 1.204464 \tValidation Loss: 1.372046 \t time: 0.3\n",
      "Epoch: 496 \tTraining Loss: 1.204387 \tValidation Loss: 1.371808 \t time: 0.3\n",
      "Epoch: 497 \tTraining Loss: 1.204285 \tValidation Loss: 1.371949 \t time: 0.3\n",
      "Epoch: 498 \tTraining Loss: 1.204174 \tValidation Loss: 1.372185 \t time: 0.3\n",
      "Epoch: 499 \tTraining Loss: 1.204086 \tValidation Loss: 1.372191 \t time: 0.3\n",
      "Epoch: 500 \tTraining Loss: 1.204004 \tValidation Loss: 1.372105 \t time: 0.3\n",
      "Epoch: 501 \tTraining Loss: 1.203908 \tValidation Loss: 1.371918 \t time: 0.3\n",
      "Epoch: 502 \tTraining Loss: 1.203817 \tValidation Loss: 1.371735 \t time: 0.3\n",
      "Epoch: 503 \tTraining Loss: 1.203731 \tValidation Loss: 1.372070 \t time: 0.3\n",
      "Epoch: 504 \tTraining Loss: 1.203619 \tValidation Loss: 1.371996 \t time: 0.3\n",
      "Epoch: 505 \tTraining Loss: 1.203537 \tValidation Loss: 1.371588 \t time: 0.3\n",
      "Epoch: 506 \tTraining Loss: 1.203438 \tValidation Loss: 1.371557 \t time: 0.3\n",
      "Epoch: 507 \tTraining Loss: 1.203332 \tValidation Loss: 1.371626 \t time: 0.3\n",
      "Epoch: 508 \tTraining Loss: 1.203248 \tValidation Loss: 1.371436 \t time: 0.3\n",
      "Epoch: 509 \tTraining Loss: 1.203156 \tValidation Loss: 1.371188 \t time: 0.3\n",
      "Epoch: 510 \tTraining Loss: 1.203087 \tValidation Loss: 1.371241 \t time: 0.3\n",
      "Epoch: 511 \tTraining Loss: 1.202991 \tValidation Loss: 1.371221 \t time: 0.3\n",
      "Epoch: 512 \tTraining Loss: 1.202901 \tValidation Loss: 1.370902 \t time: 0.3\n",
      "Epoch: 513 \tTraining Loss: 1.202808 \tValidation Loss: 1.370800 \t time: 0.3\n",
      "Epoch: 514 \tTraining Loss: 1.202718 \tValidation Loss: 1.370651 \t time: 0.3\n",
      "Epoch: 515 \tTraining Loss: 1.202638 \tValidation Loss: 1.370275 \t time: 0.3\n",
      "Validation loss decreased from 1.370570 to 1.370275. Model was saved\n",
      "Epoch: 516 \tTraining Loss: 1.202551 \tValidation Loss: 1.369531 \t time: 0.3\n",
      "Validation loss decreased from 1.370275 to 1.369531. Model was saved\n",
      "Epoch: 517 \tTraining Loss: 1.202427 \tValidation Loss: 1.368752 \t time: 0.3\n",
      "Validation loss decreased from 1.369531 to 1.368752. Model was saved\n",
      "Epoch: 518 \tTraining Loss: 1.202379 \tValidation Loss: 1.368443 \t time: 0.3\n",
      "Validation loss decreased from 1.368752 to 1.368443. Model was saved\n",
      "Epoch: 519 \tTraining Loss: 1.202317 \tValidation Loss: 1.368668 \t time: 0.3\n",
      "Epoch: 520 \tTraining Loss: 1.202231 \tValidation Loss: 1.368869 \t time: 0.3\n",
      "Epoch: 521 \tTraining Loss: 1.202183 \tValidation Loss: 1.368506 \t time: 0.3\n",
      "Epoch: 522 \tTraining Loss: 1.202104 \tValidation Loss: 1.368208 \t time: 0.3\n",
      "Validation loss decreased from 1.368443 to 1.368208. Model was saved\n",
      "Epoch: 523 \tTraining Loss: 1.202021 \tValidation Loss: 1.368159 \t time: 0.3\n",
      "Validation loss decreased from 1.368208 to 1.368159. Model was saved\n",
      "Epoch: 524 \tTraining Loss: 1.201931 \tValidation Loss: 1.367907 \t time: 0.3\n",
      "Validation loss decreased from 1.368159 to 1.367907. Model was saved\n",
      "Epoch: 525 \tTraining Loss: 1.201836 \tValidation Loss: 1.367700 \t time: 0.3\n",
      "Validation loss decreased from 1.367907 to 1.367700. Model was saved\n",
      "Epoch: 526 \tTraining Loss: 1.201708 \tValidation Loss: 1.368010 \t time: 0.3\n",
      "Epoch: 527 \tTraining Loss: 1.201620 \tValidation Loss: 1.368289 \t time: 0.3\n",
      "Epoch: 528 \tTraining Loss: 1.201563 \tValidation Loss: 1.368354 \t time: 0.3\n",
      "Epoch: 529 \tTraining Loss: 1.201493 \tValidation Loss: 1.368486 \t time: 0.3\n",
      "Epoch: 530 \tTraining Loss: 1.201427 \tValidation Loss: 1.368652 \t time: 0.3\n",
      "Epoch: 531 \tTraining Loss: 1.201366 \tValidation Loss: 1.368565 \t time: 0.3\n",
      "Epoch: 532 \tTraining Loss: 1.201306 \tValidation Loss: 1.368380 \t time: 0.3\n",
      "Epoch: 533 \tTraining Loss: 1.201243 \tValidation Loss: 1.368512 \t time: 0.3\n",
      "Epoch: 534 \tTraining Loss: 1.201162 \tValidation Loss: 1.368724 \t time: 0.3\n",
      "Epoch: 535 \tTraining Loss: 1.201047 \tValidation Loss: 1.368654 \t time: 0.3\n",
      "Epoch: 536 \tTraining Loss: 1.200907 \tValidation Loss: 1.368523 \t time: 0.3\n",
      "Epoch: 537 \tTraining Loss: 1.200807 \tValidation Loss: 1.368412 \t time: 0.3\n",
      "Epoch: 538 \tTraining Loss: 1.200746 \tValidation Loss: 1.368353 \t time: 0.3\n",
      "Epoch: 539 \tTraining Loss: 1.200708 \tValidation Loss: 1.368338 \t time: 0.3\n",
      "Epoch: 540 \tTraining Loss: 1.200665 \tValidation Loss: 1.368437 \t time: 0.3\n",
      "Epoch: 541 \tTraining Loss: 1.200621 \tValidation Loss: 1.368575 \t time: 0.3\n",
      "Epoch: 542 \tTraining Loss: 1.200554 \tValidation Loss: 1.368660 \t time: 0.3\n",
      "Epoch: 543 \tTraining Loss: 1.200498 \tValidation Loss: 1.368695 \t time: 0.3\n",
      "Epoch: 544 \tTraining Loss: 1.200448 \tValidation Loss: 1.368694 \t time: 0.3\n",
      "Epoch: 545 \tTraining Loss: 1.200389 \tValidation Loss: 1.368770 \t time: 0.3\n",
      "Epoch: 546 \tTraining Loss: 1.200338 \tValidation Loss: 1.368783 \t time: 0.3\n",
      "Epoch: 547 \tTraining Loss: 1.200285 \tValidation Loss: 1.368618 \t time: 0.3\n",
      "Epoch: 548 \tTraining Loss: 1.200218 \tValidation Loss: 1.368330 \t time: 0.3\n",
      "Epoch: 549 \tTraining Loss: 1.200139 \tValidation Loss: 1.367863 \t time: 0.3\n",
      "Epoch: 550 \tTraining Loss: 1.200020 \tValidation Loss: 1.367839 \t time: 0.3\n",
      "Epoch: 551 \tTraining Loss: 1.199943 \tValidation Loss: 1.367846 \t time: 0.3\n",
      "Epoch: 552 \tTraining Loss: 1.199885 \tValidation Loss: 1.368402 \t time: 0.3\n",
      "Epoch: 553 \tTraining Loss: 1.199822 \tValidation Loss: 1.368436 \t time: 0.3\n",
      "Epoch: 554 \tTraining Loss: 1.199752 \tValidation Loss: 1.368769 \t time: 0.3\n",
      "Epoch: 555 \tTraining Loss: 1.199771 \tValidation Loss: 1.368878 \t time: 0.3\n",
      "Epoch: 556 \tTraining Loss: 1.199562 \tValidation Loss: 1.368856 \t time: 0.3\n",
      "Epoch: 557 \tTraining Loss: 1.199493 \tValidation Loss: 1.369085 \t time: 0.3\n",
      "Epoch: 558 \tTraining Loss: 1.199416 \tValidation Loss: 1.368906 \t time: 0.3\n",
      "Epoch: 559 \tTraining Loss: 1.199323 \tValidation Loss: 1.368896 \t time: 0.3\n",
      "Epoch: 560 \tTraining Loss: 1.199266 \tValidation Loss: 1.367668 \t time: 0.3\n",
      "Validation loss decreased from 1.367700 to 1.367668. Model was saved\n",
      "Epoch: 561 \tTraining Loss: 1.199180 \tValidation Loss: 1.368021 \t time: 0.3\n",
      "Epoch: 562 \tTraining Loss: 1.199093 \tValidation Loss: 1.367217 \t time: 0.3\n",
      "Validation loss decreased from 1.367668 to 1.367217. Model was saved\n",
      "Epoch: 563 \tTraining Loss: 1.199032 \tValidation Loss: 1.368030 \t time: 0.3\n",
      "Epoch: 564 \tTraining Loss: 1.198963 \tValidation Loss: 1.367433 \t time: 0.3\n",
      "Epoch: 565 \tTraining Loss: 1.198895 \tValidation Loss: 1.367888 \t time: 0.3\n",
      "Epoch: 566 \tTraining Loss: 1.198814 \tValidation Loss: 1.367807 \t time: 0.3\n",
      "Epoch: 567 \tTraining Loss: 1.198731 \tValidation Loss: 1.367211 \t time: 0.3\n",
      "Validation loss decreased from 1.367217 to 1.367211. Model was saved\n",
      "Epoch: 568 \tTraining Loss: 1.198666 \tValidation Loss: 1.368210 \t time: 0.3\n",
      "Epoch: 569 \tTraining Loss: 1.198614 \tValidation Loss: 1.367432 \t time: 0.3\n",
      "Epoch: 570 \tTraining Loss: 1.198562 \tValidation Loss: 1.368988 \t time: 0.3\n",
      "Epoch: 571 \tTraining Loss: 1.198497 \tValidation Loss: 1.368130 \t time: 0.3\n",
      "Epoch: 572 \tTraining Loss: 1.198402 \tValidation Loss: 1.367994 \t time: 0.3\n",
      "Epoch: 573 \tTraining Loss: 1.198295 \tValidation Loss: 1.367613 \t time: 0.3\n",
      "Epoch: 574 \tTraining Loss: 1.198216 \tValidation Loss: 1.366789 \t time: 0.3\n",
      "Validation loss decreased from 1.367211 to 1.366789. Model was saved\n",
      "Epoch: 575 \tTraining Loss: 1.198153 \tValidation Loss: 1.367276 \t time: 0.3\n",
      "Epoch: 576 \tTraining Loss: 1.198110 \tValidation Loss: 1.366881 \t time: 0.3\n",
      "Epoch: 577 \tTraining Loss: 1.198072 \tValidation Loss: 1.366662 \t time: 0.3\n",
      "Validation loss decreased from 1.366789 to 1.366662. Model was saved\n",
      "Epoch: 578 \tTraining Loss: 1.198020 \tValidation Loss: 1.366471 \t time: 0.3\n",
      "Validation loss decreased from 1.366662 to 1.366471. Model was saved\n",
      "Epoch: 579 \tTraining Loss: 1.197957 \tValidation Loss: 1.366809 \t time: 0.3\n",
      "Epoch: 580 \tTraining Loss: 1.197909 \tValidation Loss: 1.366621 \t time: 0.3\n",
      "Epoch: 581 \tTraining Loss: 1.197875 \tValidation Loss: 1.366542 \t time: 0.3\n",
      "Epoch: 582 \tTraining Loss: 1.197840 \tValidation Loss: 1.366617 \t time: 0.3\n",
      "Epoch: 583 \tTraining Loss: 1.197801 \tValidation Loss: 1.366307 \t time: 0.3\n",
      "Validation loss decreased from 1.366471 to 1.366307. Model was saved\n",
      "Epoch: 584 \tTraining Loss: 1.197757 \tValidation Loss: 1.367012 \t time: 0.3\n",
      "Epoch: 585 \tTraining Loss: 1.197708 \tValidation Loss: 1.366901 \t time: 0.3\n",
      "Epoch: 586 \tTraining Loss: 1.197658 \tValidation Loss: 1.367049 \t time: 0.3\n",
      "Epoch: 587 \tTraining Loss: 1.197621 \tValidation Loss: 1.367342 \t time: 0.3\n",
      "Epoch: 588 \tTraining Loss: 1.197590 \tValidation Loss: 1.367094 \t time: 0.3\n",
      "Epoch: 589 \tTraining Loss: 1.197551 \tValidation Loss: 1.367204 \t time: 0.3\n",
      "Epoch: 590 \tTraining Loss: 1.197504 \tValidation Loss: 1.367116 \t time: 0.3\n",
      "Epoch: 591 \tTraining Loss: 1.197409 \tValidation Loss: 1.367005 \t time: 0.2\n",
      "Epoch: 592 \tTraining Loss: 1.197340 \tValidation Loss: 1.367054 \t time: 0.3\n",
      "Epoch: 593 \tTraining Loss: 1.197281 \tValidation Loss: 1.367259 \t time: 0.3\n",
      "Epoch: 594 \tTraining Loss: 1.197220 \tValidation Loss: 1.367078 \t time: 0.3\n",
      "Epoch: 595 \tTraining Loss: 1.197175 \tValidation Loss: 1.366620 \t time: 0.3\n",
      "Epoch: 596 \tTraining Loss: 1.197124 \tValidation Loss: 1.366613 \t time: 0.3\n",
      "Epoch: 597 \tTraining Loss: 1.197061 \tValidation Loss: 1.366166 \t time: 0.3\n",
      "Validation loss decreased from 1.366307 to 1.366166. Model was saved\n",
      "Epoch: 598 \tTraining Loss: 1.197008 \tValidation Loss: 1.366207 \t time: 0.3\n",
      "Epoch: 599 \tTraining Loss: 1.196965 \tValidation Loss: 1.365959 \t time: 0.3\n",
      "Validation loss decreased from 1.366166 to 1.365959. Model was saved\n",
      "Epoch: 600 \tTraining Loss: 1.196928 \tValidation Loss: 1.365733 \t time: 0.3\n",
      "Validation loss decreased from 1.365959 to 1.365733. Model was saved\n",
      "Epoch: 601 \tTraining Loss: 1.196895 \tValidation Loss: 1.365911 \t time: 0.3\n",
      "Epoch: 602 \tTraining Loss: 1.196860 \tValidation Loss: 1.365828 \t time: 0.3\n",
      "Epoch: 603 \tTraining Loss: 1.196820 \tValidation Loss: 1.366009 \t time: 0.3\n",
      "Epoch: 604 \tTraining Loss: 1.196772 \tValidation Loss: 1.366009 \t time: 0.3\n",
      "Epoch: 605 \tTraining Loss: 1.196729 \tValidation Loss: 1.365824 \t time: 0.3\n",
      "Epoch: 606 \tTraining Loss: 1.196693 \tValidation Loss: 1.365740 \t time: 0.3\n",
      "Epoch: 607 \tTraining Loss: 1.196665 \tValidation Loss: 1.365613 \t time: 0.3\n",
      "Validation loss decreased from 1.365733 to 1.365613. Model was saved\n",
      "Epoch: 608 \tTraining Loss: 1.196632 \tValidation Loss: 1.365605 \t time: 0.3\n",
      "Validation loss decreased from 1.365613 to 1.365605. Model was saved\n",
      "Epoch: 609 \tTraining Loss: 1.196602 \tValidation Loss: 1.365464 \t time: 0.3\n",
      "Validation loss decreased from 1.365605 to 1.365464. Model was saved\n",
      "Epoch: 610 \tTraining Loss: 1.196574 \tValidation Loss: 1.365413 \t time: 0.3\n",
      "Validation loss decreased from 1.365464 to 1.365413. Model was saved\n",
      "Epoch: 611 \tTraining Loss: 1.196543 \tValidation Loss: 1.365338 \t time: 0.3\n",
      "Validation loss decreased from 1.365413 to 1.365338. Model was saved\n",
      "Epoch: 612 \tTraining Loss: 1.196513 \tValidation Loss: 1.365484 \t time: 0.3\n",
      "Epoch: 613 \tTraining Loss: 1.196481 \tValidation Loss: 1.365671 \t time: 0.3\n",
      "Epoch: 614 \tTraining Loss: 1.196448 \tValidation Loss: 1.365525 \t time: 0.3\n",
      "Epoch: 615 \tTraining Loss: 1.196411 \tValidation Loss: 1.365508 \t time: 0.3\n",
      "Epoch: 616 \tTraining Loss: 1.196371 \tValidation Loss: 1.365426 \t time: 0.3\n",
      "Epoch: 617 \tTraining Loss: 1.196327 \tValidation Loss: 1.365423 \t time: 0.3\n",
      "Epoch: 618 \tTraining Loss: 1.196288 \tValidation Loss: 1.365544 \t time: 0.3\n",
      "Epoch: 619 \tTraining Loss: 1.196251 \tValidation Loss: 1.365461 \t time: 0.3\n",
      "Epoch: 620 \tTraining Loss: 1.196215 \tValidation Loss: 1.365426 \t time: 0.3\n",
      "Epoch: 621 \tTraining Loss: 1.196177 \tValidation Loss: 1.365321 \t time: 0.3\n",
      "Validation loss decreased from 1.365338 to 1.365321. Model was saved\n",
      "Epoch: 622 \tTraining Loss: 1.196128 \tValidation Loss: 1.365316 \t time: 0.3\n",
      "Validation loss decreased from 1.365321 to 1.365316. Model was saved\n",
      "Epoch: 623 \tTraining Loss: 1.196047 \tValidation Loss: 1.365317 \t time: 0.3\n",
      "Epoch: 624 \tTraining Loss: 1.195991 \tValidation Loss: 1.365356 \t time: 0.3\n",
      "Epoch: 625 \tTraining Loss: 1.195951 \tValidation Loss: 1.365301 \t time: 0.3\n",
      "Validation loss decreased from 1.365316 to 1.365301. Model was saved\n",
      "Epoch: 626 \tTraining Loss: 1.195915 \tValidation Loss: 1.365310 \t time: 0.3\n",
      "Epoch: 627 \tTraining Loss: 1.195879 \tValidation Loss: 1.365355 \t time: 0.3\n",
      "Epoch: 628 \tTraining Loss: 1.195832 \tValidation Loss: 1.365209 \t time: 0.3\n",
      "Validation loss decreased from 1.365301 to 1.365209. Model was saved\n",
      "Epoch: 629 \tTraining Loss: 1.195769 \tValidation Loss: 1.365088 \t time: 0.3\n",
      "Validation loss decreased from 1.365209 to 1.365088. Model was saved\n",
      "Epoch: 630 \tTraining Loss: 1.195653 \tValidation Loss: 1.364885 \t time: 0.3\n",
      "Validation loss decreased from 1.365088 to 1.364885. Model was saved\n",
      "Epoch: 631 \tTraining Loss: 1.195512 \tValidation Loss: 1.364601 \t time: 0.3\n",
      "Validation loss decreased from 1.364885 to 1.364601. Model was saved\n",
      "Epoch: 632 \tTraining Loss: 1.195459 \tValidation Loss: 1.364481 \t time: 0.3\n",
      "Validation loss decreased from 1.364601 to 1.364481. Model was saved\n",
      "Epoch: 633 \tTraining Loss: 1.195424 \tValidation Loss: 1.364286 \t time: 0.3\n",
      "Validation loss decreased from 1.364481 to 1.364286. Model was saved\n",
      "Epoch: 634 \tTraining Loss: 1.195379 \tValidation Loss: 1.364115 \t time: 0.3\n",
      "Validation loss decreased from 1.364286 to 1.364115. Model was saved\n",
      "Epoch: 635 \tTraining Loss: 1.195328 \tValidation Loss: 1.363919 \t time: 0.3\n",
      "Validation loss decreased from 1.364115 to 1.363919. Model was saved\n",
      "Epoch: 636 \tTraining Loss: 1.195272 \tValidation Loss: 1.364141 \t time: 0.3\n",
      "Epoch: 637 \tTraining Loss: 1.195220 \tValidation Loss: 1.364360 \t time: 0.3\n",
      "Epoch: 638 \tTraining Loss: 1.195174 \tValidation Loss: 1.364300 \t time: 0.4\n",
      "Epoch: 639 \tTraining Loss: 1.195129 \tValidation Loss: 1.364200 \t time: 0.3\n",
      "Epoch: 640 \tTraining Loss: 1.195092 \tValidation Loss: 1.364071 \t time: 0.3\n",
      "Epoch: 641 \tTraining Loss: 1.195049 \tValidation Loss: 1.363896 \t time: 0.3\n",
      "Validation loss decreased from 1.363919 to 1.363896. Model was saved\n",
      "Epoch: 642 \tTraining Loss: 1.195009 \tValidation Loss: 1.363610 \t time: 0.3\n",
      "Validation loss decreased from 1.363896 to 1.363610. Model was saved\n",
      "Epoch: 643 \tTraining Loss: 1.194980 \tValidation Loss: 1.363474 \t time: 0.3\n",
      "Validation loss decreased from 1.363610 to 1.363474. Model was saved\n",
      "Epoch: 644 \tTraining Loss: 1.194950 \tValidation Loss: 1.363402 \t time: 0.3\n",
      "Validation loss decreased from 1.363474 to 1.363402. Model was saved\n",
      "Epoch: 645 \tTraining Loss: 1.194919 \tValidation Loss: 1.363344 \t time: 0.3\n",
      "Validation loss decreased from 1.363402 to 1.363344. Model was saved\n",
      "Epoch: 646 \tTraining Loss: 1.194893 \tValidation Loss: 1.363350 \t time: 0.3\n",
      "Epoch: 647 \tTraining Loss: 1.194864 \tValidation Loss: 1.363365 \t time: 0.3\n",
      "Epoch: 648 \tTraining Loss: 1.194832 \tValidation Loss: 1.363368 \t time: 0.3\n",
      "Epoch: 649 \tTraining Loss: 1.194798 \tValidation Loss: 1.363326 \t time: 0.3\n",
      "Validation loss decreased from 1.363344 to 1.363326. Model was saved\n",
      "Epoch: 650 \tTraining Loss: 1.194737 \tValidation Loss: 1.363298 \t time: 0.3\n",
      "Validation loss decreased from 1.363326 to 1.363298. Model was saved\n",
      "Epoch: 651 \tTraining Loss: 1.194672 \tValidation Loss: 1.363340 \t time: 0.3\n",
      "Epoch: 652 \tTraining Loss: 1.194645 \tValidation Loss: 1.363505 \t time: 0.3\n",
      "Epoch: 653 \tTraining Loss: 1.194615 \tValidation Loss: 1.363629 \t time: 0.3\n",
      "Epoch: 654 \tTraining Loss: 1.194579 \tValidation Loss: 1.363597 \t time: 0.3\n",
      "Epoch: 655 \tTraining Loss: 1.194544 \tValidation Loss: 1.363603 \t time: 0.3\n",
      "Epoch: 656 \tTraining Loss: 1.194511 \tValidation Loss: 1.363746 \t time: 0.3\n",
      "Epoch: 657 \tTraining Loss: 1.194475 \tValidation Loss: 1.363876 \t time: 0.3\n",
      "Epoch: 658 \tTraining Loss: 1.194437 \tValidation Loss: 1.363856 \t time: 0.3\n",
      "Epoch: 659 \tTraining Loss: 1.194401 \tValidation Loss: 1.363711 \t time: 0.3\n",
      "Epoch: 660 \tTraining Loss: 1.194366 \tValidation Loss: 1.363547 \t time: 0.3\n",
      "Epoch: 661 \tTraining Loss: 1.194328 \tValidation Loss: 1.363421 \t time: 0.3\n",
      "Epoch: 662 \tTraining Loss: 1.194283 \tValidation Loss: 1.363365 \t time: 0.3\n",
      "Epoch: 663 \tTraining Loss: 1.194226 \tValidation Loss: 1.363287 \t time: 0.3\n",
      "Validation loss decreased from 1.363298 to 1.363287. Model was saved\n",
      "Epoch: 664 \tTraining Loss: 1.194190 \tValidation Loss: 1.363079 \t time: 0.3\n",
      "Validation loss decreased from 1.363287 to 1.363079. Model was saved\n",
      "Epoch: 665 \tTraining Loss: 1.194156 \tValidation Loss: 1.362845 \t time: 0.3\n",
      "Validation loss decreased from 1.363079 to 1.362845. Model was saved\n",
      "Epoch: 666 \tTraining Loss: 1.194108 \tValidation Loss: 1.363180 \t time: 0.3\n",
      "Epoch: 667 \tTraining Loss: 1.194107 \tValidation Loss: 1.362463 \t time: 0.3\n",
      "Validation loss decreased from 1.362845 to 1.362463. Model was saved\n",
      "Epoch: 668 \tTraining Loss: 1.194132 \tValidation Loss: 1.363547 \t time: 0.3\n",
      "Epoch: 669 \tTraining Loss: 1.194198 \tValidation Loss: 1.362683 \t time: 0.3\n",
      "Epoch: 670 \tTraining Loss: 1.194307 \tValidation Loss: 1.364102 \t time: 0.3\n",
      "Epoch: 671 \tTraining Loss: 1.194351 \tValidation Loss: 1.362067 \t time: 0.3\n",
      "Validation loss decreased from 1.362463 to 1.362067. Model was saved\n",
      "Epoch: 672 \tTraining Loss: 1.194093 \tValidation Loss: 1.362694 \t time: 0.3\n",
      "Epoch: 673 \tTraining Loss: 1.193911 \tValidation Loss: 1.363591 \t time: 0.3\n",
      "Epoch: 674 \tTraining Loss: 1.193954 \tValidation Loss: 1.362535 \t time: 0.3\n",
      "Epoch: 675 \tTraining Loss: 1.193798 \tValidation Loss: 1.363133 \t time: 0.3\n",
      "Epoch: 676 \tTraining Loss: 1.193716 \tValidation Loss: 1.364304 \t time: 0.3\n",
      "Epoch: 677 \tTraining Loss: 1.193778 \tValidation Loss: 1.363823 \t time: 0.3\n",
      "Epoch: 678 \tTraining Loss: 1.193629 \tValidation Loss: 1.363499 \t time: 0.3\n",
      "Epoch: 679 \tTraining Loss: 1.193640 \tValidation Loss: 1.364575 \t time: 0.3\n",
      "Epoch: 680 \tTraining Loss: 1.193586 \tValidation Loss: 1.364622 \t time: 0.3\n",
      "Epoch: 681 \tTraining Loss: 1.193511 \tValidation Loss: 1.364648 \t time: 0.3\n",
      "Epoch: 682 \tTraining Loss: 1.193510 \tValidation Loss: 1.364065 \t time: 0.3\n",
      "Epoch: 683 \tTraining Loss: 1.193457 \tValidation Loss: 1.363872 \t time: 0.3\n",
      "Epoch: 684 \tTraining Loss: 1.193366 \tValidation Loss: 1.363930 \t time: 0.3\n",
      "Epoch: 685 \tTraining Loss: 1.193399 \tValidation Loss: 1.363203 \t time: 0.3\n",
      "Epoch: 686 \tTraining Loss: 1.193318 \tValidation Loss: 1.363171 \t time: 0.3\n",
      "Epoch: 687 \tTraining Loss: 1.193251 \tValidation Loss: 1.363441 \t time: 0.3\n",
      "Epoch: 688 \tTraining Loss: 1.193209 \tValidation Loss: 1.363205 \t time: 0.3\n",
      "Epoch: 689 \tTraining Loss: 1.193128 \tValidation Loss: 1.363222 \t time: 0.3\n",
      "Epoch: 690 \tTraining Loss: 1.193094 \tValidation Loss: 1.362672 \t time: 0.3\n",
      "Epoch: 691 \tTraining Loss: 1.193064 \tValidation Loss: 1.362551 \t time: 0.3\n",
      "Epoch: 692 \tTraining Loss: 1.192978 \tValidation Loss: 1.362397 \t time: 0.3\n",
      "Epoch: 693 \tTraining Loss: 1.192970 \tValidation Loss: 1.362006 \t time: 0.3\n",
      "Validation loss decreased from 1.362067 to 1.362006. Model was saved\n",
      "Epoch: 694 \tTraining Loss: 1.192957 \tValidation Loss: 1.362031 \t time: 0.3\n",
      "Epoch: 695 \tTraining Loss: 1.192883 \tValidation Loss: 1.362076 \t time: 0.3\n",
      "Epoch: 696 \tTraining Loss: 1.192864 \tValidation Loss: 1.362234 \t time: 0.3\n",
      "Epoch: 697 \tTraining Loss: 1.192843 \tValidation Loss: 1.362620 \t time: 0.3\n",
      "Epoch: 698 \tTraining Loss: 1.192801 \tValidation Loss: 1.362419 \t time: 0.3\n",
      "Epoch: 699 \tTraining Loss: 1.192774 \tValidation Loss: 1.362631 \t time: 0.3\n",
      "Epoch: 700 \tTraining Loss: 1.192705 \tValidation Loss: 1.363099 \t time: 0.3\n",
      "Epoch: 701 \tTraining Loss: 1.192692 \tValidation Loss: 1.362827 \t time: 0.3\n",
      "Epoch: 702 \tTraining Loss: 1.192652 \tValidation Loss: 1.362938 \t time: 0.3\n",
      "Epoch: 703 \tTraining Loss: 1.192574 \tValidation Loss: 1.363192 \t time: 0.3\n",
      "Epoch: 704 \tTraining Loss: 1.192554 \tValidation Loss: 1.363106 \t time: 0.3\n",
      "Epoch: 705 \tTraining Loss: 1.192528 \tValidation Loss: 1.363179 \t time: 0.3\n",
      "Epoch: 706 \tTraining Loss: 1.192496 \tValidation Loss: 1.362991 \t time: 0.3\n",
      "Epoch: 707 \tTraining Loss: 1.192465 \tValidation Loss: 1.363003 \t time: 0.3\n",
      "Epoch: 708 \tTraining Loss: 1.192392 \tValidation Loss: 1.363461 \t time: 0.3\n",
      "Epoch: 709 \tTraining Loss: 1.192330 \tValidation Loss: 1.363300 \t time: 0.3\n",
      "Epoch: 710 \tTraining Loss: 1.192296 \tValidation Loss: 1.363138 \t time: 0.3\n",
      "Epoch: 711 \tTraining Loss: 1.192277 \tValidation Loss: 1.363190 \t time: 0.3\n",
      "Epoch: 712 \tTraining Loss: 1.192246 \tValidation Loss: 1.362942 \t time: 0.3\n",
      "Epoch: 713 \tTraining Loss: 1.192210 \tValidation Loss: 1.362878 \t time: 0.3\n",
      "Epoch: 714 \tTraining Loss: 1.192174 \tValidation Loss: 1.362806 \t time: 0.3\n",
      "Epoch: 715 \tTraining Loss: 1.192147 \tValidation Loss: 1.362629 \t time: 0.3\n",
      "Epoch: 716 \tTraining Loss: 1.192120 \tValidation Loss: 1.362584 \t time: 0.3\n",
      "Epoch: 717 \tTraining Loss: 1.192101 \tValidation Loss: 1.362298 \t time: 0.3\n",
      "Epoch: 718 \tTraining Loss: 1.192077 \tValidation Loss: 1.362144 \t time: 0.3\n",
      "Epoch: 719 \tTraining Loss: 1.192058 \tValidation Loss: 1.362412 \t time: 0.3\n",
      "Epoch: 720 \tTraining Loss: 1.192034 \tValidation Loss: 1.362525 \t time: 0.3\n",
      "Epoch: 721 \tTraining Loss: 1.192001 \tValidation Loss: 1.362359 \t time: 0.3\n",
      "Epoch: 722 \tTraining Loss: 1.191975 \tValidation Loss: 1.362155 \t time: 0.3\n",
      "Epoch: 723 \tTraining Loss: 1.191955 \tValidation Loss: 1.362034 \t time: 0.3\n",
      "Epoch: 724 \tTraining Loss: 1.191932 \tValidation Loss: 1.362072 \t time: 0.3\n",
      "Epoch: 725 \tTraining Loss: 1.191901 \tValidation Loss: 1.362179 \t time: 0.3\n",
      "Epoch: 726 \tTraining Loss: 1.191847 \tValidation Loss: 1.362337 \t time: 0.3\n",
      "Epoch: 727 \tTraining Loss: 1.191827 \tValidation Loss: 1.362570 \t time: 0.3\n",
      "Epoch: 728 \tTraining Loss: 1.191809 \tValidation Loss: 1.362662 \t time: 0.3\n",
      "Epoch: 729 \tTraining Loss: 1.191784 \tValidation Loss: 1.362643 \t time: 0.3\n",
      "Epoch: 730 \tTraining Loss: 1.191741 \tValidation Loss: 1.362711 \t time: 0.3\n",
      "Epoch: 731 \tTraining Loss: 1.191717 \tValidation Loss: 1.362832 \t time: 0.3\n",
      "Epoch: 732 \tTraining Loss: 1.191696 \tValidation Loss: 1.362889 \t time: 0.3\n",
      "Epoch: 733 \tTraining Loss: 1.191652 \tValidation Loss: 1.363025 \t time: 0.3\n",
      "Epoch: 734 \tTraining Loss: 1.191605 \tValidation Loss: 1.363181 \t time: 0.3\n",
      "Epoch: 735 \tTraining Loss: 1.191574 \tValidation Loss: 1.363330 \t time: 0.3\n",
      "Epoch: 736 \tTraining Loss: 1.191543 \tValidation Loss: 1.363691 \t time: 0.3\n",
      "Epoch: 737 \tTraining Loss: 1.191522 \tValidation Loss: 1.363776 \t time: 0.3\n",
      "Epoch: 738 \tTraining Loss: 1.191490 \tValidation Loss: 1.363550 \t time: 0.3\n",
      "Epoch: 739 \tTraining Loss: 1.191464 \tValidation Loss: 1.363651 \t time: 0.3\n",
      "Epoch: 740 \tTraining Loss: 1.191443 \tValidation Loss: 1.363795 \t time: 0.3\n",
      "Epoch: 741 \tTraining Loss: 1.191406 \tValidation Loss: 1.363844 \t time: 0.3\n",
      "Epoch: 742 \tTraining Loss: 1.191371 \tValidation Loss: 1.363866 \t time: 0.3\n",
      "Epoch: 743 \tTraining Loss: 1.191356 \tValidation Loss: 1.363494 \t time: 0.3\n",
      "Epoch: 744 \tTraining Loss: 1.191313 \tValidation Loss: 1.363241 \t time: 0.3\n",
      "Epoch: 745 \tTraining Loss: 1.191286 \tValidation Loss: 1.363136 \t time: 0.3\n",
      "Epoch: 746 \tTraining Loss: 1.191265 \tValidation Loss: 1.363391 \t time: 0.3\n",
      "Epoch: 747 \tTraining Loss: 1.191241 \tValidation Loss: 1.363237 \t time: 0.3\n",
      "Epoch: 748 \tTraining Loss: 1.191212 \tValidation Loss: 1.363286 \t time: 0.3\n",
      "Epoch: 749 \tTraining Loss: 1.191180 \tValidation Loss: 1.363668 \t time: 0.3\n",
      "Epoch: 750 \tTraining Loss: 1.191167 \tValidation Loss: 1.363494 \t time: 0.3\n",
      "Epoch: 751 \tTraining Loss: 1.191173 \tValidation Loss: 1.363438 \t time: 0.3\n",
      "Epoch: 752 \tTraining Loss: 1.191147 \tValidation Loss: 1.363751 \t time: 0.3\n",
      "Epoch: 753 \tTraining Loss: 1.191124 \tValidation Loss: 1.363927 \t time: 0.3\n",
      "Epoch: 754 \tTraining Loss: 1.191119 \tValidation Loss: 1.363240 \t time: 0.3\n",
      "Epoch: 755 \tTraining Loss: 1.191119 \tValidation Loss: 1.363044 \t time: 0.3\n",
      "Epoch: 756 \tTraining Loss: 1.191113 \tValidation Loss: 1.363293 \t time: 0.3\n",
      "Epoch: 757 \tTraining Loss: 1.191079 \tValidation Loss: 1.363622 \t time: 0.3\n",
      "Epoch: 758 \tTraining Loss: 1.191086 \tValidation Loss: 1.363088 \t time: 0.3\n",
      "Epoch: 759 \tTraining Loss: 1.191040 \tValidation Loss: 1.362786 \t time: 0.3\n",
      "Epoch: 760 \tTraining Loss: 1.191039 \tValidation Loss: 1.363298 \t time: 0.3\n",
      "Epoch: 761 \tTraining Loss: 1.191038 \tValidation Loss: 1.363082 \t time: 0.3\n",
      "Epoch: 762 \tTraining Loss: 1.191028 \tValidation Loss: 1.363002 \t time: 0.3\n",
      "Epoch: 763 \tTraining Loss: 1.191015 \tValidation Loss: 1.363382 \t time: 0.3\n",
      "Epoch: 764 \tTraining Loss: 1.190993 \tValidation Loss: 1.363310 \t time: 0.3\n",
      "Epoch: 765 \tTraining Loss: 1.190965 \tValidation Loss: 1.362693 \t time: 0.3\n",
      "Epoch: 766 \tTraining Loss: 1.190932 \tValidation Loss: 1.362872 \t time: 0.3\n",
      "Epoch: 767 \tTraining Loss: 1.190893 \tValidation Loss: 1.363119 \t time: 0.3\n",
      "Epoch: 768 \tTraining Loss: 1.190879 \tValidation Loss: 1.363116 \t time: 0.3\n",
      "Epoch: 769 \tTraining Loss: 1.190870 \tValidation Loss: 1.363363 \t time: 0.3\n",
      "Epoch: 770 \tTraining Loss: 1.190850 \tValidation Loss: 1.363446 \t time: 0.3\n",
      "Epoch: 771 \tTraining Loss: 1.190830 \tValidation Loss: 1.363436 \t time: 0.3\n",
      "Epoch: 772 \tTraining Loss: 1.190795 \tValidation Loss: 1.363582 \t time: 0.3\n",
      "Epoch: 773 \tTraining Loss: 1.190766 \tValidation Loss: 1.363759 \t time: 0.3\n",
      "Epoch: 774 \tTraining Loss: 1.190765 \tValidation Loss: 1.363652 \t time: 0.3\n",
      "Epoch: 775 \tTraining Loss: 1.190750 \tValidation Loss: 1.363854 \t time: 0.3\n",
      "Epoch: 776 \tTraining Loss: 1.190730 \tValidation Loss: 1.363800 \t time: 0.3\n",
      "Epoch: 777 \tTraining Loss: 1.190713 \tValidation Loss: 1.363272 \t time: 0.3\n",
      "Epoch: 778 \tTraining Loss: 1.190710 \tValidation Loss: 1.363421 \t time: 0.3\n",
      "Epoch: 779 \tTraining Loss: 1.190691 \tValidation Loss: 1.363374 \t time: 0.3\n",
      "Epoch: 780 \tTraining Loss: 1.190680 \tValidation Loss: 1.363219 \t time: 0.3\n",
      "Epoch: 781 \tTraining Loss: 1.190666 \tValidation Loss: 1.363492 \t time: 0.3\n",
      "Epoch: 782 \tTraining Loss: 1.190646 \tValidation Loss: 1.363631 \t time: 0.3\n",
      "Epoch: 783 \tTraining Loss: 1.190629 \tValidation Loss: 1.363465 \t time: 0.3\n",
      "Epoch: 784 \tTraining Loss: 1.190615 \tValidation Loss: 1.363615 \t time: 0.3\n",
      "Epoch: 785 \tTraining Loss: 1.190602 \tValidation Loss: 1.363517 \t time: 0.3\n",
      "Epoch: 786 \tTraining Loss: 1.190594 \tValidation Loss: 1.363345 \t time: 0.3\n",
      "Epoch: 787 \tTraining Loss: 1.190576 \tValidation Loss: 1.363367 \t time: 0.3\n",
      "Epoch: 788 \tTraining Loss: 1.190561 \tValidation Loss: 1.363361 \t time: 0.3\n",
      "Epoch: 789 \tTraining Loss: 1.190554 \tValidation Loss: 1.363423 \t time: 0.3\n",
      "Epoch: 790 \tTraining Loss: 1.190544 \tValidation Loss: 1.363293 \t time: 0.3\n",
      "Epoch: 791 \tTraining Loss: 1.190531 \tValidation Loss: 1.363352 \t time: 0.3\n",
      "Epoch: 792 \tTraining Loss: 1.190515 \tValidation Loss: 1.363393 \t time: 0.3\n",
      "Epoch: 793 \tTraining Loss: 1.190502 \tValidation Loss: 1.363490 \t time: 0.3\n",
      "Epoch: 794 \tTraining Loss: 1.190492 \tValidation Loss: 1.363736 \t time: 0.3\n",
      "Epoch: 795 \tTraining Loss: 1.190466 \tValidation Loss: 1.363715 \t time: 0.3\n",
      "Epoch: 796 \tTraining Loss: 1.190431 \tValidation Loss: 1.363750 \t time: 0.3\n",
      "Epoch: 797 \tTraining Loss: 1.190387 \tValidation Loss: 1.364105 \t time: 0.3\n",
      "Epoch: 798 \tTraining Loss: 1.190364 \tValidation Loss: 1.364321 \t time: 0.3\n",
      "Epoch: 799 \tTraining Loss: 1.190345 \tValidation Loss: 1.364692 \t time: 0.3\n",
      "Epoch: 800 \tTraining Loss: 1.190309 \tValidation Loss: 1.365352 \t time: 0.3\n",
      "Epoch: 801 \tTraining Loss: 1.190263 \tValidation Loss: 1.365514 \t time: 0.3\n",
      "Epoch: 802 \tTraining Loss: 1.190186 \tValidation Loss: 1.365771 \t time: 0.3\n",
      "Epoch: 803 \tTraining Loss: 1.190117 \tValidation Loss: 1.366265 \t time: 0.3\n",
      "Epoch: 804 \tTraining Loss: 1.190104 \tValidation Loss: 1.366044 \t time: 0.3\n",
      "Epoch: 805 \tTraining Loss: 1.190080 \tValidation Loss: 1.365996 \t time: 0.3\n",
      "Epoch: 806 \tTraining Loss: 1.190046 \tValidation Loss: 1.365964 \t time: 0.3\n",
      "Epoch: 807 \tTraining Loss: 1.190039 \tValidation Loss: 1.365647 \t time: 0.3\n",
      "Epoch: 808 \tTraining Loss: 1.190014 \tValidation Loss: 1.365785 \t time: 0.3\n",
      "Epoch: 809 \tTraining Loss: 1.189997 \tValidation Loss: 1.366193 \t time: 0.3\n",
      "Epoch: 810 \tTraining Loss: 1.189979 \tValidation Loss: 1.366021 \t time: 0.3\n",
      "Epoch: 811 \tTraining Loss: 1.189967 \tValidation Loss: 1.365889 \t time: 0.3\n",
      "Epoch: 812 \tTraining Loss: 1.189955 \tValidation Loss: 1.365610 \t time: 0.3\n",
      "Epoch: 813 \tTraining Loss: 1.189943 \tValidation Loss: 1.365000 \t time: 0.3\n",
      "Epoch: 814 \tTraining Loss: 1.189935 \tValidation Loss: 1.365079 \t time: 0.3\n",
      "Epoch: 815 \tTraining Loss: 1.189919 \tValidation Loss: 1.365165 \t time: 0.3\n",
      "Epoch: 816 \tTraining Loss: 1.189908 \tValidation Loss: 1.364824 \t time: 0.3\n",
      "Epoch: 817 \tTraining Loss: 1.189898 \tValidation Loss: 1.364763 \t time: 0.3\n",
      "Epoch: 818 \tTraining Loss: 1.189882 \tValidation Loss: 1.364638 \t time: 0.3\n",
      "Epoch: 819 \tTraining Loss: 1.189868 \tValidation Loss: 1.364547 \t time: 0.3\n",
      "Epoch: 820 \tTraining Loss: 1.189851 \tValidation Loss: 1.364944 \t time: 0.3\n",
      "Epoch: 821 \tTraining Loss: 1.189832 \tValidation Loss: 1.365316 \t time: 0.2\n",
      "Epoch: 822 \tTraining Loss: 1.189810 \tValidation Loss: 1.365299 \t time: 0.3\n",
      "Epoch: 823 \tTraining Loss: 1.189781 \tValidation Loss: 1.365371 \t time: 0.3\n",
      "Epoch: 824 \tTraining Loss: 1.189753 \tValidation Loss: 1.365459 \t time: 0.3\n",
      "Epoch: 825 \tTraining Loss: 1.189717 \tValidation Loss: 1.365231 \t time: 0.3\n",
      "Epoch: 826 \tTraining Loss: 1.189688 \tValidation Loss: 1.365337 \t time: 0.2\n",
      "Epoch: 827 \tTraining Loss: 1.189696 \tValidation Loss: 1.366005 \t time: 0.3\n",
      "Epoch: 828 \tTraining Loss: 1.189713 \tValidation Loss: 1.365042 \t time: 0.3\n",
      "Epoch: 829 \tTraining Loss: 1.189745 \tValidation Loss: 1.364610 \t time: 0.3\n",
      "Epoch: 830 \tTraining Loss: 1.189737 \tValidation Loss: 1.364377 \t time: 0.2\n",
      "Epoch: 831 \tTraining Loss: 1.189706 \tValidation Loss: 1.364164 \t time: 0.3\n",
      "Epoch: 832 \tTraining Loss: 1.189692 \tValidation Loss: 1.364155 \t time: 0.3\n",
      "Epoch: 833 \tTraining Loss: 1.189661 \tValidation Loss: 1.364575 \t time: 0.3\n",
      "Epoch: 834 \tTraining Loss: 1.189610 \tValidation Loss: 1.365321 \t time: 0.2\n",
      "Epoch: 835 \tTraining Loss: 1.189597 \tValidation Loss: 1.364838 \t time: 0.2\n",
      "Epoch: 836 \tTraining Loss: 1.189569 \tValidation Loss: 1.364886 \t time: 0.3\n",
      "Epoch: 837 \tTraining Loss: 1.189523 \tValidation Loss: 1.365051 \t time: 0.3\n",
      "Epoch: 838 \tTraining Loss: 1.189487 \tValidation Loss: 1.365475 \t time: 0.2\n",
      "Epoch: 839 \tTraining Loss: 1.189481 \tValidation Loss: 1.365216 \t time: 0.2\n",
      "Epoch: 840 \tTraining Loss: 1.189456 \tValidation Loss: 1.365246 \t time: 0.3\n",
      "Epoch: 841 \tTraining Loss: 1.189443 \tValidation Loss: 1.365543 \t time: 0.3\n",
      "Epoch: 842 \tTraining Loss: 1.189431 \tValidation Loss: 1.365458 \t time: 0.3\n",
      "Epoch: 843 \tTraining Loss: 1.189408 \tValidation Loss: 1.366125 \t time: 0.3\n",
      "Epoch: 844 \tTraining Loss: 1.189422 \tValidation Loss: 1.365404 \t time: 0.2\n",
      "Epoch: 845 \tTraining Loss: 1.189396 \tValidation Loss: 1.365294 \t time: 0.3\n",
      "Epoch: 846 \tTraining Loss: 1.189384 \tValidation Loss: 1.365048 \t time: 0.3\n",
      "Epoch: 847 \tTraining Loss: 1.189372 \tValidation Loss: 1.365245 \t time: 0.3\n",
      "Epoch: 848 \tTraining Loss: 1.189365 \tValidation Loss: 1.364835 \t time: 0.2\n",
      "Epoch: 849 \tTraining Loss: 1.189360 \tValidation Loss: 1.364776 \t time: 0.2\n",
      "Epoch: 850 \tTraining Loss: 1.189341 \tValidation Loss: 1.364957 \t time: 0.3\n",
      "Epoch: 851 \tTraining Loss: 1.189320 \tValidation Loss: 1.364721 \t time: 0.3\n",
      "Epoch: 852 \tTraining Loss: 1.189281 \tValidation Loss: 1.364800 \t time: 0.3\n",
      "Epoch: 853 \tTraining Loss: 1.189246 \tValidation Loss: 1.365019 \t time: 0.2\n",
      "Epoch: 854 \tTraining Loss: 1.189200 \tValidation Loss: 1.364665 \t time: 0.3\n",
      "Epoch: 855 \tTraining Loss: 1.189137 \tValidation Loss: 1.364860 \t time: 0.3\n",
      "Epoch: 856 \tTraining Loss: 1.189095 \tValidation Loss: 1.364997 \t time: 0.3\n",
      "Epoch: 857 \tTraining Loss: 1.189076 \tValidation Loss: 1.365322 \t time: 0.3\n",
      "Epoch: 858 \tTraining Loss: 1.189040 \tValidation Loss: 1.365070 \t time: 0.2\n",
      "Epoch: 859 \tTraining Loss: 1.189010 \tValidation Loss: 1.364866 \t time: 0.3\n",
      "Epoch: 860 \tTraining Loss: 1.188978 \tValidation Loss: 1.365080 \t time: 0.3\n",
      "Epoch: 861 \tTraining Loss: 1.188984 \tValidation Loss: 1.364778 \t time: 0.3\n",
      "Epoch: 862 \tTraining Loss: 1.188969 \tValidation Loss: 1.364747 \t time: 0.2\n",
      "Epoch: 863 \tTraining Loss: 1.188945 \tValidation Loss: 1.365010 \t time: 0.3\n",
      "Epoch: 864 \tTraining Loss: 1.188901 \tValidation Loss: 1.365770 \t time: 0.3\n",
      "Epoch: 865 \tTraining Loss: 1.188875 \tValidation Loss: 1.364839 \t time: 0.3\n",
      "Epoch: 866 \tTraining Loss: 1.188785 \tValidation Loss: 1.364577 \t time: 0.3\n",
      "Epoch: 867 \tTraining Loss: 1.188734 \tValidation Loss: 1.364629 \t time: 0.3\n",
      "Epoch: 868 \tTraining Loss: 1.188751 \tValidation Loss: 1.364707 \t time: 0.3\n",
      "Epoch: 869 \tTraining Loss: 1.188692 \tValidation Loss: 1.364939 \t time: 0.3\n",
      "Epoch: 870 \tTraining Loss: 1.188694 \tValidation Loss: 1.365307 \t time: 0.3\n",
      "Epoch: 871 \tTraining Loss: 1.188677 \tValidation Loss: 1.364738 \t time: 0.3\n",
      "Epoch: 872 \tTraining Loss: 1.188665 \tValidation Loss: 1.364451 \t time: 0.3\n",
      "Epoch: 873 \tTraining Loss: 1.188643 \tValidation Loss: 1.364741 \t time: 0.3\n",
      "Epoch: 874 \tTraining Loss: 1.188632 \tValidation Loss: 1.364323 \t time: 0.3\n",
      "Epoch: 875 \tTraining Loss: 1.188611 \tValidation Loss: 1.364172 \t time: 0.3\n",
      "Epoch: 876 \tTraining Loss: 1.188591 \tValidation Loss: 1.364183 \t time: 0.3\n",
      "Epoch: 877 \tTraining Loss: 1.188586 \tValidation Loss: 1.364055 \t time: 0.3\n",
      "Epoch: 878 \tTraining Loss: 1.188547 \tValidation Loss: 1.363959 \t time: 0.3\n",
      "Epoch: 879 \tTraining Loss: 1.188490 \tValidation Loss: 1.364270 \t time: 0.3\n",
      "Epoch: 880 \tTraining Loss: 1.188444 \tValidation Loss: 1.363413 \t time: 0.3\n",
      "Epoch: 881 \tTraining Loss: 1.188361 \tValidation Loss: 1.362452 \t time: 0.3\n",
      "Epoch: 882 \tTraining Loss: 1.188308 \tValidation Loss: 1.362692 \t time: 0.3\n",
      "Epoch: 883 \tTraining Loss: 1.188279 \tValidation Loss: 1.362169 \t time: 0.3\n",
      "Epoch: 884 \tTraining Loss: 1.188240 \tValidation Loss: 1.362308 \t time: 0.3\n",
      "Epoch: 885 \tTraining Loss: 1.188192 \tValidation Loss: 1.362143 \t time: 0.3\n",
      "Epoch: 886 \tTraining Loss: 1.188149 \tValidation Loss: 1.361870 \t time: 0.2\n",
      "Validation loss decreased from 1.362006 to 1.361870. Model was saved\n",
      "Epoch: 887 \tTraining Loss: 1.188106 \tValidation Loss: 1.361578 \t time: 0.2\n",
      "Validation loss decreased from 1.361870 to 1.361578. Model was saved\n",
      "Epoch: 888 \tTraining Loss: 1.188038 \tValidation Loss: 1.361542 \t time: 0.2\n",
      "Validation loss decreased from 1.361578 to 1.361542. Model was saved\n",
      "Epoch: 889 \tTraining Loss: 1.188019 \tValidation Loss: 1.361408 \t time: 0.3\n",
      "Validation loss decreased from 1.361542 to 1.361408. Model was saved\n",
      "Epoch: 890 \tTraining Loss: 1.187983 \tValidation Loss: 1.361341 \t time: 0.2\n",
      "Validation loss decreased from 1.361408 to 1.361341. Model was saved\n",
      "Epoch: 891 \tTraining Loss: 1.187952 \tValidation Loss: 1.361450 \t time: 0.3\n",
      "Epoch: 892 \tTraining Loss: 1.187943 \tValidation Loss: 1.361738 \t time: 0.3\n",
      "Epoch: 893 \tTraining Loss: 1.187938 \tValidation Loss: 1.362089 \t time: 0.2\n",
      "Epoch: 894 \tTraining Loss: 1.187914 \tValidation Loss: 1.362471 \t time: 0.2\n",
      "Epoch: 895 \tTraining Loss: 1.187903 \tValidation Loss: 1.362428 \t time: 0.3\n",
      "Epoch: 896 \tTraining Loss: 1.187893 \tValidation Loss: 1.362389 \t time: 0.2\n",
      "Epoch: 897 \tTraining Loss: 1.187876 \tValidation Loss: 1.362660 \t time: 0.3\n",
      "Epoch: 898 \tTraining Loss: 1.187868 \tValidation Loss: 1.362851 \t time: 0.3\n",
      "Epoch: 899 \tTraining Loss: 1.187859 \tValidation Loss: 1.363052 \t time: 0.2\n",
      "Epoch: 900 \tTraining Loss: 1.187839 \tValidation Loss: 1.363360 \t time: 0.3\n",
      "Epoch: 901 \tTraining Loss: 1.187834 \tValidation Loss: 1.363447 \t time: 0.3\n",
      "Epoch: 902 \tTraining Loss: 1.187830 \tValidation Loss: 1.363099 \t time: 0.3\n",
      "Epoch: 903 \tTraining Loss: 1.187816 \tValidation Loss: 1.362931 \t time: 0.3\n",
      "Epoch: 904 \tTraining Loss: 1.187810 \tValidation Loss: 1.362721 \t time: 0.3\n",
      "Epoch: 905 \tTraining Loss: 1.187799 \tValidation Loss: 1.362519 \t time: 0.3\n",
      "Epoch: 906 \tTraining Loss: 1.187786 \tValidation Loss: 1.362422 \t time: 0.3\n",
      "Epoch: 907 \tTraining Loss: 1.187773 \tValidation Loss: 1.362100 \t time: 0.3\n",
      "Epoch: 908 \tTraining Loss: 1.187749 \tValidation Loss: 1.361713 \t time: 0.3\n",
      "Epoch: 909 \tTraining Loss: 1.187702 \tValidation Loss: 1.361610 \t time: 0.3\n",
      "Epoch: 910 \tTraining Loss: 1.187619 \tValidation Loss: 1.362046 \t time: 0.2\n",
      "Epoch: 911 \tTraining Loss: 1.187549 \tValidation Loss: 1.362750 \t time: 0.3\n",
      "Epoch: 912 \tTraining Loss: 1.187510 \tValidation Loss: 1.363122 \t time: 0.2\n",
      "Epoch: 913 \tTraining Loss: 1.187492 \tValidation Loss: 1.363240 \t time: 0.3\n",
      "Epoch: 914 \tTraining Loss: 1.187485 \tValidation Loss: 1.363024 \t time: 0.3\n",
      "Epoch: 915 \tTraining Loss: 1.187473 \tValidation Loss: 1.362898 \t time: 0.3\n",
      "Epoch: 916 \tTraining Loss: 1.187458 \tValidation Loss: 1.362957 \t time: 0.3\n",
      "Epoch: 917 \tTraining Loss: 1.187441 \tValidation Loss: 1.362944 \t time: 0.3\n",
      "Epoch: 918 \tTraining Loss: 1.187428 \tValidation Loss: 1.362997 \t time: 0.3\n",
      "Epoch: 919 \tTraining Loss: 1.187417 \tValidation Loss: 1.362999 \t time: 0.3\n",
      "Epoch: 920 \tTraining Loss: 1.187408 \tValidation Loss: 1.362980 \t time: 0.2\n",
      "Epoch: 921 \tTraining Loss: 1.187400 \tValidation Loss: 1.363134 \t time: 0.2\n",
      "Epoch: 922 \tTraining Loss: 1.187391 \tValidation Loss: 1.363241 \t time: 0.3\n",
      "Epoch: 923 \tTraining Loss: 1.187382 \tValidation Loss: 1.363174 \t time: 0.3\n",
      "Epoch: 924 \tTraining Loss: 1.187375 \tValidation Loss: 1.363065 \t time: 0.3\n",
      "Epoch: 925 \tTraining Loss: 1.187369 \tValidation Loss: 1.363055 \t time: 0.3\n",
      "Epoch: 926 \tTraining Loss: 1.187362 \tValidation Loss: 1.363083 \t time: 0.3\n",
      "Epoch: 927 \tTraining Loss: 1.187356 \tValidation Loss: 1.363043 \t time: 0.3\n",
      "Epoch: 928 \tTraining Loss: 1.187350 \tValidation Loss: 1.362927 \t time: 0.3\n",
      "Epoch: 929 \tTraining Loss: 1.187344 \tValidation Loss: 1.362792 \t time: 0.2\n",
      "Epoch: 930 \tTraining Loss: 1.187336 \tValidation Loss: 1.362793 \t time: 0.2\n",
      "Epoch: 931 \tTraining Loss: 1.187329 \tValidation Loss: 1.362864 \t time: 0.3\n",
      "Epoch: 932 \tTraining Loss: 1.187323 \tValidation Loss: 1.362916 \t time: 0.3\n",
      "Epoch: 933 \tTraining Loss: 1.187316 \tValidation Loss: 1.362986 \t time: 0.3\n",
      "Epoch: 934 \tTraining Loss: 1.187308 \tValidation Loss: 1.363003 \t time: 0.2\n",
      "Epoch: 935 \tTraining Loss: 1.187299 \tValidation Loss: 1.362978 \t time: 0.3\n",
      "Epoch: 936 \tTraining Loss: 1.187285 \tValidation Loss: 1.363004 \t time: 0.3\n",
      "Epoch: 937 \tTraining Loss: 1.187261 \tValidation Loss: 1.362985 \t time: 0.2\n",
      "Epoch: 938 \tTraining Loss: 1.187244 \tValidation Loss: 1.362859 \t time: 0.2\n",
      "Epoch: 939 \tTraining Loss: 1.187231 \tValidation Loss: 1.362875 \t time: 0.2\n",
      "Epoch: 940 \tTraining Loss: 1.187222 \tValidation Loss: 1.363090 \t time: 0.2\n",
      "Epoch: 941 \tTraining Loss: 1.187209 \tValidation Loss: 1.363194 \t time: 0.3\n",
      "Epoch: 942 \tTraining Loss: 1.187192 \tValidation Loss: 1.363195 \t time: 0.3\n",
      "Epoch: 943 \tTraining Loss: 1.187164 \tValidation Loss: 1.363021 \t time: 0.3\n",
      "Epoch: 944 \tTraining Loss: 1.187127 \tValidation Loss: 1.362732 \t time: 0.3\n",
      "Epoch: 945 \tTraining Loss: 1.187059 \tValidation Loss: 1.362324 \t time: 0.3\n",
      "Epoch: 946 \tTraining Loss: 1.187074 \tValidation Loss: 1.362424 \t time: 0.3\n",
      "Epoch: 947 \tTraining Loss: 1.187055 \tValidation Loss: 1.362560 \t time: 0.3\n",
      "Epoch: 948 \tTraining Loss: 1.187056 \tValidation Loss: 1.362317 \t time: 0.2\n",
      "Epoch: 949 \tTraining Loss: 1.187046 \tValidation Loss: 1.362731 \t time: 0.2\n",
      "Epoch: 950 \tTraining Loss: 1.187048 \tValidation Loss: 1.362813 \t time: 0.3\n",
      "Epoch: 951 \tTraining Loss: 1.187064 \tValidation Loss: 1.363295 \t time: 0.3\n",
      "Epoch: 952 \tTraining Loss: 1.187037 \tValidation Loss: 1.363226 \t time: 0.3\n",
      "Epoch: 953 \tTraining Loss: 1.187022 \tValidation Loss: 1.362884 \t time: 0.2\n",
      "Epoch: 954 \tTraining Loss: 1.187018 \tValidation Loss: 1.363296 \t time: 0.3\n",
      "Epoch: 955 \tTraining Loss: 1.187018 \tValidation Loss: 1.362862 \t time: 0.3\n",
      "Epoch: 956 \tTraining Loss: 1.186983 \tValidation Loss: 1.362765 \t time: 0.3\n",
      "Epoch: 957 \tTraining Loss: 1.186969 \tValidation Loss: 1.362993 \t time: 0.3\n",
      "Epoch: 958 \tTraining Loss: 1.186974 \tValidation Loss: 1.362745 \t time: 0.2\n",
      "Epoch: 959 \tTraining Loss: 1.186964 \tValidation Loss: 1.363443 \t time: 0.3\n",
      "Epoch: 960 \tTraining Loss: 1.186971 \tValidation Loss: 1.363164 \t time: 0.2\n",
      "Epoch: 961 \tTraining Loss: 1.186958 \tValidation Loss: 1.363172 \t time: 0.2\n",
      "Epoch: 962 \tTraining Loss: 1.186934 \tValidation Loss: 1.363001 \t time: 0.2\n",
      "Epoch: 963 \tTraining Loss: 1.186921 \tValidation Loss: 1.362697 \t time: 0.2\n",
      "Epoch: 964 \tTraining Loss: 1.186912 \tValidation Loss: 1.362970 \t time: 0.3\n",
      "Epoch: 965 \tTraining Loss: 1.186892 \tValidation Loss: 1.362414 \t time: 0.3\n",
      "Epoch: 966 \tTraining Loss: 1.186874 \tValidation Loss: 1.361463 \t time: 0.3\n",
      "Epoch: 967 \tTraining Loss: 1.186860 \tValidation Loss: 1.361117 \t time: 0.2\n",
      "Validation loss decreased from 1.361341 to 1.361117. Model was saved\n",
      "Epoch: 968 \tTraining Loss: 1.186843 \tValidation Loss: 1.361792 \t time: 0.3\n",
      "Epoch: 969 \tTraining Loss: 1.186834 \tValidation Loss: 1.361767 \t time: 0.3\n",
      "Epoch: 970 \tTraining Loss: 1.186825 \tValidation Loss: 1.361137 \t time: 0.2\n",
      "Epoch: 971 \tTraining Loss: 1.186807 \tValidation Loss: 1.361215 \t time: 0.2\n",
      "Epoch: 972 \tTraining Loss: 1.186805 \tValidation Loss: 1.361463 \t time: 0.2\n",
      "Epoch: 973 \tTraining Loss: 1.186794 \tValidation Loss: 1.361596 \t time: 0.3\n",
      "Epoch: 974 \tTraining Loss: 1.186787 \tValidation Loss: 1.360970 \t time: 0.3\n",
      "Validation loss decreased from 1.361117 to 1.360970. Model was saved\n",
      "Epoch: 975 \tTraining Loss: 1.186781 \tValidation Loss: 1.361562 \t time: 0.3\n",
      "Epoch: 976 \tTraining Loss: 1.186766 \tValidation Loss: 1.361355 \t time: 0.2\n",
      "Epoch: 977 \tTraining Loss: 1.186755 \tValidation Loss: 1.360802 \t time: 0.3\n",
      "Validation loss decreased from 1.360970 to 1.360802. Model was saved\n",
      "Epoch: 978 \tTraining Loss: 1.186722 \tValidation Loss: 1.360726 \t time: 0.3\n",
      "Validation loss decreased from 1.360802 to 1.360726. Model was saved\n",
      "Epoch: 979 \tTraining Loss: 1.186682 \tValidation Loss: 1.359884 \t time: 0.3\n",
      "Validation loss decreased from 1.360726 to 1.359884. Model was saved\n",
      "Epoch: 980 \tTraining Loss: 1.186637 \tValidation Loss: 1.361281 \t time: 0.2\n",
      "Epoch: 981 \tTraining Loss: 1.186585 \tValidation Loss: 1.360490 \t time: 0.3\n",
      "Epoch: 982 \tTraining Loss: 1.186574 \tValidation Loss: 1.360949 \t time: 0.3\n",
      "Epoch: 983 \tTraining Loss: 1.186618 \tValidation Loss: 1.360386 \t time: 0.3\n",
      "Epoch: 984 \tTraining Loss: 1.186803 \tValidation Loss: 1.364078 \t time: 0.2\n",
      "Epoch: 985 \tTraining Loss: 1.187322 \tValidation Loss: 1.359570 \t time: 0.2\n",
      "Validation loss decreased from 1.359884 to 1.359570. Model was saved\n",
      "Epoch: 986 \tTraining Loss: 1.190732 \tValidation Loss: 1.397398 \t time: 0.3\n",
      "Epoch: 987 \tTraining Loss: 1.245461 \tValidation Loss: 1.533481 \t time: 0.3\n",
      "Epoch: 988 \tTraining Loss: 1.458564 \tValidation Loss: 1.417977 \t time: 0.2\n",
      "Epoch: 989 \tTraining Loss: 1.296177 \tValidation Loss: 1.435122 \t time: 0.2\n",
      "Epoch: 990 \tTraining Loss: 1.331611 \tValidation Loss: 1.451170 \t time: 0.2\n",
      "Epoch: 991 \tTraining Loss: 1.364924 \tValidation Loss: 1.426125 \t time: 0.3\n",
      "Epoch: 992 \tTraining Loss: 1.320276 \tValidation Loss: 1.390404 \t time: 0.3\n",
      "Epoch: 993 \tTraining Loss: 1.277318 \tValidation Loss: 1.411116 \t time: 0.3\n",
      "Epoch: 994 \tTraining Loss: 1.292967 \tValidation Loss: 1.422544 \t time: 0.3\n",
      "Epoch: 995 \tTraining Loss: 1.308047 \tValidation Loss: 1.410637 \t time: 0.2\n",
      "Epoch: 996 \tTraining Loss: 1.287377 \tValidation Loss: 1.393768 \t time: 0.3\n",
      "Epoch: 997 \tTraining Loss: 1.267244 \tValidation Loss: 1.398978 \t time: 0.3\n",
      "Epoch: 998 \tTraining Loss: 1.271236 \tValidation Loss: 1.419765 \t time: 0.3\n",
      "Epoch: 999 \tTraining Loss: 1.274457 \tValidation Loss: 1.409968 \t time: 0.3\n",
      "Epoch: 1000 \tTraining Loss: 1.256585 \tValidation Loss: 1.393964 \t time: 0.3\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf \n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "\n",
    "#             data = data.type((torch.FloatTensor))\n",
    "\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update accumulated training loss\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "            \n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
    "            \n",
    "#             data = data.type((torch.FloatTensor))\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update accumulated validation loss \n",
    "            valid_loss += loss.item()*data.size(0)\n",
    "            \n",
    "\n",
    "        train_loss = train_loss/len(loaders['train'].dataset)\n",
    "        valid_loss = valid_loss/len(loaders['valid'].dataset)\n",
    "        \n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\t time: {:.1f}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss,\n",
    "            time.time() - start\n",
    "            ))\n",
    "        \n",
    "        ## TODO: save the model if validation loss has decreased\n",
    "        if valid_loss < valid_loss_min:\n",
    "            print('Validation loss decreased from {:.6f} to {:.6f}. Model was saved'.format(\n",
    "                valid_loss_min,\n",
    "                valid_loss\n",
    "            ))\n",
    "            \n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            valid_loss_min = valid_loss\n",
    "    \n",
    "    # return trained model\n",
    "    return model\n",
    "\n",
    "\n",
    "# train the model\n",
    "model = train(n_epochs, loaders, model, optimizer, \n",
    "                      criterion, use_cuda, 'model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch: 215 \tTraining Loss: 1.329372 \tValidation Loss: 1.419149 \t time: 0.4\n",
    "Epoch: 216 \tTraining Loss: 1.328865 \tValidation Loss: 1.416587 \t time: 0.4\n",
    "Validation loss decreased from 1.417114 to 1.416587. Model was saved\n",
    "Kaggle score = 0.54438 (0.0006 IMPROVEMENT)\n",
    "\n",
    "Epoch: 216 \tTraining Loss: 1.326797 \tValidation Loss: 1.396590 \t time: 0.7\n",
    "Epoch: 217 \tTraining Loss: 1.326093 \tValidation Loss: 1.396514 \t time: 0.7\n",
    "Epoch: 218 \tTraining Loss: 1.325473 \tValidation Loss: 1.394434 \t time: 0.6\n",
    "Validation loss decreased from 1.396026 to 1.394434. Model was saved\n",
    "Kaggle score = 55245 (0.011 IMPROVEMENT)\n",
    "\n",
    "Epoch: 364 \tTraining Loss: 1.229904 \tValidation Loss: 1.381198 \t time: 0.3\n",
    "Validation loss decreased from 1.384373 to 1.381198. Model was saved\n",
    "Epoch: 365 \tTraining Loss: 1.231026 \tValidation Loss: 1.394960 \t time: 0.3\n",
    "Epoch: 366 \tTraining Loss: 1.231521 \tValidation Loss: 1.379200 \t time: 0.3\n",
    "Validation loss decreased from 1.381198 to 1.379200. Model was saved\n",
    "Epoch: 367 \tTraining Loss: 1.232151 \tValidation Loss: 1.395807 \t time: 0.3\n",
    "Epoch: 368 \tTraining Loss: 1.230561 \tValidation Loss: 1.391226 \t time: 0.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.298795\n",
      "\n",
      "\n",
      "Test Accuracy: 65% (32/49)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.6/site-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "def test(loaders, model, criterion, use_cuda):\n",
    "\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "        \n",
    "#         data = data.type((torch.FloatTensor))\n",
    "        # move to GPU\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average test loss \n",
    "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "        # convert output probabilities to predicted class\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        # compare predictions to true label\n",
    "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "        total += data.size(0)\n",
    "            \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "        100. * correct / total, correct, total))\n",
    "\n",
    "# load the model that got the best validation accuracy\n",
    "model.load_state_dict(torch.load('model.pt'))\n",
    "# call test function    \n",
    "test(loaders, model, criterion, use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i,l = next(iter(loaders['test']))\n",
    "# if use_cuda:\n",
    "#     i, l = i.cuda(), l.cuda()\n",
    "\n",
    "# output = model(i)\n",
    "\n",
    "# result = output.cpu().data.max(1, keepdim=True)[1].numpy()\n",
    "# result[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-dd3388f2e18c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "# d = result[:,0]\n",
    "# plt.hist(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(l.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test_data = torch.tensor(features_test.values).type((torch.FloatTensor))\n",
    "if use_cuda:\n",
    "    features_test_data = features_test_data.cuda()\n",
    "predicted_class = model(features_test_data)\n",
    "# We will look at the predicted prices to ensure we have something sensible.\n",
    "predicted_class = predicted_class.data.cpu().max(1, keepdim=True)[1].numpy()[:,0]\n",
    "predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = pd.DataFrame({\"ID\":id, \"class\":predicted_class})\n",
    "solution.to_csv(\"pokemon_sol.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
