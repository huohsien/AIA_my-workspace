Epoch: 1 	Training Loss: 1.795148 	Validation Loss: 1.803154 	 time: 0.3
Validation loss decreased from inf to 1.803154. Model was saved
Epoch: 2 	Training Loss: 1.803164 	Validation Loss: 1.796782 	 time: 0.3
Validation loss decreased from 1.803154 to 1.796782. Model was saved
Epoch: 3 	Training Loss: 1.796780 	Validation Loss: 1.792025 	 time: 0.3
Validation loss decreased from 1.796782 to 1.792025. Model was saved
Epoch: 4 	Training Loss: 1.791974 	Validation Loss: 1.790007 	 time: 0.3
Validation loss decreased from 1.792025 to 1.790007. Model was saved
Epoch: 5 	Training Loss: 1.789765 	Validation Loss: 1.787506 	 time: 0.3
Validation loss decreased from 1.790007 to 1.787506. Model was saved
Epoch: 6 	Training Loss: 1.786907 	Validation Loss: 1.784308 	 time: 0.3
Validation loss decreased from 1.787506 to 1.784308. Model was saved
Epoch: 7 	Training Loss: 1.783630 	Validation Loss: 1.778323 	 time: 0.3
Validation loss decreased from 1.784308 to 1.778323. Model was saved
Epoch: 8 	Training Loss: 1.777009 	Validation Loss: 1.769254 	 time: 0.3
Validation loss decreased from 1.778323 to 1.769254. Model was saved
Epoch: 9 	Training Loss: 1.766672 	Validation Loss: 1.757209 	 time: 0.3
Validation loss decreased from 1.769254 to 1.757209. Model was saved
Epoch: 10 	Training Loss: 1.753440 	Validation Loss: 1.743306 	 time: 0.3
Validation loss decreased from 1.757209 to 1.743306. Model was saved
Epoch: 11 	Training Loss: 1.737806 	Validation Loss: 1.728506 	 time: 0.3
Validation loss decreased from 1.743306 to 1.728506. Model was saved
Epoch: 12 	Training Loss: 1.721402 	Validation Loss: 1.711424 	 time: 0.3
Validation loss decreased from 1.728506 to 1.711424. Model was saved
Epoch: 13 	Training Loss: 1.704472 	Validation Loss: 1.695810 	 time: 0.3
Validation loss decreased from 1.711424 to 1.695810. Model was saved
Epoch: 14 	Training Loss: 1.686763 	Validation Loss: 1.682634 	 time: 0.3
Validation loss decreased from 1.695810 to 1.682634. Model was saved
Epoch: 15 	Training Loss: 1.669450 	Validation Loss: 1.669862 	 time: 0.3
Validation loss decreased from 1.682634 to 1.669862. Model was saved
Epoch: 16 	Training Loss: 1.653530 	Validation Loss: 1.658526 	 time: 0.3
Validation loss decreased from 1.669862 to 1.658526. Model was saved
Epoch: 17 	Training Loss: 1.637402 	Validation Loss: 1.648044 	 time: 0.3
Validation loss decreased from 1.658526 to 1.648044. Model was saved
Epoch: 18 	Training Loss: 1.622702 	Validation Loss: 1.639762 	 time: 0.3
Validation loss decreased from 1.648044 to 1.639762. Model was saved
Epoch: 19 	Training Loss: 1.608905 	Validation Loss: 1.633091 	 time: 0.3
Validation loss decreased from 1.639762 to 1.633091. Model was saved
Epoch: 20 	Training Loss: 1.596645 	Validation Loss: 1.627875 	 time: 0.3
Validation loss decreased from 1.633091 to 1.627875. Model was saved
Epoch: 21 	Training Loss: 1.585849 	Validation Loss: 1.619960 	 time: 0.3
Validation loss decreased from 1.627875 to 1.619960. Model was saved
Epoch: 22 	Training Loss: 1.577228 	Validation Loss: 1.618559 	 time: 0.3
Validation loss decreased from 1.619960 to 1.618559. Model was saved
Epoch: 23 	Training Loss: 1.568974 	Validation Loss: 1.608171 	 time: 0.3
Validation loss decreased from 1.618559 to 1.608171. Model was saved
Epoch: 24 	Training Loss: 1.557297 	Validation Loss: 1.600614 	 time: 0.3
Validation loss decreased from 1.608171 to 1.600614. Model was saved
Epoch: 25 	Training Loss: 1.549761 	Validation Loss: 1.592120 	 time: 0.3
Validation loss decreased from 1.600614 to 1.592120. Model was saved
Epoch: 26 	Training Loss: 1.541284 	Validation Loss: 1.585764 	 time: 0.3
Validation loss decreased from 1.592120 to 1.585764. Model was saved
Epoch: 27 	Training Loss: 1.533395 	Validation Loss: 1.582576 	 time: 0.3
Validation loss decreased from 1.585764 to 1.582576. Model was saved
Epoch: 28 	Training Loss: 1.527783 	Validation Loss: 1.573116 	 time: 0.3
Validation loss decreased from 1.582576 to 1.573116. Model was saved
Epoch: 29 	Training Loss: 1.520460 	Validation Loss: 1.568003 	 time: 0.3
Validation loss decreased from 1.573116 to 1.568003. Model was saved
Epoch: 30 	Training Loss: 1.515770 	Validation Loss: 1.565283 	 time: 0.3
Validation loss decreased from 1.568003 to 1.565283. Model was saved
Epoch: 31 	Training Loss: 1.508928 	Validation Loss: 1.563679 	 time: 0.3
Validation loss decreased from 1.565283 to 1.563679. Model was saved
Epoch: 32 	Training Loss: 1.504882 	Validation Loss: 1.558688 	 time: 0.3
Validation loss decreased from 1.563679 to 1.558688. Model was saved
Epoch: 33 	Training Loss: 1.499563 	Validation Loss: 1.556364 	 time: 0.3
Validation loss decreased from 1.558688 to 1.556364. Model was saved
Epoch: 34 	Training Loss: 1.495371 	Validation Loss: 1.553476 	 time: 0.3
Validation loss decreased from 1.556364 to 1.553476. Model was saved
Epoch: 35 	Training Loss: 1.491352 	Validation Loss: 1.549996 	 time: 0.3
Validation loss decreased from 1.553476 to 1.549996. Model was saved
Epoch: 36 	Training Loss: 1.486973 	Validation Loss: 1.548274 	 time: 0.3
Validation loss decreased from 1.549996 to 1.548274. Model was saved
Epoch: 37 	Training Loss: 1.483479 	Validation Loss: 1.544742 	 time: 0.3
Validation loss decreased from 1.548274 to 1.544742. Model was saved
Epoch: 38 	Training Loss: 1.478834 	Validation Loss: 1.541637 	 time: 0.3
Validation loss decreased from 1.544742 to 1.541637. Model was saved
Epoch: 39 	Training Loss: 1.475725 	Validation Loss: 1.540751 	 time: 0.3
Validation loss decreased from 1.541637 to 1.540751. Model was saved
Epoch: 40 	Training Loss: 1.471889 	Validation Loss: 1.536425 	 time: 0.3
Validation loss decreased from 1.540751 to 1.536425. Model was saved
Epoch: 41 	Training Loss: 1.467586 	Validation Loss: 1.534002 	 time: 0.3
Validation loss decreased from 1.536425 to 1.534002. Model was saved
Epoch: 42 	Training Loss: 1.464230 	Validation Loss: 1.532495 	 time: 0.3
Validation loss decreased from 1.534002 to 1.532495. Model was saved
Epoch: 43 	Training Loss: 1.460476 	Validation Loss: 1.531899 	 time: 0.3
Validation loss decreased from 1.532495 to 1.531899. Model was saved
Epoch: 44 	Training Loss: 1.457269 	Validation Loss: 1.526483 	 time: 0.3
Validation loss decreased from 1.531899 to 1.526483. Model was saved
Epoch: 45 	Training Loss: 1.455058 	Validation Loss: 1.527004 	 time: 0.3
Epoch: 46 	Training Loss: 1.451604 	Validation Loss: 1.527676 	 time: 0.3
Epoch: 47 	Training Loss: 1.448212 	Validation Loss: 1.521566 	 time: 0.3
Validation loss decreased from 1.526483 to 1.521566. Model was saved
Epoch: 48 	Training Loss: 1.443059 	Validation Loss: 1.519816 	 time: 0.3
Validation loss decreased from 1.521566 to 1.519816. Model was saved
Epoch: 49 	Training Loss: 1.441954 	Validation Loss: 1.522382 	 time: 0.3
Epoch: 50 	Training Loss: 1.439613 	Validation Loss: 1.520344 	 time: 0.3
Epoch: 51 	Training Loss: 1.435061 	Validation Loss: 1.515772 	 time: 0.3
Validation loss decreased from 1.519816 to 1.515772. Model was saved
Epoch: 52 	Training Loss: 1.433489 	Validation Loss: 1.515639 	 time: 0.3
Validation loss decreased from 1.515772 to 1.515639. Model was saved
Epoch: 53 	Training Loss: 1.431559 	Validation Loss: 1.518381 	 time: 0.3
Epoch: 54 	Training Loss: 1.428132 	Validation Loss: 1.514317 	 time: 0.3
Validation loss decreased from 1.515639 to 1.514317. Model was saved
Epoch: 55 	Training Loss: 1.425272 	Validation Loss: 1.512650 	 time: 0.3
Validation loss decreased from 1.514317 to 1.512650. Model was saved
Epoch: 56 	Training Loss: 1.425244 	Validation Loss: 1.514464 	 time: 0.3
Epoch: 57 	Training Loss: 1.421751 	Validation Loss: 1.514984 	 time: 0.3
Epoch: 58 	Training Loss: 1.419191 	Validation Loss: 1.509549 	 time: 0.3
Validation loss decreased from 1.512650 to 1.509549. Model was saved
Epoch: 59 	Training Loss: 1.417066 	Validation Loss: 1.509388 	 time: 0.3
Validation loss decreased from 1.509549 to 1.509388. Model was saved
Epoch: 60 	Training Loss: 1.415676 	Validation Loss: 1.512081 	 time: 0.3
Epoch: 61 	Training Loss: 1.414133 	Validation Loss: 1.507651 	 time: 0.3
Validation loss decreased from 1.509388 to 1.507651. Model was saved
Epoch: 62 	Training Loss: 1.410951 	Validation Loss: 1.502565 	 time: 0.3
Validation loss decreased from 1.507651 to 1.502565. Model was saved
Epoch: 63 	Training Loss: 1.409263 	Validation Loss: 1.504568 	 time: 0.3
Epoch: 64 	Training Loss: 1.407520 	Validation Loss: 1.505104 	 time: 0.3
Epoch: 65 	Training Loss: 1.405940 	Validation Loss: 1.499411 	 time: 0.3
Validation loss decreased from 1.502565 to 1.499411. Model was saved
Epoch: 66 	Training Loss: 1.404097 	Validation Loss: 1.498649 	 time: 0.3
Validation loss decreased from 1.499411 to 1.498649. Model was saved
Epoch: 67 	Training Loss: 1.401668 	Validation Loss: 1.500027 	 time: 0.3
Epoch: 68 	Training Loss: 1.400522 	Validation Loss: 1.495211 	 time: 0.3
Validation loss decreased from 1.498649 to 1.495211. Model was saved
Epoch: 69 	Training Loss: 1.398887 	Validation Loss: 1.494018 	 time: 0.3
Validation loss decreased from 1.495211 to 1.494018. Model was saved
Epoch: 70 	Training Loss: 1.397471 	Validation Loss: 1.494328 	 time: 0.3
Epoch: 71 	Training Loss: 1.395991 	Validation Loss: 1.490685 	 time: 0.3
Validation loss decreased from 1.494018 to 1.490685. Model was saved
Epoch: 72 	Training Loss: 1.393742 	Validation Loss: 1.487662 	 time: 0.3
Validation loss decreased from 1.490685 to 1.487662. Model was saved
Epoch: 73 	Training Loss: 1.392385 	Validation Loss: 1.489219 	 time: 0.3
Epoch: 74 	Training Loss: 1.391010 	Validation Loss: 1.485721 	 time: 0.3
Validation loss decreased from 1.487662 to 1.485721. Model was saved
Epoch: 75 	Training Loss: 1.389276 	Validation Loss: 1.483016 	 time: 0.3
Validation loss decreased from 1.485721 to 1.483016. Model was saved
Epoch: 76 	Training Loss: 1.388221 	Validation Loss: 1.485905 	 time: 0.3
Epoch: 77 	Training Loss: 1.386953 	Validation Loss: 1.482185 	 time: 0.3
Validation loss decreased from 1.483016 to 1.482185. Model was saved
Epoch: 78 	Training Loss: 1.385312 	Validation Loss: 1.479539 	 time: 0.3
Validation loss decreased from 1.482185 to 1.479539. Model was saved
Epoch: 79 	Training Loss: 1.383757 	Validation Loss: 1.481834 	 time: 0.3
Epoch: 80 	Training Loss: 1.382244 	Validation Loss: 1.479350 	 time: 0.3
Validation loss decreased from 1.479539 to 1.479350. Model was saved
Epoch: 81 	Training Loss: 1.380514 	Validation Loss: 1.477512 	 time: 0.3
Validation loss decreased from 1.479350 to 1.477512. Model was saved
Epoch: 82 	Training Loss: 1.378927 	Validation Loss: 1.479107 	 time: 0.3
Epoch: 83 	Training Loss: 1.377541 	Validation Loss: 1.476726 	 time: 0.3
Validation loss decreased from 1.477512 to 1.476726. Model was saved
Epoch: 84 	Training Loss: 1.376286 	Validation Loss: 1.475618 	 time: 0.3
Validation loss decreased from 1.476726 to 1.475618. Model was saved
Epoch: 85 	Training Loss: 1.374961 	Validation Loss: 1.475368 	 time: 0.3
Validation loss decreased from 1.475618 to 1.475368. Model was saved
Epoch: 86 	Training Loss: 1.373639 	Validation Loss: 1.475130 	 time: 0.3
Validation loss decreased from 1.475368 to 1.475130. Model was saved
Epoch: 87 	Training Loss: 1.373212 	Validation Loss: 1.474423 	 time: 0.3
Validation loss decreased from 1.475130 to 1.474423. Model was saved
Epoch: 88 	Training Loss: 1.373392 	Validation Loss: 1.477619 	 time: 0.3
Epoch: 89 	Training Loss: 1.374872 	Validation Loss: 1.473999 	 time: 0.3
Validation loss decreased from 1.474423 to 1.473999. Model was saved
Epoch: 90 	Training Loss: 1.369373 	Validation Loss: 1.470975 	 time: 0.3
Validation loss decreased from 1.473999 to 1.470975. Model was saved
Epoch: 91 	Training Loss: 1.367509 	Validation Loss: 1.471357 	 time: 0.3
Epoch: 92 	Training Loss: 1.367907 	Validation Loss: 1.471386 	 time: 0.3
Epoch: 93 	Training Loss: 1.364584 	Validation Loss: 1.470710 	 time: 0.3
Validation loss decreased from 1.470975 to 1.470710. Model was saved
Epoch: 94 	Training Loss: 1.364392 	Validation Loss: 1.467300 	 time: 0.3
Validation loss decreased from 1.470710 to 1.467300. Model was saved
Epoch: 95 	Training Loss: 1.363769 	Validation Loss: 1.465545 	 time: 0.3
Validation loss decreased from 1.467300 to 1.465545. Model was saved
Epoch: 96 	Training Loss: 1.360577 	Validation Loss: 1.467499 	 time: 0.3
Epoch: 97 	Training Loss: 1.361233 	Validation Loss: 1.461524 	 time: 0.3
Validation loss decreased from 1.465545 to 1.461524. Model was saved
Epoch: 98 	Training Loss: 1.359147 	Validation Loss: 1.455580 	 time: 0.3
Validation loss decreased from 1.461524 to 1.455580. Model was saved
Epoch: 99 	Training Loss: 1.357538 	Validation Loss: 1.461408 	 time: 0.3
Epoch: 100 	Training Loss: 1.357118 	Validation Loss: 1.454701 	 time: 0.3
Validation loss decreased from 1.455580 to 1.454701. Model was saved
Epoch: 101 	Training Loss: 1.353959 	Validation Loss: 1.449738 	 time: 0.3
Validation loss decreased from 1.454701 to 1.449738. Model was saved
Epoch: 102 	Training Loss: 1.354299 	Validation Loss: 1.454125 	 time: 0.3
Epoch: 103 	Training Loss: 1.351779 	Validation Loss: 1.451063 	 time: 0.3
Epoch: 104 	Training Loss: 1.350550 	Validation Loss: 1.445373 	 time: 0.3
Validation loss decreased from 1.449738 to 1.445373. Model was saved
Epoch: 105 	Training Loss: 1.349893 	Validation Loss: 1.444621 	 time: 0.3
Validation loss decreased from 1.445373 to 1.444621. Model was saved
Epoch: 106 	Training Loss: 1.347372 	Validation Loss: 1.446287 	 time: 0.3
Epoch: 107 	Training Loss: 1.347217 	Validation Loss: 1.441608 	 time: 0.3
Validation loss decreased from 1.444621 to 1.441608. Model was saved
Epoch: 108 	Training Loss: 1.345514 	Validation Loss: 1.438362 	 time: 0.3
Validation loss decreased from 1.441608 to 1.438362. Model was saved
Epoch: 109 	Training Loss: 1.344205 	Validation Loss: 1.442374 	 time: 0.3
Epoch: 110 	Training Loss: 1.343323 	Validation Loss: 1.438267 	 time: 0.3
Validation loss decreased from 1.438362 to 1.438267. Model was saved
Epoch: 111 	Training Loss: 1.341361 	Validation Loss: 1.437554 	 time: 0.3
Validation loss decreased from 1.438267 to 1.437554. Model was saved
Epoch: 112 	Training Loss: 1.340825 	Validation Loss: 1.438909 	 time: 0.3
Epoch: 113 	Training Loss: 1.339297 	Validation Loss: 1.438660 	 time: 0.3
Epoch: 114 	Training Loss: 1.337682 	Validation Loss: 1.436067 	 time: 0.3
Validation loss decreased from 1.437554 to 1.436067. Model was saved
Epoch: 115 	Training Loss: 1.337314 	Validation Loss: 1.439280 	 time: 0.3
Epoch: 116 	Training Loss: 1.336129 	Validation Loss: 1.437734 	 time: 0.3
Epoch: 117 	Training Loss: 1.335857 	Validation Loss: 1.442794 	 time: 0.3
Epoch: 118 	Training Loss: 1.337798 	Validation Loss: 1.434894 	 time: 0.3
Validation loss decreased from 1.436067 to 1.434894. Model was saved
Epoch: 119 	Training Loss: 1.335865 	Validation Loss: 1.446355 	 time: 0.3
Epoch: 120 	Training Loss: 1.335553 	Validation Loss: 1.432981 	 time: 0.3
Validation loss decreased from 1.434894 to 1.432981. Model was saved
Epoch: 121 	Training Loss: 1.331316 	Validation Loss: 1.433730 	 time: 0.3
Epoch: 122 	Training Loss: 1.331544 	Validation Loss: 1.448945 	 time: 0.3
Epoch: 123 	Training Loss: 1.335059 	Validation Loss: 1.431981 	 time: 0.3
Validation loss decreased from 1.432981 to 1.431981. Model was saved
Epoch: 124 	Training Loss: 1.327347 	Validation Loss: 1.433435 	 time: 0.3
Epoch: 125 	Training Loss: 1.332927 	Validation Loss: 1.453266 	 time: 0.3
Epoch: 126 	Training Loss: 1.339019 	Validation Loss: 1.437270 	 time: 0.3
Epoch: 127 	Training Loss: 1.327057 	Validation Loss: 1.432537 	 time: 0.3
Epoch: 128 	Training Loss: 1.337066 	Validation Loss: 1.434170 	 time: 0.3
Epoch: 129 	Training Loss: 1.328863 	Validation Loss: 1.448746 	 time: 0.3
Epoch: 130 	Training Loss: 1.331431 	Validation Loss: 1.434766 	 time: 0.3
Epoch: 131 	Training Loss: 1.324346 	Validation Loss: 1.431102 	 time: 0.3
Validation loss decreased from 1.431981 to 1.431102. Model was saved
Epoch: 132 	Training Loss: 1.328986 	Validation Loss: 1.426925 	 time: 0.3
Validation loss decreased from 1.431102 to 1.426925. Model was saved
Epoch: 133 	Training Loss: 1.323402 	Validation Loss: 1.439096 	 time: 0.3
Epoch: 134 	Training Loss: 1.325170 	Validation Loss: 1.430362 	 time: 0.3
Epoch: 135 	Training Loss: 1.320355 	Validation Loss: 1.426087 	 time: 0.3
Validation loss decreased from 1.426925 to 1.426087. Model was saved
Epoch: 136 	Training Loss: 1.322605 	Validation Loss: 1.418760 	 time: 0.3
Validation loss decreased from 1.426087 to 1.418760. Model was saved
Epoch: 137 	Training Loss: 1.318929 	Validation Loss: 1.426864 	 time: 0.3
Epoch: 138 	Training Loss: 1.318314 	Validation Loss: 1.426094 	 time: 0.3
Epoch: 139 	Training Loss: 1.317187 	Validation Loss: 1.418702 	 time: 0.3
Validation loss decreased from 1.418760 to 1.418702. Model was saved
Epoch: 140 	Training Loss: 1.315087 	Validation Loss: 1.415003 	 time: 0.3
Validation loss decreased from 1.418702 to 1.415003. Model was saved
Epoch: 141 	Training Loss: 1.314871 	Validation Loss: 1.417268 	 time: 0.3
Epoch: 142 	Training Loss: 1.312665 	Validation Loss: 1.421808 	 time: 0.3
Epoch: 143 	Training Loss: 1.312454 	Validation Loss: 1.417768 	 time: 0.3
Epoch: 144 	Training Loss: 1.310974 	Validation Loss: 1.413987 	 time: 0.3
Validation loss decreased from 1.415003 to 1.413987. Model was saved
Epoch: 145 	Training Loss: 1.309606 	Validation Loss: 1.412234 	 time: 0.3
Validation loss decreased from 1.413987 to 1.412234. Model was saved
Epoch: 146 	Training Loss: 1.309755 	Validation Loss: 1.413060 	 time: 0.3
Epoch: 147 	Training Loss: 1.307050 	Validation Loss: 1.412101 	 time: 0.3
Validation loss decreased from 1.412234 to 1.412101. Model was saved
Epoch: 148 	Training Loss: 1.307885 	Validation Loss: 1.406746 	 time: 0.3
Validation loss decreased from 1.412101 to 1.406746. Model was saved
Epoch: 149 	Training Loss: 1.304995 	Validation Loss: 1.407726 	 time: 0.3
Epoch: 150 	Training Loss: 1.306041 	Validation Loss: 1.406883 	 time: 0.3
Epoch: 151 	Training Loss: 1.302797 	Validation Loss: 1.408983 	 time: 0.3
Epoch: 152 	Training Loss: 1.303858 	Validation Loss: 1.404992 	 time: 0.3
Validation loss decreased from 1.406746 to 1.404992. Model was saved
Epoch: 153 	Training Loss: 1.300932 	Validation Loss: 1.404455 	 time: 0.3
Validation loss decreased from 1.404992 to 1.404455. Model was saved
Epoch: 154 	Training Loss: 1.301550 	Validation Loss: 1.402191 	 time: 0.3
Validation loss decreased from 1.404455 to 1.402191. Model was saved
Epoch: 155 	Training Loss: 1.299263 	Validation Loss: 1.405220 	 time: 0.3
Epoch: 156 	Training Loss: 1.299253 	Validation Loss: 1.403308 	 time: 0.3
Epoch: 157 	Training Loss: 1.297220 	Validation Loss: 1.402455 	 time: 0.3
Epoch: 158 	Training Loss: 1.297089 	Validation Loss: 1.403088 	 time: 0.3
Epoch: 159 	Training Loss: 1.295080 	Validation Loss: 1.406522 	 time: 0.3
Epoch: 160 	Training Loss: 1.294975 	Validation Loss: 1.403260 	 time: 0.3
Epoch: 161 	Training Loss: 1.293129 	Validation Loss: 1.402599 	 time: 0.3
Epoch: 162 	Training Loss: 1.292819 	Validation Loss: 1.403935 	 time: 0.3
Epoch: 163 	Training Loss: 1.292047 	Validation Loss: 1.407154 	 time: 0.3
Epoch: 164 	Training Loss: 1.293326 	Validation Loss: 1.406501 	 time: 0.3
Epoch: 165 	Training Loss: 1.295348 	Validation Loss: 1.414551 	 time: 0.3
Epoch: 166 	Training Loss: 1.297413 	Validation Loss: 1.401314 	 time: 0.3
Validation loss decreased from 1.402191 to 1.401314. Model was saved
Epoch: 167 	Training Loss: 1.290713 	Validation Loss: 1.400068 	 time: 0.3
Validation loss decreased from 1.401314 to 1.400068. Model was saved
Epoch: 168 	Training Loss: 1.288492 	Validation Loss: 1.408552 	 time: 0.3
Epoch: 169 	Training Loss: 1.291423 	Validation Loss: 1.398447 	 time: 0.3
Validation loss decreased from 1.400068 to 1.398447. Model was saved
Epoch: 170 	Training Loss: 1.285292 	Validation Loss: 1.400667 	 time: 0.3
Epoch: 171 	Training Loss: 1.287032 	Validation Loss: 1.406085 	 time: 0.3
Epoch: 172 	Training Loss: 1.286378 	Validation Loss: 1.399062 	 time: 0.3
Epoch: 173 	Training Loss: 1.283046 	Validation Loss: 1.399202 	 time: 0.3
Epoch: 174 	Training Loss: 1.284070 	Validation Loss: 1.400385 	 time: 0.3
Epoch: 175 	Training Loss: 1.282251 	Validation Loss: 1.398160 	 time: 0.3
Validation loss decreased from 1.398447 to 1.398160. Model was saved
Epoch: 176 	Training Loss: 1.281334 	Validation Loss: 1.394799 	 time: 0.3
Validation loss decreased from 1.398160 to 1.394799. Model was saved
Epoch: 177 	Training Loss: 1.280841 	Validation Loss: 1.396130 	 time: 0.3
Epoch: 178 	Training Loss: 1.279269 	Validation Loss: 1.401525 	 time: 0.3
Epoch: 179 	Training Loss: 1.280220 	Validation Loss: 1.395040 	 time: 0.3
Epoch: 180 	Training Loss: 1.276996 	Validation Loss: 1.394911 	 time: 0.3
Epoch: 181 	Training Loss: 1.278613 	Validation Loss: 1.400681 	 time: 0.3
Epoch: 182 	Training Loss: 1.278271 	Validation Loss: 1.399387 	 time: 0.3
Epoch: 183 	Training Loss: 1.275307 	Validation Loss: 1.396278 	 time: 0.3
Epoch: 184 	Training Loss: 1.277409 	Validation Loss: 1.397186 	 time: 0.3
Epoch: 185 	Training Loss: 1.276695 	Validation Loss: 1.396938 	 time: 0.3
Epoch: 186 	Training Loss: 1.274214 	Validation Loss: 1.397018 	 time: 0.3
Epoch: 187 	Training Loss: 1.275745 	Validation Loss: 1.394686 	 time: 0.3
Validation loss decreased from 1.394799 to 1.394686. Model was saved
Epoch: 188 	Training Loss: 1.273312 	Validation Loss: 1.395962 	 time: 0.3
Epoch: 189 	Training Loss: 1.274449 	Validation Loss: 1.393446 	 time: 0.3
Validation loss decreased from 1.394686 to 1.393446. Model was saved
Epoch: 190 	Training Loss: 1.271798 	Validation Loss: 1.394370 	 time: 0.3
Epoch: 191 	Training Loss: 1.271412 	Validation Loss: 1.395629 	 time: 0.3
Epoch: 192 	Training Loss: 1.271482 	Validation Loss: 1.394203 	 time: 0.3
Epoch: 193 	Training Loss: 1.268785 	Validation Loss: 1.395599 	 time: 0.3
Epoch: 194 	Training Loss: 1.269919 	Validation Loss: 1.392988 	 time: 0.3
Validation loss decreased from 1.393446 to 1.392988. Model was saved
Epoch: 195 	Training Loss: 1.267850 	Validation Loss: 1.391402 	 time: 0.3
Validation loss decreased from 1.392988 to 1.391402. Model was saved
Epoch: 196 	Training Loss: 1.268183 	Validation Loss: 1.390739 	 time: 0.3
Validation loss decreased from 1.391402 to 1.390739. Model was saved
Epoch: 197 	Training Loss: 1.266927 	Validation Loss: 1.390828 	 time: 0.3
Epoch: 198 	Training Loss: 1.266739 	Validation Loss: 1.388879 	 time: 0.3
Validation loss decreased from 1.390739 to 1.388879. Model was saved
Epoch: 199 	Training Loss: 1.266684 	Validation Loss: 1.386244 	 time: 0.3
Validation loss decreased from 1.388879 to 1.386244. Model was saved
Epoch: 200 	Training Loss: 1.265092 	Validation Loss: 1.387221 	 time: 0.3
Epoch: 201 	Training Loss: 1.265732 	Validation Loss: 1.389063 	 time: 0.3
Epoch: 202 	Training Loss: 1.264999 	Validation Loss: 1.387488 	 time: 0.3
Epoch: 203 	Training Loss: 1.264637 	Validation Loss: 1.383607 	 time: 0.3
Validation loss decreased from 1.386244 to 1.383607. Model was saved
Epoch: 204 	Training Loss: 1.264014 	Validation Loss: 1.383662 	 time: 0.3
Epoch: 205 	Training Loss: 1.262662 	Validation Loss: 1.385925 	 time: 0.3
Epoch: 206 	Training Loss: 1.263677 	Validation Loss: 1.383407 	 time: 0.3
Validation loss decreased from 1.383607 to 1.383407. Model was saved
Epoch: 207 	Training Loss: 1.262220 	Validation Loss: 1.380144 	 time: 0.3
Validation loss decreased from 1.383407 to 1.380144. Model was saved
Epoch: 208 	Training Loss: 1.262037 	Validation Loss: 1.381821 	 time: 0.3
Epoch: 209 	Training Loss: 1.261661 	Validation Loss: 1.381363 	 time: 0.3
Epoch: 210 	Training Loss: 1.260204 	Validation Loss: 1.380340 	 time: 0.3
Epoch: 211 	Training Loss: 1.260882 	Validation Loss: 1.382026 	 time: 0.3
Epoch: 212 	Training Loss: 1.260518 	Validation Loss: 1.384019 	 time: 0.3
Epoch: 213 	Training Loss: 1.260745 	Validation Loss: 1.384040 	 time: 0.3
Epoch: 214 	Training Loss: 1.260689 	Validation Loss: 1.379915 	 time: 0.3
Validation loss decreased from 1.380144 to 1.379915. Model was saved
Epoch: 215 	Training Loss: 1.258578 	Validation Loss: 1.381716 	 time: 0.3
Epoch: 216 	Training Loss: 1.258697 	Validation Loss: 1.381216 	 time: 0.3
Epoch: 217 	Training Loss: 1.257660 	Validation Loss: 1.379014 	 time: 0.3
Validation loss decreased from 1.379915 to 1.379014. Model was saved
Epoch: 218 	Training Loss: 1.258011 	Validation Loss: 1.382023 	 time: 0.3
Epoch: 219 	Training Loss: 1.259161 	Validation Loss: 1.380628 	 time: 0.3
Epoch: 220 	Training Loss: 1.256951 	Validation Loss: 1.379922 	 time: 0.3
Epoch: 221 	Training Loss: 1.257184 	Validation Loss: 1.378250 	 time: 0.3
Validation loss decreased from 1.379014 to 1.378250. Model was saved
Epoch: 222 	Training Loss: 1.255347 	Validation Loss: 1.380302 	 time: 0.3
Epoch: 223 	Training Loss: 1.255793 	Validation Loss: 1.381906 	 time: 0.3
Epoch: 224 	Training Loss: 1.256338 	Validation Loss: 1.378340 	 time: 0.3
Epoch: 225 	Training Loss: 1.255243 	Validation Loss: 1.380573 	 time: 0.3
Epoch: 226 	Training Loss: 1.256267 	Validation Loss: 1.377798 	 time: 0.3
Validation loss decreased from 1.378250 to 1.377798. Model was saved
Epoch: 227 	Training Loss: 1.253312 	Validation Loss: 1.376780 	 time: 0.3
Validation loss decreased from 1.377798 to 1.376780. Model was saved
Epoch: 228 	Training Loss: 1.253729 	Validation Loss: 1.380512 	 time: 0.3
Epoch: 229 	Training Loss: 1.254058 	Validation Loss: 1.380835 	 time: 0.3
Epoch: 230 	Training Loss: 1.253883 	Validation Loss: 1.383588 	 time: 0.3
Epoch: 231 	Training Loss: 1.255390 	Validation Loss: 1.378760 	 time: 0.3
Epoch: 232 	Training Loss: 1.252367 	Validation Loss: 1.380236 	 time: 0.3
Epoch: 233 	Training Loss: 1.252464 	Validation Loss: 1.376039 	 time: 0.3
Validation loss decreased from 1.376780 to 1.376039. Model was saved
Epoch: 234 	Training Loss: 1.250399 	Validation Loss: 1.375119 	 time: 0.3
Validation loss decreased from 1.376039 to 1.375119. Model was saved
Epoch: 235 	Training Loss: 1.251065 	Validation Loss: 1.381861 	 time: 0.3
Epoch: 236 	Training Loss: 1.253428 	Validation Loss: 1.378403 	 time: 0.3
Epoch: 237 	Training Loss: 1.250953 	Validation Loss: 1.377628 	 time: 0.3
Epoch: 238 	Training Loss: 1.252154 	Validation Loss: 1.376968 	 time: 0.3
Epoch: 239 	Training Loss: 1.248534 	Validation Loss: 1.378638 	 time: 0.3
Epoch: 240 	Training Loss: 1.249126 	Validation Loss: 1.377440 	 time: 0.3
Epoch: 241 	Training Loss: 1.250145 	Validation Loss: 1.375237 	 time: 0.3
Epoch: 242 	Training Loss: 1.248117 	Validation Loss: 1.381236 	 time: 0.3
Epoch: 243 	Training Loss: 1.249798 	Validation Loss: 1.375328 	 time: 0.3
Epoch: 244 	Training Loss: 1.246118 	Validation Loss: 1.374543 	 time: 0.3
Validation loss decreased from 1.375119 to 1.374543. Model was saved
Epoch: 245 	Training Loss: 1.249062 	Validation Loss: 1.384390 	 time: 0.3
Epoch: 246 	Training Loss: 1.251507 	Validation Loss: 1.381143 	 time: 0.3
Epoch: 247 	Training Loss: 1.249213 	Validation Loss: 1.376139 	 time: 0.3
Epoch: 248 	Training Loss: 1.250725 	Validation Loss: 1.375743 	 time: 0.3
Epoch: 249 	Training Loss: 1.245225 	Validation Loss: 1.385193 	 time: 0.3
Epoch: 250 	Training Loss: 1.251135 	Validation Loss: 1.382948 	 time: 0.3
Epoch: 251 	Training Loss: 1.250708 	Validation Loss: 1.377356 	 time: 0.3
Epoch: 252 	Training Loss: 1.250272 	Validation Loss: 1.384126 	 time: 0.3
Epoch: 253 	Training Loss: 1.248233 	Validation Loss: 1.384587 	 time: 0.3
Epoch: 254 	Training Loss: 1.247453 	Validation Loss: 1.376532 	 time: 0.3
Epoch: 255 	Training Loss: 1.246601 	Validation Loss: 1.376350 	 time: 0.3
Epoch: 256 	Training Loss: 1.244773 	Validation Loss: 1.378864 	 time: 0.3
Epoch: 257 	Training Loss: 1.243355 	Validation Loss: 1.382990 	 time: 0.3
Epoch: 258 	Training Loss: 1.244574 	Validation Loss: 1.377735 	 time: 0.3
Epoch: 259 	Training Loss: 1.243495 	Validation Loss: 1.374170 	 time: 0.3
Validation loss decreased from 1.374543 to 1.374170. Model was saved
Epoch: 260 	Training Loss: 1.243362 	Validation Loss: 1.378666 	 time: 0.3
Epoch: 261 	Training Loss: 1.241731 	Validation Loss: 1.380758 	 time: 0.3
Epoch: 262 	Training Loss: 1.242747 	Validation Loss: 1.372484 	 time: 0.3
Validation loss decreased from 1.374170 to 1.372484. Model was saved
Epoch: 263 	Training Loss: 1.240124 	Validation Loss: 1.372439 	 time: 0.3
Validation loss decreased from 1.372484 to 1.372439. Model was saved
Epoch: 264 	Training Loss: 1.241570 	Validation Loss: 1.377448 	 time: 0.3
Epoch: 265 	Training Loss: 1.239994 	Validation Loss: 1.380391 	 time: 0.3
Epoch: 266 	Training Loss: 1.241554 	Validation Loss: 1.372049 	 time: 0.3
Validation loss decreased from 1.372439 to 1.372049. Model was saved
Epoch: 267 	Training Loss: 1.238947 	Validation Loss: 1.373041 	 time: 0.3
Epoch: 268 	Training Loss: 1.240387 	Validation Loss: 1.376006 	 time: 0.3
Epoch: 269 	Training Loss: 1.238227 	Validation Loss: 1.377483 	 time: 0.3
Epoch: 270 	Training Loss: 1.239354 	Validation Loss: 1.370908 	 time: 0.3
Validation loss decreased from 1.372049 to 1.370908. Model was saved
Epoch: 271 	Training Loss: 1.237311 	Validation Loss: 1.371391 	 time: 0.3
Epoch: 272 	Training Loss: 1.239017 	Validation Loss: 1.375485 	 time: 0.3
Epoch: 273 	Training Loss: 1.237395 	Validation Loss: 1.378520 	 time: 0.3
Epoch: 274 	Training Loss: 1.238834 	Validation Loss: 1.371906 	 time: 0.3
Epoch: 275 	Training Loss: 1.236730 	Validation Loss: 1.371202 	 time: 0.3
Epoch: 276 	Training Loss: 1.237688 	Validation Loss: 1.375225 	 time: 0.3
Epoch: 277 	Training Loss: 1.235636 	Validation Loss: 1.380672 	 time: 0.3
Epoch: 278 	Training Loss: 1.236981 	Validation Loss: 1.372694 	 time: 0.3
Epoch: 279 	Training Loss: 1.234765 	Validation Loss: 1.372256 	 time: 0.3
Epoch: 280 	Training Loss: 1.236254 	Validation Loss: 1.376270 	 time: 0.3
Epoch: 281 	Training Loss: 1.234675 	Validation Loss: 1.380188 	 time: 0.3
Epoch: 282 	Training Loss: 1.236362 	Validation Loss: 1.373652 	 time: 0.3
Epoch: 283 	Training Loss: 1.234521 	Validation Loss: 1.371217 	 time: 0.3
Epoch: 284 	Training Loss: 1.235353 	Validation Loss: 1.374492 	 time: 0.3
Epoch: 285 	Training Loss: 1.233497 	Validation Loss: 1.378924 	 time: 0.3
Epoch: 286 	Training Loss: 1.234593 	Validation Loss: 1.372027 	 time: 0.3
Epoch: 287 	Training Loss: 1.233008 	Validation Loss: 1.372805 	 time: 0.3
Epoch: 288 	Training Loss: 1.234040 	Validation Loss: 1.374488 	 time: 0.3
Epoch: 289 	Training Loss: 1.232351 	Validation Loss: 1.378828 	 time: 0.3
Epoch: 290 	Training Loss: 1.233356 	Validation Loss: 1.374197 	 time: 0.3
Epoch: 291 	Training Loss: 1.232050 	Validation Loss: 1.371925 	 time: 0.3
Epoch: 292 	Training Loss: 1.233173 	Validation Loss: 1.375865 	 time: 0.3
Epoch: 293 	Training Loss: 1.231894 	Validation Loss: 1.379000 	 time: 0.3
Epoch: 294 	Training Loss: 1.232393 	Validation Loss: 1.373200 	 time: 0.3
Epoch: 295 	Training Loss: 1.230896 	Validation Loss: 1.372312 	 time: 0.3
Epoch: 296 	Training Loss: 1.231701 	Validation Loss: 1.375586 	 time: 0.3
Epoch: 297 	Training Loss: 1.230543 	Validation Loss: 1.379217 	 time: 0.3
Epoch: 298 	Training Loss: 1.231103 	Validation Loss: 1.373783 	 time: 0.3
Epoch: 299 	Training Loss: 1.230053 	Validation Loss: 1.372767 	 time: 0.3
Epoch: 300 	Training Loss: 1.230549 	Validation Loss: 1.377405 	 time: 0.3
Epoch: 301 	Training Loss: 1.229781 	Validation Loss: 1.379054 	 time: 0.3
Epoch: 302 	Training Loss: 1.230196 	Validation Loss: 1.375570 	 time: 0.3
Epoch: 303 	Training Loss: 1.229558 	Validation Loss: 1.373896 	 time: 0.3
Epoch: 304 	Training Loss: 1.229627 	Validation Loss: 1.379051 	 time: 0.3
Epoch: 305 	Training Loss: 1.228975 	Validation Loss: 1.379307 	 time: 0.3
Epoch: 306 	Training Loss: 1.228860 	Validation Loss: 1.375262 	 time: 0.3
Epoch: 307 	Training Loss: 1.228456 	Validation Loss: 1.375699 	 time: 0.3
Epoch: 308 	Training Loss: 1.228268 	Validation Loss: 1.378833 	 time: 0.3
Epoch: 309 	Training Loss: 1.228105 	Validation Loss: 1.377601 	 time: 0.3
Epoch: 310 	Training Loss: 1.227792 	Validation Loss: 1.373830 	 time: 0.3
Epoch: 311 	Training Loss: 1.227810 	Validation Loss: 1.375521 	 time: 0.3
Epoch: 312 	Training Loss: 1.227373 	Validation Loss: 1.376055 	 time: 0.3
Epoch: 313 	Training Loss: 1.227420 	Validation Loss: 1.374984 	 time: 0.3
Epoch: 314 	Training Loss: 1.226948 	Validation Loss: 1.373612 	 time: 0.3
Epoch: 315 	Training Loss: 1.227002 	Validation Loss: 1.375560 	 time: 0.3
Epoch: 316 	Training Loss: 1.226597 	Validation Loss: 1.375811 	 time: 0.3
Epoch: 317 	Training Loss: 1.226570 	Validation Loss: 1.374912 	 time: 0.3
Epoch: 318 	Training Loss: 1.226303 	Validation Loss: 1.375215 	 time: 0.3
Epoch: 319 	Training Loss: 1.226182 	Validation Loss: 1.376156 	 time: 0.3
Epoch: 320 	Training Loss: 1.226008 	Validation Loss: 1.376792 	 time: 0.3
Epoch: 321 	Training Loss: 1.225798 	Validation Loss: 1.375473 	 time: 0.3
Epoch: 322 	Training Loss: 1.225662 	Validation Loss: 1.376200 	 time: 0.3
Epoch: 323 	Training Loss: 1.225415 	Validation Loss: 1.376796 	 time: 0.3
Epoch: 324 	Training Loss: 1.225340 	Validation Loss: 1.376875 	 time: 0.3
Epoch: 325 	Training Loss: 1.225101 	Validation Loss: 1.375848 	 time: 0.3
Epoch: 326 	Training Loss: 1.225016 	Validation Loss: 1.377226 	 time: 0.3
Epoch: 327 	Training Loss: 1.224764 	Validation Loss: 1.376738 	 time: 0.3
Epoch: 328 	Training Loss: 1.224583 	Validation Loss: 1.376472 	 time: 0.3
Epoch: 329 	Training Loss: 1.224369 	Validation Loss: 1.375748 	 time: 0.3
Epoch: 330 	Training Loss: 1.224163 	Validation Loss: 1.376827 	 time: 0.3
Epoch: 331 	Training Loss: 1.224028 	Validation Loss: 1.375628 	 time: 0.3
Epoch: 332 	Training Loss: 1.223846 	Validation Loss: 1.376683 	 time: 0.3
Epoch: 333 	Training Loss: 1.223774 	Validation Loss: 1.375792 	 time: 0.3
Epoch: 334 	Training Loss: 1.223632 	Validation Loss: 1.377462 	 time: 0.3
Epoch: 335 	Training Loss: 1.223621 	Validation Loss: 1.376103 	 time: 0.3
Epoch: 336 	Training Loss: 1.223663 	Validation Loss: 1.378884 	 time: 0.3
Epoch: 337 	Training Loss: 1.223960 	Validation Loss: 1.377693 	 time: 0.3
Epoch: 338 	Training Loss: 1.224562 	Validation Loss: 1.383470 	 time: 0.3
Epoch: 339 	Training Loss: 1.225700 	Validation Loss: 1.380859 	 time: 0.3
Epoch: 340 	Training Loss: 1.226988 	Validation Loss: 1.386799 	 time: 0.3
Epoch: 341 	Training Loss: 1.228614 	Validation Loss: 1.383807 	 time: 0.3
Epoch: 342 	Training Loss: 1.226795 	Validation Loss: 1.378983 	 time: 0.3
Epoch: 343 	Training Loss: 1.223310 	Validation Loss: 1.382893 	 time: 0.3
Epoch: 344 	Training Loss: 1.224134 	Validation Loss: 1.382110 	 time: 0.3
Epoch: 345 	Training Loss: 1.225414 	Validation Loss: 1.380561 	 time: 0.3
Epoch: 346 	Training Loss: 1.224071 	Validation Loss: 1.379373 	 time: 0.3
Epoch: 347 	Training Loss: 1.222294 	Validation Loss: 1.379862 	 time: 0.3
Epoch: 348 	Training Loss: 1.224004 	Validation Loss: 1.380040 	 time: 0.3
Epoch: 349 	Training Loss: 1.223422 	Validation Loss: 1.378564 	 time: 0.3
Epoch: 350 	Training Loss: 1.221540 	Validation Loss: 1.379884 	 time: 0.3
Epoch: 351 	Training Loss: 1.222389 	Validation Loss: 1.382653 	 time: 0.3
Epoch: 352 	Training Loss: 1.222752 	Validation Loss: 1.379139 	 time: 0.3
Epoch: 353 	Training Loss: 1.221305 	Validation Loss: 1.380602 	 time: 0.3
Epoch: 354 	Training Loss: 1.221231 	Validation Loss: 1.381293 	 time: 0.3
Epoch: 355 	Training Loss: 1.221654 	Validation Loss: 1.379279 	 time: 0.3
Epoch: 356 	Training Loss: 1.220799 	Validation Loss: 1.381225 	 time: 0.3
Epoch: 357 	Training Loss: 1.220818 	Validation Loss: 1.380602 	 time: 0.3
Epoch: 358 	Training Loss: 1.221051 	Validation Loss: 1.378720 	 time: 0.3
Epoch: 359 	Training Loss: 1.220513 	Validation Loss: 1.382378 	 time: 0.3
Epoch: 360 	Training Loss: 1.220356 	Validation Loss: 1.377769 	 time: 0.3
Epoch: 361 	Training Loss: 1.220088 	Validation Loss: 1.376750 	 time: 0.3
Epoch: 362 	Training Loss: 1.220069 	Validation Loss: 1.383835 	 time: 0.3
Epoch: 363 	Training Loss: 1.220463 	Validation Loss: 1.376119 	 time: 0.3
Epoch: 364 	Training Loss: 1.219573 	Validation Loss: 1.376252 	 time: 0.3
Epoch: 365 	Training Loss: 1.219407 	Validation Loss: 1.384041 	 time: 0.3
Epoch: 366 	Training Loss: 1.220070 	Validation Loss: 1.376557 	 time: 0.3
Epoch: 367 	Training Loss: 1.218990 	Validation Loss: 1.376750 	 time: 0.3
Epoch: 368 	Training Loss: 1.218733 	Validation Loss: 1.382176 	 time: 0.3
Epoch: 369 	Training Loss: 1.218926 	Validation Loss: 1.377901 	 time: 0.3
Epoch: 370 	Training Loss: 1.218438 	Validation Loss: 1.376756 	 time: 0.3
Epoch: 371 	Training Loss: 1.218396 	Validation Loss: 1.378634 	 time: 0.3
Epoch: 372 	Training Loss: 1.217992 	Validation Loss: 1.378972 	 time: 0.3
Epoch: 373 	Training Loss: 1.217908 	Validation Loss: 1.377973 	 time: 0.3
Epoch: 374 	Training Loss: 1.217916 	Validation Loss: 1.378309 	 time: 0.3
Epoch: 375 	Training Loss: 1.217589 	Validation Loss: 1.382080 	 time: 0.3
Epoch: 376 	Training Loss: 1.217516 	Validation Loss: 1.379987 	 time: 0.3
Epoch: 377 	Training Loss: 1.217191 	Validation Loss: 1.380551 	 time: 0.3
Epoch: 378 	Training Loss: 1.217059 	Validation Loss: 1.383983 	 time: 0.3
Epoch: 379 	Training Loss: 1.216968 	Validation Loss: 1.382421 	 time: 0.3
Epoch: 380 	Training Loss: 1.216829 	Validation Loss: 1.382768 	 time: 0.3
Epoch: 381 	Training Loss: 1.216763 	Validation Loss: 1.382929 	 time: 0.3
Epoch: 382 	Training Loss: 1.216536 	Validation Loss: 1.383071 	 time: 0.3
Epoch: 383 	Training Loss: 1.216431 	Validation Loss: 1.381565 	 time: 0.3
Epoch: 384 	Training Loss: 1.216239 	Validation Loss: 1.381732 	 time: 0.3
Epoch: 385 	Training Loss: 1.216138 	Validation Loss: 1.382347 	 time: 0.3
Epoch: 386 	Training Loss: 1.216124 	Validation Loss: 1.380783 	 time: 0.3
Epoch: 387 	Training Loss: 1.215946 	Validation Loss: 1.381431 	 time: 0.3
Epoch: 388 	Training Loss: 1.215835 	Validation Loss: 1.380976 	 time: 0.3
Epoch: 389 	Training Loss: 1.215690 	Validation Loss: 1.381027 	 time: 0.3
Epoch: 390 	Training Loss: 1.215513 	Validation Loss: 1.380801 	 time: 0.3
Epoch: 391 	Training Loss: 1.215438 	Validation Loss: 1.381212 	 time: 0.3
Epoch: 392 	Training Loss: 1.215322 	Validation Loss: 1.381682 	 time: 0.3
Epoch: 393 	Training Loss: 1.215179 	Validation Loss: 1.381401 	 time: 0.3
Epoch: 394 	Training Loss: 1.215102 	Validation Loss: 1.382452 	 time: 0.3
Epoch: 395 	Training Loss: 1.214995 	Validation Loss: 1.382184 	 time: 0.3
Epoch: 396 	Training Loss: 1.214847 	Validation Loss: 1.382489 	 time: 0.3
Epoch: 397 	Training Loss: 1.214754 	Validation Loss: 1.382444 	 time: 0.3
Epoch: 398 	Training Loss: 1.214612 	Validation Loss: 1.383202 	 time: 0.3
Epoch: 399 	Training Loss: 1.214431 	Validation Loss: 1.382002 	 time: 0.3
Epoch: 400 	Training Loss: 1.214275 	Validation Loss: 1.383076 	 time: 0.3
Epoch: 401 	Training Loss: 1.214147 	Validation Loss: 1.382000 	 time: 0.3
Epoch: 402 	Training Loss: 1.214011 	Validation Loss: 1.381947 	 time: 0.3
Epoch: 403 	Training Loss: 1.213947 	Validation Loss: 1.382133 	 time: 0.3
Epoch: 404 	Training Loss: 1.213863 	Validation Loss: 1.381999 	 time: 0.3
Epoch: 405 	Training Loss: 1.213810 	Validation Loss: 1.381567 	 time: 0.3
Epoch: 406 	Training Loss: 1.213768 	Validation Loss: 1.382831 	 time: 0.3
Epoch: 407 	Training Loss: 1.213853 	Validation Loss: 1.381483 	 time: 0.3
Epoch: 408 	Training Loss: 1.213935 	Validation Loss: 1.383682 	 time: 0.3
Epoch: 409 	Training Loss: 1.214239 	Validation Loss: 1.382040 	 time: 0.3
Epoch: 410 	Training Loss: 1.214461 	Validation Loss: 1.384488 	 time: 0.3
Epoch: 411 	Training Loss: 1.214855 	Validation Loss: 1.382009 	 time: 0.3
Epoch: 412 	Training Loss: 1.214496 	Validation Loss: 1.382855 	 time: 0.3
Epoch: 413 	Training Loss: 1.213921 	Validation Loss: 1.381093 	 time: 0.3
Epoch: 414 	Training Loss: 1.212786 	Validation Loss: 1.380220 	 time: 0.3
Epoch: 415 	Training Loss: 1.212948 	Validation Loss: 1.384209 	 time: 0.3
Epoch: 416 	Training Loss: 1.213949 	Validation Loss: 1.382443 	 time: 0.3
Epoch: 417 	Training Loss: 1.214553 	Validation Loss: 1.385933 	 time: 0.3
Epoch: 418 	Training Loss: 1.215634 	Validation Loss: 1.383342 	 time: 0.3
Epoch: 419 	Training Loss: 1.214530 	Validation Loss: 1.383305 	 time: 0.3
Epoch: 420 	Training Loss: 1.212578 	Validation Loss: 1.382502 	 time: 0.3
Epoch: 421 	Training Loss: 1.213181 	Validation Loss: 1.381103 	 time: 0.3
Epoch: 422 	Training Loss: 1.213984 	Validation Loss: 1.387839 	 time: 0.3
Epoch: 423 	Training Loss: 1.214365 	Validation Loss: 1.379797 	 time: 0.3
Epoch: 424 	Training Loss: 1.212003 	Validation Loss: 1.378671 	 time: 0.3
Epoch: 425 	Training Loss: 1.212400 	Validation Loss: 1.385672 	 time: 0.3
Epoch: 426 	Training Loss: 1.214149 	Validation Loss: 1.381797 	 time: 0.3
Epoch: 427 	Training Loss: 1.213289 	Validation Loss: 1.379703 	 time: 0.3
Epoch: 428 	Training Loss: 1.212225 	Validation Loss: 1.382985 	 time: 0.3
Epoch: 429 	Training Loss: 1.211634 	Validation Loss: 1.381204 	 time: 0.3
Epoch: 430 	Training Loss: 1.213022 	Validation Loss: 1.381030 	 time: 0.3
Epoch: 431 	Training Loss: 1.212704 	Validation Loss: 1.381817 	 time: 0.3
Epoch: 432 	Training Loss: 1.210628 	Validation Loss: 1.381561 	 time: 0.3
Epoch: 433 	Training Loss: 1.212035 	Validation Loss: 1.380510 	 time: 0.3
Epoch: 434 	Training Loss: 1.212584 	Validation Loss: 1.379658 	 time: 0.3
Epoch: 435 	Training Loss: 1.210681 	Validation Loss: 1.381229 	 time: 0.3
Epoch: 436 	Training Loss: 1.211034 	Validation Loss: 1.382205 	 time: 0.3
Epoch: 437 	Training Loss: 1.210325 	Validation Loss: 1.378413 	 time: 0.3
Epoch: 438 	Training Loss: 1.211042 	Validation Loss: 1.381660 	 time: 0.3
Epoch: 439 	Training Loss: 1.212384 	Validation Loss: 1.384361 	 time: 0.3
Epoch: 440 	Training Loss: 1.209919 	Validation Loss: 1.377185 	 time: 0.3
Epoch: 441 	Training Loss: 1.212704 	Validation Loss: 1.386601 	 time: 0.3
Epoch: 442 	Training Loss: 1.213062 	Validation Loss: 1.382201 	 time: 0.3
Epoch: 443 	Training Loss: 1.212799 	Validation Loss: 1.376349 	 time: 0.3
Epoch: 444 	Training Loss: 1.217278 	Validation Loss: 1.382200 	 time: 0.3
Epoch: 445 	Training Loss: 1.210454 	Validation Loss: 1.390049 	 time: 0.3
Epoch: 446 	Training Loss: 1.216452 	Validation Loss: 1.381587 	 time: 0.3
Epoch: 447 	Training Loss: 1.223478 	Validation Loss: 1.388745 	 time: 0.3
Epoch: 448 	Training Loss: 1.223929 	Validation Loss: 1.412665 	 time: 0.3
Epoch: 449 	Training Loss: 1.241687 	Validation Loss: 1.392289 	 time: 0.3
Epoch: 450 	Training Loss: 1.221969 	Validation Loss: 1.386742 	 time: 0.3
Epoch: 451 	Training Loss: 1.227496 	Validation Loss: 1.394106 	 time: 0.3
Epoch: 452 	Training Loss: 1.243336 	Validation Loss: 1.424946 	 time: 0.3
Epoch: 453 	Training Loss: 1.247724 	Validation Loss: 1.404474 	 time: 0.3
Epoch: 454 	Training Loss: 1.222753 	Validation Loss: 1.393922 	 time: 0.3
Epoch: 455 	Training Loss: 1.227433 	Validation Loss: 1.376193 	 time: 0.3
Epoch: 456 	Training Loss: 1.223894 	Validation Loss: 1.381554 	 time: 0.3
Epoch: 457 	Training Loss: 1.216904 	Validation Loss: 1.395467 	 time: 0.3
Epoch: 458 	Training Loss: 1.220376 	Validation Loss: 1.393398 	 time: 0.3
Epoch: 459 	Training Loss: 1.220380 	Validation Loss: 1.372699 	 time: 0.3
Epoch: 460 	Training Loss: 1.211769 	Validation Loss: 1.374704 	 time: 0.3
Epoch: 461 	Training Loss: 1.222718 	Validation Loss: 1.383919 	 time: 0.3
Epoch: 462 	Training Loss: 1.217242 	Validation Loss: 1.387431 	 time: 0.3
Epoch: 463 	Training Loss: 1.214422 	Validation Loss: 1.388315 	 time: 0.3
Epoch: 464 	Training Loss: 1.217289 	Validation Loss: 1.377007 	 time: 0.3
Epoch: 465 	Training Loss: 1.211908 	Validation Loss: 1.373904 	 time: 0.3
Epoch: 466 	Training Loss: 1.212441 	Validation Loss: 1.375425 	 time: 0.3
Epoch: 467 	Training Loss: 1.210347 	Validation Loss: 1.376937 	 time: 0.3
Epoch: 468 	Training Loss: 1.211028 	Validation Loss: 1.375383 	 time: 0.3
Epoch: 469 	Training Loss: 1.207608 	Validation Loss: 1.375488 	 time: 0.3
Epoch: 470 	Training Loss: 1.211188 	Validation Loss: 1.374780 	 time: 0.3
Epoch: 471 	Training Loss: 1.205597 	Validation Loss: 1.373496 	 time: 0.3
Epoch: 472 	Training Loss: 1.207636 	Validation Loss: 1.371237 	 time: 0.3
Epoch: 473 	Training Loss: 1.205406 	Validation Loss: 1.370775 	 time: 0.3
Validation loss decreased from 1.370908 to 1.370775. Model was saved
Epoch: 474 	Training Loss: 1.204575 	Validation Loss: 1.367449 	 time: 0.3
Validation loss decreased from 1.370775 to 1.367449. Model was saved
Epoch: 475 	Training Loss: 1.203156 	Validation Loss: 1.369320 	 time: 0.3
Epoch: 476 	Training Loss: 1.204272 	Validation Loss: 1.369690 	 time: 0.3
Epoch: 477 	Training Loss: 1.202466 	Validation Loss: 1.365735 	 time: 0.3
Validation loss decreased from 1.367449 to 1.365735. Model was saved
Epoch: 478 	Training Loss: 1.201645 	Validation Loss: 1.359375 	 time: 0.3
Validation loss decreased from 1.365735 to 1.359375. Model was saved
Epoch: 479 	Training Loss: 1.201536 	Validation Loss: 1.360047 	 time: 0.3
Epoch: 480 	Training Loss: 1.200629 	Validation Loss: 1.364230 	 time: 0.3
Epoch: 481 	Training Loss: 1.199598 	Validation Loss: 1.367778 	 time: 0.3
Epoch: 482 	Training Loss: 1.200108 	Validation Loss: 1.366085 	 time: 0.3
Epoch: 483 	Training Loss: 1.199417 	Validation Loss: 1.365179 	 time: 0.3
Epoch: 484 	Training Loss: 1.199032 	Validation Loss: 1.367958 	 time: 0.3
Epoch: 485 	Training Loss: 1.198532 	Validation Loss: 1.367872 	 time: 0.3
Epoch: 486 	Training Loss: 1.198136 	Validation Loss: 1.366879 	 time: 0.3
Epoch: 487 	Training Loss: 1.197562 	Validation Loss: 1.368086 	 time: 0.3
Epoch: 488 	Training Loss: 1.197599 	Validation Loss: 1.368144 	 time: 0.3
Epoch: 489 	Training Loss: 1.197323 	Validation Loss: 1.366150 	 time: 0.3
Epoch: 490 	Training Loss: 1.196699 	Validation Loss: 1.364526 	 time: 0.3
Epoch: 491 	Training Loss: 1.196500 	Validation Loss: 1.365097 	 time: 0.3
Epoch: 492 	Training Loss: 1.196139 	Validation Loss: 1.365356 	 time: 0.3
Epoch: 493 	Training Loss: 1.195799 	Validation Loss: 1.365729 	 time: 0.3
Epoch: 494 	Training Loss: 1.195606 	Validation Loss: 1.366408 	 time: 0.3
Epoch: 495 	Training Loss: 1.195322 	Validation Loss: 1.366271 	 time: 0.3
Epoch: 496 	Training Loss: 1.194969 	Validation Loss: 1.364687 	 time: 0.3
Epoch: 497 	Training Loss: 1.194821 	Validation Loss: 1.364522 	 time: 0.3
Epoch: 498 	Training Loss: 1.194552 	Validation Loss: 1.365837 	 time: 0.3
Epoch: 499 	Training Loss: 1.194323 	Validation Loss: 1.365420 	 time: 0.3
Epoch: 500 	Training Loss: 1.194078 	Validation Loss: 1.363860 	 time: 0.3
Epoch: 501 	Training Loss: 1.193901 	Validation Loss: 1.364246 	 time: 0.3
Epoch: 502 	Training Loss: 1.193592 	Validation Loss: 1.365108 	 time: 0.3
Epoch: 503 	Training Loss: 1.193446 	Validation Loss: 1.363995 	 time: 0.3
Epoch: 504 	Training Loss: 1.193193 	Validation Loss: 1.364191 	 time: 0.3
Epoch: 505 	Training Loss: 1.192949 	Validation Loss: 1.365624 	 time: 0.3
Epoch: 506 	Training Loss: 1.192777 	Validation Loss: 1.364957 	 time: 0.3
Epoch: 507 	Training Loss: 1.192559 	Validation Loss: 1.363988 	 time: 0.3
Epoch: 508 	Training Loss: 1.192399 	Validation Loss: 1.364417 	 time: 0.3
Epoch: 509 	Training Loss: 1.192176 	Validation Loss: 1.364819 	 time: 0.3
Epoch: 510 	Training Loss: 1.192005 	Validation Loss: 1.364418 	 time: 0.3
Epoch: 511 	Training Loss: 1.191846 	Validation Loss: 1.364935 	 time: 0.3
Epoch: 512 	Training Loss: 1.191693 	Validation Loss: 1.365524 	 time: 0.3
Epoch: 513 	Training Loss: 1.191502 	Validation Loss: 1.364910 	 time: 0.3
Epoch: 514 	Training Loss: 1.191337 	Validation Loss: 1.364917 	 time: 0.3
Epoch: 515 	Training Loss: 1.191182 	Validation Loss: 1.365943 	 time: 0.3
Epoch: 516 	Training Loss: 1.190970 	Validation Loss: 1.365996 	 time: 0.3
Epoch: 517 	Training Loss: 1.190737 	Validation Loss: 1.365334 	 time: 0.3
Epoch: 518 	Training Loss: 1.190563 	Validation Loss: 1.365391 	 time: 0.3
Epoch: 519 	Training Loss: 1.190400 	Validation Loss: 1.365370 	 time: 0.3
Epoch: 520 	Training Loss: 1.190246 	Validation Loss: 1.364781 	 time: 0.2
Epoch: 521 	Training Loss: 1.190077 	Validation Loss: 1.364724 	 time: 0.3
Epoch: 522 	Training Loss: 1.189908 	Validation Loss: 1.364840 	 time: 0.3
Epoch: 523 	Training Loss: 1.189759 	Validation Loss: 1.364353 	 time: 0.3
Epoch: 524 	Training Loss: 1.189606 	Validation Loss: 1.364071 	 time: 0.3
Epoch: 525 	Training Loss: 1.189468 	Validation Loss: 1.364197 	 time: 0.3
Epoch: 526 	Training Loss: 1.189321 	Validation Loss: 1.363895 	 time: 0.3
Epoch: 527 	Training Loss: 1.189160 	Validation Loss: 1.363669 	 time: 0.3
Epoch: 528 	Training Loss: 1.189009 	Validation Loss: 1.363805 	 time: 0.3
Epoch: 529 	Training Loss: 1.188828 	Validation Loss: 1.363439 	 time: 0.3
Epoch: 530 	Training Loss: 1.188667 	Validation Loss: 1.363032 	 time: 0.3
Epoch: 531 	Training Loss: 1.188529 	Validation Loss: 1.363242 	 time: 0.3
Epoch: 532 	Training Loss: 1.188400 	Validation Loss: 1.363014 	 time: 0.3
Epoch: 533 	Training Loss: 1.188293 	Validation Loss: 1.362325 	 time: 0.3
Epoch: 534 	Training Loss: 1.188191 	Validation Loss: 1.362042 	 time: 0.3
Epoch: 535 	Training Loss: 1.188043 	Validation Loss: 1.361641 	 time: 0.3
Epoch: 536 	Training Loss: 1.187894 	Validation Loss: 1.360874 	 time: 0.3
Epoch: 537 	Training Loss: 1.187778 	Validation Loss: 1.360548 	 time: 0.3
Epoch: 538 	Training Loss: 1.187683 	Validation Loss: 1.360315 	 time: 0.3
Epoch: 539 	Training Loss: 1.187597 	Validation Loss: 1.359781 	 time: 0.3
Epoch: 540 	Training Loss: 1.187503 	Validation Loss: 1.359573 	 time: 0.3
Epoch: 541 	Training Loss: 1.187410 	Validation Loss: 1.359529 	 time: 0.3
Epoch: 542 	Training Loss: 1.187307 	Validation Loss: 1.359309 	 time: 0.3
Validation loss decreased from 1.359375 to 1.359309. Model was saved
Epoch: 543 	Training Loss: 1.187202 	Validation Loss: 1.359353 	 time: 0.3
Epoch: 544 	Training Loss: 1.187107 	Validation Loss: 1.359373 	 time: 0.3
Epoch: 545 	Training Loss: 1.187010 	Validation Loss: 1.359101 	 time: 0.3
Validation loss decreased from 1.359309 to 1.359101. Model was saved
Epoch: 546 	Training Loss: 1.186910 	Validation Loss: 1.359008 	 time: 0.3
Validation loss decreased from 1.359101 to 1.359008. Model was saved
Epoch: 547 	Training Loss: 1.186803 	Validation Loss: 1.358855 	 time: 0.3
Validation loss decreased from 1.359008 to 1.358855. Model was saved
Epoch: 548 	Training Loss: 1.186677 	Validation Loss: 1.358399 	 time: 0.3
Validation loss decreased from 1.358855 to 1.358399. Model was saved
Epoch: 549 	Training Loss: 1.186540 	Validation Loss: 1.358106 	 time: 0.3
Validation loss decreased from 1.358399 to 1.358106. Model was saved
Epoch: 550 	Training Loss: 1.186415 	Validation Loss: 1.357932 	 time: 0.3
Validation loss decreased from 1.358106 to 1.357932. Model was saved
Epoch: 551 	Training Loss: 1.186301 	Validation Loss: 1.357665 	 time: 0.3
Validation loss decreased from 1.357932 to 1.357665. Model was saved
Epoch: 552 	Training Loss: 1.186177 	Validation Loss: 1.357644 	 time: 0.3
Validation loss decreased from 1.357665 to 1.357644. Model was saved
Epoch: 553 	Training Loss: 1.186042 	Validation Loss: 1.357682 	 time: 0.3
Epoch: 554 	Training Loss: 1.185917 	Validation Loss: 1.357622 	 time: 0.3
Validation loss decreased from 1.357644 to 1.357622. Model was saved
Epoch: 555 	Training Loss: 1.185799 	Validation Loss: 1.357711 	 time: 0.3
Epoch: 556 	Training Loss: 1.185694 	Validation Loss: 1.357718 	 time: 0.3
Epoch: 557 	Training Loss: 1.185591 	Validation Loss: 1.357615 	 time: 0.3
Validation loss decreased from 1.357622 to 1.357615. Model was saved
Epoch: 558 	Training Loss: 1.185484 	Validation Loss: 1.357598 	 time: 0.3
Validation loss decreased from 1.357615 to 1.357598. Model was saved
Epoch: 559 	Training Loss: 1.185375 	Validation Loss: 1.357487 	 time: 0.3
Validation loss decreased from 1.357598 to 1.357487. Model was saved
Epoch: 560 	Training Loss: 1.185271 	Validation Loss: 1.357411 	 time: 0.3
Validation loss decreased from 1.357487 to 1.357411. Model was saved
Epoch: 561 	Training Loss: 1.185171 	Validation Loss: 1.357483 	 time: 0.3
Epoch: 562 	Training Loss: 1.185072 	Validation Loss: 1.357521 	 time: 0.3
Epoch: 563 	Training Loss: 1.184987 	Validation Loss: 1.357622 	 time: 0.3
Epoch: 564 	Training Loss: 1.184907 	Validation Loss: 1.357695 	 time: 0.3
Epoch: 565 	Training Loss: 1.184818 	Validation Loss: 1.357677 	 time: 0.3
Epoch: 566 	Training Loss: 1.184704 	Validation Loss: 1.357708 	 time: 0.3
Epoch: 567 	Training Loss: 1.184584 	Validation Loss: 1.357708 	 time: 0.3
Epoch: 568 	Training Loss: 1.184498 	Validation Loss: 1.357656 	 time: 0.3
Epoch: 569 	Training Loss: 1.184421 	Validation Loss: 1.357616 	 time: 0.3
Epoch: 570 	Training Loss: 1.184350 	Validation Loss: 1.357572 	 time: 0.3
Epoch: 571 	Training Loss: 1.184283 	Validation Loss: 1.357581 	 time: 0.3
Epoch: 572 	Training Loss: 1.184214 	Validation Loss: 1.357584 	 time: 0.3
Epoch: 573 	Training Loss: 1.184137 	Validation Loss: 1.357560 	 time: 0.3
Epoch: 574 	Training Loss: 1.184030 	Validation Loss: 1.357597 	 time: 0.3
Epoch: 575 	Training Loss: 1.183917 	Validation Loss: 1.357577 	 time: 0.3
Epoch: 576 	Training Loss: 1.183839 	Validation Loss: 1.357509 	 time: 0.3
Epoch: 577 	Training Loss: 1.183767 	Validation Loss: 1.357598 	 time: 0.3
Epoch: 578 	Training Loss: 1.183697 	Validation Loss: 1.357539 	 time: 0.3
Epoch: 579 	Training Loss: 1.183629 	Validation Loss: 1.357500 	 time: 0.3
Epoch: 580 	Training Loss: 1.183546 	Validation Loss: 1.357535 	 time: 0.3
Epoch: 581 	Training Loss: 1.183465 	Validation Loss: 1.357402 	 time: 0.3
Validation loss decreased from 1.357411 to 1.357402. Model was saved
Epoch: 582 	Training Loss: 1.183384 	Validation Loss: 1.357435 	 time: 0.3
Epoch: 583 	Training Loss: 1.183299 	Validation Loss: 1.357417 	 time: 0.3
Epoch: 584 	Training Loss: 1.183227 	Validation Loss: 1.357389 	 time: 0.3
Validation loss decreased from 1.357402 to 1.357389. Model was saved
Epoch: 585 	Training Loss: 1.183171 	Validation Loss: 1.357409 	 time: 0.3
Epoch: 586 	Training Loss: 1.183108 	Validation Loss: 1.357380 	 time: 0.3
Validation loss decreased from 1.357389 to 1.357380. Model was saved
Epoch: 587 	Training Loss: 1.183040 	Validation Loss: 1.357468 	 time: 0.3
Epoch: 588 	Training Loss: 1.182973 	Validation Loss: 1.357524 	 time: 0.3
Epoch: 589 	Training Loss: 1.182917 	Validation Loss: 1.357529 	 time: 0.3
Epoch: 590 	Training Loss: 1.182857 	Validation Loss: 1.357648 	 time: 0.3
Epoch: 591 	Training Loss: 1.182789 	Validation Loss: 1.357683 	 time: 0.3
Epoch: 592 	Training Loss: 1.182725 	Validation Loss: 1.357745 	 time: 0.3
Epoch: 593 	Training Loss: 1.182656 	Validation Loss: 1.357920 	 time: 0.3
Epoch: 594 	Training Loss: 1.182570 	Validation Loss: 1.358037 	 time: 0.3
Epoch: 595 	Training Loss: 1.182466 	Validation Loss: 1.358222 	 time: 0.3
Epoch: 596 	Training Loss: 1.182374 	Validation Loss: 1.358371 	 time: 0.3
Epoch: 597 	Training Loss: 1.182310 	Validation Loss: 1.358410 	 time: 0.3
Epoch: 598 	Training Loss: 1.182253 	Validation Loss: 1.358451 	 time: 0.3
Epoch: 599 	Training Loss: 1.182194 	Validation Loss: 1.358401 	 time: 0.3
Epoch: 600 	Training Loss: 1.182134 	Validation Loss: 1.358340 	 time: 0.3
Epoch: 601 	Training Loss: 1.182073 	Validation Loss: 1.358266 	 time: 0.3
Epoch: 602 	Training Loss: 1.182003 	Validation Loss: 1.358077 	 time: 0.3
Epoch: 603 	Training Loss: 1.181921 	Validation Loss: 1.357922 	 time: 0.3
Epoch: 604 	Training Loss: 1.181849 	Validation Loss: 1.357814 	 time: 0.3
Epoch: 605 	Training Loss: 1.181782 	Validation Loss: 1.357708 	 time: 0.3
Epoch: 606 	Training Loss: 1.181714 	Validation Loss: 1.357735 	 time: 0.3
Epoch: 607 	Training Loss: 1.181650 	Validation Loss: 1.357785 	 time: 0.3
Epoch: 608 	Training Loss: 1.181585 	Validation Loss: 1.357831 	 time: 0.3
Epoch: 609 	Training Loss: 1.181520 	Validation Loss: 1.357944 	 time: 0.3
Epoch: 610 	Training Loss: 1.181464 	Validation Loss: 1.358064 	 time: 0.3
Epoch: 611 	Training Loss: 1.181389 	Validation Loss: 1.358205 	 time: 0.3
Epoch: 612 	Training Loss: 1.181251 	Validation Loss: 1.358244 	 time: 0.3
Epoch: 613 	Training Loss: 1.181172 	Validation Loss: 1.358459 	 time: 0.3
Epoch: 614 	Training Loss: 1.181105 	Validation Loss: 1.358483 	 time: 0.3
Epoch: 615 	Training Loss: 1.181034 	Validation Loss: 1.358625 	 time: 0.3
Epoch: 616 	Training Loss: 1.180948 	Validation Loss: 1.358726 	 time: 0.3
Epoch: 617 	Training Loss: 1.180850 	Validation Loss: 1.358804 	 time: 0.3
Epoch: 618 	Training Loss: 1.180756 	Validation Loss: 1.358946 	 time: 0.3
Epoch: 619 	Training Loss: 1.180682 	Validation Loss: 1.358923 	 time: 0.3
Epoch: 620 	Training Loss: 1.180618 	Validation Loss: 1.358835 	 time: 0.3
Epoch: 621 	Training Loss: 1.180553 	Validation Loss: 1.358511 	 time: 0.3
Epoch: 622 	Training Loss: 1.180488 	Validation Loss: 1.358268 	 time: 0.3
Epoch: 623 	Training Loss: 1.180422 	Validation Loss: 1.357996 	 time: 0.3
Epoch: 624 	Training Loss: 1.180357 	Validation Loss: 1.357760 	 time: 0.3
Epoch: 625 	Training Loss: 1.180294 	Validation Loss: 1.357607 	 time: 0.3
Epoch: 626 	Training Loss: 1.180232 	Validation Loss: 1.357471 	 time: 0.3
Epoch: 627 	Training Loss: 1.180167 	Validation Loss: 1.357440 	 time: 0.3
Epoch: 628 	Training Loss: 1.180097 	Validation Loss: 1.357281 	 time: 0.3
Validation loss decreased from 1.357380 to 1.357281. Model was saved
Epoch: 629 	Training Loss: 1.180020 	Validation Loss: 1.357175 	 time: 0.3
Validation loss decreased from 1.357281 to 1.357175. Model was saved
Epoch: 630 	Training Loss: 1.179942 	Validation Loss: 1.356841 	 time: 0.3
Validation loss decreased from 1.357175 to 1.356841. Model was saved
Epoch: 631 	Training Loss: 1.179868 	Validation Loss: 1.356506 	 time: 0.3
Validation loss decreased from 1.356841 to 1.356506. Model was saved
Epoch: 632 	Training Loss: 1.179792 	Validation Loss: 1.356047 	 time: 0.3
Validation loss decreased from 1.356506 to 1.356047. Model was saved
Epoch: 633 	Training Loss: 1.179716 	Validation Loss: 1.355723 	 time: 0.3
Validation loss decreased from 1.356047 to 1.355723. Model was saved
Epoch: 634 	Training Loss: 1.179644 	Validation Loss: 1.355444 	 time: 0.3
Validation loss decreased from 1.355723 to 1.355444. Model was saved
Epoch: 635 	Training Loss: 1.179570 	Validation Loss: 1.355334 	 time: 0.3
Validation loss decreased from 1.355444 to 1.355334. Model was saved
Epoch: 636 	Training Loss: 1.179479 	Validation Loss: 1.355395 	 time: 0.3
Epoch: 637 	Training Loss: 1.179344 	Validation Loss: 1.355372 	 time: 0.3
Epoch: 638 	Training Loss: 1.179237 	Validation Loss: 1.355502 	 time: 0.3
Epoch: 639 	Training Loss: 1.179161 	Validation Loss: 1.355180 	 time: 0.3
Validation loss decreased from 1.355334 to 1.355180. Model was saved
Epoch: 640 	Training Loss: 1.179080 	Validation Loss: 1.355286 	 time: 0.3
Epoch: 641 	Training Loss: 1.178982 	Validation Loss: 1.354663 	 time: 0.3
Validation loss decreased from 1.355180 to 1.354663. Model was saved
Epoch: 642 	Training Loss: 1.178875 	Validation Loss: 1.354927 	 time: 0.3
Epoch: 643 	Training Loss: 1.178815 	Validation Loss: 1.354139 	 time: 0.3
Validation loss decreased from 1.354663 to 1.354139. Model was saved
Epoch: 644 	Training Loss: 1.178768 	Validation Loss: 1.355057 	 time: 0.3
Epoch: 645 	Training Loss: 1.178726 	Validation Loss: 1.353935 	 time: 0.3
Validation loss decreased from 1.354139 to 1.353935. Model was saved
Epoch: 646 	Training Loss: 1.178702 	Validation Loss: 1.355768 	 time: 0.3
Epoch: 647 	Training Loss: 1.178707 	Validation Loss: 1.353632 	 time: 0.3
Validation loss decreased from 1.353935 to 1.353632. Model was saved
Epoch: 648 	Training Loss: 1.178761 	Validation Loss: 1.356800 	 time: 0.3
Epoch: 649 	Training Loss: 1.178873 	Validation Loss: 1.353099 	 time: 0.3
Validation loss decreased from 1.353632 to 1.353099. Model was saved
Epoch: 650 	Training Loss: 1.179039 	Validation Loss: 1.357908 	 time: 0.3
Epoch: 651 	Training Loss: 1.179219 	Validation Loss: 1.352920 	 time: 0.3
Validation loss decreased from 1.353099 to 1.352920. Model was saved
Epoch: 652 	Training Loss: 1.179211 	Validation Loss: 1.357375 	 time: 0.3
Epoch: 653 	Training Loss: 1.178957 	Validation Loss: 1.352988 	 time: 0.3
Epoch: 654 	Training Loss: 1.178428 	Validation Loss: 1.353767 	 time: 0.3
Epoch: 655 	Training Loss: 1.178179 	Validation Loss: 1.356706 	 time: 0.3
Epoch: 656 	Training Loss: 1.178454 	Validation Loss: 1.352908 	 time: 0.3
Validation loss decreased from 1.352920 to 1.352908. Model was saved
Epoch: 657 	Training Loss: 1.178751 	Validation Loss: 1.358281 	 time: 0.3
Epoch: 658 	Training Loss: 1.178763 	Validation Loss: 1.353210 	 time: 0.3
Epoch: 659 	Training Loss: 1.178367 	Validation Loss: 1.355095 	 time: 0.3
Epoch: 660 	Training Loss: 1.177935 	Validation Loss: 1.355984 	 time: 0.3
Epoch: 661 	Training Loss: 1.178006 	Validation Loss: 1.352976 	 time: 0.3
Epoch: 662 	Training Loss: 1.178275 	Validation Loss: 1.357421 	 time: 0.3
Epoch: 663 	Training Loss: 1.178184 	Validation Loss: 1.353778 	 time: 0.3
Epoch: 664 	Training Loss: 1.177889 	Validation Loss: 1.355372 	 time: 0.2
Epoch: 665 	Training Loss: 1.177651 	Validation Loss: 1.356161 	 time: 0.3
Epoch: 666 	Training Loss: 1.177692 	Validation Loss: 1.353114 	 time: 0.3
Epoch: 667 	Training Loss: 1.177826 	Validation Loss: 1.356550 	 time: 0.3
Epoch: 668 	Training Loss: 1.177824 	Validation Loss: 1.352891 	 time: 0.3
Validation loss decreased from 1.352908 to 1.352891. Model was saved
Epoch: 669 	Training Loss: 1.177660 	Validation Loss: 1.354842 	 time: 0.3
Epoch: 670 	Training Loss: 1.177421 	Validation Loss: 1.355134 	 time: 0.3
Epoch: 671 	Training Loss: 1.177377 	Validation Loss: 1.353111 	 time: 0.3
Epoch: 672 	Training Loss: 1.177464 	Validation Loss: 1.356688 	 time: 0.3
Epoch: 673 	Training Loss: 1.177485 	Validation Loss: 1.352935 	 time: 0.3
Epoch: 674 	Training Loss: 1.177401 	Validation Loss: 1.355442 	 time: 0.3
Epoch: 675 	Training Loss: 1.177183 	Validation Loss: 1.353709 	 time: 0.3
Epoch: 676 	Training Loss: 1.177029 	Validation Loss: 1.352738 	 time: 0.3
Validation loss decreased from 1.352891 to 1.352738. Model was saved
Epoch: 677 	Training Loss: 1.177002 	Validation Loss: 1.354070 	 time: 0.3
Epoch: 678 	Training Loss: 1.177033 	Validation Loss: 1.351210 	 time: 0.3
Validation loss decreased from 1.352738 to 1.351210. Model was saved
Epoch: 679 	Training Loss: 1.177068 	Validation Loss: 1.354244 	 time: 0.3
Epoch: 680 	Training Loss: 1.177019 	Validation Loss: 1.351093 	 time: 0.3
Validation loss decreased from 1.351210 to 1.351093. Model was saved
Epoch: 681 	Training Loss: 1.176926 	Validation Loss: 1.353466 	 time: 0.3
Epoch: 682 	Training Loss: 1.176759 	Validation Loss: 1.351932 	 time: 0.3
Epoch: 683 	Training Loss: 1.176615 	Validation Loss: 1.351509 	 time: 0.3
Epoch: 684 	Training Loss: 1.176558 	Validation Loss: 1.353098 	 time: 0.3
Epoch: 685 	Training Loss: 1.176540 	Validation Loss: 1.351289 	 time: 0.3
Epoch: 686 	Training Loss: 1.176497 	Validation Loss: 1.354152 	 time: 0.3
Epoch: 687 	Training Loss: 1.176410 	Validation Loss: 1.352027 	 time: 0.3
Epoch: 688 	Training Loss: 1.176304 	Validation Loss: 1.353514 	 time: 0.3
Epoch: 689 	Training Loss: 1.176187 	Validation Loss: 1.352898 	 time: 0.3
Epoch: 690 	Training Loss: 1.176105 	Validation Loss: 1.352480 	 time: 0.3
Epoch: 691 	Training Loss: 1.176065 	Validation Loss: 1.354168 	 time: 0.3
Epoch: 692 	Training Loss: 1.176047 	Validation Loss: 1.353002 	 time: 0.3
Epoch: 693 	Training Loss: 1.176020 	Validation Loss: 1.355342 	 time: 0.3
Epoch: 694 	Training Loss: 1.175959 	Validation Loss: 1.354170 	 time: 0.3
Epoch: 695 	Training Loss: 1.175882 	Validation Loss: 1.355575 	 time: 0.3
Epoch: 696 	Training Loss: 1.175797 	Validation Loss: 1.355339 	 time: 0.3
Epoch: 697 	Training Loss: 1.175725 	Validation Loss: 1.355333 	 time: 0.3
Epoch: 698 	Training Loss: 1.175678 	Validation Loss: 1.356066 	 time: 0.3
Epoch: 699 	Training Loss: 1.175635 	Validation Loss: 1.355092 	 time: 0.3
Epoch: 700 	Training Loss: 1.175577 	Validation Loss: 1.356235 	 time: 0.3
Epoch: 701 	Training Loss: 1.175522 	Validation Loss: 1.355007 	 time: 0.3
Epoch: 702 	Training Loss: 1.175478 	Validation Loss: 1.356512 	 time: 0.3
Epoch: 703 	Training Loss: 1.175433 	Validation Loss: 1.355488 	 time: 0.3
Epoch: 704 	Training Loss: 1.175379 	Validation Loss: 1.356577 	 time: 0.3
Epoch: 705 	Training Loss: 1.175306 	Validation Loss: 1.355965 	 time: 0.3
Epoch: 706 	Training Loss: 1.175215 	Validation Loss: 1.355994 	 time: 0.3
Epoch: 707 	Training Loss: 1.175123 	Validation Loss: 1.356051 	 time: 0.3
Epoch: 708 	Training Loss: 1.175060 	Validation Loss: 1.355687 	 time: 0.3
Epoch: 709 	Training Loss: 1.175004 	Validation Loss: 1.356560 	 time: 0.3
Epoch: 710 	Training Loss: 1.174964 	Validation Loss: 1.356056 	 time: 0.3
Epoch: 711 	Training Loss: 1.174922 	Validation Loss: 1.357036 	 time: 0.3
Epoch: 712 	Training Loss: 1.174869 	Validation Loss: 1.356286 	 time: 0.3
Epoch: 713 	Training Loss: 1.174810 	Validation Loss: 1.356723 	 time: 0.3
Epoch: 714 	Training Loss: 1.174744 	Validation Loss: 1.356083 	 time: 0.3
Epoch: 715 	Training Loss: 1.174674 	Validation Loss: 1.356460 	 time: 0.3
Epoch: 716 	Training Loss: 1.174608 	Validation Loss: 1.356385 	 time: 0.3
Epoch: 717 	Training Loss: 1.174555 	Validation Loss: 1.356590 	 time: 0.3
Epoch: 718 	Training Loss: 1.174510 	Validation Loss: 1.356690 	 time: 0.3
Epoch: 719 	Training Loss: 1.174470 	Validation Loss: 1.356622 	 time: 0.3
Epoch: 720 	Training Loss: 1.174434 	Validation Loss: 1.356879 	 time: 0.3
Epoch: 721 	Training Loss: 1.174397 	Validation Loss: 1.356534 	 time: 0.3
Epoch: 722 	Training Loss: 1.174361 	Validation Loss: 1.356834 	 time: 0.3
Epoch: 723 	Training Loss: 1.174322 	Validation Loss: 1.356307 	 time: 0.3
Epoch: 724 	Training Loss: 1.174283 	Validation Loss: 1.356674 	 time: 0.3
Epoch: 725 	Training Loss: 1.174244 	Validation Loss: 1.356215 	 time: 0.3
Epoch: 726 	Training Loss: 1.174202 	Validation Loss: 1.356663 	 time: 0.3
Epoch: 727 	Training Loss: 1.174158 	Validation Loss: 1.356301 	 time: 0.3
Epoch: 728 	Training Loss: 1.174113 	Validation Loss: 1.356672 	 time: 0.3
Epoch: 729 	Training Loss: 1.174064 	Validation Loss: 1.356264 	 time: 0.3
Epoch: 730 	Training Loss: 1.174013 	Validation Loss: 1.356416 	 time: 0.3
Epoch: 731 	Training Loss: 1.173960 	Validation Loss: 1.356014 	 time: 0.3
Epoch: 732 	Training Loss: 1.173907 	Validation Loss: 1.355994 	 time: 0.3
Epoch: 733 	Training Loss: 1.173850 	Validation Loss: 1.355720 	 time: 0.3
Epoch: 734 	Training Loss: 1.173779 	Validation Loss: 1.355575 	 time: 0.3
Epoch: 735 	Training Loss: 1.173724 	Validation Loss: 1.355516 	 time: 0.3
Epoch: 736 	Training Loss: 1.173680 	Validation Loss: 1.355578 	 time: 0.3
Epoch: 737 	Training Loss: 1.173636 	Validation Loss: 1.355514 	 time: 0.3
Epoch: 738 	Training Loss: 1.173598 	Validation Loss: 1.355541 	 time: 0.3
Epoch: 739 	Training Loss: 1.173562 	Validation Loss: 1.355467 	 time: 0.3
Epoch: 740 	Training Loss: 1.173527 	Validation Loss: 1.355676 	 time: 0.3
Epoch: 741 	Training Loss: 1.173492 	Validation Loss: 1.355681 	 time: 0.3
Epoch: 742 	Training Loss: 1.173458 	Validation Loss: 1.355966 	 time: 0.3
Epoch: 743 	Training Loss: 1.173422 	Validation Loss: 1.355777 	 time: 0.3
Epoch: 744 	Training Loss: 1.173384 	Validation Loss: 1.356066 	 time: 0.3
Epoch: 745 	Training Loss: 1.173340 	Validation Loss: 1.355707 	 time: 0.3
Epoch: 746 	Training Loss: 1.173295 	Validation Loss: 1.356151 	 time: 0.3
Epoch: 747 	Training Loss: 1.173270 	Validation Loss: 1.355504 	 time: 0.3
Epoch: 748 	Training Loss: 1.173249 	Validation Loss: 1.356393 	 time: 0.3
Epoch: 749 	Training Loss: 1.173243 	Validation Loss: 1.355223 	 time: 0.3
Epoch: 750 	Training Loss: 1.173284 	Validation Loss: 1.357140 	 time: 0.3
Epoch: 751 	Training Loss: 1.173375 	Validation Loss: 1.355036 	 time: 0.3
Epoch: 752 	Training Loss: 1.173709 	Validation Loss: 1.359555 	 time: 0.3
Epoch: 753 	Training Loss: 1.174799 	Validation Loss: 1.358294 	 time: 0.3
Epoch: 754 	Training Loss: 1.179828 	Validation Loss: 1.390894 	 time: 0.3
Epoch: 755 	Training Loss: 1.213866 	Validation Loss: 1.430390 	 time: 0.3
Epoch: 756 	Training Loss: 1.288136 	Validation Loss: 1.383348 	 time: 0.3
Epoch: 757 	Training Loss: 1.221502 	Validation Loss: 1.446854 	 time: 0.3
Epoch: 758 	Training Loss: 1.288936 	Validation Loss: 1.382537 	 time: 0.3
Epoch: 759 	Training Loss: 1.235749 	Validation Loss: 1.405236 	 time: 0.3
Epoch: 760 	Training Loss: 1.260491 	Validation Loss: 1.408495 	 time: 0.3
Epoch: 761 	Training Loss: 1.258391 	Validation Loss: 1.392058 	 time: 0.3
Epoch: 762 	Training Loss: 1.219486 	Validation Loss: 1.412007 	 time: 0.3
Epoch: 763 	Training Loss: 1.235797 	Validation Loss: 1.418485 	 time: 0.3
Epoch: 764 	Training Loss: 1.243296 	Validation Loss: 1.379473 	 time: 0.3
Epoch: 765 	Training Loss: 1.220112 	Validation Loss: 1.386344 	 time: 0.3
Epoch: 766 	Training Loss: 1.234887 	Validation Loss: 1.395763 	 time: 0.3
Epoch: 767 	Training Loss: 1.230010 	Validation Loss: 1.394197 	 time: 0.3
Epoch: 768 	Training Loss: 1.220488 	Validation Loss: 1.370698 	 time: 0.3
Epoch: 769 	Training Loss: 1.207910 	Validation Loss: 1.368796 	 time: 0.3
Epoch: 770 	Training Loss: 1.224067 	Validation Loss: 1.357560 	 time: 0.3
Epoch: 771 	Training Loss: 1.207814 	Validation Loss: 1.363257 	 time: 0.3
Epoch: 772 	Training Loss: 1.203998 	Validation Loss: 1.369325 	 time: 0.3
Epoch: 773 	Training Loss: 1.205383 	Validation Loss: 1.373200 	 time: 0.3
Epoch: 774 	Training Loss: 1.208119 	Validation Loss: 1.374140 	 time: 0.3
Epoch: 775 	Training Loss: 1.198413 	Validation Loss: 1.366402 	 time: 0.3
Epoch: 776 	Training Loss: 1.196355 	Validation Loss: 1.363888 	 time: 0.3
Epoch: 777 	Training Loss: 1.194084 	Validation Loss: 1.356924 	 time: 0.3
Epoch: 778 	Training Loss: 1.194523 	Validation Loss: 1.355114 	 time: 0.3
Epoch: 779 	Training Loss: 1.193004 	Validation Loss: 1.346451 	 time: 0.3
Validation loss decreased from 1.351093 to 1.346451. Model was saved
Epoch: 780 	Training Loss: 1.185765 	Validation Loss: 1.349676 	 time: 0.3
Epoch: 781 	Training Loss: 1.188087 	Validation Loss: 1.349109 	 time: 0.3
Epoch: 782 	Training Loss: 1.185836 	Validation Loss: 1.350271 	 time: 0.3
Epoch: 783 	Training Loss: 1.185259 	Validation Loss: 1.346898 	 time: 0.3
Epoch: 784 	Training Loss: 1.183417 	Validation Loss: 1.350711 	 time: 0.3
Epoch: 785 	Training Loss: 1.183848 	Validation Loss: 1.347109 	 time: 0.3
Epoch: 786 	Training Loss: 1.181187 	Validation Loss: 1.344227 	 time: 0.3
Validation loss decreased from 1.346451 to 1.344227. Model was saved
Epoch: 787 	Training Loss: 1.180582 	Validation Loss: 1.346013 	 time: 0.3
Epoch: 788 	Training Loss: 1.180212 	Validation Loss: 1.346947 	 time: 0.3
Epoch: 789 	Training Loss: 1.179316 	Validation Loss: 1.344941 	 time: 0.3
Epoch: 790 	Training Loss: 1.178978 	Validation Loss: 1.341523 	 time: 0.3
Validation loss decreased from 1.344227 to 1.341523. Model was saved
Epoch: 791 	Training Loss: 1.178230 	Validation Loss: 1.339676 	 time: 0.3
Validation loss decreased from 1.341523 to 1.339676. Model was saved
Epoch: 792 	Training Loss: 1.177369 	Validation Loss: 1.339959 	 time: 0.3
Epoch: 793 	Training Loss: 1.176751 	Validation Loss: 1.338723 	 time: 0.3
Validation loss decreased from 1.339676 to 1.338723. Model was saved
Epoch: 794 	Training Loss: 1.176552 	Validation Loss: 1.338945 	 time: 0.3
Epoch: 795 	Training Loss: 1.175971 	Validation Loss: 1.339069 	 time: 0.3
Epoch: 796 	Training Loss: 1.175557 	Validation Loss: 1.337656 	 time: 0.3
Validation loss decreased from 1.338723 to 1.337656. Model was saved
Epoch: 797 	Training Loss: 1.175070 	Validation Loss: 1.337692 	 time: 0.3
Epoch: 798 	Training Loss: 1.174631 	Validation Loss: 1.338739 	 time: 0.3
Epoch: 799 	Training Loss: 1.174382 	Validation Loss: 1.339881 	 time: 0.3
Epoch: 800 	Training Loss: 1.174165 	Validation Loss: 1.339680 	 time: 0.3
Epoch: 801 	Training Loss: 1.173637 	Validation Loss: 1.338732 	 time: 0.3
Epoch: 802 	Training Loss: 1.173602 	Validation Loss: 1.338328 	 time: 0.3
Epoch: 803 	Training Loss: 1.173198 	Validation Loss: 1.337994 	 time: 0.3
Epoch: 804 	Training Loss: 1.173147 	Validation Loss: 1.336270 	 time: 0.3
Validation loss decreased from 1.337656 to 1.336270. Model was saved
Epoch: 805 	Training Loss: 1.172837 	Validation Loss: 1.335691 	 time: 0.3
Validation loss decreased from 1.336270 to 1.335691. Model was saved
Epoch: 806 	Training Loss: 1.172743 	Validation Loss: 1.336215 	 time: 0.3
Epoch: 807 	Training Loss: 1.172486 	Validation Loss: 1.336199 	 time: 0.3
Epoch: 808 	Training Loss: 1.172382 	Validation Loss: 1.335889 	 time: 0.3
Epoch: 809 	Training Loss: 1.172182 	Validation Loss: 1.336169 	 time: 0.3
Epoch: 810 	Training Loss: 1.172117 	Validation Loss: 1.336938 	 time: 0.3
Epoch: 811 	Training Loss: 1.171931 	Validation Loss: 1.337445 	 time: 0.3
Epoch: 812 	Training Loss: 1.171869 	Validation Loss: 1.337163 	 time: 0.3
Epoch: 813 	Training Loss: 1.171772 	Validation Loss: 1.336802 	 time: 0.2
Epoch: 814 	Training Loss: 1.171692 	Validation Loss: 1.336504 	 time: 0.3
Epoch: 815 	Training Loss: 1.171533 	Validation Loss: 1.336233 	 time: 0.3
Epoch: 816 	Training Loss: 1.171412 	Validation Loss: 1.335717 	 time: 0.3
Epoch: 817 	Training Loss: 1.171309 	Validation Loss: 1.335208 	 time: 0.3
Validation loss decreased from 1.335691 to 1.335208. Model was saved
Epoch: 818 	Training Loss: 1.171226 	Validation Loss: 1.334599 	 time: 0.3
Validation loss decreased from 1.335208 to 1.334599. Model was saved
Epoch: 819 	Training Loss: 1.171144 	Validation Loss: 1.334520 	 time: 0.3
Validation loss decreased from 1.334599 to 1.334520. Model was saved
Epoch: 820 	Training Loss: 1.171054 	Validation Loss: 1.335150 	 time: 0.3
Epoch: 821 	Training Loss: 1.170985 	Validation Loss: 1.335111 	 time: 0.3
Epoch: 822 	Training Loss: 1.170892 	Validation Loss: 1.334556 	 time: 0.3
Epoch: 823 	Training Loss: 1.170824 	Validation Loss: 1.334623 	 time: 0.3
Epoch: 824 	Training Loss: 1.170743 	Validation Loss: 1.334904 	 time: 0.3
Epoch: 825 	Training Loss: 1.170698 	Validation Loss: 1.334497 	 time: 0.3
Validation loss decreased from 1.334520 to 1.334497. Model was saved
Epoch: 826 	Training Loss: 1.170621 	Validation Loss: 1.334135 	 time: 0.3
Validation loss decreased from 1.334497 to 1.334135. Model was saved
Epoch: 827 	Training Loss: 1.170586 	Validation Loss: 1.334275 	 time: 0.3
Epoch: 828 	Training Loss: 1.170521 	Validation Loss: 1.334581 	 time: 0.3
Epoch: 829 	Training Loss: 1.170485 	Validation Loss: 1.334456 	 time: 0.3
Epoch: 830 	Training Loss: 1.170433 	Validation Loss: 1.334328 	 time: 0.3
Epoch: 831 	Training Loss: 1.170387 	Validation Loss: 1.334535 	 time: 0.3
Epoch: 832 	Training Loss: 1.170347 	Validation Loss: 1.334526 	 time: 0.3
Epoch: 833 	Training Loss: 1.170303 	Validation Loss: 1.334186 	 time: 0.3
Epoch: 834 	Training Loss: 1.170267 	Validation Loss: 1.334080 	 time: 0.3
Validation loss decreased from 1.334135 to 1.334080. Model was saved
Epoch: 835 	Training Loss: 1.170229 	Validation Loss: 1.334283 	 time: 0.3
Epoch: 836 	Training Loss: 1.170194 	Validation Loss: 1.334320 	 time: 0.3
Epoch: 837 	Training Loss: 1.170156 	Validation Loss: 1.334223 	 time: 0.3
Epoch: 838 	Training Loss: 1.170125 	Validation Loss: 1.334084 	 time: 0.3
Epoch: 839 	Training Loss: 1.170083 	Validation Loss: 1.333894 	 time: 0.3
Validation loss decreased from 1.334080 to 1.333894. Model was saved
Epoch: 840 	Training Loss: 1.170050 	Validation Loss: 1.333711 	 time: 0.3
Validation loss decreased from 1.333894 to 1.333711. Model was saved
Epoch: 841 	Training Loss: 1.170007 	Validation Loss: 1.333773 	 time: 0.3
Epoch: 842 	Training Loss: 1.169970 	Validation Loss: 1.333933 	 time: 0.3
Epoch: 843 	Training Loss: 1.169925 	Validation Loss: 1.333932 	 time: 0.3
Epoch: 844 	Training Loss: 1.169879 	Validation Loss: 1.333907 	 time: 0.3
Epoch: 845 	Training Loss: 1.169838 	Validation Loss: 1.334020 	 time: 0.3
Epoch: 846 	Training Loss: 1.169808 	Validation Loss: 1.334043 	 time: 0.3
Epoch: 847 	Training Loss: 1.169778 	Validation Loss: 1.333861 	 time: 0.3
Epoch: 848 	Training Loss: 1.169748 	Validation Loss: 1.333733 	 time: 0.3
Epoch: 849 	Training Loss: 1.169716 	Validation Loss: 1.333838 	 time: 0.3
Epoch: 850 	Training Loss: 1.169681 	Validation Loss: 1.333949 	 time: 0.3
Epoch: 851 	Training Loss: 1.169647 	Validation Loss: 1.333902 	 time: 0.3
Epoch: 852 	Training Loss: 1.169617 	Validation Loss: 1.333888 	 time: 0.3
Epoch: 853 	Training Loss: 1.169592 	Validation Loss: 1.333937 	 time: 0.3
Epoch: 854 	Training Loss: 1.169566 	Validation Loss: 1.333881 	 time: 0.3
Epoch: 855 	Training Loss: 1.169541 	Validation Loss: 1.333762 	 time: 0.3
Epoch: 856 	Training Loss: 1.169512 	Validation Loss: 1.333755 	 time: 0.3
Epoch: 857 	Training Loss: 1.169484 	Validation Loss: 1.333824 	 time: 0.3
Epoch: 858 	Training Loss: 1.169452 	Validation Loss: 1.333866 	 time: 0.3
Epoch: 859 	Training Loss: 1.169420 	Validation Loss: 1.333887 	 time: 0.3
Epoch: 860 	Training Loss: 1.169383 	Validation Loss: 1.333954 	 time: 0.3
Epoch: 861 	Training Loss: 1.169336 	Validation Loss: 1.334051 	 time: 0.3
Epoch: 862 	Training Loss: 1.169276 	Validation Loss: 1.334158 	 time: 0.3
Epoch: 863 	Training Loss: 1.169234 	Validation Loss: 1.334302 	 time: 0.3
Epoch: 864 	Training Loss: 1.169207 	Validation Loss: 1.334460 	 time: 0.3
Epoch: 865 	Training Loss: 1.169183 	Validation Loss: 1.334582 	 time: 0.3
Epoch: 866 	Training Loss: 1.169162 	Validation Loss: 1.334700 	 time: 0.3
Epoch: 867 	Training Loss: 1.169139 	Validation Loss: 1.334808 	 time: 0.3
Epoch: 868 	Training Loss: 1.169115 	Validation Loss: 1.334846 	 time: 0.3
Epoch: 869 	Training Loss: 1.169088 	Validation Loss: 1.334840 	 time: 0.3
Epoch: 870 	Training Loss: 1.169063 	Validation Loss: 1.334860 	 time: 0.3
Epoch: 871 	Training Loss: 1.169038 	Validation Loss: 1.334893 	 time: 0.3
Epoch: 872 	Training Loss: 1.169015 	Validation Loss: 1.334906 	 time: 0.3
Epoch: 873 	Training Loss: 1.168991 	Validation Loss: 1.334941 	 time: 0.3
Epoch: 874 	Training Loss: 1.168968 	Validation Loss: 1.335013 	 time: 0.3
Epoch: 875 	Training Loss: 1.168945 	Validation Loss: 1.335062 	 time: 0.3
Epoch: 876 	Training Loss: 1.168922 	Validation Loss: 1.335071 	 time: 0.3
Epoch: 877 	Training Loss: 1.168899 	Validation Loss: 1.335090 	 time: 0.3
Epoch: 878 	Training Loss: 1.168876 	Validation Loss: 1.335115 	 time: 0.3
Epoch: 879 	Training Loss: 1.168853 	Validation Loss: 1.335107 	 time: 0.3
Epoch: 880 	Training Loss: 1.168831 	Validation Loss: 1.335083 	 time: 0.3
Epoch: 881 	Training Loss: 1.168809 	Validation Loss: 1.335085 	 time: 0.3
Epoch: 882 	Training Loss: 1.168789 	Validation Loss: 1.335101 	 time: 0.3
Epoch: 883 	Training Loss: 1.168769 	Validation Loss: 1.335121 	 time: 0.3
Epoch: 884 	Training Loss: 1.168751 	Validation Loss: 1.335165 	 time: 0.3
Epoch: 885 	Training Loss: 1.168733 	Validation Loss: 1.335238 	 time: 0.3
Epoch: 886 	Training Loss: 1.168715 	Validation Loss: 1.335306 	 time: 0.3
Epoch: 887 	Training Loss: 1.168697 	Validation Loss: 1.335361 	 time: 0.3
Epoch: 888 	Training Loss: 1.168680 	Validation Loss: 1.335428 	 time: 0.3
Epoch: 889 	Training Loss: 1.168663 	Validation Loss: 1.335504 	 time: 0.3
Epoch: 890 	Training Loss: 1.168646 	Validation Loss: 1.335580 	 time: 0.3
Epoch: 891 	Training Loss: 1.168628 	Validation Loss: 1.335677 	 time: 0.3
Epoch: 892 	Training Loss: 1.168610 	Validation Loss: 1.335810 	 time: 0.3
Epoch: 893 	Training Loss: 1.168589 	Validation Loss: 1.335948 	 time: 0.3
Epoch: 894 	Training Loss: 1.168568 	Validation Loss: 1.336041 	 time: 0.3
Epoch: 895 	Training Loss: 1.168549 	Validation Loss: 1.336129 	 time: 0.3
Epoch: 896 	Training Loss: 1.168531 	Validation Loss: 1.336269 	 time: 0.3
Epoch: 897 	Training Loss: 1.168509 	Validation Loss: 1.336371 	 time: 0.3
Epoch: 898 	Training Loss: 1.168486 	Validation Loss: 1.336415 	 time: 0.3
Epoch: 899 	Training Loss: 1.168466 	Validation Loss: 1.336527 	 time: 0.3
Epoch: 900 	Training Loss: 1.168448 	Validation Loss: 1.336685 	 time: 0.3
Epoch: 901 	Training Loss: 1.168430 	Validation Loss: 1.336746 	 time: 0.3
Epoch: 902 	Training Loss: 1.168409 	Validation Loss: 1.336748 	 time: 0.3
Epoch: 903 	Training Loss: 1.168387 	Validation Loss: 1.336781 	 time: 0.3
Epoch: 904 	Training Loss: 1.168363 	Validation Loss: 1.336800 	 time: 0.3
Epoch: 905 	Training Loss: 1.168335 	Validation Loss: 1.336779 	 time: 0.3
Epoch: 906 	Training Loss: 1.168299 	Validation Loss: 1.336787 	 time: 0.3
Epoch: 907 	Training Loss: 1.168259 	Validation Loss: 1.336812 	 time: 0.3
Epoch: 908 	Training Loss: 1.168227 	Validation Loss: 1.336857 	 time: 0.3
Epoch: 909 	Training Loss: 1.168201 	Validation Loss: 1.336986 	 time: 0.3
Epoch: 910 	Training Loss: 1.168173 	Validation Loss: 1.337107 	 time: 0.3
Epoch: 911 	Training Loss: 1.168144 	Validation Loss: 1.337132 	 time: 0.3
Epoch: 912 	Training Loss: 1.168116 	Validation Loss: 1.337142 	 time: 0.3
Epoch: 913 	Training Loss: 1.168087 	Validation Loss: 1.337190 	 time: 0.3
Epoch: 914 	Training Loss: 1.168057 	Validation Loss: 1.337188 	 time: 0.3
Epoch: 915 	Training Loss: 1.168026 	Validation Loss: 1.337111 	 time: 0.3
Epoch: 916 	Training Loss: 1.167996 	Validation Loss: 1.337049 	 time: 0.3
Epoch: 917 	Training Loss: 1.167966 	Validation Loss: 1.337011 	 time: 0.3
Epoch: 918 	Training Loss: 1.167937 	Validation Loss: 1.336951 	 time: 0.3
Epoch: 919 	Training Loss: 1.167919 	Validation Loss: 1.336893 	 time: 0.3
Epoch: 920 	Training Loss: 1.167902 	Validation Loss: 1.336841 	 time: 0.3
Epoch: 921 	Training Loss: 1.167875 	Validation Loss: 1.336765 	 time: 0.3
Epoch: 922 	Training Loss: 1.167843 	Validation Loss: 1.336694 	 time: 0.3
Epoch: 923 	Training Loss: 1.167820 	Validation Loss: 1.336683 	 time: 0.3
Epoch: 924 	Training Loss: 1.167795 	Validation Loss: 1.336543 	 time: 0.3
Epoch: 925 	Training Loss: 1.167771 	Validation Loss: 1.336459 	 time: 0.3
Epoch: 926 	Training Loss: 1.167743 	Validation Loss: 1.336659 	 time: 0.3
Epoch: 927 	Training Loss: 1.167713 	Validation Loss: 1.336748 	 time: 0.3
Epoch: 928 	Training Loss: 1.167685 	Validation Loss: 1.336578 	 time: 0.3
Epoch: 929 	Training Loss: 1.167660 	Validation Loss: 1.336702 	 time: 0.3
Epoch: 930 	Training Loss: 1.167635 	Validation Loss: 1.336954 	 time: 0.3
Epoch: 931 	Training Loss: 1.167612 	Validation Loss: 1.336853 	 time: 0.3
Epoch: 932 	Training Loss: 1.167587 	Validation Loss: 1.336798 	 time: 0.3
Epoch: 933 	Training Loss: 1.167566 	Validation Loss: 1.336991 	 time: 0.3
Epoch: 934 	Training Loss: 1.167547 	Validation Loss: 1.337072 	 time: 0.3
Epoch: 935 	Training Loss: 1.167530 	Validation Loss: 1.337011 	 time: 0.3
Epoch: 936 	Training Loss: 1.167512 	Validation Loss: 1.336959 	 time: 0.3
Epoch: 937 	Training Loss: 1.167494 	Validation Loss: 1.336842 	 time: 0.3
Epoch: 938 	Training Loss: 1.167474 	Validation Loss: 1.336645 	 time: 0.3
Epoch: 939 	Training Loss: 1.167453 	Validation Loss: 1.336426 	 time: 0.3
Epoch: 940 	Training Loss: 1.167429 	Validation Loss: 1.336114 	 time: 0.3
Epoch: 941 	Training Loss: 1.167405 	Validation Loss: 1.335780 	 time: 0.3
Epoch: 942 	Training Loss: 1.167382 	Validation Loss: 1.335603 	 time: 0.3
Epoch: 943 	Training Loss: 1.167361 	Validation Loss: 1.335477 	 time: 0.3
Epoch: 944 	Training Loss: 1.167339 	Validation Loss: 1.335273 	 time: 0.3
Epoch: 945 	Training Loss: 1.167316 	Validation Loss: 1.335160 	 time: 0.3
Epoch: 946 	Training Loss: 1.167295 	Validation Loss: 1.335161 	 time: 0.3
Epoch: 947 	Training Loss: 1.167278 	Validation Loss: 1.335050 	 time: 0.3
Epoch: 948 	Training Loss: 1.167264 	Validation Loss: 1.334872 	 time: 0.3
Epoch: 949 	Training Loss: 1.167249 	Validation Loss: 1.334822 	 time: 0.3
Epoch: 950 	Training Loss: 1.167233 	Validation Loss: 1.334807 	 time: 0.3
Epoch: 951 	Training Loss: 1.167216 	Validation Loss: 1.334692 	 time: 0.3
Epoch: 952 	Training Loss: 1.167199 	Validation Loss: 1.334552 	 time: 0.3
Epoch: 953 	Training Loss: 1.167180 	Validation Loss: 1.334429 	 time: 0.3
Epoch: 954 	Training Loss: 1.167162 	Validation Loss: 1.334294 	 time: 0.3
Epoch: 955 	Training Loss: 1.167143 	Validation Loss: 1.334220 	 time: 0.3
Epoch: 956 	Training Loss: 1.167122 	Validation Loss: 1.334237 	 time: 0.3
Epoch: 957 	Training Loss: 1.167096 	Validation Loss: 1.334261 	 time: 0.3
Epoch: 958 	Training Loss: 1.167053 	Validation Loss: 1.334293 	 time: 0.3
Epoch: 959 	Training Loss: 1.167017 	Validation Loss: 1.334381 	 time: 0.3
Epoch: 960 	Training Loss: 1.166990 	Validation Loss: 1.334434 	 time: 0.3
Epoch: 961 	Training Loss: 1.166967 	Validation Loss: 1.334428 	 time: 0.3
Epoch: 962 	Training Loss: 1.166947 	Validation Loss: 1.334478 	 time: 0.3
Epoch: 963 	Training Loss: 1.166927 	Validation Loss: 1.334632 	 time: 0.3
Epoch: 964 	Training Loss: 1.166903 	Validation Loss: 1.334841 	 time: 0.3
Epoch: 965 	Training Loss: 1.166873 	Validation Loss: 1.335069 	 time: 0.3
Epoch: 966 	Training Loss: 1.166839 	Validation Loss: 1.335283 	 time: 0.3
Epoch: 967 	Training Loss: 1.166805 	Validation Loss: 1.335494 	 time: 0.3
Epoch: 968 	Training Loss: 1.166770 	Validation Loss: 1.335672 	 time: 0.3
Epoch: 969 	Training Loss: 1.166733 	Validation Loss: 1.335768 	 time: 0.3
Epoch: 970 	Training Loss: 1.166698 	Validation Loss: 1.335816 	 time: 0.3
Epoch: 971 	Training Loss: 1.166664 	Validation Loss: 1.335817 	 time: 0.3
Epoch: 972 	Training Loss: 1.166633 	Validation Loss: 1.335662 	 time: 0.3
Epoch: 973 	Training Loss: 1.166606 	Validation Loss: 1.335380 	 time: 0.3
Epoch: 974 	Training Loss: 1.166583 	Validation Loss: 1.335075 	 time: 0.3
Epoch: 975 	Training Loss: 1.166560 	Validation Loss: 1.334688 	 time: 0.3
Epoch: 976 	Training Loss: 1.166539 	Validation Loss: 1.334310 	 time: 0.3
Epoch: 977 	Training Loss: 1.166519 	Validation Loss: 1.334140 	 time: 0.3
Epoch: 978 	Training Loss: 1.166499 	Validation Loss: 1.334111 	 time: 0.3
Epoch: 979 	Training Loss: 1.166481 	Validation Loss: 1.334088 	 time: 0.3
Epoch: 980 	Training Loss: 1.166464 	Validation Loss: 1.334113 	 time: 0.3
Epoch: 981 	Training Loss: 1.166447 	Validation Loss: 1.334190 	 time: 0.3
Epoch: 982 	Training Loss: 1.166430 	Validation Loss: 1.334200 	 time: 0.3
Epoch: 983 	Training Loss: 1.166412 	Validation Loss: 1.334166 	 time: 0.3
Epoch: 984 	Training Loss: 1.166395 	Validation Loss: 1.334184 	 time: 0.3
Epoch: 985 	Training Loss: 1.166377 	Validation Loss: 1.334218 	 time: 0.3
Epoch: 986 	Training Loss: 1.166358 	Validation Loss: 1.334236 	 time: 0.3
Epoch: 987 	Training Loss: 1.166338 	Validation Loss: 1.334274 	 time: 0.3
Epoch: 988 	Training Loss: 1.166318 	Validation Loss: 1.334303 	 time: 0.3
Epoch: 989 	Training Loss: 1.166297 	Validation Loss: 1.334300 	 time: 0.3
Epoch: 990 	Training Loss: 1.166275 	Validation Loss: 1.334321 	 time: 0.3
Epoch: 991 	Training Loss: 1.166252 	Validation Loss: 1.334352 	 time: 0.3
Epoch: 992 	Training Loss: 1.166229 	Validation Loss: 1.334354 	 time: 0.3
Epoch: 993 	Training Loss: 1.166207 	Validation Loss: 1.334362 	 time: 0.3
Epoch: 994 	Training Loss: 1.166185 	Validation Loss: 1.334375 	 time: 0.3
Epoch: 995 	Training Loss: 1.166163 	Validation Loss: 1.334358 	 time: 0.3
Epoch: 996 	Training Loss: 1.166139 	Validation Loss: 1.334352 	 time: 0.3
Epoch: 997 	Training Loss: 1.166113 	Validation Loss: 1.334378 	 time: 0.3
Epoch: 998 	Training Loss: 1.166087 	Validation Loss: 1.334408 	 time: 0.3
Epoch: 999 	Training Loss: 1.166067 	Validation Loss: 1.334452 	 time: 0.3
Epoch: 1000 	Training Loss: 1.166051 	Validation Loss: 1.334505 	 time: 0.3
Epoch: 1001 	Training Loss: 1.166034 	Validation Loss: 1.334531 	 time: 0.3
Epoch: 1002 	Training Loss: 1.166011 	Validation Loss: 1.334559 	 time: 0.3
Epoch: 1003 	Training Loss: 1.165988 	Validation Loss: 1.334615 	 time: 0.3
Epoch: 1004 	Training Loss: 1.165966 	Validation Loss: 1.334648 	 time: 0.3
Epoch: 1005 	Training Loss: 1.165947 	Validation Loss: 1.334635 	 time: 0.3
Epoch: 1006 	Training Loss: 1.165930 	Validation Loss: 1.334590 	 time: 0.3
Epoch: 1007 	Training Loss: 1.165911 	Validation Loss: 1.334503 	 time: 0.3
Epoch: 1008 	Training Loss: 1.165891 	Validation Loss: 1.334405 	 time: 0.3
Epoch: 1009 	Training Loss: 1.165870 	Validation Loss: 1.334332 	 time: 0.3
Epoch: 1010 	Training Loss: 1.165847 	Validation Loss: 1.334271 	 time: 0.3
Epoch: 1011 	Training Loss: 1.165823 	Validation Loss: 1.334225 	 time: 0.3
Epoch: 1012 	Training Loss: 1.165797 	Validation Loss: 1.334211 	 time: 0.3
Epoch: 1013 	Training Loss: 1.165765 	Validation Loss: 1.334198 	 time: 0.3
Epoch: 1014 	Training Loss: 1.165729 	Validation Loss: 1.334188 	 time: 0.3
Epoch: 1015 	Training Loss: 1.165694 	Validation Loss: 1.334187 	 time: 0.3
Epoch: 1016 	Training Loss: 1.165659 	Validation Loss: 1.334119 	 time: 0.3
Epoch: 1017 	Training Loss: 1.165619 	Validation Loss: 1.334083 	 time: 0.3
Epoch: 1018 	Training Loss: 1.165559 	Validation Loss: 1.334029 	 time: 0.3
Epoch: 1019 	Training Loss: 1.165514 	Validation Loss: 1.333935 	 time: 0.3
Epoch: 1020 	Training Loss: 1.165488 	Validation Loss: 1.334215 	 time: 0.3
Epoch: 1021 	Training Loss: 1.165463 	Validation Loss: 1.334358 	 time: 0.3
Epoch: 1022 	Training Loss: 1.165439 	Validation Loss: 1.334393 	 time: 0.3
Epoch: 1023 	Training Loss: 1.165416 	Validation Loss: 1.334571 	 time: 0.3
Epoch: 1024 	Training Loss: 1.165390 	Validation Loss: 1.334438 	 time: 0.3
Epoch: 1025 	Training Loss: 1.165364 	Validation Loss: 1.334371 	 time: 0.3
Epoch: 1026 	Training Loss: 1.165338 	Validation Loss: 1.334443 	 time: 0.3
Epoch: 1027 	Training Loss: 1.165310 	Validation Loss: 1.334235 	 time: 0.3
Epoch: 1028 	Training Loss: 1.165282 	Validation Loss: 1.334173 	 time: 0.3
Epoch: 1029 	Training Loss: 1.165253 	Validation Loss: 1.334129 	 time: 0.3
Epoch: 1030 	Training Loss: 1.165223 	Validation Loss: 1.333909 	 time: 0.3
Epoch: 1031 	Training Loss: 1.165192 	Validation Loss: 1.333784 	 time: 0.3
Epoch: 1032 	Training Loss: 1.165163 	Validation Loss: 1.333535 	 time: 0.3
Validation loss decreased from 1.333711 to 1.333535. Model was saved
Epoch: 1033 	Training Loss: 1.165136 	Validation Loss: 1.333368 	 time: 0.3
Validation loss decreased from 1.333535 to 1.333368. Model was saved
Epoch: 1034 	Training Loss: 1.165113 	Validation Loss: 1.333263 	 time: 0.3
Validation loss decreased from 1.333368 to 1.333263. Model was saved
Epoch: 1035 	Training Loss: 1.165092 	Validation Loss: 1.333045 	 time: 0.3
Validation loss decreased from 1.333263 to 1.333045. Model was saved
Epoch: 1036 	Training Loss: 1.165068 	Validation Loss: 1.332929 	 time: 0.3
Validation loss decreased from 1.333045 to 1.332929. Model was saved
Epoch: 1037 	Training Loss: 1.165046 	Validation Loss: 1.332783 	 time: 0.3
Validation loss decreased from 1.332929 to 1.332783. Model was saved
Epoch: 1038 	Training Loss: 1.165024 	Validation Loss: 1.332684 	 time: 0.3
Validation loss decreased from 1.332783 to 1.332684. Model was saved
Epoch: 1039 	Training Loss: 1.165003 	Validation Loss: 1.332613 	 time: 0.3
Validation loss decreased from 1.332684 to 1.332613. Model was saved
Epoch: 1040 	Training Loss: 1.164981 	Validation Loss: 1.332422 	 time: 0.3
Validation loss decreased from 1.332613 to 1.332422. Model was saved
Epoch: 1041 	Training Loss: 1.164958 	Validation Loss: 1.332336 	 time: 0.3
Validation loss decreased from 1.332422 to 1.332336. Model was saved
Epoch: 1042 	Training Loss: 1.164934 	Validation Loss: 1.332227 	 time: 0.3
Validation loss decreased from 1.332336 to 1.332227. Model was saved
Epoch: 1043 	Training Loss: 1.164912 	Validation Loss: 1.332125 	 time: 0.3
Validation loss decreased from 1.332227 to 1.332125. Model was saved
Epoch: 1044 	Training Loss: 1.164889 	Validation Loss: 1.332004 	 time: 0.3
Validation loss decreased from 1.332125 to 1.332004. Model was saved
Epoch: 1045 	Training Loss: 1.164867 	Validation Loss: 1.331878 	 time: 0.3
Validation loss decreased from 1.332004 to 1.331878. Model was saved
Epoch: 1046 	Training Loss: 1.164847 	Validation Loss: 1.331823 	 time: 0.3
Validation loss decreased from 1.331878 to 1.331823. Model was saved
Epoch: 1047 	Training Loss: 1.164828 	Validation Loss: 1.331713 	 time: 0.3
Validation loss decreased from 1.331823 to 1.331713. Model was saved
Epoch: 1048 	Training Loss: 1.164808 	Validation Loss: 1.331665 	 time: 0.3
Validation loss decreased from 1.331713 to 1.331665. Model was saved
Epoch: 1049 	Training Loss: 1.164788 	Validation Loss: 1.331560 	 time: 0.3
Validation loss decreased from 1.331665 to 1.331560. Model was saved
Epoch: 1050 	Training Loss: 1.164766 	Validation Loss: 1.331534 	 time: 0.3
Validation loss decreased from 1.331560 to 1.331534. Model was saved
Epoch: 1051 	Training Loss: 1.164743 	Validation Loss: 1.331457 	 time: 0.3
Validation loss decreased from 1.331534 to 1.331457. Model was saved
Epoch: 1052 	Training Loss: 1.164716 	Validation Loss: 1.331484 	 time: 0.3
Epoch: 1053 	Training Loss: 1.164678 	Validation Loss: 1.331403 	 time: 0.3
Validation loss decreased from 1.331457 to 1.331403. Model was saved
Epoch: 1054 	Training Loss: 1.164635 	Validation Loss: 1.331568 	 time: 0.3
Epoch: 1055 	Training Loss: 1.164607 	Validation Loss: 1.331367 	 time: 0.3
Validation loss decreased from 1.331403 to 1.331367. Model was saved
Epoch: 1056 	Training Loss: 1.164596 	Validation Loss: 1.331511 	 time: 0.3
Epoch: 1057 	Training Loss: 1.164589 	Validation Loss: 1.330917 	 time: 0.3
Validation loss decreased from 1.331367 to 1.330917. Model was saved
Epoch: 1058 	Training Loss: 1.164619 	Validation Loss: 1.331347 	 time: 0.3
Epoch: 1059 	Training Loss: 1.164570 	Validation Loss: 1.330660 	 time: 0.3
Validation loss decreased from 1.330917 to 1.330660. Model was saved
Epoch: 1060 	Training Loss: 1.164523 	Validation Loss: 1.330743 	 time: 0.3
Epoch: 1061 	Training Loss: 1.164476 	Validation Loss: 1.330772 	 time: 0.3
Epoch: 1062 	Training Loss: 1.164462 	Validation Loss: 1.330566 	 time: 0.3
Validation loss decreased from 1.330660 to 1.330566. Model was saved
Epoch: 1063 	Training Loss: 1.164469 	Validation Loss: 1.330931 	 time: 0.3
Epoch: 1064 	Training Loss: 1.164460 	Validation Loss: 1.330572 	 time: 0.3
Epoch: 1065 	Training Loss: 1.164461 	Validation Loss: 1.330833 	 time: 0.3
Epoch: 1066 	Training Loss: 1.164395 	Validation Loss: 1.330585 	 time: 0.3
Epoch: 1067 	Training Loss: 1.164360 	Validation Loss: 1.330433 	 time: 0.3
Validation loss decreased from 1.330566 to 1.330433. Model was saved
Epoch: 1068 	Training Loss: 1.164349 	Validation Loss: 1.330682 	 time: 0.3
Epoch: 1069 	Training Loss: 1.164344 	Validation Loss: 1.330342 	 time: 0.3
Validation loss decreased from 1.330433 to 1.330342. Model was saved
Epoch: 1070 	Training Loss: 1.164342 	Validation Loss: 1.330562 	 time: 0.3
Epoch: 1071 	Training Loss: 1.164294 	Validation Loss: 1.330129 	 time: 0.3
Validation loss decreased from 1.330342 to 1.330129. Model was saved
Epoch: 1072 	Training Loss: 1.164251 	Validation Loss: 1.330005 	 time: 0.3
Validation loss decreased from 1.330129 to 1.330005. Model was saved
Epoch: 1073 	Training Loss: 1.164209 	Validation Loss: 1.329726 	 time: 0.3
Validation loss decreased from 1.330005 to 1.329726. Model was saved
Epoch: 1074 	Training Loss: 1.164178 	Validation Loss: 1.329396 	 time: 0.3
Validation loss decreased from 1.329726 to 1.329396. Model was saved
Epoch: 1075 	Training Loss: 1.164137 	Validation Loss: 1.328894 	 time: 0.3
Validation loss decreased from 1.329396 to 1.328894. Model was saved
Epoch: 1076 	Training Loss: 1.164058 	Validation Loss: 1.328930 	 time: 0.3
Epoch: 1077 	Training Loss: 1.164006 	Validation Loss: 1.328624 	 time: 0.3
Validation loss decreased from 1.328894 to 1.328624. Model was saved
Epoch: 1078 	Training Loss: 1.163953 	Validation Loss: 1.328474 	 time: 0.3
Validation loss decreased from 1.328624 to 1.328474. Model was saved
Epoch: 1079 	Training Loss: 1.163892 	Validation Loss: 1.328409 	 time: 0.3
Validation loss decreased from 1.328474 to 1.328409. Model was saved
Epoch: 1080 	Training Loss: 1.163858 	Validation Loss: 1.328145 	 time: 0.3
Validation loss decreased from 1.328409 to 1.328145. Model was saved
Epoch: 1081 	Training Loss: 1.163825 	Validation Loss: 1.328353 	 time: 0.3
Epoch: 1082 	Training Loss: 1.163780 	Validation Loss: 1.327903 	 time: 0.3
Validation loss decreased from 1.328145 to 1.327903. Model was saved
Epoch: 1083 	Training Loss: 1.163697 	Validation Loss: 1.327436 	 time: 0.3
Validation loss decreased from 1.327903 to 1.327436. Model was saved
Epoch: 1084 	Training Loss: 1.163607 	Validation Loss: 1.326604 	 time: 0.3
Validation loss decreased from 1.327436 to 1.326604. Model was saved
Epoch: 1085 	Training Loss: 1.163551 	Validation Loss: 1.325722 	 time: 0.3
Validation loss decreased from 1.326604 to 1.325722. Model was saved
Epoch: 1086 	Training Loss: 1.163514 	Validation Loss: 1.325014 	 time: 0.3
Validation loss decreased from 1.325722 to 1.325014. Model was saved
Epoch: 1087 	Training Loss: 1.163475 	Validation Loss: 1.324493 	 time: 0.3
Validation loss decreased from 1.325014 to 1.324493. Model was saved
Epoch: 1088 	Training Loss: 1.163425 	Validation Loss: 1.323997 	 time: 0.3
Validation loss decreased from 1.324493 to 1.323997. Model was saved
Epoch: 1089 	Training Loss: 1.163371 	Validation Loss: 1.323580 	 time: 0.3
Validation loss decreased from 1.323997 to 1.323580. Model was saved
Epoch: 1090 	Training Loss: 1.163326 	Validation Loss: 1.323165 	 time: 0.3
Validation loss decreased from 1.323580 to 1.323165. Model was saved
Epoch: 1091 	Training Loss: 1.163285 	Validation Loss: 1.322634 	 time: 0.3
Validation loss decreased from 1.323165 to 1.322634. Model was saved
Epoch: 1092 	Training Loss: 1.163238 	Validation Loss: 1.322295 	 time: 0.3
Validation loss decreased from 1.322634 to 1.322295. Model was saved
Epoch: 1093 	Training Loss: 1.163195 	Validation Loss: 1.321855 	 time: 0.3
Validation loss decreased from 1.322295 to 1.321855. Model was saved
Epoch: 1094 	Training Loss: 1.163169 	Validation Loss: 1.321993 	 time: 0.3
Epoch: 1095 	Training Loss: 1.163147 	Validation Loss: 1.321458 	 time: 0.3
Validation loss decreased from 1.321855 to 1.321458. Model was saved
Epoch: 1096 	Training Loss: 1.163142 	Validation Loss: 1.322050 	 time: 0.3
Epoch: 1097 	Training Loss: 1.163142 	Validation Loss: 1.321393 	 time: 0.3
Validation loss decreased from 1.321458 to 1.321393. Model was saved
Epoch: 1098 	Training Loss: 1.163233 	Validation Loss: 1.322096 	 time: 0.3
Epoch: 1099 	Training Loss: 1.163108 	Validation Loss: 1.321115 	 time: 0.3
Validation loss decreased from 1.321393 to 1.321115. Model was saved
Epoch: 1100 	Training Loss: 1.162977 	Validation Loss: 1.320932 	 time: 0.3
Validation loss decreased from 1.321115 to 1.320932. Model was saved
Epoch: 1101 	Training Loss: 1.162859 	Validation Loss: 1.321445 	 time: 0.3
Epoch: 1102 	Training Loss: 1.162853 	Validation Loss: 1.321214 	 time: 0.3
Epoch: 1103 	Training Loss: 1.162917 	Validation Loss: 1.321868 	 time: 0.3
Epoch: 1104 	Training Loss: 1.162903 	Validation Loss: 1.321478 	 time: 0.3
Epoch: 1105 	Training Loss: 1.162889 	Validation Loss: 1.321385 	 time: 0.3
Epoch: 1106 	Training Loss: 1.162660 	Validation Loss: 1.321505 	 time: 0.3
Epoch: 1107 	Training Loss: 1.162566 	Validation Loss: 1.321518 	 time: 0.3
Epoch: 1108 	Training Loss: 1.162574 	Validation Loss: 1.322504 	 time: 0.3
Epoch: 1109 	Training Loss: 1.162601 	Validation Loss: 1.321177 	 time: 0.3
Epoch: 1110 	Training Loss: 1.162688 	Validation Loss: 1.322154 	 time: 0.3
Epoch: 1111 	Training Loss: 1.162577 	Validation Loss: 1.321084 	 time: 0.3
Epoch: 1112 	Training Loss: 1.162400 	Validation Loss: 1.320917 	 time: 0.3
Validation loss decreased from 1.320932 to 1.320917. Model was saved
Epoch: 1113 	Training Loss: 1.162353 	Validation Loss: 1.321698 	 time: 0.3
Epoch: 1114 	Training Loss: 1.162403 	Validation Loss: 1.320851 	 time: 0.3
Validation loss decreased from 1.320917 to 1.320851. Model was saved
Epoch: 1115 	Training Loss: 1.162409 	Validation Loss: 1.321935 	 time: 0.3
Epoch: 1116 	Training Loss: 1.162337 	Validation Loss: 1.321095 	 time: 0.3
Epoch: 1117 	Training Loss: 1.162207 	Validation Loss: 1.320845 	 time: 0.3
Validation loss decreased from 1.320851 to 1.320845. Model was saved
Epoch: 1118 	Training Loss: 1.162100 	Validation Loss: 1.321684 	 time: 0.3
Epoch: 1119 	Training Loss: 1.162110 	Validation Loss: 1.321024 	 time: 0.3
Epoch: 1120 	Training Loss: 1.162126 	Validation Loss: 1.322345 	 time: 0.3
Epoch: 1121 	Training Loss: 1.162084 	Validation Loss: 1.321127 	 time: 0.3
Epoch: 1122 	Training Loss: 1.161969 	Validation Loss: 1.321399 	 time: 0.3
Epoch: 1123 	Training Loss: 1.161857 	Validation Loss: 1.322301 	 time: 0.3
Epoch: 1124 	Training Loss: 1.161818 	Validation Loss: 1.321855 	 time: 0.3
Epoch: 1125 	Training Loss: 1.161902 	Validation Loss: 1.323062 	 time: 0.3
Epoch: 1126 	Training Loss: 1.161826 	Validation Loss: 1.321481 	 time: 0.3
Epoch: 1127 	Training Loss: 1.161714 	Validation Loss: 1.321763 	 time: 0.3
Epoch: 1128 	Training Loss: 1.161677 	Validation Loss: 1.322843 	 time: 0.3
Epoch: 1129 	Training Loss: 1.161593 	Validation Loss: 1.323143 	 time: 0.3
Epoch: 1130 	Training Loss: 1.161610 	Validation Loss: 1.324041 	 time: 0.3
Epoch: 1131 	Training Loss: 1.161545 	Validation Loss: 1.323020 	 time: 0.3
Epoch: 1132 	Training Loss: 1.161489 	Validation Loss: 1.323229 	 time: 0.3
Epoch: 1133 	Training Loss: 1.161454 	Validation Loss: 1.323607 	 time: 0.3
Epoch: 1134 	Training Loss: 1.161426 	Validation Loss: 1.323860 	 time: 0.3
Epoch: 1135 	Training Loss: 1.161362 	Validation Loss: 1.324379 	 time: 0.3
Epoch: 1136 	Training Loss: 1.161365 	Validation Loss: 1.324212 	 time: 0.3
Epoch: 1137 	Training Loss: 1.161322 	Validation Loss: 1.324264 	 time: 0.3
Epoch: 1138 	Training Loss: 1.161292 	Validation Loss: 1.324101 	 time: 0.3
Epoch: 1139 	Training Loss: 1.161262 	Validation Loss: 1.324334 	 time: 0.3
Epoch: 1140 	Training Loss: 1.161219 	Validation Loss: 1.324749 	 time: 0.3
Epoch: 1141 	Training Loss: 1.161171 	Validation Loss: 1.324742 	 time: 0.3
Epoch: 1142 	Training Loss: 1.161142 	Validation Loss: 1.324861 	 time: 0.3
Epoch: 1143 	Training Loss: 1.161085 	Validation Loss: 1.324627 	 time: 0.3
Epoch: 1144 	Training Loss: 1.161046 	Validation Loss: 1.324606 	 time: 0.3
Epoch: 1145 	Training Loss: 1.161025 	Validation Loss: 1.324788 	 time: 0.3
Epoch: 1146 	Training Loss: 1.160993 	Validation Loss: 1.324912 	 time: 0.3
Epoch: 1147 	Training Loss: 1.160965 	Validation Loss: 1.325135 	 time: 0.3
Epoch: 1148 	Training Loss: 1.160942 	Validation Loss: 1.325106 	 time: 0.3
Epoch: 1149 	Training Loss: 1.160901 	Validation Loss: 1.325204 	 time: 0.3
Epoch: 1150 	Training Loss: 1.160861 	Validation Loss: 1.325319 	 time: 0.3
Epoch: 1151 	Training Loss: 1.160813 	Validation Loss: 1.325407 	 time: 0.3
Epoch: 1152 	Training Loss: 1.160759 	Validation Loss: 1.325798 	 time: 0.3
Epoch: 1153 	Training Loss: 1.160721 	Validation Loss: 1.325577 	 time: 0.3
Epoch: 1154 	Training Loss: 1.160696 	Validation Loss: 1.326209 	 time: 0.3
Epoch: 1155 	Training Loss: 1.160676 	Validation Loss: 1.325295 	 time: 0.3
Epoch: 1156 	Training Loss: 1.160664 	Validation Loss: 1.325777 	 time: 0.3
Epoch: 1157 	Training Loss: 1.160647 	Validation Loss: 1.325069 	 time: 0.3
Epoch: 1158 	Training Loss: 1.160607 	Validation Loss: 1.325694 	 time: 0.3
Epoch: 1159 	Training Loss: 1.160575 	Validation Loss: 1.325206 	 time: 0.3
Epoch: 1160 	Training Loss: 1.160552 	Validation Loss: 1.325418 	 time: 0.3
Epoch: 1161 	Training Loss: 1.160514 	Validation Loss: 1.325087 	 time: 0.3
Epoch: 1162 	Training Loss: 1.160477 	Validation Loss: 1.324992 	 time: 0.3
Epoch: 1163 	Training Loss: 1.160452 	Validation Loss: 1.325077 	 time: 0.3
Epoch: 1164 	Training Loss: 1.160436 	Validation Loss: 1.324842 	 time: 0.3
Epoch: 1165 	Training Loss: 1.160421 	Validation Loss: 1.325058 	 time: 0.3
Epoch: 1166 	Training Loss: 1.160403 	Validation Loss: 1.324627 	 time: 0.3
Epoch: 1167 	Training Loss: 1.160389 	Validation Loss: 1.325005 	 time: 0.3
Epoch: 1168 	Training Loss: 1.160376 	Validation Loss: 1.324528 	 time: 0.3
Epoch: 1169 	Training Loss: 1.160370 	Validation Loss: 1.324916 	 time: 0.3
Epoch: 1170 	Training Loss: 1.160324 	Validation Loss: 1.324495 	 time: 0.3
Epoch: 1171 	Training Loss: 1.160257 	Validation Loss: 1.324663 	 time: 0.3
Epoch: 1172 	Training Loss: 1.160184 	Validation Loss: 1.324510 	 time: 0.3
Epoch: 1173 	Training Loss: 1.160128 	Validation Loss: 1.324504 	 time: 0.3
Epoch: 1174 	Training Loss: 1.160108 	Validation Loss: 1.324696 	 time: 0.3
Epoch: 1175 	Training Loss: 1.160088 	Validation Loss: 1.324487 	 time: 0.3
Epoch: 1176 	Training Loss: 1.160074 	Validation Loss: 1.324808 	 time: 0.3
Epoch: 1177 	Training Loss: 1.160151 	Validation Loss: 1.324133 	 time: 0.3
Epoch: 1178 	Training Loss: 1.160559 	Validation Loss: 1.326064 	 time: 0.3
Epoch: 1179 	Training Loss: 1.160647 	Validation Loss: 1.324854 	 time: 0.3
Epoch: 1180 	Training Loss: 1.160914 	Validation Loss: 1.326327 	 time: 0.3
Epoch: 1181 	Training Loss: 1.160541 	Validation Loss: 1.325071 	 time: 0.3
Epoch: 1182 	Training Loss: 1.160089 	Validation Loss: 1.324974 	 time: 0.3
Epoch: 1183 	Training Loss: 1.160467 	Validation Loss: 1.328183 	 time: 0.3
Epoch: 1184 	Training Loss: 1.161238 	Validation Loss: 1.326058 	 time: 0.3
Epoch: 1185 	Training Loss: 1.161914 	Validation Loss: 1.333122 	 time: 0.3
Epoch: 1186 	Training Loss: 1.161529 	Validation Loss: 1.325763 	 time: 0.3
Epoch: 1187 	Training Loss: 1.160388 	Validation Loss: 1.325606 	 time: 0.3
Epoch: 1188 	Training Loss: 1.160797 	Validation Loss: 1.332963 	 time: 0.3
Epoch: 1189 	Training Loss: 1.162049 	Validation Loss: 1.326373 	 time: 0.3
Epoch: 1190 	Training Loss: 1.163385 	Validation Loss: 1.327759 	 time: 0.3
Epoch: 1191 	Training Loss: 1.161723 	Validation Loss: 1.330398 	 time: 0.3
Epoch: 1192 	Training Loss: 1.162566 	Validation Loss: 1.332250 	 time: 0.3
Epoch: 1193 	Training Loss: 1.169766 	Validation Loss: 1.366559 	 time: 0.3
Epoch: 1194 	Training Loss: 1.188206 	Validation Loss: 1.382623 	 time: 0.3
Epoch: 1195 	Training Loss: 1.237538 	Validation Loss: 1.376112 	 time: 0.3
Epoch: 1196 	Training Loss: 1.224629 	Validation Loss: 1.362206 	 time: 0.3
Epoch: 1197 	Training Loss: 1.207702 	Validation Loss: 1.386993 	 time: 0.3
Epoch: 1198 	Training Loss: 1.214064 	Validation Loss: 1.364468 	 time: 0.3
Epoch: 1199 	Training Loss: 1.214339 	Validation Loss: 1.349648 	 time: 0.3
Epoch: 1200 	Training Loss: 1.207113 	Validation Loss: 1.367739 	 time: 0.3
Epoch: 1201 	Training Loss: 1.202940 	Validation Loss: 1.369320 	 time: 0.3
Epoch: 1202 	Training Loss: 1.193669 	Validation Loss: 1.361743 	 time: 0.3
Epoch: 1203 	Training Loss: 1.196378 	Validation Loss: 1.348616 	 time: 0.3
Epoch: 1204 	Training Loss: 1.186785 	Validation Loss: 1.357624 	 time: 0.3
Epoch: 1205 	Training Loss: 1.189100 	Validation Loss: 1.344255 	 time: 0.3
Epoch: 1206 	Training Loss: 1.179563 	Validation Loss: 1.350798 	 time: 0.3
Epoch: 1207 	Training Loss: 1.183732 	Validation Loss: 1.349099 	 time: 0.3
Epoch: 1208 	Training Loss: 1.179456 	Validation Loss: 1.345374 	 time: 0.3
Epoch: 1209 	Training Loss: 1.174713 	Validation Loss: 1.353826 	 time: 0.3
Epoch: 1210 	Training Loss: 1.178648 	Validation Loss: 1.351099 	 time: 0.3
Epoch: 1211 	Training Loss: 1.173434 	Validation Loss: 1.347858 	 time: 0.3
Epoch: 1212 	Training Loss: 1.175677 	Validation Loss: 1.341732 	 time: 0.3
Epoch: 1213 	Training Loss: 1.171279 	Validation Loss: 1.345056 	 time: 0.3
Epoch: 1214 	Training Loss: 1.171362 	Validation Loss: 1.348832 	 time: 0.3
Epoch: 1215 	Training Loss: 1.171322 	Validation Loss: 1.342736 	 time: 0.3
Epoch: 1216 	Training Loss: 1.170909 	Validation Loss: 1.344222 	 time: 0.3
Epoch: 1217 	Training Loss: 1.169674 	Validation Loss: 1.339085 	 time: 0.3
Epoch: 1218 	Training Loss: 1.167559 	Validation Loss: 1.338780 	 time: 0.3
Epoch: 1219 	Training Loss: 1.168192 	Validation Loss: 1.338861 	 time: 0.3
Epoch: 1220 	Training Loss: 1.167117 	Validation Loss: 1.343973 	 time: 0.3
Epoch: 1221 	Training Loss: 1.166597 	Validation Loss: 1.345050 	 time: 0.3
Epoch: 1222 	Training Loss: 1.166013 	Validation Loss: 1.341381 	 time: 0.3
Epoch: 1223 	Training Loss: 1.165184 	Validation Loss: 1.339394 	 time: 0.3
Epoch: 1224 	Training Loss: 1.165256 	Validation Loss: 1.337699 	 time: 0.3
Epoch: 1225 	Training Loss: 1.164413 	Validation Loss: 1.339070 	 time: 0.3
Epoch: 1226 	Training Loss: 1.164415 	Validation Loss: 1.336299 	 time: 0.3
Epoch: 1227 	Training Loss: 1.163777 	Validation Loss: 1.338536 	 time: 0.3
Epoch: 1228 	Training Loss: 1.163419 	Validation Loss: 1.341417 	 time: 0.3
Epoch: 1229 	Training Loss: 1.163263 	Validation Loss: 1.339388 	 time: 0.3
Epoch: 1230 	Training Loss: 1.162919 	Validation Loss: 1.339718 	 time: 0.3
Epoch: 1231 	Training Loss: 1.162691 	Validation Loss: 1.340551 	 time: 0.3
Epoch: 1232 	Training Loss: 1.162470 	Validation Loss: 1.338490 	 time: 0.3
Epoch: 1233 	Training Loss: 1.162142 	Validation Loss: 1.337680 	 time: 0.3
Epoch: 1234 	Training Loss: 1.161952 	Validation Loss: 1.337616 	 time: 0.3
Epoch: 1235 	Training Loss: 1.161734 	Validation Loss: 1.335941 	 time: 0.3
Epoch: 1236 	Training Loss: 1.161545 	Validation Loss: 1.334465 	 time: 0.3
Epoch: 1237 	Training Loss: 1.161401 	Validation Loss: 1.335093 	 time: 0.3
Epoch: 1238 	Training Loss: 1.161149 	Validation Loss: 1.335892 	 time: 0.3
Epoch: 1239 	Training Loss: 1.160900 	Validation Loss: 1.335360 	 time: 0.3
Epoch: 1240 	Training Loss: 1.160691 	Validation Loss: 1.334935 	 time: 0.3
Epoch: 1241 	Training Loss: 1.160485 	Validation Loss: 1.335202 	 time: 0.3
Epoch: 1242 	Training Loss: 1.160399 	Validation Loss: 1.334591 	 time: 0.3
Epoch: 1243 	Training Loss: 1.160276 	Validation Loss: 1.334299 	 time: 0.3
Epoch: 1244 	Training Loss: 1.160159 	Validation Loss: 1.334412 	 time: 0.3
Epoch: 1245 	Training Loss: 1.160011 	Validation Loss: 1.333959 	 time: 0.3
Epoch: 1246 	Training Loss: 1.159828 	Validation Loss: 1.333435 	 time: 0.3
Epoch: 1247 	Training Loss: 1.159690 	Validation Loss: 1.333460 	 time: 0.3
Epoch: 1248 	Training Loss: 1.159559 	Validation Loss: 1.333489 	 time: 0.3
Epoch: 1249 	Training Loss: 1.159465 	Validation Loss: 1.333150 	 time: 0.3
Epoch: 1250 	Training Loss: 1.159387 	Validation Loss: 1.333001 	 time: 0.3
Epoch: 1251 	Training Loss: 1.159340 	Validation Loss: 1.332875 	 time: 0.3
Epoch: 1252 	Training Loss: 1.159279 	Validation Loss: 1.332049 	 time: 0.3
Epoch: 1253 	Training Loss: 1.159177 	Validation Loss: 1.331229 	 time: 0.3
Epoch: 1254 	Training Loss: 1.159097 	Validation Loss: 1.331187 	 time: 0.3
Epoch: 1255 	Training Loss: 1.159034 	Validation Loss: 1.331375 	 time: 0.3
Epoch: 1256 	Training Loss: 1.158985 	Validation Loss: 1.331566 	 time: 0.3
Epoch: 1257 	Training Loss: 1.158919 	Validation Loss: 1.332149 	 time: 0.3
Epoch: 1258 	Training Loss: 1.158868 	Validation Loss: 1.332877 	 time: 0.3
Epoch: 1259 	Training Loss: 1.158837 	Validation Loss: 1.333130 	 time: 0.3
Epoch: 1260 	Training Loss: 1.158796 	Validation Loss: 1.333125 	 time: 0.3
Epoch: 1261 	Training Loss: 1.158751 	Validation Loss: 1.333252 	 time: 0.3
Epoch: 1262 	Training Loss: 1.158710 	Validation Loss: 1.333350 	 time: 0.3
Epoch: 1263 	Training Loss: 1.158674 	Validation Loss: 1.333246 	 time: 0.3
Epoch: 1264 	Training Loss: 1.158637 	Validation Loss: 1.333161 	 time: 0.3
Epoch: 1265 	Training Loss: 1.158599 	Validation Loss: 1.333221 	 time: 0.3
Epoch: 1266 	Training Loss: 1.158560 	Validation Loss: 1.333178 	 time: 0.3
Epoch: 1267 	Training Loss: 1.158527 	Validation Loss: 1.333028 	 time: 0.3
Epoch: 1268 	Training Loss: 1.158495 	Validation Loss: 1.333026 	 time: 0.3
Epoch: 1269 	Training Loss: 1.158462 	Validation Loss: 1.333200 	 time: 0.3
Epoch: 1270 	Training Loss: 1.158432 	Validation Loss: 1.333338 	 time: 0.3
Epoch: 1271 	Training Loss: 1.158399 	Validation Loss: 1.333461 	 time: 0.3
Epoch: 1272 	Training Loss: 1.158364 	Validation Loss: 1.333658 	 time: 0.3
Epoch: 1273 	Training Loss: 1.158319 	Validation Loss: 1.333869 	 time: 0.3
Epoch: 1274 	Training Loss: 1.158255 	Validation Loss: 1.334068 	 time: 0.3
Epoch: 1275 	Training Loss: 1.158195 	Validation Loss: 1.334316 	 time: 0.3
Epoch: 1276 	Training Loss: 1.158160 	Validation Loss: 1.334455 	 time: 0.3
Epoch: 1277 	Training Loss: 1.158130 	Validation Loss: 1.334389 	 time: 0.3
Epoch: 1278 	Training Loss: 1.158097 	Validation Loss: 1.334310 	 time: 0.3
Epoch: 1279 	Training Loss: 1.158064 	Validation Loss: 1.334327 	 time: 0.3
Epoch: 1280 	Training Loss: 1.158033 	Validation Loss: 1.334388 	 time: 0.3
Epoch: 1281 	Training Loss: 1.158004 	Validation Loss: 1.334479 	 time: 0.3
Epoch: 1282 	Training Loss: 1.157978 	Validation Loss: 1.334630 	 time: 0.3
Epoch: 1283 	Training Loss: 1.157948 	Validation Loss: 1.334849 	 time: 0.3
Epoch: 1284 	Training Loss: 1.157918 	Validation Loss: 1.335105 	 time: 0.3
Epoch: 1285 	Training Loss: 1.157888 	Validation Loss: 1.335391 	 time: 0.3
Epoch: 1286 	Training Loss: 1.157858 	Validation Loss: 1.335671 	 time: 0.3
Epoch: 1287 	Training Loss: 1.157829 	Validation Loss: 1.335860 	 time: 0.3
Epoch: 1288 	Training Loss: 1.157797 	Validation Loss: 1.335912 	 time: 0.3
Epoch: 1289 	Training Loss: 1.157760 	Validation Loss: 1.335842 	 time: 0.3
Epoch: 1290 	Training Loss: 1.157723 	Validation Loss: 1.335690 	 time: 0.3
Epoch: 1291 	Training Loss: 1.157692 	Validation Loss: 1.335471 	 time: 0.3
Epoch: 1292 	Training Loss: 1.157665 	Validation Loss: 1.335219 	 time: 0.3
Epoch: 1293 	Training Loss: 1.157639 	Validation Loss: 1.335015 	 time: 0.3
Epoch: 1294 	Training Loss: 1.157609 	Validation Loss: 1.334892 	 time: 0.3
Epoch: 1295 	Training Loss: 1.157569 	Validation Loss: 1.334806 	 time: 0.3
Epoch: 1296 	Training Loss: 1.157529 	Validation Loss: 1.334706 	 time: 0.3
Epoch: 1297 	Training Loss: 1.157499 	Validation Loss: 1.334598 	 time: 0.3
Epoch: 1298 	Training Loss: 1.157476 	Validation Loss: 1.334490 	 time: 0.3
Epoch: 1299 	Training Loss: 1.157460 	Validation Loss: 1.334383 	 time: 0.3
Epoch: 1300 	Training Loss: 1.157444 	Validation Loss: 1.334276 	 time: 0.3
Epoch: 1301 	Training Loss: 1.157427 	Validation Loss: 1.334170 	 time: 0.3
Epoch: 1302 	Training Loss: 1.157408 	Validation Loss: 1.334053 	 time: 0.3
Epoch: 1303 	Training Loss: 1.157388 	Validation Loss: 1.333916 	 time: 0.3
Epoch: 1304 	Training Loss: 1.157368 	Validation Loss: 1.333763 	 time: 0.3
Epoch: 1305 	Training Loss: 1.157351 	Validation Loss: 1.333620 	 time: 0.3
Epoch: 1306 	Training Loss: 1.157334 	Validation Loss: 1.333510 	 time: 0.3
Epoch: 1307 	Training Loss: 1.157318 	Validation Loss: 1.333394 	 time: 0.3
Epoch: 1308 	Training Loss: 1.157301 	Validation Loss: 1.333242 	 time: 0.3
Epoch: 1309 	Training Loss: 1.157285 	Validation Loss: 1.333093 	 time: 0.3
Epoch: 1310 	Training Loss: 1.157270 	Validation Loss: 1.332960 	 time: 0.3
Epoch: 1311 	Training Loss: 1.157254 	Validation Loss: 1.332785 	 time: 0.3
Epoch: 1312 	Training Loss: 1.157238 	Validation Loss: 1.332576 	 time: 0.3
Epoch: 1313 	Training Loss: 1.157221 	Validation Loss: 1.332407 	 time: 0.3
Epoch: 1314 	Training Loss: 1.157202 	Validation Loss: 1.332269 	 time: 0.3
Epoch: 1315 	Training Loss: 1.157183 	Validation Loss: 1.332111 	 time: 0.3
Epoch: 1316 	Training Loss: 1.157160 	Validation Loss: 1.331948 	 time: 0.3
Epoch: 1317 	Training Loss: 1.157134 	Validation Loss: 1.331812 	 time: 0.3
Epoch: 1318 	Training Loss: 1.157101 	Validation Loss: 1.331683 	 time: 0.3
Epoch: 1319 	Training Loss: 1.157063 	Validation Loss: 1.331551 	 time: 0.3
Epoch: 1320 	Training Loss: 1.157030 	Validation Loss: 1.331444 	 time: 0.3
Epoch: 1321 	Training Loss: 1.157008 	Validation Loss: 1.331357 	 time: 0.3
Epoch: 1322 	Training Loss: 1.156988 	Validation Loss: 1.331243 	 time: 0.3
Epoch: 1323 	Training Loss: 1.156968 	Validation Loss: 1.331117 	 time: 0.3
Epoch: 1324 	Training Loss: 1.156948 	Validation Loss: 1.331015 	 time: 0.3
Epoch: 1325 	Training Loss: 1.156928 	Validation Loss: 1.330908 	 time: 0.3
Epoch: 1326 	Training Loss: 1.156908 	Validation Loss: 1.330782 	 time: 0.3
Epoch: 1327 	Training Loss: 1.156889 	Validation Loss: 1.330669 	 time: 0.3
Epoch: 1328 	Training Loss: 1.156870 	Validation Loss: 1.330561 	 time: 0.3
Epoch: 1329 	Training Loss: 1.156851 	Validation Loss: 1.330423 	 time: 0.3
Epoch: 1330 	Training Loss: 1.156829 	Validation Loss: 1.330285 	 time: 0.3
Epoch: 1331 	Training Loss: 1.156803 	Validation Loss: 1.330200 	 time: 0.3
Epoch: 1332 	Training Loss: 1.156781 	Validation Loss: 1.330141 	 time: 0.3
Epoch: 1333 	Training Loss: 1.156763 	Validation Loss: 1.330064 	 time: 0.3
Epoch: 1334 	Training Loss: 1.156747 	Validation Loss: 1.329987 	 time: 0.3
Epoch: 1335 	Training Loss: 1.156732 	Validation Loss: 1.329910 	 time: 0.3
Epoch: 1336 	Training Loss: 1.156717 	Validation Loss: 1.329806 	 time: 0.3
Epoch: 1337 	Training Loss: 1.156701 	Validation Loss: 1.329690 	 time: 0.3
Epoch: 1338 	Training Loss: 1.156687 	Validation Loss: 1.329579 	 time: 0.3
Epoch: 1339 	Training Loss: 1.156672 	Validation Loss: 1.329459 	 time: 0.3
Epoch: 1340 	Training Loss: 1.156659 	Validation Loss: 1.329322 	 time: 0.3
Epoch: 1341 	Training Loss: 1.156644 	Validation Loss: 1.329182 	 time: 0.3
Epoch: 1342 	Training Loss: 1.156630 	Validation Loss: 1.329048 	 time: 0.3
Epoch: 1343 	Training Loss: 1.156614 	Validation Loss: 1.328920 	 time: 0.3
Epoch: 1344 	Training Loss: 1.156595 	Validation Loss: 1.328784 	 time: 0.3
Epoch: 1345 	Training Loss: 1.156574 	Validation Loss: 1.328643 	 time: 0.3
Epoch: 1346 	Training Loss: 1.156565 	Validation Loss: 1.328559 	 time: 0.3
Epoch: 1347 	Training Loss: 1.156548 	Validation Loss: 1.328430 	 time: 0.3
Epoch: 1348 	Training Loss: 1.156527 	Validation Loss: 1.328204 	 time: 0.3
Epoch: 1349 	Training Loss: 1.156512 	Validation Loss: 1.327988 	 time: 0.3
Epoch: 1350 	Training Loss: 1.156499 	Validation Loss: 1.327812 	 time: 0.3
Epoch: 1351 	Training Loss: 1.156486 	Validation Loss: 1.327611 	 time: 0.3
Epoch: 1352 	Training Loss: 1.156473 	Validation Loss: 1.327373 	 time: 0.3
Epoch: 1353 	Training Loss: 1.156461 	Validation Loss: 1.327167 	 time: 0.3
Epoch: 1354 	Training Loss: 1.156449 	Validation Loss: 1.327017 	 time: 0.3
Epoch: 1355 	Training Loss: 1.156437 	Validation Loss: 1.326883 	 time: 0.3
Epoch: 1356 	Training Loss: 1.156425 	Validation Loss: 1.326741 	 time: 0.3
Epoch: 1357 	Training Loss: 1.156413 	Validation Loss: 1.326573 	 time: 0.3
Epoch: 1358 	Training Loss: 1.156400 	Validation Loss: 1.326336 	 time: 0.3
Epoch: 1359 	Training Loss: 1.156387 	Validation Loss: 1.326063 	 time: 0.3
Epoch: 1360 	Training Loss: 1.156374 	Validation Loss: 1.325840 	 time: 0.3
Epoch: 1361 	Training Loss: 1.156360 	Validation Loss: 1.325647 	 time: 0.3
Epoch: 1362 	Training Loss: 1.156347 	Validation Loss: 1.325405 	 time: 0.3
Epoch: 1363 	Training Loss: 1.156334 	Validation Loss: 1.325147 	 time: 0.3
Epoch: 1364 	Training Loss: 1.156319 	Validation Loss: 1.324969 	 time: 0.3
Epoch: 1365 	Training Loss: 1.156303 	Validation Loss: 1.324873 	 time: 0.3
Epoch: 1366 	Training Loss: 1.156287 	Validation Loss: 1.324805 	 time: 0.3
Epoch: 1367 	Training Loss: 1.156272 	Validation Loss: 1.324728 	 time: 0.3
Epoch: 1368 	Training Loss: 1.156258 	Validation Loss: 1.324646 	 time: 0.3
Epoch: 1369 	Training Loss: 1.156243 	Validation Loss: 1.324593 	 time: 0.4
Epoch: 1370 	Training Loss: 1.156228 	Validation Loss: 1.324563 	 time: 0.3
Epoch: 1371 	Training Loss: 1.156212 	Validation Loss: 1.324518 	 time: 0.3
Epoch: 1372 	Training Loss: 1.156196 	Validation Loss: 1.324446 	 time: 0.3
Epoch: 1373 	Training Loss: 1.156178 	Validation Loss: 1.324354 	 time: 0.3
Epoch: 1374 	Training Loss: 1.156157 	Validation Loss: 1.324217 	 time: 0.3
Epoch: 1375 	Training Loss: 1.156135 	Validation Loss: 1.323993 	 time: 0.3
Epoch: 1376 	Training Loss: 1.156116 	Validation Loss: 1.323729 	 time: 0.3
Epoch: 1377 	Training Loss: 1.156101 	Validation Loss: 1.323482 	 time: 0.3
Epoch: 1378 	Training Loss: 1.156088 	Validation Loss: 1.323256 	 time: 0.3
Epoch: 1379 	Training Loss: 1.156073 	Validation Loss: 1.323104 	 time: 0.3
Epoch: 1380 	Training Loss: 1.156059 	Validation Loss: 1.323034 	 time: 0.3
Epoch: 1381 	Training Loss: 1.156045 	Validation Loss: 1.322973 	 time: 0.3
Epoch: 1382 	Training Loss: 1.156032 	Validation Loss: 1.322910 	 time: 0.3
Epoch: 1383 	Training Loss: 1.156021 	Validation Loss: 1.322894 	 time: 0.3
Epoch: 1384 	Training Loss: 1.156009 	Validation Loss: 1.322919 	 time: 0.3
Epoch: 1385 	Training Loss: 1.155997 	Validation Loss: 1.322949 	 time: 0.3
Epoch: 1386 	Training Loss: 1.155984 	Validation Loss: 1.322956 	 time: 0.3
Epoch: 1387 	Training Loss: 1.155973 	Validation Loss: 1.322926 	 time: 0.3
Epoch: 1388 	Training Loss: 1.155960 	Validation Loss: 1.322870 	 time: 0.3
Epoch: 1389 	Training Loss: 1.155946 	Validation Loss: 1.322793 	 time: 0.3
Epoch: 1390 	Training Loss: 1.155927 	Validation Loss: 1.322673 	 time: 0.3
Epoch: 1391 	Training Loss: 1.155905 	Validation Loss: 1.322540 	 time: 0.3
Epoch: 1392 	Training Loss: 1.155890 	Validation Loss: 1.322474 	 time: 0.3
Epoch: 1393 	Training Loss: 1.155875 	Validation Loss: 1.322471 	 time: 0.3
Epoch: 1394 	Training Loss: 1.155857 	Validation Loss: 1.322466 	 time: 0.3
Epoch: 1395 	Training Loss: 1.155836 	Validation Loss: 1.322462 	 time: 0.3
Epoch: 1396 	Training Loss: 1.155813 	Validation Loss: 1.322464 	 time: 0.3
Epoch: 1397 	Training Loss: 1.155788 	Validation Loss: 1.322426 	 time: 0.3
Epoch: 1398 	Training Loss: 1.155758 	Validation Loss: 1.322357 	 time: 0.3
Epoch: 1399 	Training Loss: 1.155712 	Validation Loss: 1.322265 	 time: 0.3
Epoch: 1400 	Training Loss: 1.155644 	Validation Loss: 1.321964 	 time: 0.3
Epoch: 1401 	Training Loss: 1.155601 	Validation Loss: 1.321683 	 time: 0.3
Epoch: 1402 	Training Loss: 1.155572 	Validation Loss: 1.321660 	 time: 0.3
Epoch: 1403 	Training Loss: 1.155542 	Validation Loss: 1.321599 	 time: 0.3
Epoch: 1404 	Training Loss: 1.155514 	Validation Loss: 1.321475 	 time: 0.3
Epoch: 1405 	Training Loss: 1.155490 	Validation Loss: 1.321507 	 time: 0.3
Epoch: 1406 	Training Loss: 1.155468 	Validation Loss: 1.321618 	 time: 0.3
Epoch: 1407 	Training Loss: 1.155447 	Validation Loss: 1.321693 	 time: 0.3
Epoch: 1408 	Training Loss: 1.155425 	Validation Loss: 1.321782 	 time: 0.3
Epoch: 1409 	Training Loss: 1.155401 	Validation Loss: 1.321911 	 time: 0.3
Epoch: 1410 	Training Loss: 1.155377 	Validation Loss: 1.322012 	 time: 0.3
Epoch: 1411 	Training Loss: 1.155354 	Validation Loss: 1.322015 	 time: 0.3
Epoch: 1412 	Training Loss: 1.155333 	Validation Loss: 1.321939 	 time: 0.3
Epoch: 1413 	Training Loss: 1.155315 	Validation Loss: 1.321902 	 time: 0.3
Epoch: 1414 	Training Loss: 1.155297 	Validation Loss: 1.321923 	 time: 0.3
Epoch: 1415 	Training Loss: 1.155280 	Validation Loss: 1.321954 	 time: 0.3
Epoch: 1416 	Training Loss: 1.155262 	Validation Loss: 1.322070 	 time: 0.3
Epoch: 1417 	Training Loss: 1.155242 	Validation Loss: 1.322275 	 time: 0.3
Epoch: 1418 	Training Loss: 1.155222 	Validation Loss: 1.322401 	 time: 0.3
Epoch: 1419 	Training Loss: 1.155201 	Validation Loss: 1.322449 	 time: 0.3
Epoch: 1420 	Training Loss: 1.155180 	Validation Loss: 1.322505 	 time: 0.3
Epoch: 1421 	Training Loss: 1.155161 	Validation Loss: 1.322433 	 time: 0.3
Epoch: 1422 	Training Loss: 1.155140 	Validation Loss: 1.322174 	 time: 0.3
Epoch: 1423 	Training Loss: 1.155119 	Validation Loss: 1.321937 	 time: 0.3
Epoch: 1424 	Training Loss: 1.155099 	Validation Loss: 1.321796 	 time: 0.3
Epoch: 1425 	Training Loss: 1.155083 	Validation Loss: 1.321645 	 time: 0.3
Epoch: 1426 	Training Loss: 1.155069 	Validation Loss: 1.321519 	 time: 0.3
Epoch: 1427 	Training Loss: 1.155054 	Validation Loss: 1.321492 	 time: 0.3
Epoch: 1428 	Training Loss: 1.155038 	Validation Loss: 1.321493 	 time: 0.3
Epoch: 1429 	Training Loss: 1.155022 	Validation Loss: 1.321430 	 time: 0.3
Epoch: 1430 	Training Loss: 1.155005 	Validation Loss: 1.321340 	 time: 0.3
Epoch: 1431 	Training Loss: 1.154981 	Validation Loss: 1.321299 	 time: 0.3
Epoch: 1432 	Training Loss: 1.154944 	Validation Loss: 1.321301 	 time: 0.3
Epoch: 1433 	Training Loss: 1.154890 	Validation Loss: 1.321309 	 time: 0.3
Epoch: 1434 	Training Loss: 1.154847 	Validation Loss: 1.321377 	 time: 0.3
Epoch: 1435 	Training Loss: 1.154824 	Validation Loss: 1.321562 	 time: 0.3
Epoch: 1436 	Training Loss: 1.154806 	Validation Loss: 1.321761 	 time: 0.3
Epoch: 1437 	Training Loss: 1.154789 	Validation Loss: 1.321871 	 time: 0.3
Epoch: 1438 	Training Loss: 1.154770 	Validation Loss: 1.321968 	 time: 0.3
Epoch: 1439 	Training Loss: 1.154748 	Validation Loss: 1.322106 	 time: 0.3
Epoch: 1440 	Training Loss: 1.154725 	Validation Loss: 1.322212 	 time: 0.3
Epoch: 1441 	Training Loss: 1.154680 	Validation Loss: 1.322218 	 time: 0.3
Epoch: 1442 	Training Loss: 1.154580 	Validation Loss: 1.322241 	 time: 0.3
Epoch: 1443 	Training Loss: 1.154546 	Validation Loss: 1.322223 	 time: 0.3
Epoch: 1444 	Training Loss: 1.154528 	Validation Loss: 1.322090 	 time: 0.3
Epoch: 1445 	Training Loss: 1.154508 	Validation Loss: 1.321950 	 time: 0.3
Epoch: 1446 	Training Loss: 1.154482 	Validation Loss: 1.321879 	 time: 0.3
Epoch: 1447 	Training Loss: 1.154454 	Validation Loss: 1.321876 	 time: 0.3
Epoch: 1448 	Training Loss: 1.154446 	Validation Loss: 1.321967 	 time: 0.3
Epoch: 1449 	Training Loss: 1.154423 	Validation Loss: 1.322051 	 time: 0.3
Epoch: 1450 	Training Loss: 1.154405 	Validation Loss: 1.322064 	 time: 0.3
Epoch: 1451 	Training Loss: 1.154394 	Validation Loss: 1.322067 	 time: 0.3
Epoch: 1452 	Training Loss: 1.154379 	Validation Loss: 1.322065 	 time: 0.3
Epoch: 1453 	Training Loss: 1.154365 	Validation Loss: 1.322017 	 time: 0.3
Epoch: 1454 	Training Loss: 1.154350 	Validation Loss: 1.321958 	 time: 0.3
Epoch: 1455 	Training Loss: 1.154338 	Validation Loss: 1.321933 	 time: 0.3
Epoch: 1456 	Training Loss: 1.154322 	Validation Loss: 1.321955 	 time: 0.3
Epoch: 1457 	Training Loss: 1.154301 	Validation Loss: 1.321989 	 time: 0.3
Epoch: 1458 	Training Loss: 1.154279 	Validation Loss: 1.322009 	 time: 0.3
Epoch: 1459 	Training Loss: 1.154257 	Validation Loss: 1.322042 	 time: 0.3
Epoch: 1460 	Training Loss: 1.154232 	Validation Loss: 1.322083 	 time: 0.3
Epoch: 1461 	Training Loss: 1.154210 	Validation Loss: 1.322084 	 time: 0.3
Epoch: 1462 	Training Loss: 1.154193 	Validation Loss: 1.322048 	 time: 0.3
Epoch: 1463 	Training Loss: 1.154178 	Validation Loss: 1.322052 	 time: 0.3
Epoch: 1464 	Training Loss: 1.154164 	Validation Loss: 1.322096 	 time: 0.3
Epoch: 1465 	Training Loss: 1.154152 	Validation Loss: 1.322121 	 time: 0.3
Epoch: 1466 	Training Loss: 1.154141 	Validation Loss: 1.322107 	 time: 0.3
Epoch: 1467 	Training Loss: 1.154129 	Validation Loss: 1.322108 	 time: 0.3
Epoch: 1468 	Training Loss: 1.154118 	Validation Loss: 1.322157 	 time: 0.3
Epoch: 1469 	Training Loss: 1.154106 	Validation Loss: 1.322214 	 time: 0.3
Epoch: 1470 	Training Loss: 1.154096 	Validation Loss: 1.322261 	 time: 0.3
Epoch: 1471 	Training Loss: 1.154084 	Validation Loss: 1.322310 	 time: 0.3
Epoch: 1472 	Training Loss: 1.154072 	Validation Loss: 1.322363 	 time: 0.3
Epoch: 1473 	Training Loss: 1.154060 	Validation Loss: 1.322424 	 time: 0.3
Epoch: 1474 	Training Loss: 1.154049 	Validation Loss: 1.322479 	 time: 0.3
Epoch: 1475 	Training Loss: 1.154038 	Validation Loss: 1.322479 	 time: 0.3
Epoch: 1476 	Training Loss: 1.154028 	Validation Loss: 1.322423 	 time: 0.3
Epoch: 1477 	Training Loss: 1.154017 	Validation Loss: 1.322353 	 time: 0.3
Epoch: 1478 	Training Loss: 1.154007 	Validation Loss: 1.322291 	 time: 0.3
Epoch: 1479 	Training Loss: 1.153997 	Validation Loss: 1.322271 	 time: 0.3
Epoch: 1480 	Training Loss: 1.153987 	Validation Loss: 1.322315 	 time: 0.3
Epoch: 1481 	Training Loss: 1.153975 	Validation Loss: 1.322389 	 time: 0.3
Epoch: 1482 	Training Loss: 1.153962 	Validation Loss: 1.322447 	 time: 0.3
Epoch: 1483 	Training Loss: 1.153949 	Validation Loss: 1.322475 	 time: 0.3
Epoch: 1484 	Training Loss: 1.153935 	Validation Loss: 1.322488 	 time: 0.3
Epoch: 1485 	Training Loss: 1.153921 	Validation Loss: 1.322502 	 time: 0.3
Epoch: 1486 	Training Loss: 1.153908 	Validation Loss: 1.322511 	 time: 0.3
Epoch: 1487 	Training Loss: 1.153896 	Validation Loss: 1.322509 	 time: 0.3
Epoch: 1488 	Training Loss: 1.153885 	Validation Loss: 1.322510 	 time: 0.3
Epoch: 1489 	Training Loss: 1.153874 	Validation Loss: 1.322511 	 time: 0.3
Epoch: 1490 	Training Loss: 1.153865 	Validation Loss: 1.322482 	 time: 0.3
Epoch: 1491 	Training Loss: 1.153854 	Validation Loss: 1.322414 	 time: 0.3
Epoch: 1492 	Training Loss: 1.153843 	Validation Loss: 1.322322 	 time: 0.3
Epoch: 1493 	Training Loss: 1.153829 	Validation Loss: 1.322189 	 time: 0.3
Epoch: 1494 	Training Loss: 1.153809 	Validation Loss: 1.322025 	 time: 0.3
Epoch: 1495 	Training Loss: 1.153792 	Validation Loss: 1.321881 	 time: 0.3
Epoch: 1496 	Training Loss: 1.153781 	Validation Loss: 1.321743 	 time: 0.3
Epoch: 1497 	Training Loss: 1.153770 	Validation Loss: 1.321611 	 time: 0.3
Epoch: 1498 	Training Loss: 1.153760 	Validation Loss: 1.321511 	 time: 0.3
Epoch: 1499 	Training Loss: 1.153750 	Validation Loss: 1.321457 	 time: 0.3
Epoch: 1500 	Training Loss: 1.153740 	Validation Loss: 1.321456 	 time: 0.3
Epoch: 1501 	Training Loss: 1.153730 	Validation Loss: 1.321508 	 time: 0.3
Epoch: 1502 	Training Loss: 1.153721 	Validation Loss: 1.321577 	 time: 0.3
Epoch: 1503 	Training Loss: 1.153712 	Validation Loss: 1.321630 	 time: 0.3
Epoch: 1504 	Training Loss: 1.153703 	Validation Loss: 1.321659 	 time: 0.3
Epoch: 1505 	Training Loss: 1.153694 	Validation Loss: 1.321664 	 time: 0.3
Epoch: 1506 	Training Loss: 1.153685 	Validation Loss: 1.321658 	 time: 0.3
Epoch: 1507 	Training Loss: 1.153676 	Validation Loss: 1.321663 	 time: 0.3
Epoch: 1508 	Training Loss: 1.153667 	Validation Loss: 1.321697 	 time: 0.3
Epoch: 1509 	Training Loss: 1.153658 	Validation Loss: 1.321765 	 time: 0.3
Epoch: 1510 	Training Loss: 1.153649 	Validation Loss: 1.321853 	 time: 0.3
Epoch: 1511 	Training Loss: 1.153640 	Validation Loss: 1.321934 	 time: 0.3
Epoch: 1512 	Training Loss: 1.153630 	Validation Loss: 1.322007 	 time: 0.3
Epoch: 1513 	Training Loss: 1.153620 	Validation Loss: 1.322081 	 time: 0.3
Epoch: 1514 	Training Loss: 1.153609 	Validation Loss: 1.322158 	 time: 0.3
Epoch: 1515 	Training Loss: 1.153597 	Validation Loss: 1.322236 	 time: 0.3
Epoch: 1516 	Training Loss: 1.153582 	Validation Loss: 1.322305 	 time: 0.3
Epoch: 1517 	Training Loss: 1.153567 	Validation Loss: 1.322341 	 time: 0.3
Epoch: 1518 	Training Loss: 1.153553 	Validation Loss: 1.322347 	 time: 0.3
Epoch: 1519 	Training Loss: 1.153542 	Validation Loss: 1.322347 	 time: 0.3
Epoch: 1520 	Training Loss: 1.153532 	Validation Loss: 1.322348 	 time: 0.3
Epoch: 1521 	Training Loss: 1.153523 	Validation Loss: 1.322354 	 time: 0.3
Epoch: 1522 	Training Loss: 1.153514 	Validation Loss: 1.322368 	 time: 0.3
Epoch: 1523 	Training Loss: 1.153505 	Validation Loss: 1.322374 	 time: 0.3
Epoch: 1524 	Training Loss: 1.153496 	Validation Loss: 1.322359 	 time: 0.3
Epoch: 1525 	Training Loss: 1.153486 	Validation Loss: 1.322325 	 time: 0.3
Epoch: 1526 	Training Loss: 1.153472 	Validation Loss: 1.322284 	 time: 0.3
Epoch: 1527 	Training Loss: 1.153455 	Validation Loss: 1.322273 	 time: 0.3
Epoch: 1528 	Training Loss: 1.153440 	Validation Loss: 1.322323 	 time: 0.3
Epoch: 1529 	Training Loss: 1.153430 	Validation Loss: 1.322343 	 time: 0.3
Epoch: 1530 	Training Loss: 1.153421 	Validation Loss: 1.322271 	 time: 0.3
Epoch: 1531 	Training Loss: 1.153410 	Validation Loss: 1.322162 	 time: 0.3
Epoch: 1532 	Training Loss: 1.153399 	Validation Loss: 1.322092 	 time: 0.3
Epoch: 1533 	Training Loss: 1.153390 	Validation Loss: 1.322055 	 time: 0.3
Epoch: 1534 	Training Loss: 1.153380 	Validation Loss: 1.321989 	 time: 0.3
Epoch: 1535 	Training Loss: 1.153372 	Validation Loss: 1.321914 	 time: 0.3
Epoch: 1536 	Training Loss: 1.153364 	Validation Loss: 1.321903 	 time: 0.3
Epoch: 1537 	Training Loss: 1.153356 	Validation Loss: 1.321928 	 time: 0.3
Epoch: 1538 	Training Loss: 1.153349 	Validation Loss: 1.321925 	 time: 0.3
Epoch: 1539 	Training Loss: 1.153340 	Validation Loss: 1.321894 	 time: 0.3
Epoch: 1540 	Training Loss: 1.153329 	Validation Loss: 1.321826 	 time: 0.3
Epoch: 1541 	Training Loss: 1.153314 	Validation Loss: 1.321733 	 time: 0.3
Epoch: 1542 	Training Loss: 1.153297 	Validation Loss: 1.321750 	 time: 0.3
Epoch: 1543 	Training Loss: 1.153281 	Validation Loss: 1.321925 	 time: 0.3
Epoch: 1544 	Training Loss: 1.153267 	Validation Loss: 1.322119 	 time: 0.3
Epoch: 1545 	Training Loss: 1.153251 	Validation Loss: 1.322261 	 time: 0.3
Epoch: 1546 	Training Loss: 1.153229 	Validation Loss: 1.322340 	 time: 0.3
Epoch: 1547 	Training Loss: 1.153212 	Validation Loss: 1.322270 	 time: 0.3
Epoch: 1548 	Training Loss: 1.153202 	Validation Loss: 1.322082 	 time: 0.3
Epoch: 1549 	Training Loss: 1.153194 	Validation Loss: 1.321828 	 time: 0.3
Epoch: 1550 	Training Loss: 1.153186 	Validation Loss: 1.321553 	 time: 0.3
Epoch: 1551 	Training Loss: 1.153178 	Validation Loss: 1.321301 	 time: 0.3
Epoch: 1552 	Training Loss: 1.153169 	Validation Loss: 1.321092 	 time: 0.3
Epoch: 1553 	Training Loss: 1.153157 	Validation Loss: 1.320939 	 time: 0.3
Epoch: 1554 	Training Loss: 1.153144 	Validation Loss: 1.320806 	 time: 0.3
Validation loss decreased from 1.320845 to 1.320806. Model was saved
Epoch: 1555 	Training Loss: 1.153125 	Validation Loss: 1.320616 	 time: 0.3
Validation loss decreased from 1.320806 to 1.320616. Model was saved
Epoch: 1556 	Training Loss: 1.153096 	Validation Loss: 1.320416 	 time: 0.3
Validation loss decreased from 1.320616 to 1.320416. Model was saved
Epoch: 1557 	Training Loss: 1.153068 	Validation Loss: 1.320403 	 time: 0.3
Validation loss decreased from 1.320416 to 1.320403. Model was saved
Epoch: 1558 	Training Loss: 1.153044 	Validation Loss: 1.320566 	 time: 0.3
Epoch: 1559 	Training Loss: 1.153012 	Validation Loss: 1.320860 	 time: 0.3
Epoch: 1560 	Training Loss: 1.152994 	Validation Loss: 1.321210 	 time: 0.3
Epoch: 1561 	Training Loss: 1.152973 	Validation Loss: 1.321520 	 time: 0.3
Epoch: 1562 	Training Loss: 1.152948 	Validation Loss: 1.321737 	 time: 0.3
Epoch: 1563 	Training Loss: 1.152922 	Validation Loss: 1.321838 	 time: 0.3
Epoch: 1564 	Training Loss: 1.152889 	Validation Loss: 1.321838 	 time: 0.3
Epoch: 1565 	Training Loss: 1.152843 	Validation Loss: 1.321828 	 time: 0.3
Epoch: 1566 	Training Loss: 1.152792 	Validation Loss: 1.321831 	 time: 0.3
Epoch: 1567 	Training Loss: 1.152746 	Validation Loss: 1.321746 	 time: 0.3
Epoch: 1568 	Training Loss: 1.152708 	Validation Loss: 1.321693 	 time: 0.3
Epoch: 1569 	Training Loss: 1.152689 	Validation Loss: 1.321737 	 time: 0.3
Epoch: 1570 	Training Loss: 1.152670 	Validation Loss: 1.321754 	 time: 0.3
Epoch: 1571 	Training Loss: 1.152640 	Validation Loss: 1.321658 	 time: 0.3
Epoch: 1572 	Training Loss: 1.152618 	Validation Loss: 1.321553 	 time: 0.3
Epoch: 1573 	Training Loss: 1.152609 	Validation Loss: 1.321533 	 time: 0.3
Epoch: 1574 	Training Loss: 1.152589 	Validation Loss: 1.321538 	 time: 0.3
Epoch: 1575 	Training Loss: 1.152568 	Validation Loss: 1.321493 	 time: 0.3
Epoch: 1576 	Training Loss: 1.152551 	Validation Loss: 1.321490 	 time: 0.3
Epoch: 1577 	Training Loss: 1.152526 	Validation Loss: 1.321605 	 time: 0.3
Epoch: 1578 	Training Loss: 1.152473 	Validation Loss: 1.321836 	 time: 0.3
Epoch: 1579 	Training Loss: 1.152405 	Validation Loss: 1.322146 	 time: 0.3
Epoch: 1580 	Training Loss: 1.152377 	Validation Loss: 1.322514 	 time: 0.3
Epoch: 1581 	Training Loss: 1.152353 	Validation Loss: 1.322847 	 time: 0.3
Epoch: 1582 	Training Loss: 1.152334 	Validation Loss: 1.322894 	 time: 0.3
Epoch: 1583 	Training Loss: 1.152324 	Validation Loss: 1.322843 	 time: 0.3
Epoch: 1584 	Training Loss: 1.152315 	Validation Loss: 1.322658 	 time: 0.3
Epoch: 1585 	Training Loss: 1.152302 	Validation Loss: 1.322262 	 time: 0.3
Epoch: 1586 	Training Loss: 1.152283 	Validation Loss: 1.321908 	 time: 0.3
Epoch: 1587 	Training Loss: 1.152269 	Validation Loss: 1.321713 	 time: 0.3
Epoch: 1588 	Training Loss: 1.152264 	Validation Loss: 1.321490 	 time: 0.3
Epoch: 1589 	Training Loss: 1.152255 	Validation Loss: 1.321340 	 time: 0.3
Epoch: 1590 	Training Loss: 1.152242 	Validation Loss: 1.321334 	 time: 0.3
Epoch: 1591 	Training Loss: 1.152233 	Validation Loss: 1.321286 	 time: 0.3
Epoch: 1592 	Training Loss: 1.152227 	Validation Loss: 1.321110 	 time: 0.3
Epoch: 1593 	Training Loss: 1.152218 	Validation Loss: 1.320801 	 time: 0.3
Epoch: 1594 	Training Loss: 1.152208 	Validation Loss: 1.320548 	 time: 0.3
Epoch: 1595 	Training Loss: 1.152202 	Validation Loss: 1.320555 	 time: 0.3
Epoch: 1596 	Training Loss: 1.152194 	Validation Loss: 1.320749 	 time: 0.3
Epoch: 1597 	Training Loss: 1.152185 	Validation Loss: 1.320869 	 time: 0.3
Epoch: 1598 	Training Loss: 1.152179 	Validation Loss: 1.320838 	 time: 0.3
Epoch: 1599 	Training Loss: 1.152173 	Validation Loss: 1.320759 	 time: 0.3
Epoch: 1600 	Training Loss: 1.152166 	Validation Loss: 1.320630 	 time: 0.3
Epoch: 1601 	Training Loss: 1.152160 	Validation Loss: 1.320434 	 time: 0.3
Epoch: 1602 	Training Loss: 1.152154 	Validation Loss: 1.320296 	 time: 0.3
Validation loss decreased from 1.320403 to 1.320296. Model was saved
Epoch: 1603 	Training Loss: 1.152148 	Validation Loss: 1.320261 	 time: 0.3
Validation loss decreased from 1.320296 to 1.320261. Model was saved
Epoch: 1604 	Training Loss: 1.152143 	Validation Loss: 1.320282 	 time: 0.3
Epoch: 1605 	Training Loss: 1.152136 	Validation Loss: 1.320358 	 time: 0.3
Epoch: 1606 	Training Loss: 1.152130 	Validation Loss: 1.320448 	 time: 0.3
Epoch: 1607 	Training Loss: 1.152124 	Validation Loss: 1.320474 	 time: 0.3
Epoch: 1608 	Training Loss: 1.152118 	Validation Loss: 1.320479 	 time: 0.3
Epoch: 1609 	Training Loss: 1.152115 	Validation Loss: 1.320502 	 time: 0.3
Epoch: 1610 	Training Loss: 1.152109 	Validation Loss: 1.320495 	 time: 0.3
Epoch: 1611 	Training Loss: 1.152103 	Validation Loss: 1.320464 	 time: 0.3
Epoch: 1612 	Training Loss: 1.152097 	Validation Loss: 1.320478 	 time: 0.3
Epoch: 1613 	Training Loss: 1.152092 	Validation Loss: 1.320536 	 time: 0.3
Epoch: 1614 	Training Loss: 1.152087 	Validation Loss: 1.320592 	 time: 0.3
Epoch: 1615 	Training Loss: 1.152081 	Validation Loss: 1.320605 	 time: 0.3
Epoch: 1616 	Training Loss: 1.152073 	Validation Loss: 1.320567 	 time: 0.3
Epoch: 1617 	Training Loss: 1.152063 	Validation Loss: 1.320522 	 time: 0.3
Epoch: 1618 	Training Loss: 1.152052 	Validation Loss: 1.320501 	 time: 0.3
Epoch: 1619 	Training Loss: 1.152037 	Validation Loss: 1.320472 	 time: 0.3
Epoch: 1620 	Training Loss: 1.152017 	Validation Loss: 1.320419 	 time: 0.3
Epoch: 1621 	Training Loss: 1.152003 	Validation Loss: 1.320396 	 time: 0.3
Epoch: 1622 	Training Loss: 1.151996 	Validation Loss: 1.320411 	 time: 0.3
Epoch: 1623 	Training Loss: 1.151991 	Validation Loss: 1.320407 	 time: 0.3
Epoch: 1624 	Training Loss: 1.151985 	Validation Loss: 1.320372 	 time: 0.3
Epoch: 1625 	Training Loss: 1.151979 	Validation Loss: 1.320331 	 time: 0.3
Epoch: 1626 	Training Loss: 1.151972 	Validation Loss: 1.320285 	 time: 0.3
Epoch: 1627 	Training Loss: 1.151964 	Validation Loss: 1.320242 	 time: 0.3
Validation loss decreased from 1.320261 to 1.320242. Model was saved
Epoch: 1628 	Training Loss: 1.151955 	Validation Loss: 1.320221 	 time: 0.3
Validation loss decreased from 1.320242 to 1.320221. Model was saved
Epoch: 1629 	Training Loss: 1.151944 	Validation Loss: 1.320213 	 time: 0.3
Validation loss decreased from 1.320221 to 1.320213. Model was saved
Epoch: 1630 	Training Loss: 1.151929 	Validation Loss: 1.320202 	 time: 0.3
Validation loss decreased from 1.320213 to 1.320202. Model was saved
Epoch: 1631 	Training Loss: 1.151912 	Validation Loss: 1.320207 	 time: 0.3
Epoch: 1632 	Training Loss: 1.151900 	Validation Loss: 1.320208 	 time: 0.3
Epoch: 1633 	Training Loss: 1.151891 	Validation Loss: 1.320106 	 time: 0.3
Validation loss decreased from 1.320202 to 1.320106. Model was saved
Epoch: 1634 	Training Loss: 1.151881 	Validation Loss: 1.319924 	 time: 0.3
Validation loss decreased from 1.320106 to 1.319924. Model was saved
Epoch: 1635 	Training Loss: 1.151873 	Validation Loss: 1.319824 	 time: 0.3
Validation loss decreased from 1.319924 to 1.319824. Model was saved
Epoch: 1636 	Training Loss: 1.151865 	Validation Loss: 1.319814 	 time: 0.3
Validation loss decreased from 1.319824 to 1.319814. Model was saved
Epoch: 1637 	Training Loss: 1.151858 	Validation Loss: 1.319773 	 time: 0.3
Validation loss decreased from 1.319814 to 1.319773. Model was saved
Epoch: 1638 	Training Loss: 1.151851 	Validation Loss: 1.319729 	 time: 0.3
Validation loss decreased from 1.319773 to 1.319729. Model was saved
Epoch: 1639 	Training Loss: 1.151846 	Validation Loss: 1.319740 	 time: 0.3
Epoch: 1640 	Training Loss: 1.151839 	Validation Loss: 1.319752 	 time: 0.3
Epoch: 1641 	Training Loss: 1.151834 	Validation Loss: 1.319736 	 time: 0.3
Epoch: 1642 	Training Loss: 1.151828 	Validation Loss: 1.319741 	 time: 0.3
Epoch: 1643 	Training Loss: 1.151823 	Validation Loss: 1.319806 	 time: 0.3
Epoch: 1644 	Training Loss: 1.151817 	Validation Loss: 1.319911 	 time: 0.3
Epoch: 1645 	Training Loss: 1.151812 	Validation Loss: 1.320002 	 time: 0.3
Epoch: 1646 	Training Loss: 1.151807 	Validation Loss: 1.320050 	 time: 0.3
Epoch: 1647 	Training Loss: 1.151803 	Validation Loss: 1.320070 	 time: 0.3
Epoch: 1648 	Training Loss: 1.151799 	Validation Loss: 1.320052 	 time: 0.3
Epoch: 1649 	Training Loss: 1.151794 	Validation Loss: 1.319991 	 time: 0.3
Epoch: 1650 	Training Loss: 1.151789 	Validation Loss: 1.319934 	 time: 0.3
Epoch: 1651 	Training Loss: 1.151785 	Validation Loss: 1.319914 	 time: 0.3
Epoch: 1652 	Training Loss: 1.151781 	Validation Loss: 1.319921 	 time: 0.3
Epoch: 1653 	Training Loss: 1.151777 	Validation Loss: 1.319959 	 time: 0.3
Epoch: 1654 	Training Loss: 1.151773 	Validation Loss: 1.320021 	 time: 0.3
Epoch: 1655 	Training Loss: 1.151769 	Validation Loss: 1.320067 	 time: 0.3
Epoch: 1656 	Training Loss: 1.151765 	Validation Loss: 1.320097 	 time: 0.3
Epoch: 1657 	Training Loss: 1.151761 	Validation Loss: 1.320133 	 time: 0.3
Epoch: 1658 	Training Loss: 1.151757 	Validation Loss: 1.320165 	 time: 0.3
Epoch: 1659 	Training Loss: 1.151753 	Validation Loss: 1.320196 	 time: 0.3
Epoch: 1660 	Training Loss: 1.151750 	Validation Loss: 1.320251 	 time: 0.3
Epoch: 1661 	Training Loss: 1.151746 	Validation Loss: 1.320319 	 time: 0.3
Epoch: 1662 	Training Loss: 1.151742 	Validation Loss: 1.320374 	 time: 0.3
Epoch: 1663 	Training Loss: 1.151739 	Validation Loss: 1.320404 	 time: 0.3
Epoch: 1664 	Training Loss: 1.151735 	Validation Loss: 1.320423 	 time: 0.3
Epoch: 1665 	Training Loss: 1.151731 	Validation Loss: 1.320454 	 time: 0.3
Epoch: 1666 	Training Loss: 1.151728 	Validation Loss: 1.320485 	 time: 0.3
Epoch: 1667 	Training Loss: 1.151724 	Validation Loss: 1.320511 	 time: 0.3
Epoch: 1668 	Training Loss: 1.151720 	Validation Loss: 1.320557 	 time: 0.3
Epoch: 1669 	Training Loss: 1.151716 	Validation Loss: 1.320620 	 time: 0.3
Epoch: 1670 	Training Loss: 1.151712 	Validation Loss: 1.320666 	 time: 0.3
Epoch: 1671 	Training Loss: 1.151707 	Validation Loss: 1.320705 	 time: 0.3
Epoch: 1672 	Training Loss: 1.151701 	Validation Loss: 1.320795 	 time: 0.3
Epoch: 1673 	Training Loss: 1.151691 	Validation Loss: 1.320989 	 time: 0.3
Epoch: 1674 	Training Loss: 1.151670 	Validation Loss: 1.321148 	 time: 0.3
Epoch: 1675 	Training Loss: 1.151656 	Validation Loss: 1.321047 	 time: 0.3
Epoch: 1676 	Training Loss: 1.151651 	Validation Loss: 1.320878 	 time: 0.3
Epoch: 1677 	Training Loss: 1.151647 	Validation Loss: 1.320823 	 time: 0.3
Epoch: 1678 	Training Loss: 1.151644 	Validation Loss: 1.320803 	 time: 0.3
Epoch: 1679 	Training Loss: 1.151641 	Validation Loss: 1.320815 	 time: 0.3
Epoch: 1680 	Training Loss: 1.151638 	Validation Loss: 1.321058 	 time: 0.3
Epoch: 1681 	Training Loss: 1.151634 	Validation Loss: 1.321329 	 time: 0.3
Epoch: 1682 	Training Loss: 1.151631 	Validation Loss: 1.321382 	 time: 0.3
Epoch: 1683 	Training Loss: 1.151627 	Validation Loss: 1.321382 	 time: 0.3
Epoch: 1684 	Training Loss: 1.151622 	Validation Loss: 1.321400 	 time: 0.3
Epoch: 1685 	Training Loss: 1.151619 	Validation Loss: 1.321387 	 time: 0.3
Epoch: 1686 	Training Loss: 1.151615 	Validation Loss: 1.321408 	 time: 0.3
Epoch: 1687 	Training Loss: 1.151611 	Validation Loss: 1.321456 	 time: 0.3
Epoch: 1688 	Training Loss: 1.151607 	Validation Loss: 1.321549 	 time: 0.3
Epoch: 1689 	Training Loss: 1.151603 	Validation Loss: 1.321649 	 time: 0.3
Epoch: 1690 	Training Loss: 1.151599 	Validation Loss: 1.321610 	 time: 0.3
Epoch: 1691 	Training Loss: 1.151595 	Validation Loss: 1.321521 	 time: 0.3
Epoch: 1692 	Training Loss: 1.151591 	Validation Loss: 1.321515 	 time: 0.3
Epoch: 1693 	Training Loss: 1.151587 	Validation Loss: 1.321518 	 time: 0.3
Epoch: 1694 	Training Loss: 1.151583 	Validation Loss: 1.321517 	 time: 0.3
Epoch: 1695 	Training Loss: 1.151580 	Validation Loss: 1.321606 	 time: 0.3
Epoch: 1696 	Training Loss: 1.151576 	Validation Loss: 1.321712 	 time: 0.3
Epoch: 1697 	Training Loss: 1.151573 	Validation Loss: 1.321740 	 time: 0.2
Epoch: 1698 	Training Loss: 1.151570 	Validation Loss: 1.321722 	 time: 0.3
Epoch: 1699 	Training Loss: 1.151566 	Validation Loss: 1.321686 	 time: 0.3
Epoch: 1700 	Training Loss: 1.151563 	Validation Loss: 1.321671 	 time: 0.3
Epoch: 1701 	Training Loss: 1.151560 	Validation Loss: 1.321698 	 time: 0.3
Epoch: 1702 	Training Loss: 1.151556 	Validation Loss: 1.321716 	 time: 0.3
Epoch: 1703 	Training Loss: 1.151553 	Validation Loss: 1.321752 	 time: 0.3
Epoch: 1704 	Training Loss: 1.151550 	Validation Loss: 1.321805 	 time: 0.3
Epoch: 1705 	Training Loss: 1.151547 	Validation Loss: 1.321802 	 time: 0.3
Epoch: 1706 	Training Loss: 1.151543 	Validation Loss: 1.321774 	 time: 0.3
Epoch: 1707 	Training Loss: 1.151540 	Validation Loss: 1.321771 	 time: 0.3
Epoch: 1708 	Training Loss: 1.151536 	Validation Loss: 1.321766 	 time: 0.3
Epoch: 1709 	Training Loss: 1.151533 	Validation Loss: 1.321780 	 time: 0.3
Epoch: 1710 	Training Loss: 1.151529 	Validation Loss: 1.321814 	 time: 0.3
Epoch: 1711 	Training Loss: 1.151524 	Validation Loss: 1.321812 	 time: 0.3
Epoch: 1712 	Training Loss: 1.151519 	Validation Loss: 1.321775 	 time: 0.3
Epoch: 1713 	Training Loss: 1.151510 	Validation Loss: 1.321711 	 time: 0.3
Epoch: 1714 	Training Loss: 1.151496 	Validation Loss: 1.321635 	 time: 0.3
Epoch: 1715 	Training Loss: 1.151485 	Validation Loss: 1.321640 	 time: 0.3
Epoch: 1716 	Training Loss: 1.151481 	Validation Loss: 1.321717 	 time: 0.3
Epoch: 1717 	Training Loss: 1.151477 	Validation Loss: 1.321772 	 time: 0.3
Epoch: 1718 	Training Loss: 1.151474 	Validation Loss: 1.321888 	 time: 0.3
Epoch: 1719 	Training Loss: 1.151471 	Validation Loss: 1.322030 	 time: 0.3
Epoch: 1720 	Training Loss: 1.151467 	Validation Loss: 1.322055 	 time: 0.3
Epoch: 1721 	Training Loss: 1.151464 	Validation Loss: 1.322077 	 time: 0.3
Epoch: 1722 	Training Loss: 1.151460 	Validation Loss: 1.322138 	 time: 0.3
Epoch: 1723 	Training Loss: 1.151457 	Validation Loss: 1.322127 	 time: 0.3
Epoch: 1724 	Training Loss: 1.151453 	Validation Loss: 1.322121 	 time: 0.3
Epoch: 1725 	Training Loss: 1.151450 	Validation Loss: 1.322154 	 time: 0.3
Epoch: 1726 	Training Loss: 1.151446 	Validation Loss: 1.322148 	 time: 0.3
Epoch: 1727 	Training Loss: 1.151443 	Validation Loss: 1.322133 	 time: 0.3
Epoch: 1728 	Training Loss: 1.151440 	Validation Loss: 1.322126 	 time: 0.3
Epoch: 1729 	Training Loss: 1.151436 	Validation Loss: 1.322135 	 time: 0.3
Epoch: 1730 	Training Loss: 1.151433 	Validation Loss: 1.322185 	 time: 0.3
Epoch: 1731 	Training Loss: 1.151430 	Validation Loss: 1.322226 	 time: 0.3
Epoch: 1732 	Training Loss: 1.151427 	Validation Loss: 1.322258 	 time: 0.3
Epoch: 1733 	Training Loss: 1.151423 	Validation Loss: 1.322315 	 time: 0.3
Epoch: 1734 	Training Loss: 1.151420 	Validation Loss: 1.322331 	 time: 0.3
Epoch: 1735 	Training Loss: 1.151417 	Validation Loss: 1.322307 	 time: 0.3
Epoch: 1736 	Training Loss: 1.151414 	Validation Loss: 1.322321 	 time: 0.3
Epoch: 1737 	Training Loss: 1.151411 	Validation Loss: 1.322328 	 time: 0.3
Epoch: 1738 	Training Loss: 1.151408 	Validation Loss: 1.322309 	 time: 0.3
Epoch: 1739 	Training Loss: 1.151405 	Validation Loss: 1.322318 	 time: 0.3
Epoch: 1740 	Training Loss: 1.151402 	Validation Loss: 1.322334 	 time: 0.3
Epoch: 1741 	Training Loss: 1.151399 	Validation Loss: 1.322341 	 time: 0.3
Epoch: 1742 	Training Loss: 1.151396 	Validation Loss: 1.322351 	 time: 0.3
Epoch: 1743 	Training Loss: 1.151394 	Validation Loss: 1.322355 	 time: 0.3
Epoch: 1744 	Training Loss: 1.151391 	Validation Loss: 1.322378 	 time: 0.3
Epoch: 1745 	Training Loss: 1.151388 	Validation Loss: 1.322405 	 time: 0.3
Epoch: 1746 	Training Loss: 1.151385 	Validation Loss: 1.322412 	 time: 0.3
Epoch: 1747 	Training Loss: 1.151382 	Validation Loss: 1.322435 	 time: 0.3
Epoch: 1748 	Training Loss: 1.151379 	Validation Loss: 1.322468 	 time: 0.3
Epoch: 1749 	Training Loss: 1.151376 	Validation Loss: 1.322481 	 time: 0.3
Epoch: 1750 	Training Loss: 1.151373 	Validation Loss: 1.322507 	 time: 0.3
Epoch: 1751 	Training Loss: 1.151370 	Validation Loss: 1.322554 	 time: 0.3
Epoch: 1752 	Training Loss: 1.151367 	Validation Loss: 1.322594 	 time: 0.3
Epoch: 1753 	Training Loss: 1.151363 	Validation Loss: 1.322640 	 time: 0.3
Epoch: 1754 	Training Loss: 1.151359 	Validation Loss: 1.322698 	 time: 0.3
Epoch: 1755 	Training Loss: 1.151353 	Validation Loss: 1.322758 	 time: 0.3
Epoch: 1756 	Training Loss: 1.151345 	Validation Loss: 1.322820 	 time: 0.3
Epoch: 1757 	Training Loss: 1.151334 	Validation Loss: 1.322856 	 time: 0.3
Epoch: 1758 	Training Loss: 1.151317 	Validation Loss: 1.322843 	 time: 0.3
Epoch: 1759 	Training Loss: 1.151289 	Validation Loss: 1.322765 	 time: 0.3
Epoch: 1760 	Training Loss: 1.151250 	Validation Loss: 1.322615 	 time: 0.3
Epoch: 1761 	Training Loss: 1.151220 	Validation Loss: 1.322433 	 time: 0.3
Epoch: 1762 	Training Loss: 1.151204 	Validation Loss: 1.322280 	 time: 0.3
Epoch: 1763 	Training Loss: 1.151195 	Validation Loss: 1.322204 	 time: 0.3
Epoch: 1764 	Training Loss: 1.151189 	Validation Loss: 1.322178 	 time: 0.3
Epoch: 1765 	Training Loss: 1.151183 	Validation Loss: 1.322196 	 time: 0.3
Epoch: 1766 	Training Loss: 1.151177 	Validation Loss: 1.322226 	 time: 0.3
Epoch: 1767 	Training Loss: 1.151171 	Validation Loss: 1.322191 	 time: 0.3
Epoch: 1768 	Training Loss: 1.151164 	Validation Loss: 1.322132 	 time: 0.3
Epoch: 1769 	Training Loss: 1.151156 	Validation Loss: 1.322086 	 time: 0.3
Epoch: 1770 	Training Loss: 1.151140 	Validation Loss: 1.322022 	 time: 0.3
Epoch: 1771 	Training Loss: 1.151108 	Validation Loss: 1.321978 	 time: 0.3
Epoch: 1772 	Training Loss: 1.151108 	Validation Loss: 1.321974 	 time: 0.3
Epoch: 1773 	Training Loss: 1.151112 	Validation Loss: 1.321911 	 time: 0.3
Epoch: 1774 	Training Loss: 1.151100 	Validation Loss: 1.321751 	 time: 0.3
Epoch: 1775 	Training Loss: 1.151082 	Validation Loss: 1.321663 	 time: 0.3
Epoch: 1776 	Training Loss: 1.151073 	Validation Loss: 1.321610 	 time: 0.3
Epoch: 1777 	Training Loss: 1.151074 	Validation Loss: 1.321526 	 time: 0.3
Epoch: 1778 	Training Loss: 1.151068 	Validation Loss: 1.321543 	 time: 0.3
Epoch: 1779 	Training Loss: 1.151056 	Validation Loss: 1.321689 	 time: 0.3
Epoch: 1780 	Training Loss: 1.151047 	Validation Loss: 1.321816 	 time: 0.3
Epoch: 1781 	Training Loss: 1.151042 	Validation Loss: 1.321906 	 time: 0.3
Epoch: 1782 	Training Loss: 1.151037 	Validation Loss: 1.321937 	 time: 0.3
Epoch: 1783 	Training Loss: 1.151027 	Validation Loss: 1.321909 	 time: 0.3
Epoch: 1784 	Training Loss: 1.151016 	Validation Loss: 1.321884 	 time: 0.3
Epoch: 1785 	Training Loss: 1.151004 	Validation Loss: 1.321816 	 time: 0.3
Epoch: 1786 	Training Loss: 1.150991 	Validation Loss: 1.321808 	 time: 0.3
Epoch: 1787 	Training Loss: 1.150971 	Validation Loss: 1.321975 	 time: 0.3
Epoch: 1788 	Training Loss: 1.150913 	Validation Loss: 1.322283 	 time: 0.3
Epoch: 1789 	Training Loss: 1.150861 	Validation Loss: 1.321945 	 time: 0.3
Epoch: 1790 	Training Loss: 1.150852 	Validation Loss: 1.321509 	 time: 0.3
Epoch: 1791 	Training Loss: 1.150848 	Validation Loss: 1.321151 	 time: 0.3
Epoch: 1792 	Training Loss: 1.150841 	Validation Loss: 1.320542 	 time: 0.3
Epoch: 1793 	Training Loss: 1.150866 	Validation Loss: 1.320597 	 time: 0.3
Epoch: 1794 	Training Loss: 1.150871 	Validation Loss: 1.320431 	 time: 0.3
Epoch: 1795 	Training Loss: 1.150860 	Validation Loss: 1.320897 	 time: 0.3
Epoch: 1796 	Training Loss: 1.150811 	Validation Loss: 1.320459 	 time: 0.3
Epoch: 1797 	Training Loss: 1.150775 	Validation Loss: 1.320644 	 time: 0.3
Epoch: 1798 	Training Loss: 1.150769 	Validation Loss: 1.321738 	 time: 0.3
Epoch: 1799 	Training Loss: 1.150787 	Validation Loss: 1.321654 	 time: 0.3
Epoch: 1800 	Training Loss: 1.150820 	Validation Loss: 1.321910 	 time: 0.3
Epoch: 1801 	Training Loss: 1.150810 	Validation Loss: 1.321841 	 time: 0.3
Epoch: 1802 	Training Loss: 1.150750 	Validation Loss: 1.322582 	 time: 0.3
Epoch: 1803 	Training Loss: 1.150797 	Validation Loss: 1.321994 	 time: 0.3
Epoch: 1804 	Training Loss: 1.150833 	Validation Loss: 1.321339 	 time: 0.3
Epoch: 1805 	Training Loss: 1.150840 	Validation Loss: 1.322989 	 time: 0.3
Epoch: 1806 	Training Loss: 1.150803 	Validation Loss: 1.322905 	 time: 0.3
Epoch: 1807 	Training Loss: 1.150796 	Validation Loss: 1.322561 	 time: 0.3
Epoch: 1808 	Training Loss: 1.150778 	Validation Loss: 1.323345 	 time: 0.3
Epoch: 1809 	Training Loss: 1.150751 	Validation Loss: 1.322280 	 time: 0.3
Epoch: 1810 	Training Loss: 1.150734 	Validation Loss: 1.322224 	 time: 0.3
Epoch: 1811 	Training Loss: 1.150692 	Validation Loss: 1.323361 	 time: 0.3
Epoch: 1812 	Training Loss: 1.150753 	Validation Loss: 1.322542 	 time: 0.3
Epoch: 1813 	Training Loss: 1.150731 	Validation Loss: 1.323326 	 time: 0.3
Epoch: 1814 	Training Loss: 1.150802 	Validation Loss: 1.322673 	 time: 0.3
Epoch: 1815 	Training Loss: 1.150672 	Validation Loss: 1.323194 	 time: 0.3
Epoch: 1816 	Training Loss: 1.150655 	Validation Loss: 1.323336 	 time: 0.3
Epoch: 1817 	Training Loss: 1.150722 	Validation Loss: 1.322824 	 time: 0.3
Epoch: 1818 	Training Loss: 1.150744 	Validation Loss: 1.323399 	 time: 0.3
Epoch: 1819 	Training Loss: 1.150706 	Validation Loss: 1.323032 	 time: 0.3
Epoch: 1820 	Training Loss: 1.150717 	Validation Loss: 1.323513 	 time: 0.3
Epoch: 1821 	Training Loss: 1.150709 	Validation Loss: 1.324249 	 time: 0.3
Epoch: 1822 	Training Loss: 1.150687 	Validation Loss: 1.322514 	 time: 0.3
Epoch: 1823 	Training Loss: 1.150668 	Validation Loss: 1.321917 	 time: 0.3
Epoch: 1824 	Training Loss: 1.150672 	Validation Loss: 1.323846 	 time: 0.3
Epoch: 1825 	Training Loss: 1.150660 	Validation Loss: 1.324378 	 time: 0.3
Epoch: 1826 	Training Loss: 1.150623 	Validation Loss: 1.323999 	 time: 0.3
Epoch: 1827 	Training Loss: 1.150632 	Validation Loss: 1.323999 	 time: 0.3
Epoch: 1828 	Training Loss: 1.150608 	Validation Loss: 1.323533 	 time: 0.3
Epoch: 1829 	Training Loss: 1.150587 	Validation Loss: 1.324046 	 time: 0.3
Epoch: 1830 	Training Loss: 1.150573 	Validation Loss: 1.324248 	 time: 0.3
Epoch: 1831 	Training Loss: 1.150558 	Validation Loss: 1.322735 	 time: 0.3
Epoch: 1832 	Training Loss: 1.150545 	Validation Loss: 1.322891 	 time: 0.3
Epoch: 1833 	Training Loss: 1.150498 	Validation Loss: 1.324111 	 time: 0.3
Epoch: 1834 	Training Loss: 1.150443 	Validation Loss: 1.324418 	 time: 0.3
Epoch: 1835 	Training Loss: 1.150397 	Validation Loss: 1.323739 	 time: 0.3
Epoch: 1836 	Training Loss: 1.150388 	Validation Loss: 1.323433 	 time: 0.3
Epoch: 1837 	Training Loss: 1.150377 	Validation Loss: 1.323077 	 time: 0.3
Epoch: 1838 	Training Loss: 1.150370 	Validation Loss: 1.323519 	 time: 0.3
Epoch: 1839 	Training Loss: 1.150348 	Validation Loss: 1.323292 	 time: 0.3
Epoch: 1840 	Training Loss: 1.150347 	Validation Loss: 1.323149 	 time: 0.3
Epoch: 1841 	Training Loss: 1.150331 	Validation Loss: 1.323733 	 time: 0.3
Epoch: 1842 	Training Loss: 1.150326 	Validation Loss: 1.324197 	 time: 0.3
Epoch: 1843 	Training Loss: 1.150314 	Validation Loss: 1.323032 	 time: 0.3
Epoch: 1844 	Training Loss: 1.150303 	Validation Loss: 1.323162 	 time: 0.3
Epoch: 1845 	Training Loss: 1.150281 	Validation Loss: 1.322729 	 time: 0.3
Epoch: 1846 	Training Loss: 1.150272 	Validation Loss: 1.322017 	 time: 0.3
Epoch: 1847 	Training Loss: 1.150264 	Validation Loss: 1.322831 	 time: 0.3
Epoch: 1848 	Training Loss: 1.150259 	Validation Loss: 1.322345 	 time: 0.3
Epoch: 1849 	Training Loss: 1.150247 	Validation Loss: 1.322514 	 time: 0.3
Epoch: 1850 	Training Loss: 1.150243 	Validation Loss: 1.322729 	 time: 0.3
Epoch: 1851 	Training Loss: 1.150233 	Validation Loss: 1.322277 	 time: 0.3
Epoch: 1852 	Training Loss: 1.150224 	Validation Loss: 1.322201 	 time: 0.3
Epoch: 1853 	Training Loss: 1.150212 	Validation Loss: 1.322150 	 time: 0.3
Epoch: 1854 	Training Loss: 1.150204 	Validation Loss: 1.321750 	 time: 0.3
Epoch: 1855 	Training Loss: 1.150185 	Validation Loss: 1.321778 	 time: 0.3
Epoch: 1856 	Training Loss: 1.150124 	Validation Loss: 1.321807 	 time: 0.3
Epoch: 1857 	Training Loss: 1.150088 	Validation Loss: 1.321596 	 time: 0.3
Epoch: 1858 	Training Loss: 1.150093 	Validation Loss: 1.321631 	 time: 0.3
Epoch: 1859 	Training Loss: 1.150084 	Validation Loss: 1.322494 	 time: 0.3
Epoch: 1860 	Training Loss: 1.150066 	Validation Loss: 1.322654 	 time: 0.3
Epoch: 1861 	Training Loss: 1.150065 	Validation Loss: 1.322901 	 time: 0.3
Epoch: 1862 	Training Loss: 1.150079 	Validation Loss: 1.322051 	 time: 0.3
Epoch: 1863 	Training Loss: 1.150072 	Validation Loss: 1.322105 	 time: 0.3
Epoch: 1864 	Training Loss: 1.150048 	Validation Loss: 1.320770 	 time: 0.3
Epoch: 1865 	Training Loss: 1.150027 	Validation Loss: 1.320967 	 time: 0.3
Epoch: 1866 	Training Loss: 1.150011 	Validation Loss: 1.321451 	 time: 0.3
Epoch: 1867 	Training Loss: 1.150005 	Validation Loss: 1.321064 	 time: 0.3
Epoch: 1868 	Training Loss: 1.150015 	Validation Loss: 1.321626 	 time: 0.3
Epoch: 1869 	Training Loss: 1.150025 	Validation Loss: 1.321276 	 time: 0.3
Epoch: 1870 	Training Loss: 1.150029 	Validation Loss: 1.321745 	 time: 0.3
Epoch: 1871 	Training Loss: 1.150030 	Validation Loss: 1.321096 	 time: 0.3
Epoch: 1872 	Training Loss: 1.150012 	Validation Loss: 1.321372 	 time: 0.3
Epoch: 1873 	Training Loss: 1.149981 	Validation Loss: 1.321241 	 time: 0.3
Epoch: 1874 	Training Loss: 1.149971 	Validation Loss: 1.321391 	 time: 0.3
Epoch: 1875 	Training Loss: 1.149986 	Validation Loss: 1.321730 	 time: 0.3
Epoch: 1876 	Training Loss: 1.150022 	Validation Loss: 1.321157 	 time: 0.3
Epoch: 1877 	Training Loss: 1.150044 	Validation Loss: 1.322317 	 time: 0.3
Epoch: 1878 	Training Loss: 1.150006 	Validation Loss: 1.321107 	 time: 0.3
Epoch: 1879 	Training Loss: 1.149966 	Validation Loss: 1.321116 	 time: 0.3
Epoch: 1880 	Training Loss: 1.150028 	Validation Loss: 1.322103 	 time: 0.3
Epoch: 1881 	Training Loss: 1.150182 	Validation Loss: 1.320374 	 time: 0.3
Epoch: 1882 	Training Loss: 1.150203 	Validation Loss: 1.321313 	 time: 0.3
Epoch: 1883 	Training Loss: 1.150090 	Validation Loss: 1.322284 	 time: 0.3
Epoch: 1884 	Training Loss: 1.150099 	Validation Loss: 1.321501 	 time: 0.3
Epoch: 1885 	Training Loss: 1.150187 	Validation Loss: 1.323697 	 time: 0.3
Epoch: 1886 	Training Loss: 1.150130 	Validation Loss: 1.320727 	 time: 0.3
Epoch: 1887 	Training Loss: 1.150028 	Validation Loss: 1.319396 	 time: 0.3
Validation loss decreased from 1.319729 to 1.319396. Model was saved
Epoch: 1888 	Training Loss: 1.150113 	Validation Loss: 1.320892 	 time: 0.3
Epoch: 1889 	Training Loss: 1.150042 	Validation Loss: 1.323342 	 time: 0.3
Epoch: 1890 	Training Loss: 1.150025 	Validation Loss: 1.323462 	 time: 0.3
Epoch: 1891 	Training Loss: 1.150046 	Validation Loss: 1.322040 	 time: 0.3
Epoch: 1892 	Training Loss: 1.150003 	Validation Loss: 1.320664 	 time: 0.3
Epoch: 1893 	Training Loss: 1.149999 	Validation Loss: 1.321568 	 time: 0.3
Epoch: 1894 	Training Loss: 1.149990 	Validation Loss: 1.322208 	 time: 0.3
Epoch: 1895 	Training Loss: 1.150003 	Validation Loss: 1.319536 	 time: 0.3
Epoch: 1896 	Training Loss: 1.149981 	Validation Loss: 1.319510 	 time: 0.3
Epoch: 1897 	Training Loss: 1.149978 	Validation Loss: 1.321187 	 time: 0.3
Epoch: 1898 	Training Loss: 1.149968 	Validation Loss: 1.321898 	 time: 0.3
Epoch: 1899 	Training Loss: 1.149978 	Validation Loss: 1.319844 	 time: 0.3
Epoch: 1900 	Training Loss: 1.149948 	Validation Loss: 1.319662 	 time: 0.3
Epoch: 1901 	Training Loss: 1.149951 	Validation Loss: 1.320925 	 time: 0.3
Epoch: 1902 	Training Loss: 1.149939 	Validation Loss: 1.321177 	 time: 0.3
Epoch: 1903 	Training Loss: 1.149949 	Validation Loss: 1.320770 	 time: 0.3
Epoch: 1904 	Training Loss: 1.149933 	Validation Loss: 1.320651 	 time: 0.3
Epoch: 1905 	Training Loss: 1.149923 	Validation Loss: 1.321267 	 time: 0.3
Epoch: 1906 	Training Loss: 1.149921 	Validation Loss: 1.321883 	 time: 0.3
Epoch: 1907 	Training Loss: 1.149911 	Validation Loss: 1.320801 	 time: 0.3
Epoch: 1908 	Training Loss: 1.149909 	Validation Loss: 1.320765 	 time: 0.3
Epoch: 1909 	Training Loss: 1.149901 	Validation Loss: 1.322564 	 time: 0.3
Epoch: 1910 	Training Loss: 1.149904 	Validation Loss: 1.321777 	 time: 0.3
Epoch: 1911 	Training Loss: 1.149887 	Validation Loss: 1.321475 	 time: 0.3
Epoch: 1912 	Training Loss: 1.149884 	Validation Loss: 1.321958 	 time: 0.3
Epoch: 1913 	Training Loss: 1.149866 	Validation Loss: 1.322197 	 time: 0.3
Epoch: 1914 	Training Loss: 1.149850 	Validation Loss: 1.322065 	 time: 0.3
Epoch: 1915 	Training Loss: 1.149837 	Validation Loss: 1.322147 	 time: 0.3
Epoch: 1916 	Training Loss: 1.149834 	Validation Loss: 1.322417 	 time: 0.3
Epoch: 1917 	Training Loss: 1.149833 	Validation Loss: 1.322472 	 time: 0.3
Epoch: 1918 	Training Loss: 1.149826 	Validation Loss: 1.322322 	 time: 0.3
Epoch: 1919 	Training Loss: 1.149822 	Validation Loss: 1.321637 	 time: 0.3
Epoch: 1920 	Training Loss: 1.149814 	Validation Loss: 1.322485 	 time: 0.3
Epoch: 1921 	Training Loss: 1.149807 	Validation Loss: 1.322685 	 time: 0.3
Epoch: 1922 	Training Loss: 1.149797 	Validation Loss: 1.322455 	 time: 0.2
Epoch: 1923 	Training Loss: 1.149789 	Validation Loss: 1.322308 	 time: 0.3
Epoch: 1924 	Training Loss: 1.149785 	Validation Loss: 1.323022 	 time: 0.3
Epoch: 1925 	Training Loss: 1.149785 	Validation Loss: 1.322585 	 time: 0.3
Epoch: 1926 	Training Loss: 1.149781 	Validation Loss: 1.323496 	 time: 0.3
Epoch: 1927 	Training Loss: 1.149778 	Validation Loss: 1.323306 	 time: 0.3
Epoch: 1928 	Training Loss: 1.149775 	Validation Loss: 1.323932 	 time: 0.3
Epoch: 1929 	Training Loss: 1.149786 	Validation Loss: 1.323871 	 time: 0.3
Epoch: 1930 	Training Loss: 1.149785 	Validation Loss: 1.324049 	 time: 0.3
Epoch: 1931 	Training Loss: 1.149785 	Validation Loss: 1.323930 	 time: 0.3
Epoch: 1932 	Training Loss: 1.149789 	Validation Loss: 1.324244 	 time: 0.3
Epoch: 1933 	Training Loss: 1.149805 	Validation Loss: 1.324064 	 time: 0.3
Epoch: 1934 	Training Loss: 1.149785 	Validation Loss: 1.324224 	 time: 0.3
Epoch: 1935 	Training Loss: 1.149741 	Validation Loss: 1.324055 	 time: 0.3
Epoch: 1936 	Training Loss: 1.149737 	Validation Loss: 1.324355 	 time: 0.3
Epoch: 1937 	Training Loss: 1.149760 	Validation Loss: 1.324439 	 time: 0.3
Epoch: 1938 	Training Loss: 1.149842 	Validation Loss: 1.323830 	 time: 0.3
Epoch: 1939 	Training Loss: 1.149822 	Validation Loss: 1.324789 	 time: 0.3
Epoch: 1940 	Training Loss: 1.149756 	Validation Loss: 1.323832 	 time: 0.3
Epoch: 1941 	Training Loss: 1.149789 	Validation Loss: 1.323927 	 time: 0.3
Epoch: 1942 	Training Loss: 1.149912 	Validation Loss: 1.326266 	 time: 0.3
Epoch: 1943 	Training Loss: 1.150158 	Validation Loss: 1.322661 	 time: 0.3
Epoch: 1944 	Training Loss: 1.150116 	Validation Loss: 1.322597 	 time: 0.3
Epoch: 1945 	Training Loss: 1.149944 	Validation Loss: 1.323640 	 time: 0.3
Epoch: 1946 	Training Loss: 1.150005 	Validation Loss: 1.324144 	 time: 0.3
Epoch: 1947 	Training Loss: 1.149973 	Validation Loss: 1.326407 	 time: 0.3
Epoch: 1948 	Training Loss: 1.149913 	Validation Loss: 1.326363 	 time: 0.3
Epoch: 1949 	Training Loss: 1.150022 	Validation Loss: 1.321651 	 time: 0.3
Epoch: 1950 	Training Loss: 1.150313 	Validation Loss: 1.325268 	 time: 0.3
Epoch: 1951 	Training Loss: 1.150360 	Validation Loss: 1.324554 	 time: 0.3
Epoch: 1952 	Training Loss: 1.150176 	Validation Loss: 1.322530 	 time: 0.3
Epoch: 1953 	Training Loss: 1.150105 	Validation Loss: 1.323476 	 time: 0.3
Epoch: 1954 	Training Loss: 1.150227 	Validation Loss: 1.321780 	 time: 0.3
Epoch: 1955 	Training Loss: 1.150335 	Validation Loss: 1.326719 	 time: 0.3
Epoch: 1956 	Training Loss: 1.150040 	Validation Loss: 1.328459 	 time: 0.3
Epoch: 1957 	Training Loss: 1.149979 	Validation Loss: 1.325826 	 time: 0.3
Epoch: 1958 	Training Loss: 1.150086 	Validation Loss: 1.329406 	 time: 0.2
Epoch: 1959 	Training Loss: 1.150018 	Validation Loss: 1.329238 	 time: 0.3
Epoch: 1960 	Training Loss: 1.149999 	Validation Loss: 1.326648 	 time: 0.3
Epoch: 1961 	Training Loss: 1.149958 	Validation Loss: 1.324008 	 time: 0.3
Epoch: 1962 	Training Loss: 1.149949 	Validation Loss: 1.324092 	 time: 0.3
Epoch: 1963 	Training Loss: 1.149869 	Validation Loss: 1.326101 	 time: 0.2
Epoch: 1964 	Training Loss: 1.149939 	Validation Loss: 1.323966 	 time: 0.3
Epoch: 1965 	Training Loss: 1.149831 	Validation Loss: 1.323221 	 time: 0.3
Epoch: 1966 	Training Loss: 1.149869 	Validation Loss: 1.324489 	 time: 0.3
Epoch: 1967 	Training Loss: 1.149800 	Validation Loss: 1.325428 	 time: 0.3
Epoch: 1968 	Training Loss: 1.149814 	Validation Loss: 1.325562 	 time: 0.2
Epoch: 1969 	Training Loss: 1.149744 	Validation Loss: 1.327634 	 time: 0.3
Epoch: 1970 	Training Loss: 1.149786 	Validation Loss: 1.327828 	 time: 0.3
Epoch: 1971 	Training Loss: 1.149771 	Validation Loss: 1.324786 	 time: 0.3
Epoch: 1972 	Training Loss: 1.149792 	Validation Loss: 1.323207 	 time: 0.3
Epoch: 1973 	Training Loss: 1.149803 	Validation Loss: 1.323303 	 time: 0.3
Epoch: 1974 	Training Loss: 1.149773 	Validation Loss: 1.323560 	 time: 0.3
Epoch: 1975 	Training Loss: 1.149754 	Validation Loss: 1.323915 	 time: 0.3
Epoch: 1976 	Training Loss: 1.149757 	Validation Loss: 1.324181 	 time: 0.3
Epoch: 1977 	Training Loss: 1.149717 	Validation Loss: 1.324854 	 time: 0.3
Epoch: 1978 	Training Loss: 1.149713 	Validation Loss: 1.326053 	 time: 0.3
Epoch: 1979 	Training Loss: 1.149686 	Validation Loss: 1.326297 	 time: 0.3
Epoch: 1980 	Training Loss: 1.149699 	Validation Loss: 1.326200 	 time: 0.3
Epoch: 1981 	Training Loss: 1.149702 	Validation Loss: 1.326654 	 time: 0.3
Epoch: 1982 	Training Loss: 1.149660 	Validation Loss: 1.327369 	 time: 0.3
Epoch: 1983 	Training Loss: 1.149676 	Validation Loss: 1.327429 	 time: 0.3
Epoch: 1984 	Training Loss: 1.149721 	Validation Loss: 1.326773 	 time: 0.3
Epoch: 1985 	Training Loss: 1.149719 	Validation Loss: 1.326261 	 time: 0.3
Epoch: 1986 	Training Loss: 1.149649 	Validation Loss: 1.326434 	 time: 0.3
Epoch: 1987 	Training Loss: 1.149667 	Validation Loss: 1.326648 	 time: 0.3
Epoch: 1988 	Training Loss: 1.149663 	Validation Loss: 1.325787 	 time: 0.3
Epoch: 1989 	Training Loss: 1.149643 	Validation Loss: 1.326242 	 time: 0.3
Epoch: 1990 	Training Loss: 1.149581 	Validation Loss: 1.327979 	 time: 0.3
Epoch: 1991 	Training Loss: 1.149590 	Validation Loss: 1.326958 	 time: 0.3
Epoch: 1992 	Training Loss: 1.149615 	Validation Loss: 1.327508 	 time: 0.3
Epoch: 1993 	Training Loss: 1.149535 	Validation Loss: 1.327265 	 time: 0.3
Epoch: 1994 	Training Loss: 1.149530 	Validation Loss: 1.327442 	 time: 0.3
Epoch: 1995 	Training Loss: 1.149539 	Validation Loss: 1.328839 	 time: 0.3
Epoch: 1996 	Training Loss: 1.149542 	Validation Loss: 1.327648 	 time: 0.3
Epoch: 1997 	Training Loss: 1.149593 	Validation Loss: 1.328100 	 time: 0.3
Epoch: 1998 	Training Loss: 1.149685 	Validation Loss: 1.328285 	 time: 0.3
Epoch: 1999 	Training Loss: 1.149548 	Validation Loss: 1.328398 	 time: 0.3
Epoch: 2000 	Training Loss: 1.149586 	Validation Loss: 1.327809 	 time: 0.3
Epoch: 2001 	Training Loss: 1.149635 	Validation Loss: 1.327974 	 time: 0.3
Epoch: 2002 	Training Loss: 1.149591 	Validation Loss: 1.329500 	 time: 0.3
Epoch: 2003 	Training Loss: 1.149537 	Validation Loss: 1.328418 	 time: 0.3
Epoch: 2004 	Training Loss: 1.149575 	Validation Loss: 1.327390 	 time: 0.3
Epoch: 2005 	Training Loss: 1.149809 	Validation Loss: 1.331252 	 time: 0.3
Epoch: 2006 	Training Loss: 1.150077 	Validation Loss: 1.328213 	 time: 0.3
Epoch: 2007 	Training Loss: 1.149734 	Validation Loss: 1.324466 	 time: 0.3
Epoch: 2008 	Training Loss: 1.149724 	Validation Loss: 1.324802 	 time: 0.3
Epoch: 2009 	Training Loss: 1.149935 	Validation Loss: 1.326318 	 time: 0.3
Epoch: 2010 	Training Loss: 1.150795 	Validation Loss: 1.331142 	 time: 0.3
Epoch: 2011 	Training Loss: 1.152304 	Validation Loss: 1.335725 	 time: 0.3
Epoch: 2012 	Training Loss: 1.155770 	Validation Loss: 1.339550 	 time: 0.3
Epoch: 2013 	Training Loss: 1.168594 	Validation Loss: 1.413531 	 time: 0.3
Epoch: 2014 	Training Loss: 1.252148 	Validation Loss: 1.423497 	 time: 0.3
Epoch: 2015 	Training Loss: 1.275021 	Validation Loss: 1.393939 	 time: 0.3
Epoch: 2016 	Training Loss: 1.258864 	Validation Loss: 1.385958 	 time: 0.3
Epoch: 2017 	Training Loss: 1.223313 	Validation Loss: 1.446280 	 time: 0.3
Epoch: 2018 	Training Loss: 1.280788 	Validation Loss: 1.410192 	 time: 0.3
Epoch: 2019 	Training Loss: 1.237894 	Validation Loss: 1.356375 	 time: 0.3
Epoch: 2020 	Training Loss: 1.208542 	Validation Loss: 1.407593 	 time: 0.3
Epoch: 2021 	Training Loss: 1.244744 	Validation Loss: 1.384624 	 time: 0.3
Epoch: 2022 	Training Loss: 1.215368 	Validation Loss: 1.385420 	 time: 0.3
Epoch: 2023 	Training Loss: 1.206184 	Validation Loss: 1.374773 	 time: 0.2
Epoch: 2024 	Training Loss: 1.206649 	Validation Loss: 1.364124 	 time: 0.3
Epoch: 2025 	Training Loss: 1.203070 	Validation Loss: 1.356603 	 time: 0.3
Epoch: 2026 	Training Loss: 1.199297 	Validation Loss: 1.368840 	 time: 0.3
Epoch: 2027 	Training Loss: 1.189981 	Validation Loss: 1.371382 	 time: 0.3
Epoch: 2028 	Training Loss: 1.187359 	Validation Loss: 1.360826 	 time: 0.3
Epoch: 2029 	Training Loss: 1.190696 	Validation Loss: 1.344403 	 time: 0.3
Epoch: 2030 	Training Loss: 1.178078 	Validation Loss: 1.331089 	 time: 0.3
Epoch: 2031 	Training Loss: 1.179108 	Validation Loss: 1.332908 	 time: 0.3
Epoch: 2032 	Training Loss: 1.175928 	Validation Loss: 1.341997 	 time: 0.3
Epoch: 2033 	Training Loss: 1.175097 	Validation Loss: 1.344718 	 time: 0.3
Epoch: 2034 	Training Loss: 1.174123 	Validation Loss: 1.339652 	 time: 0.3
Epoch: 2035 	Training Loss: 1.170138 	Validation Loss: 1.335820 	 time: 0.3
Epoch: 2036 	Training Loss: 1.167339 	Validation Loss: 1.331839 	 time: 0.3
Epoch: 2037 	Training Loss: 1.166102 	Validation Loss: 1.330358 	 time: 0.3
Epoch: 2038 	Training Loss: 1.163354 	Validation Loss: 1.331350 	 time: 0.3
Epoch: 2039 	Training Loss: 1.161963 	Validation Loss: 1.330368 	 time: 0.3
Epoch: 2040 	Training Loss: 1.161154 	Validation Loss: 1.325859 	 time: 0.3
Epoch: 2041 	Training Loss: 1.159711 	Validation Loss: 1.323877 	 time: 0.3
Epoch: 2042 	Training Loss: 1.159217 	Validation Loss: 1.321008 	 time: 0.3
Epoch: 2043 	Training Loss: 1.158606 	Validation Loss: 1.319750 	 time: 0.3
Epoch: 2044 	Training Loss: 1.157728 	Validation Loss: 1.320034 	 time: 0.3
Epoch: 2045 	Training Loss: 1.156847 	Validation Loss: 1.313887 	 time: 0.3
Validation loss decreased from 1.319396 to 1.313887. Model was saved
Epoch: 2046 	Training Loss: 1.155896 	Validation Loss: 1.314293 	 time: 0.3
Epoch: 2047 	Training Loss: 1.155552 	Validation Loss: 1.317857 	 time: 0.3
Epoch: 2048 	Training Loss: 1.155020 	Validation Loss: 1.318704 	 time: 0.3
Epoch: 2049 	Training Loss: 1.154318 	Validation Loss: 1.320402 	 time: 0.3
Epoch: 2050 	Training Loss: 1.154125 	Validation Loss: 1.320377 	 time: 0.3
Epoch: 2051 	Training Loss: 1.153779 	Validation Loss: 1.321986 	 time: 0.3
Epoch: 2052 	Training Loss: 1.153207 	Validation Loss: 1.322291 	 time: 0.3
Epoch: 2053 	Training Loss: 1.152980 	Validation Loss: 1.322804 	 time: 0.3
Epoch: 2054 	Training Loss: 1.152862 	Validation Loss: 1.323149 	 time: 0.3
Epoch: 2055 	Training Loss: 1.152780 	Validation Loss: 1.322956 	 time: 0.3
Epoch: 2056 	Training Loss: 1.152436 	Validation Loss: 1.321484 	 time: 0.3
Epoch: 2057 	Training Loss: 1.152207 	Validation Loss: 1.319941 	 time: 0.3
Epoch: 2058 	Training Loss: 1.151973 	Validation Loss: 1.319759 	 time: 0.3
Epoch: 2059 	Training Loss: 1.151891 	Validation Loss: 1.321043 	 time: 0.3
Epoch: 2060 	Training Loss: 1.151735 	Validation Loss: 1.321525 	 time: 0.3
Epoch: 2061 	Training Loss: 1.151438 	Validation Loss: 1.322103 	 time: 0.2
Epoch: 2062 	Training Loss: 1.151390 	Validation Loss: 1.322983 	 time: 0.3
Epoch: 2063 	Training Loss: 1.151214 	Validation Loss: 1.322983 	 time: 0.3
Epoch: 2064 	Training Loss: 1.151029 	Validation Loss: 1.322229 	 time: 0.3
Epoch: 2065 	Training Loss: 1.151000 	Validation Loss: 1.321856 	 time: 0.3
Epoch: 2066 	Training Loss: 1.150888 	Validation Loss: 1.321460 	 time: 0.3
Epoch: 2067 	Training Loss: 1.150838 	Validation Loss: 1.320634 	 time: 0.3
Epoch: 2068 	Training Loss: 1.150771 	Validation Loss: 1.319438 	 time: 0.3
Epoch: 2069 	Training Loss: 1.150713 	Validation Loss: 1.318060 	 time: 0.3
Epoch: 2070 	Training Loss: 1.150619 	Validation Loss: 1.317105 	 time: 0.3
Epoch: 2071 	Training Loss: 1.150550 	Validation Loss: 1.316465 	 time: 0.3
Epoch: 2072 	Training Loss: 1.150497 	Validation Loss: 1.316339 	 time: 0.3
Epoch: 2073 	Training Loss: 1.150398 	Validation Loss: 1.316454 	 time: 0.3
Epoch: 2074 	Training Loss: 1.150330 	Validation Loss: 1.316170 	 time: 0.3
Epoch: 2075 	Training Loss: 1.150275 	Validation Loss: 1.315711 	 time: 0.3
Epoch: 2076 	Training Loss: 1.150224 	Validation Loss: 1.315263 	 time: 0.3
Epoch: 2077 	Training Loss: 1.150136 	Validation Loss: 1.314851 	 time: 0.3
Epoch: 2078 	Training Loss: 1.150072 	Validation Loss: 1.314670 	 time: 0.3
Epoch: 2079 	Training Loss: 1.150008 	Validation Loss: 1.314649 	 time: 0.2
Epoch: 2080 	Training Loss: 1.149951 	Validation Loss: 1.314524 	 time: 0.3
Epoch: 2081 	Training Loss: 1.149905 	Validation Loss: 1.314151 	 time: 0.3
Epoch: 2082 	Training Loss: 1.149845 	Validation Loss: 1.313787 	 time: 0.3
Validation loss decreased from 1.313887 to 1.313787. Model was saved
Epoch: 2083 	Training Loss: 1.149792 	Validation Loss: 1.313480 	 time: 0.3
Validation loss decreased from 1.313787 to 1.313480. Model was saved
Epoch: 2084 	Training Loss: 1.149760 	Validation Loss: 1.313211 	 time: 0.3
Validation loss decreased from 1.313480 to 1.313211. Model was saved
Epoch: 2085 	Training Loss: 1.149734 	Validation Loss: 1.313112 	 time: 0.3
Validation loss decreased from 1.313211 to 1.313112. Model was saved
Epoch: 2086 	Training Loss: 1.149707 	Validation Loss: 1.313165 	 time: 0.3
Epoch: 2087 	Training Loss: 1.149676 	Validation Loss: 1.313202 	 time: 0.3
Epoch: 2088 	Training Loss: 1.149631 	Validation Loss: 1.313138 	 time: 0.3
Epoch: 2089 	Training Loss: 1.149594 	Validation Loss: 1.312995 	 time: 0.3
Validation loss decreased from 1.313112 to 1.312995. Model was saved
Epoch: 2090 	Training Loss: 1.149562 	Validation Loss: 1.312788 	 time: 0.3
Validation loss decreased from 1.312995 to 1.312788. Model was saved
Epoch: 2091 	Training Loss: 1.149526 	Validation Loss: 1.312647 	 time: 0.3
Validation loss decreased from 1.312788 to 1.312647. Model was saved
Epoch: 2092 	Training Loss: 1.149488 	Validation Loss: 1.312732 	 time: 0.3
Epoch: 2093 	Training Loss: 1.149452 	Validation Loss: 1.312969 	 time: 0.3
Epoch: 2094 	Training Loss: 1.149417 	Validation Loss: 1.313025 	 time: 0.3
Epoch: 2095 	Training Loss: 1.149383 	Validation Loss: 1.312845 	 time: 0.3
Epoch: 2096 	Training Loss: 1.149342 	Validation Loss: 1.312637 	 time: 0.3
Validation loss decreased from 1.312647 to 1.312637. Model was saved
Epoch: 2097 	Training Loss: 1.149311 	Validation Loss: 1.312484 	 time: 0.3
Validation loss decreased from 1.312637 to 1.312484. Model was saved
Epoch: 2098 	Training Loss: 1.149292 	Validation Loss: 1.312501 	 time: 0.2
Epoch: 2099 	Training Loss: 1.149258 	Validation Loss: 1.312642 	 time: 0.3
Epoch: 2100 	Training Loss: 1.149196 	Validation Loss: 1.312735 	 time: 0.3
Epoch: 2101 	Training Loss: 1.149065 	Validation Loss: 1.312523 	 time: 0.2
Epoch: 2102 	Training Loss: 1.149009 	Validation Loss: 1.311847 	 time: 0.3
Validation loss decreased from 1.312484 to 1.311847. Model was saved
Epoch: 2103 	Training Loss: 1.149000 	Validation Loss: 1.310900 	 time: 0.3
Validation loss decreased from 1.311847 to 1.310900. Model was saved
Epoch: 2104 	Training Loss: 1.148989 	Validation Loss: 1.310563 	 time: 0.3
Validation loss decreased from 1.310900 to 1.310563. Model was saved
Epoch: 2105 	Training Loss: 1.148929 	Validation Loss: 1.310807 	 time: 0.3
Epoch: 2106 	Training Loss: 1.148902 	Validation Loss: 1.311202 	 time: 0.3
Epoch: 2107 	Training Loss: 1.148883 	Validation Loss: 1.311301 	 time: 0.3
Epoch: 2108 	Training Loss: 1.148861 	Validation Loss: 1.310974 	 time: 0.3
Epoch: 2109 	Training Loss: 1.148815 	Validation Loss: 1.310593 	 time: 0.3
Epoch: 2110 	Training Loss: 1.148771 	Validation Loss: 1.310351 	 time: 0.3
Validation loss decreased from 1.310563 to 1.310351. Model was saved
Epoch: 2111 	Training Loss: 1.148745 	Validation Loss: 1.310226 	 time: 0.3
Validation loss decreased from 1.310351 to 1.310226. Model was saved
Epoch: 2112 	Training Loss: 1.148713 	Validation Loss: 1.310168 	 time: 0.3
Validation loss decreased from 1.310226 to 1.310168. Model was saved
Epoch: 2113 	Training Loss: 1.148685 	Validation Loss: 1.310245 	 time: 0.3
Epoch: 2114 	Training Loss: 1.148643 	Validation Loss: 1.310511 	 time: 0.3
Epoch: 2115 	Training Loss: 1.148585 	Validation Loss: 1.310716 	 time: 0.3
Epoch: 2116 	Training Loss: 1.148531 	Validation Loss: 1.310691 	 time: 0.3
Epoch: 2117 	Training Loss: 1.148502 	Validation Loss: 1.310660 	 time: 0.3
Epoch: 2118 	Training Loss: 1.148465 	Validation Loss: 1.310783 	 time: 0.3
Epoch: 2119 	Training Loss: 1.148414 	Validation Loss: 1.311100 	 time: 0.3
Epoch: 2120 	Training Loss: 1.148365 	Validation Loss: 1.311428 	 time: 0.3
Epoch: 2121 	Training Loss: 1.148320 	Validation Loss: 1.311388 	 time: 0.3
Epoch: 2122 	Training Loss: 1.148286 	Validation Loss: 1.311220 	 time: 0.3
Epoch: 2123 	Training Loss: 1.148258 	Validation Loss: 1.311256 	 time: 0.3
Epoch: 2124 	Training Loss: 1.148239 	Validation Loss: 1.311563 	 time: 0.3
Epoch: 2125 	Training Loss: 1.148223 	Validation Loss: 1.312005 	 time: 0.3
Epoch: 2126 	Training Loss: 1.148203 	Validation Loss: 1.312127 	 time: 0.3
Epoch: 2127 	Training Loss: 1.148177 	Validation Loss: 1.311913 	 time: 0.3
Epoch: 2128 	Training Loss: 1.148146 	Validation Loss: 1.311911 	 time: 0.3
Epoch: 2129 	Training Loss: 1.148112 	Validation Loss: 1.312241 	 time: 0.3
Epoch: 2130 	Training Loss: 1.148087 	Validation Loss: 1.312677 	 time: 0.3
Epoch: 2131 	Training Loss: 1.148066 	Validation Loss: 1.312965 	 time: 0.3
Epoch: 2132 	Training Loss: 1.148044 	Validation Loss: 1.313179 	 time: 0.3
Epoch: 2133 	Training Loss: 1.148021 	Validation Loss: 1.313600 	 time: 0.3
Epoch: 2134 	Training Loss: 1.147985 	Validation Loss: 1.314238 	 time: 0.3
Epoch: 2135 	Training Loss: 1.147944 	Validation Loss: 1.314826 	 time: 0.3
Epoch: 2136 	Training Loss: 1.147914 	Validation Loss: 1.315263 	 time: 0.3
Epoch: 2137 	Training Loss: 1.147890 	Validation Loss: 1.315569 	 time: 0.3
Epoch: 2138 	Training Loss: 1.147868 	Validation Loss: 1.315702 	 time: 0.3
Epoch: 2139 	Training Loss: 1.147852 	Validation Loss: 1.315630 	 time: 0.3
Epoch: 2140 	Training Loss: 1.147836 	Validation Loss: 1.315467 	 time: 0.3
Epoch: 2141 	Training Loss: 1.147824 	Validation Loss: 1.315369 	 time: 0.3
Epoch: 2142 	Training Loss: 1.147811 	Validation Loss: 1.315471 	 time: 0.3
Epoch: 2143 	Training Loss: 1.147788 	Validation Loss: 1.315767 	 time: 0.3
Epoch: 2144 	Training Loss: 1.147764 	Validation Loss: 1.316048 	 time: 0.3
Epoch: 2145 	Training Loss: 1.147747 	Validation Loss: 1.316019 	 time: 0.3
Epoch: 2146 	Training Loss: 1.147732 	Validation Loss: 1.315847 	 time: 0.3
Epoch: 2147 	Training Loss: 1.147718 	Validation Loss: 1.315916 	 time: 0.3
Epoch: 2148 	Training Loss: 1.147709 	Validation Loss: 1.316125 	 time: 0.3
Epoch: 2149 	Training Loss: 1.147698 	Validation Loss: 1.316382 	 time: 0.3
Epoch: 2150 	Training Loss: 1.147686 	Validation Loss: 1.316430 	 time: 0.3
Epoch: 2151 	Training Loss: 1.147670 	Validation Loss: 1.316289 	 time: 0.3
Epoch: 2152 	Training Loss: 1.147655 	Validation Loss: 1.316191 	 time: 0.3
Epoch: 2153 	Training Loss: 1.147646 	Validation Loss: 1.316214 	 time: 0.3
Epoch: 2154 	Training Loss: 1.147636 	Validation Loss: 1.316401 	 time: 0.3
Epoch: 2155 	Training Loss: 1.147626 	Validation Loss: 1.316713 	 time: 0.3
Epoch: 2156 	Training Loss: 1.147616 	Validation Loss: 1.316914 	 time: 0.3
Epoch: 2157 	Training Loss: 1.147609 	Validation Loss: 1.316847 	 time: 0.3
Epoch: 2158 	Training Loss: 1.147601 	Validation Loss: 1.316665 	 time: 0.3
Epoch: 2159 	Training Loss: 1.147594 	Validation Loss: 1.316573 	 time: 0.3
Epoch: 2160 	Training Loss: 1.147590 	Validation Loss: 1.316549 	 time: 0.3
Epoch: 2161 	Training Loss: 1.147583 	Validation Loss: 1.316549 	 time: 0.3
Epoch: 2162 	Training Loss: 1.147576 	Validation Loss: 1.316637 	 time: 0.3
Epoch: 2163 	Training Loss: 1.147568 	Validation Loss: 1.316822 	 time: 0.3
Epoch: 2164 	Training Loss: 1.147561 	Validation Loss: 1.316934 	 time: 0.3
Epoch: 2165 	Training Loss: 1.147553 	Validation Loss: 1.316875 	 time: 0.3
Epoch: 2166 	Training Loss: 1.147545 	Validation Loss: 1.316833 	 time: 0.3
Epoch: 2167 	Training Loss: 1.147539 	Validation Loss: 1.316934 	 time: 0.3
Epoch: 2168 	Training Loss: 1.147532 	Validation Loss: 1.317118 	 time: 0.3
Epoch: 2169 	Training Loss: 1.147522 	Validation Loss: 1.317174 	 time: 0.3
Epoch: 2170 	Training Loss: 1.147505 	Validation Loss: 1.317087 	 time: 0.3
Epoch: 2171 	Training Loss: 1.147491 	Validation Loss: 1.317127 	 time: 0.3
Epoch: 2172 	Training Loss: 1.147484 	Validation Loss: 1.317229 	 time: 0.3
Epoch: 2173 	Training Loss: 1.147478 	Validation Loss: 1.317362 	 time: 0.3
Epoch: 2174 	Training Loss: 1.147472 	Validation Loss: 1.317557 	 time: 0.3
Epoch: 2175 	Training Loss: 1.147466 	Validation Loss: 1.317703 	 time: 0.3
Epoch: 2176 	Training Loss: 1.147460 	Validation Loss: 1.317723 	 time: 0.3
Epoch: 2177 	Training Loss: 1.147454 	Validation Loss: 1.317768 	 time: 0.3
Epoch: 2178 	Training Loss: 1.147449 	Validation Loss: 1.317947 	 time: 0.3
Epoch: 2179 	Training Loss: 1.147442 	Validation Loss: 1.318107 	 time: 0.3
Epoch: 2180 	Training Loss: 1.147432 	Validation Loss: 1.318135 	 time: 0.3
Epoch: 2181 	Training Loss: 1.147421 	Validation Loss: 1.318172 	 time: 0.3
Epoch: 2182 	Training Loss: 1.147408 	Validation Loss: 1.318293 	 time: 0.3
Epoch: 2183 	Training Loss: 1.147392 	Validation Loss: 1.318400 	 time: 0.3
Epoch: 2184 	Training Loss: 1.147383 	Validation Loss: 1.318429 	 time: 0.3
Epoch: 2185 	Training Loss: 1.147375 	Validation Loss: 1.318487 	 time: 0.3
Epoch: 2186 	Training Loss: 1.147367 	Validation Loss: 1.318636 	 time: 0.3
Epoch: 2187 	Training Loss: 1.147359 	Validation Loss: 1.318795 	 time: 0.3
Epoch: 2188 	Training Loss: 1.147354 	Validation Loss: 1.318867 	 time: 0.3
Epoch: 2189 	Training Loss: 1.147350 	Validation Loss: 1.318862 	 time: 0.3
Epoch: 2190 	Training Loss: 1.147345 	Validation Loss: 1.318831 	 time: 0.3
Epoch: 2191 	Training Loss: 1.147340 	Validation Loss: 1.318824 	 time: 0.3
Epoch: 2192 	Training Loss: 1.147335 	Validation Loss: 1.318892 	 time: 0.3
Epoch: 2193 	Training Loss: 1.147329 	Validation Loss: 1.318983 	 time: 0.3
Epoch: 2194 	Training Loss: 1.147323 	Validation Loss: 1.318992 	 time: 0.3
Epoch: 2195 	Training Loss: 1.147315 	Validation Loss: 1.318948 	 time: 0.3
Epoch: 2196 	Training Loss: 1.147306 	Validation Loss: 1.318967 	 time: 0.3
Epoch: 2197 	Training Loss: 1.147297 	Validation Loss: 1.319114 	 time: 0.3
Epoch: 2198 	Training Loss: 1.147292 	Validation Loss: 1.319219 	 time: 0.3
Epoch: 2199 	Training Loss: 1.147287 	Validation Loss: 1.319180 	 time: 0.3
Epoch: 2200 	Training Loss: 1.147282 	Validation Loss: 1.319187 	 time: 0.3
Epoch: 2201 	Training Loss: 1.147276 	Validation Loss: 1.319320 	 time: 0.3
Epoch: 2202 	Training Loss: 1.147271 	Validation Loss: 1.319384 	 time: 0.3
Epoch: 2203 	Training Loss: 1.147268 	Validation Loss: 1.319339 	 time: 0.3
Epoch: 2204 	Training Loss: 1.147265 	Validation Loss: 1.319350 	 time: 0.3
Epoch: 2205 	Training Loss: 1.147261 	Validation Loss: 1.319454 	 time: 0.3
Epoch: 2206 	Training Loss: 1.147257 	Validation Loss: 1.319501 	 time: 0.3
Epoch: 2207 	Training Loss: 1.147254 	Validation Loss: 1.319412 	 time: 0.3
Epoch: 2208 	Training Loss: 1.147250 	Validation Loss: 1.319344 	 time: 0.3
Epoch: 2209 	Training Loss: 1.147246 	Validation Loss: 1.319369 	 time: 0.3
Epoch: 2210 	Training Loss: 1.147242 	Validation Loss: 1.319438 	 time: 0.3
Epoch: 2211 	Training Loss: 1.147238 	Validation Loss: 1.319477 	 time: 0.3
Epoch: 2212 	Training Loss: 1.147232 	Validation Loss: 1.319481 	 time: 0.3
Epoch: 2213 	Training Loss: 1.147225 	Validation Loss: 1.319492 	 time: 0.3
Epoch: 2214 	Training Loss: 1.147218 	Validation Loss: 1.319573 	 time: 0.3
Epoch: 2215 	Training Loss: 1.147212 	Validation Loss: 1.319731 	 time: 0.3
Epoch: 2216 	Training Loss: 1.147207 	Validation Loss: 1.319852 	 time: 0.3
Epoch: 2217 	Training Loss: 1.147204 	Validation Loss: 1.319891 	 time: 0.3
Epoch: 2218 	Training Loss: 1.147201 	Validation Loss: 1.319912 	 time: 0.3
Epoch: 2219 	Training Loss: 1.147197 	Validation Loss: 1.319933 	 time: 0.3
Epoch: 2220 	Training Loss: 1.147194 	Validation Loss: 1.319890 	 time: 0.3
Epoch: 2221 	Training Loss: 1.147189 	Validation Loss: 1.319821 	 time: 0.3
Epoch: 2222 	Training Loss: 1.147184 	Validation Loss: 1.319823 	 time: 0.3
Epoch: 2223 	Training Loss: 1.147179 	Validation Loss: 1.319886 	 time: 0.3
Epoch: 2224 	Training Loss: 1.147175 	Validation Loss: 1.319892 	 time: 0.3
Epoch: 2225 	Training Loss: 1.147171 	Validation Loss: 1.319817 	 time: 0.3
Epoch: 2226 	Training Loss: 1.147166 	Validation Loss: 1.319765 	 time: 0.3
Epoch: 2227 	Training Loss: 1.147162 	Validation Loss: 1.319776 	 time: 0.3
Epoch: 2228 	Training Loss: 1.147156 	Validation Loss: 1.319800 	 time: 0.3
Epoch: 2229 	Training Loss: 1.147150 	Validation Loss: 1.319800 	 time: 0.3
Epoch: 2230 	Training Loss: 1.147143 	Validation Loss: 1.319806 	 time: 0.3
Epoch: 2231 	Training Loss: 1.147133 	Validation Loss: 1.319848 	 time: 0.3
Epoch: 2232 	Training Loss: 1.147119 	Validation Loss: 1.319913 	 time: 0.3
Epoch: 2233 	Training Loss: 1.147096 	Validation Loss: 1.319967 	 time: 0.3
Epoch: 2234 	Training Loss: 1.147060 	Validation Loss: 1.319976 	 time: 0.3
Epoch: 2235 	Training Loss: 1.147013 	Validation Loss: 1.319924 	 time: 0.3
Epoch: 2236 	Training Loss: 1.146976 	Validation Loss: 1.319841 	 time: 0.3
Epoch: 2237 	Training Loss: 1.146963 	Validation Loss: 1.319733 	 time: 0.3
Epoch: 2238 	Training Loss: 1.146960 	Validation Loss: 1.319623 	 time: 0.3
Epoch: 2239 	Training Loss: 1.146957 	Validation Loss: 1.319521 	 time: 0.3
Epoch: 2240 	Training Loss: 1.146952 	Validation Loss: 1.319438 	 time: 0.3
Epoch: 2241 	Training Loss: 1.146946 	Validation Loss: 1.319416 	 time: 0.3
Epoch: 2242 	Training Loss: 1.146940 	Validation Loss: 1.319503 	 time: 0.3
Epoch: 2243 	Training Loss: 1.146933 	Validation Loss: 1.319587 	 time: 0.3
Epoch: 2244 	Training Loss: 1.146927 	Validation Loss: 1.319668 	 time: 0.3
Epoch: 2245 	Training Loss: 1.146922 	Validation Loss: 1.319672 	 time: 0.3
Epoch: 2246 	Training Loss: 1.146919 	Validation Loss: 1.319726 	 time: 0.3
Epoch: 2247 	Training Loss: 1.146913 	Validation Loss: 1.319662 	 time: 0.3
Epoch: 2248 	Training Loss: 1.146916 	Validation Loss: 1.319658 	 time: 0.3
Epoch: 2249 	Training Loss: 1.146899 	Validation Loss: 1.319625 	 time: 0.3
Epoch: 2250 	Training Loss: 1.146888 	Validation Loss: 1.319659 	 time: 0.3
Epoch: 2251 	Training Loss: 1.146876 	Validation Loss: 1.319649 	 time: 0.3
Epoch: 2252 	Training Loss: 1.146869 	Validation Loss: 1.319694 	 time: 0.3
Epoch: 2253 	Training Loss: 1.146865 	Validation Loss: 1.319840 	 time: 0.3
Epoch: 2254 	Training Loss: 1.146862 	Validation Loss: 1.319988 	 time: 0.3
Epoch: 2255 	Training Loss: 1.146865 	Validation Loss: 1.320102 	 time: 0.3
Epoch: 2256 	Training Loss: 1.146862 	Validation Loss: 1.320282 	 time: 0.3
Epoch: 2257 	Training Loss: 1.146867 	Validation Loss: 1.320407 	 time: 0.3
Epoch: 2258 	Training Loss: 1.146834 	Validation Loss: 1.320693 	 time: 0.3
Epoch: 2259 	Training Loss: 1.146812 	Validation Loss: 1.321046 	 time: 0.3
Epoch: 2260 	Training Loss: 1.146804 	Validation Loss: 1.321438 	 time: 0.3
Epoch: 2261 	Training Loss: 1.146802 	Validation Loss: 1.322049 	 time: 0.3
Epoch: 2262 	Training Loss: 1.146801 	Validation Loss: 1.322644 	 time: 0.3
Epoch: 2263 	Training Loss: 1.146720 	Validation Loss: 1.322940 	 time: 0.3
Epoch: 2264 	Training Loss: 1.146691 	Validation Loss: 1.322818 	 time: 0.3
Epoch: 2265 	Training Loss: 1.146712 	Validation Loss: 1.322551 	 time: 0.3
Epoch: 2266 	Training Loss: 1.146743 	Validation Loss: 1.322334 	 time: 0.3
Epoch: 2267 	Training Loss: 1.146791 	Validation Loss: 1.322215 	 time: 0.3
Epoch: 2268 	Training Loss: 1.146722 	Validation Loss: 1.322409 	 time: 0.3
Epoch: 2269 	Training Loss: 1.146769 	Validation Loss: 1.322237 	 time: 0.3
Epoch: 2270 	Training Loss: 1.146810 	Validation Loss: 1.321470 	 time: 0.3
Epoch: 2271 	Training Loss: 1.146715 	Validation Loss: 1.321611 	 time: 0.3
Epoch: 2272 	Training Loss: 1.146808 	Validation Loss: 1.321737 	 time: 0.3
Epoch: 2273 	Training Loss: 1.146777 	Validation Loss: 1.321587 	 time: 0.3
Epoch: 2274 	Training Loss: 1.146705 	Validation Loss: 1.321365 	 time: 0.3
Epoch: 2275 	Training Loss: 1.146787 	Validation Loss: 1.321335 	 time: 0.3
Epoch: 2276 	Training Loss: 1.146806 	Validation Loss: 1.321195 	 time: 0.3
Epoch: 2277 	Training Loss: 1.146716 	Validation Loss: 1.321006 	 time: 0.3
Epoch: 2278 	Training Loss: 1.146710 	Validation Loss: 1.320584 	 time: 0.3
Epoch: 2279 	Training Loss: 1.146730 	Validation Loss: 1.320650 	 time: 0.3
Epoch: 2280 	Training Loss: 1.146730 	Validation Loss: 1.320885 	 time: 0.3
Epoch: 2281 	Training Loss: 1.146701 	Validation Loss: 1.320936 	 time: 0.3
Epoch: 2282 	Training Loss: 1.146685 	Validation Loss: 1.320650 	 time: 0.3
Epoch: 2283 	Training Loss: 1.146670 	Validation Loss: 1.320469 	 time: 0.3
Epoch: 2284 	Training Loss: 1.146637 	Validation Loss: 1.320728 	 time: 0.3
Epoch: 2285 	Training Loss: 1.146677 	Validation Loss: 1.320781 	 time: 0.3
Epoch: 2286 	Training Loss: 1.146679 	Validation Loss: 1.320918 	 time: 0.3
Epoch: 2287 	Training Loss: 1.146637 	Validation Loss: 1.320942 	 time: 0.3
Epoch: 2288 	Training Loss: 1.146688 	Validation Loss: 1.320571 	 time: 0.3
Epoch: 2289 	Training Loss: 1.146641 	Validation Loss: 1.320316 	 time: 0.3
Epoch: 2290 	Training Loss: 1.146569 	Validation Loss: 1.320469 	 time: 0.3
Epoch: 2291 	Training Loss: 1.146571 	Validation Loss: 1.321196 	 time: 0.3
Epoch: 2292 	Training Loss: 1.146525 	Validation Loss: 1.321773 	 time: 0.3
Epoch: 2293 	Training Loss: 1.146535 	Validation Loss: 1.321677 	 time: 0.3
Epoch: 2294 	Training Loss: 1.146506 	Validation Loss: 1.321347 	 time: 0.3
Epoch: 2295 	Training Loss: 1.146493 	Validation Loss: 1.321363 	 time: 0.3
Epoch: 2296 	Training Loss: 1.146475 	Validation Loss: 1.321371 	 time: 0.3
Epoch: 2297 	Training Loss: 1.146468 	Validation Loss: 1.320667 	 time: 0.3
Epoch: 2298 	Training Loss: 1.146452 	Validation Loss: 1.320553 	 time: 0.3
Epoch: 2299 	Training Loss: 1.146400 	Validation Loss: 1.321053 	 time: 0.3
Epoch: 2300 	Training Loss: 1.146396 	Validation Loss: 1.321315 	 time: 0.3
Epoch: 2301 	Training Loss: 1.146492 	Validation Loss: 1.321127 	 time: 0.3
Epoch: 2302 	Training Loss: 1.146400 	Validation Loss: 1.321336 	 time: 0.3
Epoch: 2303 	Training Loss: 1.146448 	Validation Loss: 1.321509 	 time: 0.3
Epoch: 2304 	Training Loss: 1.146498 	Validation Loss: 1.321196 	 time: 0.3
Epoch: 2305 	Training Loss: 1.146348 	Validation Loss: 1.321081 	 time: 0.3
Epoch: 2306 	Training Loss: 1.146514 	Validation Loss: 1.320738 	 time: 0.3
Epoch: 2307 	Training Loss: 1.146395 	Validation Loss: 1.321371 	 time: 0.3
Epoch: 2308 	Training Loss: 1.146376 	Validation Loss: 1.321602 	 time: 0.3
Epoch: 2309 	Training Loss: 1.146386 	Validation Loss: 1.320859 	 time: 0.3
Epoch: 2310 	Training Loss: 1.146300 	Validation Loss: 1.321129 	 time: 0.3
Epoch: 2311 	Training Loss: 1.146358 	Validation Loss: 1.321623 	 time: 0.3
Epoch: 2312 	Training Loss: 1.146370 	Validation Loss: 1.320984 	 time: 0.3
Epoch: 2313 	Training Loss: 1.146351 	Validation Loss: 1.320939 	 time: 0.3
Epoch: 2314 	Training Loss: 1.146337 	Validation Loss: 1.321659 	 time: 0.3
Epoch: 2315 	Training Loss: 1.146341 	Validation Loss: 1.322050 	 time: 0.3
Epoch: 2316 	Training Loss: 1.146296 	Validation Loss: 1.320580 	 time: 0.3
Epoch: 2317 	Training Loss: 1.146303 	Validation Loss: 1.319847 	 time: 0.3
Epoch: 2318 	Training Loss: 1.146301 	Validation Loss: 1.320762 	 time: 0.3
Epoch: 2319 	Training Loss: 1.146262 	Validation Loss: 1.321927 	 time: 0.3
Epoch: 2320 	Training Loss: 1.146267 	Validation Loss: 1.321502 	 time: 0.3
Epoch: 2321 	Training Loss: 1.146259 	Validation Loss: 1.320800 	 time: 0.3
Epoch: 2322 	Training Loss: 1.146246 	Validation Loss: 1.320716 	 time: 0.3
Epoch: 2323 	Training Loss: 1.146244 	Validation Loss: 1.321134 	 time: 0.3
Epoch: 2324 	Training Loss: 1.146244 	Validation Loss: 1.320878 	 time: 0.3
Epoch: 2325 	Training Loss: 1.146224 	Validation Loss: 1.321028 	 time: 0.3
Epoch: 2326 	Training Loss: 1.146228 	Validation Loss: 1.321338 	 time: 0.3
Epoch: 2327 	Training Loss: 1.146212 	Validation Loss: 1.321511 	 time: 0.3
Epoch: 2328 	Training Loss: 1.146206 	Validation Loss: 1.320915 	 time: 0.3
Epoch: 2329 	Training Loss: 1.146181 	Validation Loss: 1.320642 	 time: 0.3
Epoch: 2330 	Training Loss: 1.146191 	Validation Loss: 1.321208 	 time: 0.3
Epoch: 2331 	Training Loss: 1.146196 	Validation Loss: 1.321555 	 time: 0.3
Epoch: 2332 	Training Loss: 1.146218 	Validation Loss: 1.321450 	 time: 0.3
Epoch: 2333 	Training Loss: 1.146167 	Validation Loss: 1.321147 	 time: 0.3
Epoch: 2334 	Training Loss: 1.146226 	Validation Loss: 1.320893 	 time: 0.3
Epoch: 2335 	Training Loss: 1.146292 	Validation Loss: 1.321062 	 time: 0.3
Epoch: 2336 	Training Loss: 1.146206 	Validation Loss: 1.321239 	 time: 0.3
Epoch: 2337 	Training Loss: 1.146294 	Validation Loss: 1.321258 	 time: 0.3
Epoch: 2338 	Training Loss: 1.146252 	Validation Loss: 1.321293 	 time: 0.3
Epoch: 2339 	Training Loss: 1.146232 	Validation Loss: 1.321488 	 time: 0.3
Epoch: 2340 	Training Loss: 1.146267 	Validation Loss: 1.320210 	 time: 0.3
Epoch: 2341 	Training Loss: 1.146236 	Validation Loss: 1.320345 	 time: 0.3
Epoch: 2342 	Training Loss: 1.146241 	Validation Loss: 1.321349 	 time: 0.3
Epoch: 2343 	Training Loss: 1.146186 	Validation Loss: 1.321852 	 time: 0.3
Epoch: 2344 	Training Loss: 1.146207 	Validation Loss: 1.320934 	 time: 0.3
Epoch: 2345 	Training Loss: 1.146211 	Validation Loss: 1.320668 	 time: 0.3
Epoch: 2346 	Training Loss: 1.146171 	Validation Loss: 1.321160 	 time: 0.3
Epoch: 2347 	Training Loss: 1.146238 	Validation Loss: 1.320514 	 time: 0.3
Epoch: 2348 	Training Loss: 1.146186 	Validation Loss: 1.320611 	 time: 0.3
Epoch: 2349 	Training Loss: 1.146205 	Validation Loss: 1.321508 	 time: 0.3
Epoch: 2350 	Training Loss: 1.146209 	Validation Loss: 1.321816 	 time: 0.3
Epoch: 2351 	Training Loss: 1.146152 	Validation Loss: 1.321226 	 time: 0.3
Epoch: 2352 	Training Loss: 1.146148 	Validation Loss: 1.320719 	 time: 0.3
Epoch: 2353 	Training Loss: 1.146215 	Validation Loss: 1.320606 	 time: 0.3
Epoch: 2354 	Training Loss: 1.146216 	Validation Loss: 1.321304 	 time: 0.3
Epoch: 2355 	Training Loss: 1.146162 	Validation Loss: 1.321850 	 time: 0.3
Epoch: 2356 	Training Loss: 1.146193 	Validation Loss: 1.321823 	 time: 0.3
Epoch: 2357 	Training Loss: 1.146148 	Validation Loss: 1.321203 	 time: 0.3
Epoch: 2358 	Training Loss: 1.146116 	Validation Loss: 1.320686 	 time: 0.3
Epoch: 2359 	Training Loss: 1.146140 	Validation Loss: 1.320256 	 time: 0.3
Epoch: 2360 	Training Loss: 1.146125 	Validation Loss: 1.321129 	 time: 0.3
Epoch: 2361 	Training Loss: 1.146071 	Validation Loss: 1.321878 	 time: 0.3
Epoch: 2362 	Training Loss: 1.146101 	Validation Loss: 1.320848 	 time: 0.3
Epoch: 2363 	Training Loss: 1.146027 	Validation Loss: 1.320225 	 time: 0.3
Epoch: 2364 	Training Loss: 1.146006 	Validation Loss: 1.321075 	 time: 0.3
Epoch: 2365 	Training Loss: 1.146014 	Validation Loss: 1.321086 	 time: 0.3
Epoch: 2366 	Training Loss: 1.146040 	Validation Loss: 1.321071 	 time: 0.3
Epoch: 2367 	Training Loss: 1.146019 	Validation Loss: 1.320888 	 time: 0.3
Epoch: 2368 	Training Loss: 1.145991 	Validation Loss: 1.321264 	 time: 0.3
Epoch: 2369 	Training Loss: 1.145983 	Validation Loss: 1.320719 	 time: 0.3
Epoch: 2370 	Training Loss: 1.145956 	Validation Loss: 1.320375 	 time: 0.3
Epoch: 2371 	Training Loss: 1.145963 	Validation Loss: 1.320640 	 time: 0.3
Epoch: 2372 	Training Loss: 1.145943 	Validation Loss: 1.321257 	 time: 0.3
Epoch: 2373 	Training Loss: 1.145928 	Validation Loss: 1.320865 	 time: 0.3
Epoch: 2374 	Training Loss: 1.145932 	Validation Loss: 1.320638 	 time: 0.3
Epoch: 2375 	Training Loss: 1.145906 	Validation Loss: 1.320496 	 time: 0.3
Epoch: 2376 	Training Loss: 1.145914 	Validation Loss: 1.320339 	 time: 0.3
Epoch: 2377 	Training Loss: 1.145913 	Validation Loss: 1.320029 	 time: 0.3
Epoch: 2378 	Training Loss: 1.145915 	Validation Loss: 1.319992 	 time: 0.3
Epoch: 2379 	Training Loss: 1.145934 	Validation Loss: 1.319926 	 time: 0.3
Epoch: 2380 	Training Loss: 1.145949 	Validation Loss: 1.319779 	 time: 0.3
Epoch: 2381 	Training Loss: 1.145907 	Validation Loss: 1.319339 	 time: 0.3
Epoch: 2382 	Training Loss: 1.145896 	Validation Loss: 1.318785 	 time: 0.3
Epoch: 2383 	Training Loss: 1.145916 	Validation Loss: 1.319054 	 time: 0.3
Epoch: 2384 	Training Loss: 1.145948 	Validation Loss: 1.319376 	 time: 0.3
Epoch: 2385 	Training Loss: 1.145942 	Validation Loss: 1.318989 	 time: 0.3
Epoch: 2386 	Training Loss: 1.145929 	Validation Loss: 1.319146 	 time: 0.3
Epoch: 2387 	Training Loss: 1.146018 	Validation Loss: 1.319802 	 time: 0.3
Epoch: 2388 	Training Loss: 1.145909 	Validation Loss: 1.319825 	 time: 0.3
Epoch: 2389 	Training Loss: 1.146023 	Validation Loss: 1.319507 	 time: 0.3
Epoch: 2390 	Training Loss: 1.146195 	Validation Loss: 1.319287 	 time: 0.3
Epoch: 2391 	Training Loss: 1.145946 	Validation Loss: 1.320310 	 time: 0.3
Epoch: 2392 	Training Loss: 1.146086 	Validation Loss: 1.318749 	 time: 0.3
Epoch: 2393 	Training Loss: 1.145957 	Validation Loss: 1.317713 	 time: 0.3
Epoch: 2394 	Training Loss: 1.146022 	Validation Loss: 1.318976 	 time: 0.3
Epoch: 2395 	Training Loss: 1.145929 	Validation Loss: 1.320381 	 time: 0.3
Epoch: 2396 	Training Loss: 1.145964 	Validation Loss: 1.320637 	 time: 0.3
Epoch: 2397 	Training Loss: 1.145969 	Validation Loss: 1.320006 	 time: 0.3
Epoch: 2398 	Training Loss: 1.145900 	Validation Loss: 1.319743 	 time: 0.3
Epoch: 2399 	Training Loss: 1.145963 	Validation Loss: 1.320735 	 time: 0.3
Epoch: 2400 	Training Loss: 1.145960 	Validation Loss: 1.319603 	 time: 0.3
Epoch: 2401 	Training Loss: 1.145964 	Validation Loss: 1.317922 	 time: 0.3
Epoch: 2402 	Training Loss: 1.145949 	Validation Loss: 1.317735 	 time: 0.3
Epoch: 2403 	Training Loss: 1.145933 	Validation Loss: 1.319621 	 time: 0.3
Epoch: 2404 	Training Loss: 1.145919 	Validation Loss: 1.320091 	 time: 0.3
Epoch: 2405 	Training Loss: 1.145906 	Validation Loss: 1.319329 	 time: 0.3
Epoch: 2406 	Training Loss: 1.145897 	Validation Loss: 1.319664 	 time: 0.3
Epoch: 2407 	Training Loss: 1.145856 	Validation Loss: 1.320565 	 time: 0.3
Epoch: 2408 	Training Loss: 1.145836 	Validation Loss: 1.319585 	 time: 0.3
Epoch: 2409 	Training Loss: 1.145849 	Validation Loss: 1.318995 	 time: 0.3
Epoch: 2410 	Training Loss: 1.145840 	Validation Loss: 1.318867 	 time: 0.3
Epoch: 2411 	Training Loss: 1.145820 	Validation Loss: 1.319819 	 time: 0.3
Epoch: 2412 	Training Loss: 1.145787 	Validation Loss: 1.320095 	 time: 0.3
Epoch: 2413 	Training Loss: 1.145786 	Validation Loss: 1.319505 	 time: 0.3
Epoch: 2414 	Training Loss: 1.145769 	Validation Loss: 1.319095 	 time: 0.3
Epoch: 2415 	Training Loss: 1.145730 	Validation Loss: 1.319901 	 time: 0.3
Epoch: 2416 	Training Loss: 1.145684 	Validation Loss: 1.321387 	 time: 0.3
Epoch: 2417 	Training Loss: 1.145686 	Validation Loss: 1.320339 	 time: 0.3
Epoch: 2418 	Training Loss: 1.145675 	Validation Loss: 1.318928 	 time: 0.3
Epoch: 2419 	Training Loss: 1.145655 	Validation Loss: 1.319367 	 time: 0.3
Epoch: 2420 	Training Loss: 1.145663 	Validation Loss: 1.322098 	 time: 0.3
Epoch: 2421 	Training Loss: 1.145682 	Validation Loss: 1.319876 	 time: 0.3
Epoch: 2422 	Training Loss: 1.145597 	Validation Loss: 1.318641 	 time: 0.3
Epoch: 2423 	Training Loss: 1.145640 	Validation Loss: 1.318522 	 time: 0.3
Epoch: 2424 	Training Loss: 1.145616 	Validation Loss: 1.320441 	 time: 0.3
Epoch: 2425 	Training Loss: 1.145614 	Validation Loss: 1.319640 	 time: 0.3
Epoch: 2426 	Training Loss: 1.145624 	Validation Loss: 1.317974 	 time: 0.3
Epoch: 2427 	Training Loss: 1.145637 	Validation Loss: 1.317621 	 time: 0.3
Epoch: 2428 	Training Loss: 1.145611 	Validation Loss: 1.318095 	 time: 0.3
Epoch: 2429 	Training Loss: 1.145602 	Validation Loss: 1.318692 	 time: 0.3
Epoch: 2430 	Training Loss: 1.145634 	Validation Loss: 1.318632 	 time: 0.3
Epoch: 2431 	Training Loss: 1.145621 	Validation Loss: 1.318966 	 time: 0.3
Epoch: 2432 	Training Loss: 1.145582 	Validation Loss: 1.319234 	 time: 0.3
Epoch: 2433 	Training Loss: 1.145530 	Validation Loss: 1.318617 	 time: 0.3
Epoch: 2434 	Training Loss: 1.145631 	Validation Loss: 1.317852 	 time: 0.3
Epoch: 2435 	Training Loss: 1.145579 	Validation Loss: 1.317410 	 time: 0.3
Epoch: 2436 	Training Loss: 1.145529 	Validation Loss: 1.318421 	 time: 0.3
Epoch: 2437 	Training Loss: 1.145545 	Validation Loss: 1.319742 	 time: 0.3
Epoch: 2438 	Training Loss: 1.145513 	Validation Loss: 1.318347 	 time: 0.3
Epoch: 2439 	Training Loss: 1.145481 	Validation Loss: 1.317849 	 time: 0.3
Epoch: 2440 	Training Loss: 1.145473 	Validation Loss: 1.318290 	 time: 0.3
Epoch: 2441 	Training Loss: 1.145448 	Validation Loss: 1.318776 	 time: 0.3
Epoch: 2442 	Training Loss: 1.145457 	Validation Loss: 1.317753 	 time: 0.3
Epoch: 2443 	Training Loss: 1.145432 	Validation Loss: 1.317549 	 time: 0.3
Epoch: 2444 	Training Loss: 1.145457 	Validation Loss: 1.318205 	 time: 0.3
Epoch: 2445 	Training Loss: 1.145502 	Validation Loss: 1.319457 	 time: 0.3
Epoch: 2446 	Training Loss: 1.145431 	Validation Loss: 1.318637 	 time: 0.3
Epoch: 2447 	Training Loss: 1.145432 	Validation Loss: 1.318080 	 time: 0.3
Epoch: 2448 	Training Loss: 1.145440 	Validation Loss: 1.317806 	 time: 0.3
Epoch: 2449 	Training Loss: 1.145406 	Validation Loss: 1.317957 	 time: 0.3
Epoch: 2450 	Training Loss: 1.145436 	Validation Loss: 1.317723 	 time: 0.3
Epoch: 2451 	Training Loss: 1.145456 	Validation Loss: 1.318001 	 time: 0.3
Epoch: 2452 	Training Loss: 1.145424 	Validation Loss: 1.318780 	 time: 0.3
Epoch: 2453 	Training Loss: 1.145433 	Validation Loss: 1.319267 	 time: 0.3
Epoch: 2454 	Training Loss: 1.145481 	Validation Loss: 1.319155 	 time: 0.3
Epoch: 2455 	Training Loss: 1.145386 	Validation Loss: 1.318195 	 time: 0.3
Epoch: 2456 	Training Loss: 1.145429 	Validation Loss: 1.318014 	 time: 0.3
Epoch: 2457 	Training Loss: 1.145496 	Validation Loss: 1.318264 	 time: 0.3
Epoch: 2458 	Training Loss: 1.145344 	Validation Loss: 1.319590 	 time: 0.3
Epoch: 2459 	Training Loss: 1.145507 	Validation Loss: 1.318198 	 time: 0.3
Epoch: 2460 	Training Loss: 1.145396 	Validation Loss: 1.317618 	 time: 0.3
Epoch: 2461 	Training Loss: 1.145397 	Validation Loss: 1.317955 	 time: 0.3
Epoch: 2462 	Training Loss: 1.145512 	Validation Loss: 1.318146 	 time: 0.3
Epoch: 2463 	Training Loss: 1.145299 	Validation Loss: 1.318471 	 time: 0.3
Epoch: 2464 	Training Loss: 1.145450 	Validation Loss: 1.318219 	 time: 0.3
Epoch: 2465 	Training Loss: 1.145354 	Validation Loss: 1.318429 	 time: 0.3
Epoch: 2466 	Training Loss: 1.145354 	Validation Loss: 1.318605 	 time: 0.3
Epoch: 2467 	Training Loss: 1.145357 	Validation Loss: 1.318806 	 time: 0.3
Epoch: 2468 	Training Loss: 1.145282 	Validation Loss: 1.318474 	 time: 0.3
Epoch: 2469 	Training Loss: 1.145307 	Validation Loss: 1.318231 	 time: 0.3
Epoch: 2470 	Training Loss: 1.145276 	Validation Loss: 1.318607 	 time: 0.3
Epoch: 2471 	Training Loss: 1.145270 	Validation Loss: 1.319181 	 time: 0.3
Epoch: 2472 	Training Loss: 1.145270 	Validation Loss: 1.318736 	 time: 0.3
Epoch: 2473 	Training Loss: 1.145270 	Validation Loss: 1.318310 	 time: 0.3
Epoch: 2474 	Training Loss: 1.145260 	Validation Loss: 1.318501 	 time: 0.3
Epoch: 2475 	Training Loss: 1.145249 	Validation Loss: 1.318970 	 time: 0.3
Epoch: 2476 	Training Loss: 1.145249 	Validation Loss: 1.318657 	 time: 0.3
Epoch: 2477 	Training Loss: 1.145237 	Validation Loss: 1.318678 	 time: 0.3
Epoch: 2478 	Training Loss: 1.145231 	Validation Loss: 1.318603 	 time: 0.3
Epoch: 2479 	Training Loss: 1.145220 	Validation Loss: 1.318430 	 time: 0.3
Epoch: 2480 	Training Loss: 1.145217 	Validation Loss: 1.318189 	 time: 0.3
Epoch: 2481 	Training Loss: 1.145205 	Validation Loss: 1.318238 	 time: 0.3
Epoch: 2482 	Training Loss: 1.145201 	Validation Loss: 1.318462 	 time: 0.3
Epoch: 2483 	Training Loss: 1.145181 	Validation Loss: 1.318469 	 time: 0.3
Epoch: 2484 	Training Loss: 1.145184 	Validation Loss: 1.318108 	 time: 0.3
Epoch: 2485 	Training Loss: 1.145185 	Validation Loss: 1.318159 	 time: 0.3
Epoch: 2486 	Training Loss: 1.145175 	Validation Loss: 1.317923 	 time: 0.3
Epoch: 2487 	Training Loss: 1.145164 	Validation Loss: 1.317844 	 time: 0.3
Epoch: 2488 	Training Loss: 1.145154 	Validation Loss: 1.317922 	 time: 0.3
Epoch: 2489 	Training Loss: 1.145158 	Validation Loss: 1.318033 	 time: 0.3
Epoch: 2490 	Training Loss: 1.145163 	Validation Loss: 1.318281 	 time: 0.3
Epoch: 2491 	Training Loss: 1.145155 	Validation Loss: 1.317924 	 time: 0.3
Epoch: 2492 	Training Loss: 1.145137 	Validation Loss: 1.317468 	 time: 0.3
Epoch: 2493 	Training Loss: 1.145136 	Validation Loss: 1.317586 	 time: 0.3
Epoch: 2494 	Training Loss: 1.145141 	Validation Loss: 1.318023 	 time: 0.3
Epoch: 2495 	Training Loss: 1.145154 	Validation Loss: 1.318241 	 time: 0.3
Epoch: 2496 	Training Loss: 1.145127 	Validation Loss: 1.317729 	 time: 0.3
Epoch: 2497 	Training Loss: 1.145109 	Validation Loss: 1.317439 	 time: 0.3
Epoch: 2498 	Training Loss: 1.145123 	Validation Loss: 1.317623 	 time: 0.3
Epoch: 2499 	Training Loss: 1.145145 	Validation Loss: 1.317692 	 time: 0.3
Epoch: 2500 	Training Loss: 1.145180 	Validation Loss: 1.317689 	 time: 0.3
Epoch: 2501 	Training Loss: 1.145149 	Validation Loss: 1.317655 	 time: 0.3
Epoch: 2502 	Training Loss: 1.145139 	Validation Loss: 1.317247 	 time: 0.3
Epoch: 2503 	Training Loss: 1.145170 	Validation Loss: 1.317060 	 time: 0.3
Epoch: 2504 	Training Loss: 1.145131 	Validation Loss: 1.317037 	 time: 0.3
Epoch: 2505 	Training Loss: 1.145116 	Validation Loss: 1.316921 	 time: 0.3
Epoch: 2506 	Training Loss: 1.145134 	Validation Loss: 1.316886 	 time: 0.3
Epoch: 2507 	Training Loss: 1.145118 	Validation Loss: 1.317045 	 time: 0.3
Epoch: 2508 	Training Loss: 1.145098 	Validation Loss: 1.317339 	 time: 0.3
Epoch: 2509 	Training Loss: 1.145111 	Validation Loss: 1.317612 	 time: 0.3
Epoch: 2510 	Training Loss: 1.145093 	Validation Loss: 1.317311 	 time: 0.3
Epoch: 2511 	Training Loss: 1.145079 	Validation Loss: 1.317239 	 time: 0.3
Epoch: 2512 	Training Loss: 1.145091 	Validation Loss: 1.317603 	 time: 0.3
Epoch: 2513 	Training Loss: 1.145078 	Validation Loss: 1.317770 	 time: 0.3
Epoch: 2514 	Training Loss: 1.145077 	Validation Loss: 1.317528 	 time: 0.3
Epoch: 2515 	Training Loss: 1.145074 	Validation Loss: 1.317506 	 time: 0.3
Epoch: 2516 	Training Loss: 1.145071 	Validation Loss: 1.317688 	 time: 0.3
Epoch: 2517 	Training Loss: 1.145068 	Validation Loss: 1.317525 	 time: 0.3
Epoch: 2518 	Training Loss: 1.145064 	Validation Loss: 1.317466 	 time: 0.3
Epoch: 2519 	Training Loss: 1.145069 	Validation Loss: 1.317633 	 time: 0.3
Epoch: 2520 	Training Loss: 1.145067 	Validation Loss: 1.317558 	 time: 0.3
Epoch: 2521 	Training Loss: 1.145074 	Validation Loss: 1.317493 	 time: 0.3
Epoch: 2522 	Training Loss: 1.145064 	Validation Loss: 1.317638 	 time: 0.3
Epoch: 2523 	Training Loss: 1.145053 	Validation Loss: 1.317551 	 time: 0.3
Epoch: 2524 	Training Loss: 1.145051 	Validation Loss: 1.317358 	 time: 0.3
Epoch: 2525 	Training Loss: 1.145059 	Validation Loss: 1.317488 	 time: 0.3
Epoch: 2526 	Training Loss: 1.145084 	Validation Loss: 1.317691 	 time: 0.3
Epoch: 2527 	Training Loss: 1.145083 	Validation Loss: 1.317446 	 time: 0.3
Epoch: 2528 	Training Loss: 1.145053 	Validation Loss: 1.317198 	 time: 0.3
Epoch: 2529 	Training Loss: 1.145063 	Validation Loss: 1.317269 	 time: 0.3
Epoch: 2530 	Training Loss: 1.145070 	Validation Loss: 1.317505 	 time: 0.3
Epoch: 2531 	Training Loss: 1.145133 	Validation Loss: 1.317523 	 time: 0.3
Epoch: 2532 	Training Loss: 1.145099 	Validation Loss: 1.317359 	 time: 0.3
Epoch: 2533 	Training Loss: 1.145066 	Validation Loss: 1.317050 	 time: 0.3
Epoch: 2534 	Training Loss: 1.145116 	Validation Loss: 1.316923 	 time: 0.3
Epoch: 2535 	Training Loss: 1.145060 	Validation Loss: 1.317074 	 time: 0.3
Epoch: 2536 	Training Loss: 1.145042 	Validation Loss: 1.317010 	 time: 0.3
Epoch: 2537 	Training Loss: 1.145070 	Validation Loss: 1.316892 	 time: 0.3
Epoch: 2538 	Training Loss: 1.145046 	Validation Loss: 1.316981 	 time: 0.3
Epoch: 2539 	Training Loss: 1.145036 	Validation Loss: 1.317261 	 time: 0.3
Epoch: 2540 	Training Loss: 1.145075 	Validation Loss: 1.317539 	 time: 0.3
Epoch: 2541 	Training Loss: 1.145069 	Validation Loss: 1.317104 	 time: 0.3
Epoch: 2542 	Training Loss: 1.145014 	Validation Loss: 1.317083 	 time: 0.3
Epoch: 2543 	Training Loss: 1.145050 	Validation Loss: 1.317503 	 time: 0.3
Epoch: 2544 	Training Loss: 1.145105 	Validation Loss: 1.317632 	 time: 0.3
Epoch: 2545 	Training Loss: 1.145178 	Validation Loss: 1.317320 	 time: 0.3
Epoch: 2546 	Training Loss: 1.145083 	Validation Loss: 1.317116 	 time: 0.3
Epoch: 2547 	Training Loss: 1.145093 	Validation Loss: 1.316719 	 time: 0.3
Epoch: 2548 	Training Loss: 1.145163 	Validation Loss: 1.316504 	 time: 0.3
Epoch: 2549 	Training Loss: 1.145072 	Validation Loss: 1.316573 	 time: 0.3
Epoch: 2550 	Training Loss: 1.145117 	Validation Loss: 1.317013 	 time: 0.3
Epoch: 2551 	Training Loss: 1.145120 	Validation Loss: 1.316879 	 time: 0.3
Epoch: 2552 	Training Loss: 1.145084 	Validation Loss: 1.316149 	 time: 0.3
Epoch: 2553 	Training Loss: 1.145091 	Validation Loss: 1.316528 	 time: 0.3
Epoch: 2554 	Training Loss: 1.145091 	Validation Loss: 1.317229 	 time: 0.3
Epoch: 2555 	Training Loss: 1.145100 	Validation Loss: 1.316708 	 time: 0.3
Epoch: 2556 	Training Loss: 1.145060 	Validation Loss: 1.316721 	 time: 0.3
Epoch: 2557 	Training Loss: 1.145082 	Validation Loss: 1.317086 	 time: 0.3
Epoch: 2558 	Training Loss: 1.145073 	Validation Loss: 1.317277 	 time: 0.3
Epoch: 2559 	Training Loss: 1.145066 	Validation Loss: 1.316751 	 time: 0.3
Epoch: 2560 	Training Loss: 1.145061 	Validation Loss: 1.316919 	 time: 0.3
Epoch: 2561 	Training Loss: 1.145044 	Validation Loss: 1.317743 	 time: 0.3
Epoch: 2562 	Training Loss: 1.145049 	Validation Loss: 1.316692 	 time: 0.3
Epoch: 2563 	Training Loss: 1.145035 	Validation Loss: 1.316312 	 time: 0.3
Epoch: 2564 	Training Loss: 1.145029 	Validation Loss: 1.316776 	 time: 0.3
Epoch: 2565 	Training Loss: 1.145028 	Validation Loss: 1.317237 	 time: 0.3
Epoch: 2566 	Training Loss: 1.145035 	Validation Loss: 1.316791 	 time: 0.3
Epoch: 2567 	Training Loss: 1.145019 	Validation Loss: 1.316580 	 time: 0.3
Epoch: 2568 	Training Loss: 1.145011 	Validation Loss: 1.316815 	 time: 0.3
Epoch: 2569 	Training Loss: 1.145015 	Validation Loss: 1.316766 	 time: 0.3
Epoch: 2570 	Training Loss: 1.145000 	Validation Loss: 1.316633 	 time: 0.3
Epoch: 2571 	Training Loss: 1.144999 	Validation Loss: 1.316780 	 time: 0.3
Epoch: 2572 	Training Loss: 1.144986 	Validation Loss: 1.316986 	 time: 0.3
Epoch: 2573 	Training Loss: 1.145003 	Validation Loss: 1.316835 	 time: 0.3
Epoch: 2574 	Training Loss: 1.145030 	Validation Loss: 1.316825 	 time: 0.3
Epoch: 2575 	Training Loss: 1.145042 	Validation Loss: 1.317203 	 time: 0.3
Epoch: 2576 	Training Loss: 1.144994 	Validation Loss: 1.317076 	 time: 0.3
Epoch: 2577 	Training Loss: 1.145044 	Validation Loss: 1.316700 	 time: 0.3
Epoch: 2578 	Training Loss: 1.145034 	Validation Loss: 1.316912 	 time: 0.3
Epoch: 2579 	Training Loss: 1.145015 	Validation Loss: 1.316954 	 time: 0.3
Epoch: 2580 	Training Loss: 1.145006 	Validation Loss: 1.316675 	 time: 0.3
Epoch: 2581 	Training Loss: 1.145059 	Validation Loss: 1.316774 	 time: 0.3
Epoch: 2582 	Training Loss: 1.145120 	Validation Loss: 1.317190 	 time: 0.3
Epoch: 2583 	Training Loss: 1.145023 	Validation Loss: 1.317564 	 time: 0.3
Epoch: 2584 	Training Loss: 1.145083 	Validation Loss: 1.316954 	 time: 0.3
Epoch: 2585 	Training Loss: 1.145063 	Validation Loss: 1.316064 	 time: 0.3
Epoch: 2586 	Training Loss: 1.145031 	Validation Loss: 1.315625 	 time: 0.3
Epoch: 2587 	Training Loss: 1.145055 	Validation Loss: 1.316367 	 time: 0.3
Epoch: 2588 	Training Loss: 1.145025 	Validation Loss: 1.317191 	 time: 0.3
Epoch: 2589 	Training Loss: 1.145028 	Validation Loss: 1.317052 	 time: 0.3
Epoch: 2590 	Training Loss: 1.145028 	Validation Loss: 1.316561 	 time: 0.3
Epoch: 2591 	Training Loss: 1.145022 	Validation Loss: 1.316673 	 time: 0.3
Epoch: 2592 	Training Loss: 1.144996 	Validation Loss: 1.316499 	 time: 0.3
Epoch: 2593 	Training Loss: 1.145000 	Validation Loss: 1.316760 	 time: 0.3
Epoch: 2594 	Training Loss: 1.144986 	Validation Loss: 1.317090 	 time: 0.3
Epoch: 2595 	Training Loss: 1.144995 	Validation Loss: 1.317294 	 time: 0.3
Epoch: 2596 	Training Loss: 1.144976 	Validation Loss: 1.316531 	 time: 0.3
Epoch: 2597 	Training Loss: 1.144981 	Validation Loss: 1.316766 	 time: 0.3
Epoch: 2598 	Training Loss: 1.144984 	Validation Loss: 1.317254 	 time: 0.3
Epoch: 2599 	Training Loss: 1.144989 	Validation Loss: 1.317142 	 time: 0.3
Epoch: 2600 	Training Loss: 1.144991 	Validation Loss: 1.316665 	 time: 0.3
Epoch: 2601 	Training Loss: 1.144960 	Validation Loss: 1.316422 	 time: 0.3
Epoch: 2602 	Training Loss: 1.144992 	Validation Loss: 1.316832 	 time: 0.3
Epoch: 2603 	Training Loss: 1.145075 	Validation Loss: 1.316874 	 time: 0.3
Epoch: 2604 	Training Loss: 1.144998 	Validation Loss: 1.316607 	 time: 0.3
Epoch: 2605 	Training Loss: 1.145009 	Validation Loss: 1.316729 	 time: 0.3
Epoch: 2606 	Training Loss: 1.144991 	Validation Loss: 1.316832 	 time: 0.3
Epoch: 2607 	Training Loss: 1.144973 	Validation Loss: 1.316501 	 time: 0.3
Epoch: 2608 	Training Loss: 1.144965 	Validation Loss: 1.316817 	 time: 0.3
Epoch: 2609 	Training Loss: 1.144993 	Validation Loss: 1.317315 	 time: 0.3
Epoch: 2610 	Training Loss: 1.144974 	Validation Loss: 1.316972 	 time: 0.3
Epoch: 2611 	Training Loss: 1.144958 	Validation Loss: 1.316734 	 time: 0.3
Epoch: 2612 	Training Loss: 1.144999 	Validation Loss: 1.316814 	 time: 0.3
Epoch: 2613 	Training Loss: 1.144970 	Validation Loss: 1.317199 	 time: 0.3
Epoch: 2614 	Training Loss: 1.144994 	Validation Loss: 1.316835 	 time: 0.3
Epoch: 2615 	Training Loss: 1.144956 	Validation Loss: 1.316507 	 time: 0.3
Epoch: 2616 	Training Loss: 1.145010 	Validation Loss: 1.316898 	 time: 0.3
Epoch: 2617 	Training Loss: 1.145012 	Validation Loss: 1.317207 	 time: 0.3
Epoch: 2618 	Training Loss: 1.144962 	Validation Loss: 1.316869 	 time: 0.3
Epoch: 2619 	Training Loss: 1.144972 	Validation Loss: 1.316793 	 time: 0.3
Epoch: 2620 	Training Loss: 1.144903 	Validation Loss: 1.316849 	 time: 0.3
Epoch: 2621 	Training Loss: 1.144902 	Validation Loss: 1.316999 	 time: 0.3
Epoch: 2622 	Training Loss: 1.144857 	Validation Loss: 1.317211 	 time: 0.3
Epoch: 2623 	Training Loss: 1.144868 	Validation Loss: 1.317489 	 time: 0.3
Epoch: 2624 	Training Loss: 1.144839 	Validation Loss: 1.317397 	 time: 0.3
Epoch: 2625 	Training Loss: 1.144849 	Validation Loss: 1.317090 	 time: 0.3
Epoch: 2626 	Training Loss: 1.144857 	Validation Loss: 1.317440 	 time: 0.3
Epoch: 2627 	Training Loss: 1.144931 	Validation Loss: 1.317893 	 time: 0.3
Epoch: 2628 	Training Loss: 1.144884 	Validation Loss: 1.317578 	 time: 0.3
Epoch: 2629 	Training Loss: 1.144869 	Validation Loss: 1.317191 	 time: 0.3
Epoch: 2630 	Training Loss: 1.144914 	Validation Loss: 1.317102 	 time: 0.3
Epoch: 2631 	Training Loss: 1.144913 	Validation Loss: 1.317655 	 time: 0.3
Epoch: 2632 	Training Loss: 1.144971 	Validation Loss: 1.317827 	 time: 0.3
Epoch: 2633 	Training Loss: 1.144937 	Validation Loss: 1.317417 	 time: 0.3
Epoch: 2634 	Training Loss: 1.144915 	Validation Loss: 1.317036 	 time: 0.3
Epoch: 2635 	Training Loss: 1.144953 	Validation Loss: 1.316753 	 time: 0.3
Epoch: 2636 	Training Loss: 1.144899 	Validation Loss: 1.316925 	 time: 0.3
Epoch: 2637 	Training Loss: 1.144900 	Validation Loss: 1.316918 	 time: 0.3
Epoch: 2638 	Training Loss: 1.144906 	Validation Loss: 1.316839 	 time: 0.3
Epoch: 2639 	Training Loss: 1.144887 	Validation Loss: 1.316977 	 time: 0.3
Epoch: 2640 	Training Loss: 1.144863 	Validation Loss: 1.317162 	 time: 0.3
Epoch: 2641 	Training Loss: 1.144890 	Validation Loss: 1.317400 	 time: 0.3
Epoch: 2642 	Training Loss: 1.144843 	Validation Loss: 1.317112 	 time: 0.3
Epoch: 2643 	Training Loss: 1.144838 	Validation Loss: 1.316895 	 time: 0.3
Epoch: 2644 	Training Loss: 1.144810 	Validation Loss: 1.316945 	 time: 0.3
Epoch: 2645 	Training Loss: 1.144775 	Validation Loss: 1.317121 	 time: 0.3
Epoch: 2646 	Training Loss: 1.144783 	Validation Loss: 1.317089 	 time: 0.3
Epoch: 2647 	Training Loss: 1.144852 	Validation Loss: 1.316883 	 time: 0.3
Epoch: 2648 	Training Loss: 1.144821 	Validation Loss: 1.316962 	 time: 0.3
Epoch: 2649 	Training Loss: 1.144781 	Validation Loss: 1.316693 	 time: 0.3
Epoch: 2650 	Training Loss: 1.144821 	Validation Loss: 1.316302 	 time: 0.3
Epoch: 2651 	Training Loss: 1.144869 	Validation Loss: 1.316673 	 time: 0.3
Epoch: 2652 	Training Loss: 1.144845 	Validation Loss: 1.316878 	 time: 0.3
Epoch: 2653 	Training Loss: 1.144806 	Validation Loss: 1.316843 	 time: 0.3
Epoch: 2654 	Training Loss: 1.144834 	Validation Loss: 1.316040 	 time: 0.3
Epoch: 2655 	Training Loss: 1.144810 	Validation Loss: 1.315947 	 time: 0.3
Epoch: 2656 	Training Loss: 1.144796 	Validation Loss: 1.316513 	 time: 0.3
Epoch: 2657 	Training Loss: 1.144799 	Validation Loss: 1.316058 	 time: 0.3
Epoch: 2658 	Training Loss: 1.144767 	Validation Loss: 1.315628 	 time: 0.3
Epoch: 2659 	Training Loss: 1.144783 	Validation Loss: 1.315698 	 time: 0.3
Epoch: 2660 	Training Loss: 1.144750 	Validation Loss: 1.316157 	 time: 0.3
Epoch: 2661 	Training Loss: 1.144748 	Validation Loss: 1.315927 	 time: 0.3
Epoch: 2662 	Training Loss: 1.144734 	Validation Loss: 1.315927 	 time: 0.3
Epoch: 2663 	Training Loss: 1.144720 	Validation Loss: 1.316096 	 time: 0.3
Epoch: 2664 	Training Loss: 1.144729 	Validation Loss: 1.316012 	 time: 0.3
Epoch: 2665 	Training Loss: 1.144716 	Validation Loss: 1.315698 	 time: 0.3
Epoch: 2666 	Training Loss: 1.144706 	Validation Loss: 1.315868 	 time: 0.3
Epoch: 2667 	Training Loss: 1.144705 	Validation Loss: 1.316277 	 time: 0.3
Epoch: 2668 	Training Loss: 1.144692 	Validation Loss: 1.316457 	 time: 0.3
Epoch: 2669 	Training Loss: 1.144692 	Validation Loss: 1.316290 	 time: 0.3
Epoch: 2670 	Training Loss: 1.144698 	Validation Loss: 1.316158 	 time: 0.3
Epoch: 2671 	Training Loss: 1.144680 	Validation Loss: 1.316127 	 time: 0.3
Epoch: 2672 	Training Loss: 1.144690 	Validation Loss: 1.316046 	 time: 0.3
Epoch: 2673 	Training Loss: 1.144682 	Validation Loss: 1.315930 	 time: 0.3
Epoch: 2674 	Training Loss: 1.144689 	Validation Loss: 1.315975 	 time: 0.3
Epoch: 2675 	Training Loss: 1.144687 	Validation Loss: 1.316095 	 time: 0.3
Epoch: 2676 	Training Loss: 1.144660 	Validation Loss: 1.315842 	 time: 0.3
Epoch: 2677 	Training Loss: 1.144647 	Validation Loss: 1.315563 	 time: 0.3
Epoch: 2678 	Training Loss: 1.144660 	Validation Loss: 1.315728 	 time: 0.3
Epoch: 2679 	Training Loss: 1.144637 	Validation Loss: 1.315821 	 time: 0.3
Epoch: 2680 	Training Loss: 1.144635 	Validation Loss: 1.315675 	 time: 0.3
Epoch: 2681 	Training Loss: 1.144622 	Validation Loss: 1.315672 	 time: 0.3
Epoch: 2682 	Training Loss: 1.144596 	Validation Loss: 1.315657 	 time: 0.3
Epoch: 2683 	Training Loss: 1.144589 	Validation Loss: 1.315458 	 time: 0.3
Epoch: 2684 	Training Loss: 1.144578 	Validation Loss: 1.315335 	 time: 0.3
Epoch: 2685 	Training Loss: 1.144573 	Validation Loss: 1.315461 	 time: 0.3
Epoch: 2686 	Training Loss: 1.144582 	Validation Loss: 1.315628 	 time: 0.3
Epoch: 2687 	Training Loss: 1.144603 	Validation Loss: 1.315587 	 time: 0.3
Epoch: 2688 	Training Loss: 1.144645 	Validation Loss: 1.315475 	 time: 0.3
Epoch: 2689 	Training Loss: 1.144575 	Validation Loss: 1.315116 	 time: 0.3
Epoch: 2690 	Training Loss: 1.144567 	Validation Loss: 1.315271 	 time: 0.3
Epoch: 2691 	Training Loss: 1.144655 	Validation Loss: 1.315472 	 time: 0.3
Epoch: 2692 	Training Loss: 1.144601 	Validation Loss: 1.315333 	 time: 0.3
Epoch: 2693 	Training Loss: 1.144595 	Validation Loss: 1.315124 	 time: 0.3
Epoch: 2694 	Training Loss: 1.144544 	Validation Loss: 1.314950 	 time: 0.3
Epoch: 2695 	Training Loss: 1.144548 	Validation Loss: 1.314340 	 time: 0.3
Epoch: 2696 	Training Loss: 1.144517 	Validation Loss: 1.314492 	 time: 0.3
Epoch: 2697 	Training Loss: 1.144463 	Validation Loss: 1.314873 	 time: 0.3
Epoch: 2698 	Training Loss: 1.144472 	Validation Loss: 1.314763 	 time: 0.3
Epoch: 2699 	Training Loss: 1.144464 	Validation Loss: 1.314342 	 time: 0.3
Epoch: 2700 	Training Loss: 1.144528 	Validation Loss: 1.314633 	 time: 0.3
Epoch: 2701 	Training Loss: 1.144702 	Validation Loss: 1.315511 	 time: 0.3
Epoch: 2702 	Training Loss: 1.144602 	Validation Loss: 1.314280 	 time: 0.3
Epoch: 2703 	Training Loss: 1.144563 	Validation Loss: 1.314346 	 time: 0.3
Epoch: 2704 	Training Loss: 1.144568 	Validation Loss: 1.314409 	 time: 0.3
Epoch: 2705 	Training Loss: 1.144660 	Validation Loss: 1.313839 	 time: 0.3
Epoch: 2706 	Training Loss: 1.144558 	Validation Loss: 1.313936 	 time: 0.3
Epoch: 2707 	Training Loss: 1.144535 	Validation Loss: 1.314582 	 time: 0.3
Epoch: 2708 	Training Loss: 1.144552 	Validation Loss: 1.312843 	 time: 0.3
Epoch: 2709 	Training Loss: 1.144509 	Validation Loss: 1.311785 	 time: 0.3
Epoch: 2710 	Training Loss: 1.144532 	Validation Loss: 1.312906 	 time: 0.3
Epoch: 2711 	Training Loss: 1.144495 	Validation Loss: 1.313012 	 time: 0.3
Epoch: 2712 	Training Loss: 1.144576 	Validation Loss: 1.313577 	 time: 0.3
Epoch: 2713 	Training Loss: 1.144517 	Validation Loss: 1.314056 	 time: 0.3
Epoch: 2714 	Training Loss: 1.144444 	Validation Loss: 1.314354 	 time: 0.3
Epoch: 2715 	Training Loss: 1.144568 	Validation Loss: 1.313834 	 time: 0.3
Epoch: 2716 	Training Loss: 1.144519 	Validation Loss: 1.313066 	 time: 0.3
Epoch: 2717 	Training Loss: 1.144476 	Validation Loss: 1.312792 	 time: 0.3
Epoch: 2718 	Training Loss: 1.144486 	Validation Loss: 1.313199 	 time: 0.3
Epoch: 2719 	Training Loss: 1.144426 	Validation Loss: 1.312590 	 time: 0.3
Epoch: 2720 	Training Loss: 1.144457 	Validation Loss: 1.312986 	 time: 0.3
Epoch: 2721 	Training Loss: 1.144491 	Validation Loss: 1.313452 	 time: 0.3
Epoch: 2722 	Training Loss: 1.144408 	Validation Loss: 1.312291 	 time: 0.3
Epoch: 2723 	Training Loss: 1.144444 	Validation Loss: 1.311830 	 time: 0.3
Epoch: 2724 	Training Loss: 1.144384 	Validation Loss: 1.312482 	 time: 0.3
Epoch: 2725 	Training Loss: 1.144475 	Validation Loss: 1.313261 	 time: 0.3
Epoch: 2726 	Training Loss: 1.144402 	Validation Loss: 1.312102 	 time: 0.3
Epoch: 2727 	Training Loss: 1.144568 	Validation Loss: 1.312712 	 time: 0.3
Epoch: 2728 	Training Loss: 1.144884 	Validation Loss: 1.315290 	 time: 0.3
Epoch: 2729 	Training Loss: 1.144644 	Validation Loss: 1.311466 	 time: 0.3
Epoch: 2730 	Training Loss: 1.144520 	Validation Loss: 1.311720 	 time: 0.3
Epoch: 2731 	Training Loss: 1.144973 	Validation Loss: 1.313725 	 time: 0.3
Epoch: 2732 	Training Loss: 1.145058 	Validation Loss: 1.312270 	 time: 0.3
Epoch: 2733 	Training Loss: 1.144651 	Validation Loss: 1.313014 	 time: 0.3
Epoch: 2734 	Training Loss: 1.145032 	Validation Loss: 1.315675 	 time: 0.3
Epoch: 2735 	Training Loss: 1.146864 	Validation Loss: 1.315239 	 time: 0.3
Epoch: 2736 	Training Loss: 1.150663 	Validation Loss: 1.339373 	 time: 0.3
Epoch: 2737 	Training Loss: 1.163653 	Validation Loss: 1.404313 	 time: 0.3
Epoch: 2738 	Training Loss: 1.219176 	Validation Loss: 1.328944 	 time: 0.3
Epoch: 2739 	Training Loss: 1.160888 	Validation Loss: 1.370074 	 time: 0.3
Epoch: 2740 	Training Loss: 1.229855 	Validation Loss: 1.373731 	 time: 0.3
Epoch: 2741 	Training Loss: 1.200441 	Validation Loss: 1.381816 	 time: 0.3
Epoch: 2742 	Training Loss: 1.216241 	Validation Loss: 1.356652 	 time: 0.3
Epoch: 2743 	Training Loss: 1.190125 	Validation Loss: 1.345703 	 time: 0.3
Epoch: 2744 	Training Loss: 1.187587 	Validation Loss: 1.361057 	 time: 0.3
Epoch: 2745 	Training Loss: 1.205453 	Validation Loss: 1.350471 	 time: 0.3
Epoch: 2746 	Training Loss: 1.173079 	Validation Loss: 1.368854 	 time: 0.3
Epoch: 2747 	Training Loss: 1.179993 	Validation Loss: 1.373045 	 time: 0.3
Epoch: 2748 	Training Loss: 1.191693 	Validation Loss: 1.341837 	 time: 0.3
Epoch: 2749 	Training Loss: 1.166563 	Validation Loss: 1.341538 	 time: 0.3
Epoch: 2750 	Training Loss: 1.177227 	Validation Loss: 1.341054 	 time: 0.3
Epoch: 2751 	Training Loss: 1.175367 	Validation Loss: 1.334685 	 time: 0.3
Epoch: 2752 	Training Loss: 1.166796 	Validation Loss: 1.333868 	 time: 0.3
Epoch: 2753 	Training Loss: 1.168506 	Validation Loss: 1.337756 	 time: 0.3
Epoch: 2754 	Training Loss: 1.168633 	Validation Loss: 1.334120 	 time: 0.3
Epoch: 2755 	Training Loss: 1.161129 	Validation Loss: 1.336173 	 time: 0.3
Epoch: 2756 	Training Loss: 1.161753 	Validation Loss: 1.328888 	 time: 0.3
Epoch: 2757 	Training Loss: 1.164346 	Validation Loss: 1.324628 	 time: 0.3
Epoch: 2758 	Training Loss: 1.156469 	Validation Loss: 1.330466 	 time: 0.3
Epoch: 2759 	Training Loss: 1.158752 	Validation Loss: 1.330261 	 time: 0.3
Epoch: 2760 	Training Loss: 1.157523 	Validation Loss: 1.324526 	 time: 0.3
Epoch: 2761 	Training Loss: 1.154525 	Validation Loss: 1.319869 	 time: 0.3
Epoch: 2762 	Training Loss: 1.154129 	Validation Loss: 1.320679 	 time: 0.3
Epoch: 2763 	Training Loss: 1.154768 	Validation Loss: 1.323522 	 time: 0.3
Epoch: 2764 	Training Loss: 1.152229 	Validation Loss: 1.325515 	 time: 0.3
Epoch: 2765 	Training Loss: 1.151849 	Validation Loss: 1.325960 	 time: 0.3
Epoch: 2766 	Training Loss: 1.150692 	Validation Loss: 1.326407 	 time: 0.3
Epoch: 2767 	Training Loss: 1.150267 	Validation Loss: 1.324172 	 time: 0.3
Epoch: 2768 	Training Loss: 1.149850 	Validation Loss: 1.327610 	 time: 0.3
Epoch: 2769 	Training Loss: 1.149612 	Validation Loss: 1.329937 	 time: 0.3
Epoch: 2770 	Training Loss: 1.148733 	Validation Loss: 1.329748 	 time: 0.3
Epoch: 2771 	Training Loss: 1.148414 	Validation Loss: 1.333013 	 time: 0.3
Epoch: 2772 	Training Loss: 1.148088 	Validation Loss: 1.338642 	 time: 0.3
Epoch: 2773 	Training Loss: 1.147742 	Validation Loss: 1.329265 	 time: 0.3
Epoch: 2774 	Training Loss: 1.147196 	Validation Loss: 1.330714 	 time: 0.3
Epoch: 2775 	Training Loss: 1.146488 	Validation Loss: 1.332941 	 time: 0.3
Epoch: 2776 	Training Loss: 1.146370 	Validation Loss: 1.331813 	 time: 0.3
Epoch: 2777 	Training Loss: 1.145823 	Validation Loss: 1.330451 	 time: 0.3
Epoch: 2778 	Training Loss: 1.145806 	Validation Loss: 1.333321 	 time: 0.3
Epoch: 2779 	Training Loss: 1.146191 	Validation Loss: 1.330711 	 time: 0.3
Epoch: 2780 	Training Loss: 1.145739 	Validation Loss: 1.327341 	 time: 0.3
Epoch: 2781 	Training Loss: 1.145828 	Validation Loss: 1.331911 	 time: 0.3
Epoch: 2782 	Training Loss: 1.145386 	Validation Loss: 1.330447 	 time: 0.3
Epoch: 2783 	Training Loss: 1.145130 	Validation Loss: 1.334110 	 time: 0.3
Epoch: 2784 	Training Loss: 1.145119 	Validation Loss: 1.337247 	 time: 0.3
Epoch: 2785 	Training Loss: 1.144883 	Validation Loss: 1.336876 	 time: 0.3
Epoch: 2786 	Training Loss: 1.144699 	Validation Loss: 1.333355 	 time: 0.3
Epoch: 2787 	Training Loss: 1.144455 	Validation Loss: 1.329414 	 time: 0.3
Epoch: 2788 	Training Loss: 1.144256 	Validation Loss: 1.326756 	 time: 0.3
Epoch: 2789 	Training Loss: 1.144137 	Validation Loss: 1.325642 	 time: 0.3
Epoch: 2790 	Training Loss: 1.143960 	Validation Loss: 1.324816 	 time: 0.3
Epoch: 2791 	Training Loss: 1.143999 	Validation Loss: 1.327017 	 time: 0.3
Epoch: 2792 	Training Loss: 1.143750 	Validation Loss: 1.328228 	 time: 0.3
Epoch: 2793 	Training Loss: 1.143693 	Validation Loss: 1.328308 	 time: 0.3
Epoch: 2794 	Training Loss: 1.143632 	Validation Loss: 1.329215 	 time: 0.3
Epoch: 2795 	Training Loss: 1.143533 	Validation Loss: 1.328169 	 time: 0.3
Epoch: 2796 	Training Loss: 1.143449 	Validation Loss: 1.326972 	 time: 0.3
Epoch: 2797 	Training Loss: 1.143327 	Validation Loss: 1.325423 	 time: 0.3
Epoch: 2798 	Training Loss: 1.143311 	Validation Loss: 1.325878 	 time: 0.3
Epoch: 2799 	Training Loss: 1.143199 	Validation Loss: 1.325884 	 time: 0.3
Epoch: 2800 	Training Loss: 1.143192 	Validation Loss: 1.324849 	 time: 0.3
Epoch: 2801 	Training Loss: 1.143099 	Validation Loss: 1.325006 	 time: 0.3
Epoch: 2802 	Training Loss: 1.143020 	Validation Loss: 1.325332 	 time: 0.3
Epoch: 2803 	Training Loss: 1.142983 	Validation Loss: 1.325319 	 time: 0.3
Epoch: 2804 	Training Loss: 1.142922 	Validation Loss: 1.324956 	 time: 0.3
Epoch: 2805 	Training Loss: 1.142914 	Validation Loss: 1.324072 	 time: 0.3
Epoch: 2806 	Training Loss: 1.142892 	Validation Loss: 1.323324 	 time: 0.3
Epoch: 2807 	Training Loss: 1.142887 	Validation Loss: 1.323373 	 time: 0.3
Epoch: 2808 	Training Loss: 1.142853 	Validation Loss: 1.322964 	 time: 0.3
Epoch: 2809 	Training Loss: 1.142827 	Validation Loss: 1.321846 	 time: 0.3
Epoch: 2810 	Training Loss: 1.142815 	Validation Loss: 1.321581 	 time: 0.3
Epoch: 2811 	Training Loss: 1.142780 	Validation Loss: 1.322048 	 time: 0.3
Epoch: 2812 	Training Loss: 1.142743 	Validation Loss: 1.322552 	 time: 0.3
Epoch: 2813 	Training Loss: 1.142762 	Validation Loss: 1.323004 	 time: 0.3
Epoch: 2814 	Training Loss: 1.142717 	Validation Loss: 1.323325 	 time: 0.3
Epoch: 2815 	Training Loss: 1.142700 	Validation Loss: 1.323725 	 time: 0.3
Epoch: 2816 	Training Loss: 1.142659 	Validation Loss: 1.323864 	 time: 0.3
Epoch: 2817 	Training Loss: 1.142647 	Validation Loss: 1.324411 	 time: 0.3
Epoch: 2818 	Training Loss: 1.142633 	Validation Loss: 1.325005 	 time: 0.3
Epoch: 2819 	Training Loss: 1.142578 	Validation Loss: 1.324883 	 time: 0.3
Epoch: 2820 	Training Loss: 1.142603 	Validation Loss: 1.325215 	 time: 0.3
Epoch: 2821 	Training Loss: 1.142576 	Validation Loss: 1.324730 	 time: 0.3
Epoch: 2822 	Training Loss: 1.142531 	Validation Loss: 1.323612 	 time: 0.3
Epoch: 2823 	Training Loss: 1.142569 	Validation Loss: 1.323306 	 time: 0.3
Epoch: 2824 	Training Loss: 1.142476 	Validation Loss: 1.323163 	 time: 0.3
Epoch: 2825 	Training Loss: 1.142512 	Validation Loss: 1.322653 	 time: 0.3
Epoch: 2826 	Training Loss: 1.142489 	Validation Loss: 1.321936 	 time: 0.3
Epoch: 2827 	Training Loss: 1.142506 	Validation Loss: 1.322449 	 time: 0.3
Epoch: 2828 	Training Loss: 1.142454 	Validation Loss: 1.322618 	 time: 0.3
Epoch: 2829 	Training Loss: 1.142441 	Validation Loss: 1.322750 	 time: 0.3
Epoch: 2830 	Training Loss: 1.142467 	Validation Loss: 1.323813 	 time: 0.3
Epoch: 2831 	Training Loss: 1.142413 	Validation Loss: 1.323987 	 time: 0.3
Epoch: 2832 	Training Loss: 1.142397 	Validation Loss: 1.323875 	 time: 0.3
Epoch: 2833 	Training Loss: 1.142387 	Validation Loss: 1.324148 	 time: 0.3
Epoch: 2834 	Training Loss: 1.142372 	Validation Loss: 1.323884 	 time: 0.3
Epoch: 2835 	Training Loss: 1.142362 	Validation Loss: 1.323559 	 time: 0.3
Epoch: 2836 	Training Loss: 1.142358 	Validation Loss: 1.323915 	 time: 0.3
Epoch: 2837 	Training Loss: 1.142347 	Validation Loss: 1.323965 	 time: 0.3
Epoch: 2838 	Training Loss: 1.142336 	Validation Loss: 1.323907 	 time: 0.3
Epoch: 2839 	Training Loss: 1.142324 	Validation Loss: 1.323806 	 time: 0.3
Epoch: 2840 	Training Loss: 1.142330 	Validation Loss: 1.323370 	 time: 0.3
Epoch: 2841 	Training Loss: 1.142313 	Validation Loss: 1.323327 	 time: 0.3
Epoch: 2842 	Training Loss: 1.142305 	Validation Loss: 1.323218 	 time: 0.3
Epoch: 2843 	Training Loss: 1.142288 	Validation Loss: 1.323306 	 time: 0.3
Epoch: 2844 	Training Loss: 1.142289 	Validation Loss: 1.323643 	 time: 0.3
Epoch: 2845 	Training Loss: 1.142309 	Validation Loss: 1.323309 	 time: 0.3
Epoch: 2846 	Training Loss: 1.142273 	Validation Loss: 1.323160 	 time: 0.3
Epoch: 2847 	Training Loss: 1.142278 	Validation Loss: 1.323471 	 time: 0.3
Epoch: 2848 	Training Loss: 1.142304 	Validation Loss: 1.323506 	 time: 0.3
Epoch: 2849 	Training Loss: 1.142263 	Validation Loss: 1.323375 	 time: 0.3
Epoch: 2850 	Training Loss: 1.142357 	Validation Loss: 1.323743 	 time: 0.3
Epoch: 2851 	Training Loss: 1.142251 	Validation Loss: 1.323559 	 time: 0.3
Epoch: 2852 	Training Loss: 1.142289 	Validation Loss: 1.322848 	 time: 0.3
Epoch: 2853 	Training Loss: 1.142246 	Validation Loss: 1.322645 	 time: 0.3
Epoch: 2854 	Training Loss: 1.142234 	Validation Loss: 1.322827 	 time: 0.3
Epoch: 2855 	Training Loss: 1.142234 	Validation Loss: 1.322640 	 time: 0.3
Epoch: 2856 	Training Loss: 1.142238 	Validation Loss: 1.322608 	 time: 0.3
Epoch: 2857 	Training Loss: 1.142236 	Validation Loss: 1.322243 	 time: 0.3
Epoch: 2858 	Training Loss: 1.142208 	Validation Loss: 1.322118 	 time: 0.3
Epoch: 2859 	Training Loss: 1.142215 	Validation Loss: 1.322312 	 time: 0.3
Epoch: 2860 	Training Loss: 1.142219 	Validation Loss: 1.322034 	 time: 0.3
Epoch: 2861 	Training Loss: 1.142168 	Validation Loss: 1.321537 	 time: 0.3
Epoch: 2862 	Training Loss: 1.142251 	Validation Loss: 1.321847 	 time: 0.3
Epoch: 2863 	Training Loss: 1.142136 	Validation Loss: 1.321564 	 time: 0.3
Epoch: 2864 	Training Loss: 1.142149 	Validation Loss: 1.320902 	 time: 0.3
Epoch: 2865 	Training Loss: 1.142164 	Validation Loss: 1.320722 	 time: 0.3
Epoch: 2866 	Training Loss: 1.142112 	Validation Loss: 1.320555 	 time: 0.3
Epoch: 2867 	Training Loss: 1.142104 	Validation Loss: 1.320629 	 time: 0.3
Epoch: 2868 	Training Loss: 1.142106 	Validation Loss: 1.320999 	 time: 0.3
Epoch: 2869 	Training Loss: 1.142138 	Validation Loss: 1.320918 	 time: 0.3
Epoch: 2870 	Training Loss: 1.142072 	Validation Loss: 1.320737 	 time: 0.3
Epoch: 2871 	Training Loss: 1.142165 	Validation Loss: 1.321175 	 time: 0.3
Epoch: 2872 	Training Loss: 1.142063 	Validation Loss: 1.321686 	 time: 0.3
Epoch: 2873 	Training Loss: 1.142053 	Validation Loss: 1.322076 	 time: 0.3
Epoch: 2874 	Training Loss: 1.142058 	Validation Loss: 1.322529 	 time: 0.3
Epoch: 2875 	Training Loss: 1.142050 	Validation Loss: 1.322303 	 time: 0.3
Epoch: 2876 	Training Loss: 1.142004 	Validation Loss: 1.321821 	 time: 0.3
Epoch: 2877 	Training Loss: 1.142125 	Validation Loss: 1.322108 	 time: 0.3
Epoch: 2878 	Training Loss: 1.142005 	Validation Loss: 1.322293 	 time: 0.3
Epoch: 2879 	Training Loss: 1.142036 	Validation Loss: 1.322188 	 time: 0.3
Epoch: 2880 	Training Loss: 1.142062 	Validation Loss: 1.321675 	 time: 0.3
Epoch: 2881 	Training Loss: 1.141991 	Validation Loss: 1.321193 	 time: 0.3
Epoch: 2882 	Training Loss: 1.142014 	Validation Loss: 1.320751 	 time: 0.3
Epoch: 2883 	Training Loss: 1.141972 	Validation Loss: 1.320410 	 time: 0.3
Epoch: 2884 	Training Loss: 1.141997 	Validation Loss: 1.320367 	 time: 0.3
Epoch: 2885 	Training Loss: 1.142024 	Validation Loss: 1.320210 	 time: 0.3
Epoch: 2886 	Training Loss: 1.142009 	Validation Loss: 1.320588 	 time: 0.3
Epoch: 2887 	Training Loss: 1.141971 	Validation Loss: 1.321198 	 time: 0.3
Epoch: 2888 	Training Loss: 1.141979 	Validation Loss: 1.321269 	 time: 0.3
Epoch: 2889 	Training Loss: 1.142030 	Validation Loss: 1.320536 	 time: 0.3
Epoch: 2890 	Training Loss: 1.141965 	Validation Loss: 1.319703 	 time: 0.3
Epoch: 2891 	Training Loss: 1.142062 	Validation Loss: 1.319725 	 time: 0.3
Epoch: 2892 	Training Loss: 1.141995 	Validation Loss: 1.319682 	 time: 0.3
Epoch: 2893 	Training Loss: 1.142003 	Validation Loss: 1.319522 	 time: 0.3
Epoch: 2894 	Training Loss: 1.141964 	Validation Loss: 1.319406 	 time: 0.3
Epoch: 2895 	Training Loss: 1.141937 	Validation Loss: 1.319203 	 time: 0.3
Epoch: 2896 	Training Loss: 1.141950 	Validation Loss: 1.318768 	 time: 0.3
Epoch: 2897 	Training Loss: 1.141936 	Validation Loss: 1.318927 	 time: 0.3
Epoch: 2898 	Training Loss: 1.141922 	Validation Loss: 1.319242 	 time: 0.3
Epoch: 2899 	Training Loss: 1.141912 	Validation Loss: 1.319396 	 time: 0.3
Epoch: 2900 	Training Loss: 1.141905 	Validation Loss: 1.319415 	 time: 0.3
Epoch: 2901 	Training Loss: 1.141913 	Validation Loss: 1.319454 	 time: 0.3
Epoch: 2902 	Training Loss: 1.141896 	Validation Loss: 1.319357 	 time: 0.3
Epoch: 2903 	Training Loss: 1.141896 	Validation Loss: 1.319065 	 time: 0.3
Epoch: 2904 	Training Loss: 1.141888 	Validation Loss: 1.318818 	 time: 0.3
Epoch: 2905 	Training Loss: 1.141891 	Validation Loss: 1.319136 	 time: 0.3
Epoch: 2906 	Training Loss: 1.141896 	Validation Loss: 1.319154 	 time: 0.3
Epoch: 2907 	Training Loss: 1.141873 	Validation Loss: 1.319078 	 time: 0.3
Epoch: 2908 	Training Loss: 1.141864 	Validation Loss: 1.318927 	 time: 0.3
Epoch: 2909 	Training Loss: 1.141862 	Validation Loss: 1.318910 	 time: 0.3
Epoch: 2910 	Training Loss: 1.141859 	Validation Loss: 1.319170 	 time: 0.3
Epoch: 2911 	Training Loss: 1.141871 	Validation Loss: 1.319217 	 time: 0.3
Epoch: 2912 	Training Loss: 1.141873 	Validation Loss: 1.319677 	 time: 0.3
Epoch: 2913 	Training Loss: 1.141896 	Validation Loss: 1.319750 	 time: 0.3
Epoch: 2914 	Training Loss: 1.141858 	Validation Loss: 1.319565 	 time: 0.3
Epoch: 2915 	Training Loss: 1.141986 	Validation Loss: 1.320070 	 time: 0.3
Epoch: 2916 	Training Loss: 1.141911 	Validation Loss: 1.319914 	 time: 0.3
Epoch: 2917 	Training Loss: 1.141887 	Validation Loss: 1.319297 	 time: 0.3
Epoch: 2918 	Training Loss: 1.141951 	Validation Loss: 1.319111 	 time: 0.3
Epoch: 2919 	Training Loss: 1.141907 	Validation Loss: 1.318905 	 time: 0.3
Epoch: 2920 	Training Loss: 1.141899 	Validation Loss: 1.318703 	 time: 0.3
Epoch: 2921 	Training Loss: 1.141893 	Validation Loss: 1.318729 	 time: 0.3
Epoch: 2922 	Training Loss: 1.141898 	Validation Loss: 1.318334 	 time: 0.3
Epoch: 2923 	Training Loss: 1.141848 	Validation Loss: 1.317981 	 time: 0.3
Epoch: 2924 	Training Loss: 1.141894 	Validation Loss: 1.318300 	 time: 0.3
Epoch: 2925 	Training Loss: 1.141928 	Validation Loss: 1.318472 	 time: 0.3
Epoch: 2926 	Training Loss: 1.141932 	Validation Loss: 1.318585 	 time: 0.3
Epoch: 2927 	Training Loss: 1.141912 	Validation Loss: 1.318521 	 time: 0.3
Epoch: 2928 	Training Loss: 1.141931 	Validation Loss: 1.318245 	 time: 0.3
Epoch: 2929 	Training Loss: 1.141906 	Validation Loss: 1.317875 	 time: 0.3
Epoch: 2930 	Training Loss: 1.141886 	Validation Loss: 1.317775 	 time: 0.3
Epoch: 2931 	Training Loss: 1.141905 	Validation Loss: 1.317980 	 time: 0.3
Epoch: 2932 	Training Loss: 1.141893 	Validation Loss: 1.318560 	 time: 0.3
Epoch: 2933 	Training Loss: 1.141875 	Validation Loss: 1.319074 	 time: 0.3
Epoch: 2934 	Training Loss: 1.141867 	Validation Loss: 1.319255 	 time: 0.3
Epoch: 2935 	Training Loss: 1.141874 	Validation Loss: 1.319155 	 time: 0.3
Epoch: 2936 	Training Loss: 1.141861 	Validation Loss: 1.318883 	 time: 0.3
Epoch: 2937 	Training Loss: 1.141838 	Validation Loss: 1.318616 	 time: 0.3
Epoch: 2938 	Training Loss: 1.141832 	Validation Loss: 1.318367 	 time: 0.3
Epoch: 2939 	Training Loss: 1.141829 	Validation Loss: 1.318166 	 time: 0.3
Epoch: 2940 	Training Loss: 1.141825 	Validation Loss: 1.318199 	 time: 0.3
Epoch: 2941 	Training Loss: 1.141819 	Validation Loss: 1.318405 	 time: 0.3
Epoch: 2942 	Training Loss: 1.141825 	Validation Loss: 1.318363 	 time: 0.3
Epoch: 2943 	Training Loss: 1.141822 	Validation Loss: 1.318082 	 time: 0.3
Epoch: 2944 	Training Loss: 1.141808 	Validation Loss: 1.317789 	 time: 0.3
Epoch: 2945 	Training Loss: 1.141802 	Validation Loss: 1.317836 	 time: 0.3
Epoch: 2946 	Training Loss: 1.141803 	Validation Loss: 1.317883 	 time: 0.3
Epoch: 2947 	Training Loss: 1.141802 	Validation Loss: 1.317982 	 time: 0.3
Epoch: 2948 	Training Loss: 1.141798 	Validation Loss: 1.318110 	 time: 0.3
Epoch: 2949 	Training Loss: 1.141794 	Validation Loss: 1.318180 	 time: 0.3
Epoch: 2950 	Training Loss: 1.141794 	Validation Loss: 1.318273 	 time: 0.3
Epoch: 2951 	Training Loss: 1.141782 	Validation Loss: 1.318282 	 time: 0.3
Epoch: 2952 	Training Loss: 1.141780 	Validation Loss: 1.318115 	 time: 0.3
Epoch: 2953 	Training Loss: 1.141775 	Validation Loss: 1.317953 	 time: 0.3
Epoch: 2954 	Training Loss: 1.141771 	Validation Loss: 1.317981 	 time: 0.3
Epoch: 2955 	Training Loss: 1.141766 	Validation Loss: 1.318010 	 time: 0.3
Epoch: 2956 	Training Loss: 1.141763 	Validation Loss: 1.318002 	 time: 0.3
Epoch: 2957 	Training Loss: 1.141757 	Validation Loss: 1.317946 	 time: 0.3
Epoch: 2958 	Training Loss: 1.141755 	Validation Loss: 1.317866 	 time: 0.3
Epoch: 2959 	Training Loss: 1.141751 	Validation Loss: 1.317810 	 time: 0.3
Epoch: 2960 	Training Loss: 1.141748 	Validation Loss: 1.317706 	 time: 0.3
Epoch: 2961 	Training Loss: 1.141745 	Validation Loss: 1.317654 	 time: 0.3
Epoch: 2962 	Training Loss: 1.141742 	Validation Loss: 1.317730 	 time: 0.3
Epoch: 2963 	Training Loss: 1.141738 	Validation Loss: 1.317727 	 time: 0.3
Epoch: 2964 	Training Loss: 1.141732 	Validation Loss: 1.317669 	 time: 0.3
Epoch: 2965 	Training Loss: 1.141721 	Validation Loss: 1.317524 	 time: 0.3
Epoch: 2966 	Training Loss: 1.141698 	Validation Loss: 1.317280 	 time: 0.3
Epoch: 2967 	Training Loss: 1.141658 	Validation Loss: 1.317257 	 time: 0.3
Epoch: 2968 	Training Loss: 1.141647 	Validation Loss: 1.317321 	 time: 0.3
Epoch: 2969 	Training Loss: 1.141641 	Validation Loss: 1.317182 	 time: 0.3
Epoch: 2970 	Training Loss: 1.141619 	Validation Loss: 1.316931 	 time: 0.3
Epoch: 2971 	Training Loss: 1.141600 	Validation Loss: 1.316833 	 time: 0.3
Epoch: 2972 	Training Loss: 1.141590 	Validation Loss: 1.316814 	 time: 0.4
Epoch: 2973 	Training Loss: 1.141585 	Validation Loss: 1.316899 	 time: 0.3
Epoch: 2974 	Training Loss: 1.141579 	Validation Loss: 1.316873 	 time: 0.3
Epoch: 2975 	Training Loss: 1.141570 	Validation Loss: 1.316615 	 time: 0.3
Epoch: 2976 	Training Loss: 1.141562 	Validation Loss: 1.316339 	 time: 0.3
Epoch: 2977 	Training Loss: 1.141558 	Validation Loss: 1.316231 	 time: 0.3
Epoch: 2978 	Training Loss: 1.141555 	Validation Loss: 1.316276 	 time: 0.3
Epoch: 2979 	Training Loss: 1.141549 	Validation Loss: 1.316303 	 time: 0.3
Epoch: 2980 	Training Loss: 1.141545 	Validation Loss: 1.316196 	 time: 0.3
Epoch: 2981 	Training Loss: 1.141541 	Validation Loss: 1.316193 	 time: 0.3
Epoch: 2982 	Training Loss: 1.141537 	Validation Loss: 1.316247 	 time: 0.3
Epoch: 2983 	Training Loss: 1.141530 	Validation Loss: 1.316317 	 time: 0.3
Epoch: 2984 	Training Loss: 1.141520 	Validation Loss: 1.316265 	 time: 0.3
Epoch: 2985 	Training Loss: 1.141507 	Validation Loss: 1.316339 	 time: 0.3
Epoch: 2986 	Training Loss: 1.141506 	Validation Loss: 1.316482 	 time: 0.3
Epoch: 2987 	Training Loss: 1.141498 	Validation Loss: 1.316607 	 time: 0.3
Epoch: 2988 	Training Loss: 1.141494 	Validation Loss: 1.316926 	 time: 0.3
Epoch: 2989 	Training Loss: 1.141478 	Validation Loss: 1.316957 	 time: 0.3
Epoch: 2990 	Training Loss: 1.141460 	Validation Loss: 1.317059 	 time: 0.3
Epoch: 2991 	Training Loss: 1.141414 	Validation Loss: 1.317185 	 time: 0.3
Epoch: 2992 	Training Loss: 1.141296 	Validation Loss: 1.317201 	 time: 0.3
Epoch: 2993 	Training Loss: 1.141315 	Validation Loss: 1.317122 	 time: 0.3
Epoch: 2994 	Training Loss: 1.141297 	Validation Loss: 1.316908 	 time: 0.3
Epoch: 2995 	Training Loss: 1.141266 	Validation Loss: 1.316771 	 time: 0.3
Epoch: 2996 	Training Loss: 1.141257 	Validation Loss: 1.316804 	 time: 0.3
Epoch: 2997 	Training Loss: 1.141242 	Validation Loss: 1.316868 	 time: 0.3
Epoch: 2998 	Training Loss: 1.141194 	Validation Loss: 1.316901 	 time: 0.3
Epoch: 2999 	Training Loss: 1.141248 	Validation Loss: 1.318077 	 time: 0.3
Epoch: 3000 	Training Loss: 1.141369 	Validation Loss: 1.317497 	 time: 0.3
Epoch: 3001 	Training Loss: 1.141258 	Validation Loss: 1.317008 	 time: 0.3
Epoch: 3002 	Training Loss: 1.141307 	Validation Loss: 1.317329 	 time: 0.3
Epoch: 3003 	Training Loss: 1.141332 	Validation Loss: 1.316593 	 time: 0.3
Epoch: 3004 	Training Loss: 1.141276 	Validation Loss: 1.316521 	 time: 0.3
Epoch: 3005 	Training Loss: 1.141255 	Validation Loss: 1.316302 	 time: 0.3
Epoch: 3006 	Training Loss: 1.141252 	Validation Loss: 1.315560 	 time: 0.3
Epoch: 3007 	Training Loss: 1.141218 	Validation Loss: 1.314722 	 time: 0.3
Epoch: 3008 	Training Loss: 1.141221 	Validation Loss: 1.315031 	 time: 0.3
Epoch: 3009 	Training Loss: 1.141202 	Validation Loss: 1.316127 	 time: 0.3
Epoch: 3010 	Training Loss: 1.141183 	Validation Loss: 1.316213 	 time: 0.3
Epoch: 3011 	Training Loss: 1.141145 	Validation Loss: 1.315900 	 time: 0.3
Epoch: 3012 	Training Loss: 1.141164 	Validation Loss: 1.316535 	 time: 0.3
Epoch: 3013 	Training Loss: 1.141214 	Validation Loss: 1.316105 	 time: 0.3
Epoch: 3014 	Training Loss: 1.141167 	Validation Loss: 1.315972 	 time: 0.3
Epoch: 3015 	Training Loss: 1.141163 	Validation Loss: 1.315894 	 time: 0.3
Epoch: 3016 	Training Loss: 1.141190 	Validation Loss: 1.315170 	 time: 0.3
Epoch: 3017 	Training Loss: 1.141144 	Validation Loss: 1.314863 	 time: 0.3
Epoch: 3018 	Training Loss: 1.141156 	Validation Loss: 1.315186 	 time: 0.3
Epoch: 3019 	Training Loss: 1.141140 	Validation Loss: 1.314872 	 time: 0.3
Epoch: 3020 	Training Loss: 1.141134 	Validation Loss: 1.314174 	 time: 0.3
Epoch: 3021 	Training Loss: 1.141139 	Validation Loss: 1.313716 	 time: 0.3
Epoch: 3022 	Training Loss: 1.141104 	Validation Loss: 1.313601 	 time: 0.3
Epoch: 3023 	Training Loss: 1.141105 	Validation Loss: 1.313539 	 time: 0.3
Epoch: 3024 	Training Loss: 1.141103 	Validation Loss: 1.314008 	 time: 0.3
Epoch: 3025 	Training Loss: 1.141086 	Validation Loss: 1.314203 	 time: 0.3
Epoch: 3026 	Training Loss: 1.141098 	Validation Loss: 1.314581 	 time: 0.3
Epoch: 3027 	Training Loss: 1.141081 	Validation Loss: 1.314219 	 time: 0.3
Epoch: 3028 	Training Loss: 1.141066 	Validation Loss: 1.314364 	 time: 0.3
Epoch: 3029 	Training Loss: 1.141078 	Validation Loss: 1.315199 	 time: 0.3
Epoch: 3030 	Training Loss: 1.141067 	Validation Loss: 1.315264 	 time: 0.3
Epoch: 3031 	Training Loss: 1.141069 	Validation Loss: 1.315146 	 time: 0.3
Epoch: 3032 	Training Loss: 1.141060 	Validation Loss: 1.314640 	 time: 0.3
Epoch: 3033 	Training Loss: 1.141052 	Validation Loss: 1.314397 	 time: 0.3
Epoch: 3034 	Training Loss: 1.141055 	Validation Loss: 1.314769 	 time: 0.3
Epoch: 3035 	Training Loss: 1.141060 	Validation Loss: 1.314623 	 time: 0.3
Epoch: 3036 	Training Loss: 1.141047 	Validation Loss: 1.314655 	 time: 0.3
Epoch: 3037 	Training Loss: 1.141046 	Validation Loss: 1.314696 	 time: 0.3
Epoch: 3038 	Training Loss: 1.141048 	Validation Loss: 1.314441 	 time: 0.3
Epoch: 3039 	Training Loss: 1.141048 	Validation Loss: 1.314797 	 time: 0.3
Epoch: 3040 	Training Loss: 1.141057 	Validation Loss: 1.314615 	 time: 0.3
Epoch: 3041 	Training Loss: 1.141038 	Validation Loss: 1.314337 	 time: 0.3
Epoch: 3042 	Training Loss: 1.141049 	Validation Loss: 1.314561 	 time: 0.3
Epoch: 3043 	Training Loss: 1.141082 	Validation Loss: 1.314213 	 time: 0.3
Epoch: 3044 	Training Loss: 1.141036 	Validation Loss: 1.314333 	 time: 0.3
Epoch: 3045 	Training Loss: 1.141051 	Validation Loss: 1.314877 	 time: 0.3
Epoch: 3046 	Training Loss: 1.141121 	Validation Loss: 1.314147 	 time: 0.3
Epoch: 3047 	Training Loss: 1.141030 	Validation Loss: 1.313498 	 time: 0.3
Epoch: 3048 	Training Loss: 1.141140 	Validation Loss: 1.314746 	 time: 0.3
Epoch: 3049 	Training Loss: 1.141206 	Validation Loss: 1.314564 	 time: 0.3
Epoch: 3050 	Training Loss: 1.141094 	Validation Loss: 1.313832 	 time: 0.3
Epoch: 3051 	Training Loss: 1.141179 	Validation Loss: 1.313491 	 time: 0.3
Epoch: 3052 	Training Loss: 1.141136 	Validation Loss: 1.313508 	 time: 0.3
Epoch: 3053 	Training Loss: 1.141146 	Validation Loss: 1.312951 	 time: 0.3
Epoch: 3054 	Training Loss: 1.141119 	Validation Loss: 1.313251 	 time: 0.3
Epoch: 3055 	Training Loss: 1.141111 	Validation Loss: 1.313933 	 time: 0.3
Epoch: 3056 	Training Loss: 1.141106 	Validation Loss: 1.313734 	 time: 0.3
Epoch: 3057 	Training Loss: 1.141067 	Validation Loss: 1.313126 	 time: 0.3
Epoch: 3058 	Training Loss: 1.141082 	Validation Loss: 1.313394 	 time: 0.3
Epoch: 3059 	Training Loss: 1.141032 	Validation Loss: 1.314114 	 time: 0.3
Epoch: 3060 	Training Loss: 1.141079 	Validation Loss: 1.313640 	 time: 0.3
Epoch: 3061 	Training Loss: 1.141094 	Validation Loss: 1.313627 	 time: 0.3
Epoch: 3062 	Training Loss: 1.141116 	Validation Loss: 1.313657 	 time: 0.3
Epoch: 3063 	Training Loss: 1.141089 	Validation Loss: 1.313655 	 time: 0.3
Epoch: 3064 	Training Loss: 1.141096 	Validation Loss: 1.313794 	 time: 0.3
Epoch: 3065 	Training Loss: 1.141092 	Validation Loss: 1.313701 	 time: 0.3
Epoch: 3066 	Training Loss: 1.141087 	Validation Loss: 1.313919 	 time: 0.3
Epoch: 3067 	Training Loss: 1.141030 	Validation Loss: 1.314213 	 time: 0.3
Epoch: 3068 	Training Loss: 1.141033 	Validation Loss: 1.314744 	 time: 0.3
Epoch: 3069 	Training Loss: 1.141192 	Validation Loss: 1.313833 	 time: 0.3
Epoch: 3070 	Training Loss: 1.141101 	Validation Loss: 1.311810 	 time: 0.3
Epoch: 3071 	Training Loss: 1.141100 	Validation Loss: 1.312214 	 time: 0.3
Epoch: 3072 	Training Loss: 1.141143 	Validation Loss: 1.313184 	 time: 0.3
Epoch: 3073 	Training Loss: 1.141039 	Validation Loss: 1.314323 	 time: 0.3
Epoch: 3074 	Training Loss: 1.141070 	Validation Loss: 1.313427 	 time: 0.3
Epoch: 3075 	Training Loss: 1.141006 	Validation Loss: 1.312320 	 time: 0.3
Epoch: 3076 	Training Loss: 1.140973 	Validation Loss: 1.312792 	 time: 0.3
Epoch: 3077 	Training Loss: 1.141020 	Validation Loss: 1.314617 	 time: 0.3
Epoch: 3078 	Training Loss: 1.141365 	Validation Loss: 1.314848 	 time: 0.3
Epoch: 3079 	Training Loss: 1.141155 	Validation Loss: 1.312607 	 time: 0.3
Epoch: 3080 	Training Loss: 1.141171 	Validation Loss: 1.312070 	 time: 0.3
Epoch: 3081 	Training Loss: 1.141106 	Validation Loss: 1.312542 	 time: 0.3
Epoch: 3082 	Training Loss: 1.141137 	Validation Loss: 1.313971 	 time: 0.3
Epoch: 3083 	Training Loss: 1.141082 	Validation Loss: 1.312915 	 time: 0.3
Epoch: 3084 	Training Loss: 1.141202 	Validation Loss: 1.312569 	 time: 0.3
Epoch: 3085 	Training Loss: 1.141067 	Validation Loss: 1.311870 	 time: 0.3
Epoch: 3086 	Training Loss: 1.141295 	Validation Loss: 1.313855 	 time: 0.3
Epoch: 3087 	Training Loss: 1.141311 	Validation Loss: 1.315233 	 time: 0.3
Epoch: 3088 	Training Loss: 1.141124 	Validation Loss: 1.313500 	 time: 0.3
Epoch: 3089 	Training Loss: 1.141359 	Validation Loss: 1.314054 	 time: 0.3
Epoch: 3090 	Training Loss: 1.141440 	Validation Loss: 1.314861 	 time: 0.3
Epoch: 3091 	Training Loss: 1.141220 	Validation Loss: 1.312647 	 time: 0.3
Epoch: 3092 	Training Loss: 1.141241 	Validation Loss: 1.314453 	 time: 0.3
Epoch: 3093 	Training Loss: 1.141146 	Validation Loss: 1.314881 	 time: 0.3
Epoch: 3094 	Training Loss: 1.141204 	Validation Loss: 1.315199 	 time: 0.3
Epoch: 3095 	Training Loss: 1.141146 	Validation Loss: 1.314705 	 time: 0.3
Epoch: 3096 	Training Loss: 1.141118 	Validation Loss: 1.314631 	 time: 0.3
Epoch: 3097 	Training Loss: 1.141021 	Validation Loss: 1.314370 	 time: 0.3
Epoch: 3098 	Training Loss: 1.141049 	Validation Loss: 1.315061 	 time: 0.3
Epoch: 3099 	Training Loss: 1.141062 	Validation Loss: 1.314371 	 time: 0.3
Epoch: 3100 	Training Loss: 1.141165 	Validation Loss: 1.314940 	 time: 0.3
Epoch: 3101 	Training Loss: 1.140948 	Validation Loss: 1.315394 	 time: 0.3
Epoch: 3102 	Training Loss: 1.140993 	Validation Loss: 1.313132 	 time: 0.3
Epoch: 3103 	Training Loss: 1.140987 	Validation Loss: 1.312317 	 time: 0.3
Epoch: 3104 	Training Loss: 1.140977 	Validation Loss: 1.312896 	 time: 0.3
Epoch: 3105 	Training Loss: 1.141001 	Validation Loss: 1.314025 	 time: 0.3
Epoch: 3106 	Training Loss: 1.140978 	Validation Loss: 1.313650 	 time: 0.3
Epoch: 3107 	Training Loss: 1.140979 	Validation Loss: 1.312809 	 time: 0.3
Epoch: 3108 	Training Loss: 1.140909 	Validation Loss: 1.313051 	 time: 0.3
Epoch: 3109 	Training Loss: 1.140862 	Validation Loss: 1.314419 	 time: 0.3
Epoch: 3110 	Training Loss: 1.140897 	Validation Loss: 1.313501 	 time: 0.3
Epoch: 3111 	Training Loss: 1.140962 	Validation Loss: 1.313285 	 time: 0.3
Epoch: 3112 	Training Loss: 1.140841 	Validation Loss: 1.313744 	 time: 0.3
Epoch: 3113 	Training Loss: 1.140895 	Validation Loss: 1.314566 	 time: 0.2
Epoch: 3114 	Training Loss: 1.140778 	Validation Loss: 1.316394 	 time: 0.3
Epoch: 3115 	Training Loss: 1.140754 	Validation Loss: 1.317029 	 time: 0.3
Epoch: 3116 	Training Loss: 1.140881 	Validation Loss: 1.315334 	 time: 0.3
Epoch: 3117 	Training Loss: 1.140736 	Validation Loss: 1.314899 	 time: 0.3
Epoch: 3118 	Training Loss: 1.140856 	Validation Loss: 1.315727 	 time: 0.3
Epoch: 3119 	Training Loss: 1.140726 	Validation Loss: 1.316108 	 time: 0.3
Epoch: 3120 	Training Loss: 1.140854 	Validation Loss: 1.314383 	 time: 0.3
Epoch: 3121 	Training Loss: 1.140752 	Validation Loss: 1.312932 	 time: 0.3
Epoch: 3122 	Training Loss: 1.140817 	Validation Loss: 1.312746 	 time: 0.3
Epoch: 3123 	Training Loss: 1.140722 	Validation Loss: 1.313843 	 time: 0.3
Epoch: 3124 	Training Loss: 1.140754 	Validation Loss: 1.314036 	 time: 0.3
Epoch: 3125 	Training Loss: 1.140744 	Validation Loss: 1.312701 	 time: 0.3
Epoch: 3126 	Training Loss: 1.140734 	Validation Loss: 1.312660 	 time: 0.3
Epoch: 3127 	Training Loss: 1.140723 	Validation Loss: 1.314266 	 time: 0.3
Epoch: 3128 	Training Loss: 1.140694 	Validation Loss: 1.314863 	 time: 0.3
Epoch: 3129 	Training Loss: 1.140726 	Validation Loss: 1.314821 	 time: 0.3
Epoch: 3130 	Training Loss: 1.140724 	Validation Loss: 1.314056 	 time: 0.3
Epoch: 3131 	Training Loss: 1.140684 	Validation Loss: 1.313876 	 time: 0.3
Epoch: 3132 	Training Loss: 1.140758 	Validation Loss: 1.315198 	 time: 0.3
Epoch: 3133 	Training Loss: 1.140755 	Validation Loss: 1.315231 	 time: 0.3
Epoch: 3134 	Training Loss: 1.140780 	Validation Loss: 1.313991 	 time: 0.3
Epoch: 3135 	Training Loss: 1.140695 	Validation Loss: 1.313578 	 time: 0.3
Epoch: 3136 	Training Loss: 1.140814 	Validation Loss: 1.314651 	 time: 0.3
Epoch: 3137 	Training Loss: 1.140943 	Validation Loss: 1.314781 	 time: 0.3
Epoch: 3138 	Training Loss: 1.140859 	Validation Loss: 1.312831 	 time: 0.3
Epoch: 3139 	Training Loss: 1.140924 	Validation Loss: 1.314445 	 time: 0.3
Epoch: 3140 	Training Loss: 1.140915 	Validation Loss: 1.313271 	 time: 0.3
Epoch: 3141 	Training Loss: 1.140736 	Validation Loss: 1.315045 	 time: 0.3
Epoch: 3142 	Training Loss: 1.140841 	Validation Loss: 1.313581 	 time: 0.3
Epoch: 3143 	Training Loss: 1.140784 	Validation Loss: 1.313961 	 time: 0.3
Epoch: 3144 	Training Loss: 1.140754 	Validation Loss: 1.312767 	 time: 0.3
Epoch: 3145 	Training Loss: 1.140732 	Validation Loss: 1.312808 	 time: 0.3
Epoch: 3146 	Training Loss: 1.140734 	Validation Loss: 1.313133 	 time: 0.3
Epoch: 3147 	Training Loss: 1.140733 	Validation Loss: 1.312015 	 time: 0.3
Epoch: 3148 	Training Loss: 1.140661 	Validation Loss: 1.311959 	 time: 0.3
Epoch: 3149 	Training Loss: 1.140685 	Validation Loss: 1.313525 	 time: 0.3
Epoch: 3150 	Training Loss: 1.140651 	Validation Loss: 1.314356 	 time: 0.3
Epoch: 3151 	Training Loss: 1.140668 	Validation Loss: 1.312965 	 time: 0.3
Epoch: 3152 	Training Loss: 1.140629 	Validation Loss: 1.312479 	 time: 0.3
Epoch: 3153 	Training Loss: 1.140613 	Validation Loss: 1.312629 	 time: 0.3
Epoch: 3154 	Training Loss: 1.140598 	Validation Loss: 1.313278 	 time: 0.3
Epoch: 3155 	Training Loss: 1.140595 	Validation Loss: 1.313677 	 time: 0.3
Epoch: 3156 	Training Loss: 1.140624 	Validation Loss: 1.312064 	 time: 0.3
Epoch: 3157 	Training Loss: 1.140596 	Validation Loss: 1.310840 	 time: 0.3
Epoch: 3158 	Training Loss: 1.140583 	Validation Loss: 1.311175 	 time: 0.3
Epoch: 3159 	Training Loss: 1.140574 	Validation Loss: 1.311766 	 time: 0.3
Epoch: 3160 	Training Loss: 1.140567 	Validation Loss: 1.312791 	 time: 0.3
Epoch: 3161 	Training Loss: 1.140550 	Validation Loss: 1.312881 	 time: 0.3
Epoch: 3162 	Training Loss: 1.140562 	Validation Loss: 1.312597 	 time: 0.3
Epoch: 3163 	Training Loss: 1.140547 	Validation Loss: 1.312313 	 time: 0.3
Epoch: 3164 	Training Loss: 1.140535 	Validation Loss: 1.311921 	 time: 0.3
Epoch: 3165 	Training Loss: 1.140557 	Validation Loss: 1.312871 	 time: 0.3
Epoch: 3166 	Training Loss: 1.140581 	Validation Loss: 1.312773 	 time: 0.3
Epoch: 3167 	Training Loss: 1.140561 	Validation Loss: 1.312952 	 time: 0.3
Epoch: 3168 	Training Loss: 1.140531 	Validation Loss: 1.313165 	 time: 0.3
Epoch: 3169 	Training Loss: 1.140549 	Validation Loss: 1.312420 	 time: 0.3
Epoch: 3170 	Training Loss: 1.140554 	Validation Loss: 1.313244 	 time: 0.3
Epoch: 3171 	Training Loss: 1.140545 	Validation Loss: 1.313660 	 time: 0.3
Epoch: 3172 	Training Loss: 1.140539 	Validation Loss: 1.313583 	 time: 0.3
Epoch: 3173 	Training Loss: 1.140560 	Validation Loss: 1.313029 	 time: 0.3
Epoch: 3174 	Training Loss: 1.140549 	Validation Loss: 1.312223 	 time: 0.3
Epoch: 3175 	Training Loss: 1.140526 	Validation Loss: 1.312060 	 time: 0.3
Epoch: 3176 	Training Loss: 1.140526 	Validation Loss: 1.312391 	 time: 0.3
Epoch: 3177 	Training Loss: 1.140514 	Validation Loss: 1.312358 	 time: 0.3
Epoch: 3178 	Training Loss: 1.140527 	Validation Loss: 1.312201 	 time: 0.3
Epoch: 3179 	Training Loss: 1.140511 	Validation Loss: 1.311957 	 time: 0.3
Epoch: 3180 	Training Loss: 1.140500 	Validation Loss: 1.311847 	 time: 0.3
Epoch: 3181 	Training Loss: 1.140499 	Validation Loss: 1.312382 	 time: 0.3
Epoch: 3182 	Training Loss: 1.140476 	Validation Loss: 1.313096 	 time: 0.3
Epoch: 3183 	Training Loss: 1.140481 	Validation Loss: 1.313079 	 time: 0.3
Epoch: 3184 	Training Loss: 1.140482 	Validation Loss: 1.313213 	 time: 0.3
Epoch: 3185 	Training Loss: 1.140483 	Validation Loss: 1.312894 	 time: 0.3
Epoch: 3186 	Training Loss: 1.140490 	Validation Loss: 1.313221 	 time: 0.3
Epoch: 3187 	Training Loss: 1.140501 	Validation Loss: 1.312641 	 time: 0.3
Epoch: 3188 	Training Loss: 1.140483 	Validation Loss: 1.312902 	 time: 0.3
Epoch: 3189 	Training Loss: 1.140475 	Validation Loss: 1.312775 	 time: 0.3
Epoch: 3190 	Training Loss: 1.140458 	Validation Loss: 1.312560 	 time: 0.3
Epoch: 3191 	Training Loss: 1.140477 	Validation Loss: 1.312901 	 time: 0.3
Epoch: 3192 	Training Loss: 1.140544 	Validation Loss: 1.312084 	 time: 0.3
Epoch: 3193 	Training Loss: 1.140484 	Validation Loss: 1.312162 	 time: 0.3
Epoch: 3194 	Training Loss: 1.140476 	Validation Loss: 1.313015 	 time: 0.3
Epoch: 3195 	Training Loss: 1.140537 	Validation Loss: 1.312606 	 time: 0.3
Epoch: 3196 	Training Loss: 1.140469 	Validation Loss: 1.312886 	 time: 0.3
Epoch: 3197 	Training Loss: 1.140459 	Validation Loss: 1.313023 	 time: 0.3
Epoch: 3198 	Training Loss: 1.140451 	Validation Loss: 1.312727 	 time: 0.3
Epoch: 3199 	Training Loss: 1.140482 	Validation Loss: 1.313223 	 time: 0.3
Epoch: 3200 	Training Loss: 1.140537 	Validation Loss: 1.312720 	 time: 0.3
Epoch: 3201 	Training Loss: 1.140509 	Validation Loss: 1.312759 	 time: 0.3
Epoch: 3202 	Training Loss: 1.140469 	Validation Loss: 1.312848 	 time: 0.3
Epoch: 3203 	Training Loss: 1.140494 	Validation Loss: 1.312010 	 time: 0.3
Epoch: 3204 	Training Loss: 1.140497 	Validation Loss: 1.312716 	 time: 0.3
Epoch: 3205 	Training Loss: 1.140548 	Validation Loss: 1.312568 	 time: 0.3
Epoch: 3206 	Training Loss: 1.140507 	Validation Loss: 1.312424 	 time: 0.3
Epoch: 3207 	Training Loss: 1.140538 	Validation Loss: 1.313237 	 time: 0.3
Epoch: 3208 	Training Loss: 1.140719 	Validation Loss: 1.312188 	 time: 0.3
Epoch: 3209 	Training Loss: 1.140561 	Validation Loss: 1.311213 	 time: 0.3
Epoch: 3210 	Training Loss: 1.140792 	Validation Loss: 1.312506 	 time: 0.3
Epoch: 3211 	Training Loss: 1.140675 	Validation Loss: 1.311613 	 time: 0.3
Epoch: 3212 	Training Loss: 1.140719 	Validation Loss: 1.310056 	 time: 0.3
Validation loss decreased from 1.310168 to 1.310056. Model was saved
Epoch: 3213 	Training Loss: 1.140651 	Validation Loss: 1.309514 	 time: 0.3
Validation loss decreased from 1.310056 to 1.309514. Model was saved
Epoch: 3214 	Training Loss: 1.140806 	Validation Loss: 1.313757 	 time: 0.3
Epoch: 3215 	Training Loss: 1.141275 	Validation Loss: 1.312302 	 time: 0.3
Epoch: 3216 	Training Loss: 1.140842 	Validation Loss: 1.310592 	 time: 0.3
Epoch: 3217 	Training Loss: 1.140901 	Validation Loss: 1.312427 	 time: 0.3
Epoch: 3218 	Training Loss: 1.140880 	Validation Loss: 1.313092 	 time: 0.3
Epoch: 3219 	Training Loss: 1.140857 	Validation Loss: 1.312786 	 time: 0.3
Epoch: 3220 	Training Loss: 1.140860 	Validation Loss: 1.314223 	 time: 0.3
Epoch: 3221 	Training Loss: 1.140884 	Validation Loss: 1.315591 	 time: 0.3
Epoch: 3222 	Training Loss: 1.140947 	Validation Loss: 1.313094 	 time: 0.3
Epoch: 3223 	Training Loss: 1.140831 	Validation Loss: 1.310154 	 time: 0.3
Epoch: 3224 	Training Loss: 1.140953 	Validation Loss: 1.311816 	 time: 0.3
Epoch: 3225 	Training Loss: 1.140945 	Validation Loss: 1.312556 	 time: 0.3
Epoch: 3226 	Training Loss: 1.140875 	Validation Loss: 1.313672 	 time: 0.3
Epoch: 3227 	Training Loss: 1.140819 	Validation Loss: 1.312395 	 time: 0.3
Epoch: 3228 	Training Loss: 1.140806 	Validation Loss: 1.312631 	 time: 0.3
Epoch: 3229 	Training Loss: 1.140729 	Validation Loss: 1.314337 	 time: 0.3
Epoch: 3230 	Training Loss: 1.140771 	Validation Loss: 1.318287 	 time: 0.3
Epoch: 3231 	Training Loss: 1.141026 	Validation Loss: 1.314117 	 time: 0.3
Epoch: 3232 	Training Loss: 1.140750 	Validation Loss: 1.311049 	 time: 0.3
Epoch: 3233 	Training Loss: 1.140872 	Validation Loss: 1.310095 	 time: 0.3
Epoch: 3234 	Training Loss: 1.140863 	Validation Loss: 1.312150 	 time: 0.3
Epoch: 3235 	Training Loss: 1.140907 	Validation Loss: 1.310455 	 time: 0.3
Epoch: 3236 	Training Loss: 1.140764 	Validation Loss: 1.309961 	 time: 0.3
Epoch: 3237 	Training Loss: 1.140832 	Validation Loss: 1.309873 	 time: 0.3
Epoch: 3238 	Training Loss: 1.140693 	Validation Loss: 1.310749 	 time: 0.3
Epoch: 3239 	Training Loss: 1.140741 	Validation Loss: 1.309103 	 time: 0.3
Validation loss decreased from 1.309514 to 1.309103. Model was saved
Epoch: 3240 	Training Loss: 1.140670 	Validation Loss: 1.309653 	 time: 0.3
Epoch: 3241 	Training Loss: 1.140620 	Validation Loss: 1.310875 	 time: 0.3
Epoch: 3242 	Training Loss: 1.140583 	Validation Loss: 1.311123 	 time: 0.3
Epoch: 3243 	Training Loss: 1.140569 	Validation Loss: 1.312191 	 time: 0.3
Epoch: 3244 	Training Loss: 1.140582 	Validation Loss: 1.311763 	 time: 0.3
Epoch: 3245 	Training Loss: 1.140540 	Validation Loss: 1.311959 	 time: 0.3
Epoch: 3246 	Training Loss: 1.140576 	Validation Loss: 1.309938 	 time: 0.3
Epoch: 3247 	Training Loss: 1.140491 	Validation Loss: 1.309152 	 time: 0.3
Epoch: 3248 	Training Loss: 1.140506 	Validation Loss: 1.307877 	 time: 0.3
Validation loss decreased from 1.309103 to 1.307877. Model was saved
Epoch: 3249 	Training Loss: 1.140493 	Validation Loss: 1.308662 	 time: 0.3
Epoch: 3250 	Training Loss: 1.140525 	Validation Loss: 1.310807 	 time: 0.3
Epoch: 3251 	Training Loss: 1.140553 	Validation Loss: 1.312327 	 time: 0.3
Epoch: 3252 	Training Loss: 1.140531 	Validation Loss: 1.310593 	 time: 0.3
Epoch: 3253 	Training Loss: 1.140514 	Validation Loss: 1.311051 	 time: 0.3
Epoch: 3254 	Training Loss: 1.140513 	Validation Loss: 1.310822 	 time: 0.3
Epoch: 3255 	Training Loss: 1.140433 	Validation Loss: 1.312496 	 time: 0.3
Epoch: 3256 	Training Loss: 1.140468 	Validation Loss: 1.311119 	 time: 0.3
Epoch: 3257 	Training Loss: 1.140429 	Validation Loss: 1.310474 	 time: 0.3
Epoch: 3258 	Training Loss: 1.140419 	Validation Loss: 1.310117 	 time: 0.3
Epoch: 3259 	Training Loss: 1.140411 	Validation Loss: 1.313832 	 time: 0.3
Epoch: 3260 	Training Loss: 1.140431 	Validation Loss: 1.312724 	 time: 0.3
Epoch: 3261 	Training Loss: 1.140365 	Validation Loss: 1.311941 	 time: 0.3
Epoch: 3262 	Training Loss: 1.140374 	Validation Loss: 1.312636 	 time: 0.3
Epoch: 3263 	Training Loss: 1.140379 	Validation Loss: 1.311746 	 time: 0.3
Epoch: 3264 	Training Loss: 1.140331 	Validation Loss: 1.310763 	 time: 0.3
Epoch: 3265 	Training Loss: 1.140326 	Validation Loss: 1.310631 	 time: 0.3
Epoch: 3266 	Training Loss: 1.140360 	Validation Loss: 1.309722 	 time: 0.3
Epoch: 3267 	Training Loss: 1.140345 	Validation Loss: 1.309848 	 time: 0.3
Epoch: 3268 	Training Loss: 1.140321 	Validation Loss: 1.310551 	 time: 0.3
Epoch: 3269 	Training Loss: 1.140295 	Validation Loss: 1.309822 	 time: 0.3
Epoch: 3270 	Training Loss: 1.140317 	Validation Loss: 1.310066 	 time: 0.3
Epoch: 3271 	Training Loss: 1.140282 	Validation Loss: 1.309216 	 time: 0.3
Epoch: 3272 	Training Loss: 1.140318 	Validation Loss: 1.309867 	 time: 0.3
Epoch: 3273 	Training Loss: 1.140296 	Validation Loss: 1.309981 	 time: 0.3
Epoch: 3274 	Training Loss: 1.140257 	Validation Loss: 1.309609 	 time: 0.3
Epoch: 3275 	Training Loss: 1.140360 	Validation Loss: 1.310934 	 time: 0.3
Epoch: 3276 	Training Loss: 1.140548 	Validation Loss: 1.309828 	 time: 0.3
Epoch: 3277 	Training Loss: 1.140363 	Validation Loss: 1.308987 	 time: 0.3
Epoch: 3278 	Training Loss: 1.140507 	Validation Loss: 1.309836 	 time: 0.3
Epoch: 3279 	Training Loss: 1.140406 	Validation Loss: 1.310159 	 time: 0.3
Epoch: 3280 	Training Loss: 1.140484 	Validation Loss: 1.309299 	 time: 0.3
Epoch: 3281 	Training Loss: 1.140374 	Validation Loss: 1.309291 	 time: 0.3
Epoch: 3282 	Training Loss: 1.140440 	Validation Loss: 1.310746 	 time: 0.3
Epoch: 3283 	Training Loss: 1.140362 	Validation Loss: 1.310711 	 time: 0.3
Epoch: 3284 	Training Loss: 1.140357 	Validation Loss: 1.310292 	 time: 0.3
Epoch: 3285 	Training Loss: 1.140399 	Validation Loss: 1.310835 	 time: 0.3
Epoch: 3286 	Training Loss: 1.140267 	Validation Loss: 1.311463 	 time: 0.3
Epoch: 3287 	Training Loss: 1.140319 	Validation Loss: 1.309519 	 time: 0.3
Epoch: 3288 	Training Loss: 1.140290 	Validation Loss: 1.311252 	 time: 0.3
Epoch: 3289 	Training Loss: 1.140296 	Validation Loss: 1.309463 	 time: 0.3
Epoch: 3290 	Training Loss: 1.140242 	Validation Loss: 1.308645 	 time: 0.3
Epoch: 3291 	Training Loss: 1.140359 	Validation Loss: 1.309345 	 time: 0.3
Epoch: 3292 	Training Loss: 1.140292 	Validation Loss: 1.309944 	 time: 0.3
Epoch: 3293 	Training Loss: 1.140264 	Validation Loss: 1.308949 	 time: 0.3
Epoch: 3294 	Training Loss: 1.140288 	Validation Loss: 1.308945 	 time: 0.3
Epoch: 3295 	Training Loss: 1.140224 	Validation Loss: 1.308666 	 time: 0.3
Epoch: 3296 	Training Loss: 1.140221 	Validation Loss: 1.308384 	 time: 0.3
Epoch: 3297 	Training Loss: 1.140261 	Validation Loss: 1.310303 	 time: 0.3
Epoch: 3298 	Training Loss: 1.140410 	Validation Loss: 1.309264 	 time: 0.3
Epoch: 3299 	Training Loss: 1.140264 	Validation Loss: 1.307588 	 time: 0.3
Validation loss decreased from 1.307877 to 1.307588. Model was saved
Epoch: 3300 	Training Loss: 1.140277 	Validation Loss: 1.308254 	 time: 0.3
Epoch: 3301 	Training Loss: 1.140255 	Validation Loss: 1.308135 	 time: 0.3
Epoch: 3302 	Training Loss: 1.140203 	Validation Loss: 1.308529 	 time: 0.3
Epoch: 3303 	Training Loss: 1.140185 	Validation Loss: 1.308378 	 time: 0.3
Epoch: 3304 	Training Loss: 1.140189 	Validation Loss: 1.306975 	 time: 0.3
Validation loss decreased from 1.307588 to 1.306975. Model was saved
Epoch: 3305 	Training Loss: 1.140197 	Validation Loss: 1.307308 	 time: 0.3
Epoch: 3306 	Training Loss: 1.140081 	Validation Loss: 1.307295 	 time: 0.3
Epoch: 3307 	Training Loss: 1.140123 	Validation Loss: 1.307439 	 time: 0.3
Epoch: 3308 	Training Loss: 1.140143 	Validation Loss: 1.308215 	 time: 0.3
Epoch: 3309 	Training Loss: 1.140161 	Validation Loss: 1.307118 	 time: 0.3
Epoch: 3310 	Training Loss: 1.140133 	Validation Loss: 1.307565 	 time: 0.3
Epoch: 3311 	Training Loss: 1.140185 	Validation Loss: 1.307560 	 time: 0.3
Epoch: 3312 	Training Loss: 1.140010 	Validation Loss: 1.307891 	 time: 0.3
Epoch: 3313 	Training Loss: 1.140080 	Validation Loss: 1.308110 	 time: 0.3
Epoch: 3314 	Training Loss: 1.140097 	Validation Loss: 1.307578 	 time: 0.3
Epoch: 3315 	Training Loss: 1.140074 	Validation Loss: 1.307650 	 time: 0.3
Epoch: 3316 	Training Loss: 1.140078 	Validation Loss: 1.307972 	 time: 0.3
Epoch: 3317 	Training Loss: 1.140049 	Validation Loss: 1.307631 	 time: 0.3
Epoch: 3318 	Training Loss: 1.140041 	Validation Loss: 1.308002 	 time: 0.3
Epoch: 3319 	Training Loss: 1.140087 	Validation Loss: 1.307712 	 time: 0.3
Epoch: 3320 	Training Loss: 1.140097 	Validation Loss: 1.307253 	 time: 0.3
Epoch: 3321 	Training Loss: 1.140077 	Validation Loss: 1.306691 	 time: 0.3
Validation loss decreased from 1.306975 to 1.306691. Model was saved
Epoch: 3322 	Training Loss: 1.140072 	Validation Loss: 1.306541 	 time: 0.3
Validation loss decreased from 1.306691 to 1.306541. Model was saved
Epoch: 3323 	Training Loss: 1.140062 	Validation Loss: 1.306419 	 time: 0.3
Validation loss decreased from 1.306541 to 1.306419. Model was saved
Epoch: 3324 	Training Loss: 1.140072 	Validation Loss: 1.306395 	 time: 0.3
Validation loss decreased from 1.306419 to 1.306395. Model was saved
Epoch: 3325 	Training Loss: 1.140073 	Validation Loss: 1.306028 	 time: 0.3
Validation loss decreased from 1.306395 to 1.306028. Model was saved
Epoch: 3326 	Training Loss: 1.140056 	Validation Loss: 1.306209 	 time: 0.3
Epoch: 3327 	Training Loss: 1.140042 	Validation Loss: 1.306294 	 time: 0.3
Epoch: 3328 	Training Loss: 1.140044 	Validation Loss: 1.306092 	 time: 0.3
Epoch: 3329 	Training Loss: 1.140043 	Validation Loss: 1.306380 	 time: 0.3
Epoch: 3330 	Training Loss: 1.140032 	Validation Loss: 1.306297 	 time: 0.3
Epoch: 3331 	Training Loss: 1.140033 	Validation Loss: 1.306392 	 time: 0.3
Epoch: 3332 	Training Loss: 1.140025 	Validation Loss: 1.306359 	 time: 0.3
Epoch: 3333 	Training Loss: 1.140022 	Validation Loss: 1.306110 	 time: 0.3
Epoch: 3334 	Training Loss: 1.140013 	Validation Loss: 1.306035 	 time: 0.3
Epoch: 3335 	Training Loss: 1.140009 	Validation Loss: 1.306249 	 time: 0.3
Epoch: 3336 	Training Loss: 1.140010 	Validation Loss: 1.306560 	 time: 0.3
Epoch: 3337 	Training Loss: 1.140003 	Validation Loss: 1.306929 	 time: 0.3
Epoch: 3338 	Training Loss: 1.140000 	Validation Loss: 1.307063 	 time: 0.3
Epoch: 3339 	Training Loss: 1.140000 	Validation Loss: 1.307030 	 time: 0.3
Epoch: 3340 	Training Loss: 1.139991 	Validation Loss: 1.306931 	 time: 0.3
Epoch: 3341 	Training Loss: 1.139983 	Validation Loss: 1.306874 	 time: 0.3
Epoch: 3342 	Training Loss: 1.139982 	Validation Loss: 1.307118 	 time: 0.3
Epoch: 3343 	Training Loss: 1.139983 	Validation Loss: 1.307504 	 time: 0.3
Epoch: 3344 	Training Loss: 1.139976 	Validation Loss: 1.307511 	 time: 0.3
Epoch: 3345 	Training Loss: 1.139979 	Validation Loss: 1.307221 	 time: 0.3
Epoch: 3346 	Training Loss: 1.139974 	Validation Loss: 1.306891 	 time: 0.3
Epoch: 3347 	Training Loss: 1.139971 	Validation Loss: 1.306792 	 time: 0.3
Epoch: 3348 	Training Loss: 1.139972 	Validation Loss: 1.307006 	 time: 0.3
Epoch: 3349 	Training Loss: 1.139968 	Validation Loss: 1.307238 	 time: 0.3
Epoch: 3350 	Training Loss: 1.139965 	Validation Loss: 1.307338 	 time: 0.3
Epoch: 3351 	Training Loss: 1.139966 	Validation Loss: 1.307237 	 time: 0.3
Epoch: 3352 	Training Loss: 1.139963 	Validation Loss: 1.307086 	 time: 0.3
Epoch: 3353 	Training Loss: 1.139960 	Validation Loss: 1.307047 	 time: 0.3
Epoch: 3354 	Training Loss: 1.139961 	Validation Loss: 1.307108 	 time: 0.3
Epoch: 3355 	Training Loss: 1.139957 	Validation Loss: 1.307212 	 time: 0.3
Epoch: 3356 	Training Loss: 1.139954 	Validation Loss: 1.307301 	 time: 0.3
Epoch: 3357 	Training Loss: 1.139950 	Validation Loss: 1.307330 	 time: 0.3
Epoch: 3358 	Training Loss: 1.139926 	Validation Loss: 1.307294 	 time: 0.3
Epoch: 3359 	Training Loss: 1.139913 	Validation Loss: 1.307276 	 time: 0.3
Epoch: 3360 	Training Loss: 1.139910 	Validation Loss: 1.307370 	 time: 0.3
Epoch: 3361 	Training Loss: 1.139909 	Validation Loss: 1.307500 	 time: 0.3
Epoch: 3362 	Training Loss: 1.139907 	Validation Loss: 1.307596 	 time: 0.3
Epoch: 3363 	Training Loss: 1.139904 	Validation Loss: 1.307602 	 time: 0.3
Epoch: 3364 	Training Loss: 1.139885 	Validation Loss: 1.307579 	 time: 0.3
Epoch: 3365 	Training Loss: 1.139864 	Validation Loss: 1.307593 	 time: 0.3
Epoch: 3366 	Training Loss: 1.139863 	Validation Loss: 1.307716 	 time: 0.3
Epoch: 3367 	Training Loss: 1.139861 	Validation Loss: 1.307809 	 time: 0.3
Epoch: 3368 	Training Loss: 1.139861 	Validation Loss: 1.307822 	 time: 0.3
Epoch: 3369 	Training Loss: 1.139849 	Validation Loss: 1.307775 	 time: 0.3
Epoch: 3370 	Training Loss: 1.139851 	Validation Loss: 1.307781 	 time: 0.3
Epoch: 3371 	Training Loss: 1.139848 	Validation Loss: 1.307810 	 time: 0.3
Epoch: 3372 	Training Loss: 1.139841 	Validation Loss: 1.307792 	 time: 0.3
Epoch: 3373 	Training Loss: 1.139843 	Validation Loss: 1.307643 	 time: 0.3
Epoch: 3374 	Training Loss: 1.139843 	Validation Loss: 1.307489 	 time: 0.3
Epoch: 3375 	Training Loss: 1.139838 	Validation Loss: 1.307500 	 time: 0.3
Epoch: 3376 	Training Loss: 1.139838 	Validation Loss: 1.307654 	 time: 0.3
Epoch: 3377 	Training Loss: 1.139838 	Validation Loss: 1.307769 	 time: 0.3
Epoch: 3378 	Training Loss: 1.139835 	Validation Loss: 1.307733 	 time: 0.3
Epoch: 3379 	Training Loss: 1.139834 	Validation Loss: 1.307609 	 time: 0.3
Epoch: 3380 	Training Loss: 1.139834 	Validation Loss: 1.307576 	 time: 0.3
Epoch: 3381 	Training Loss: 1.139833 	Validation Loss: 1.307672 	 time: 0.3
Epoch: 3382 	Training Loss: 1.139831 	Validation Loss: 1.307754 	 time: 0.3
Epoch: 3383 	Training Loss: 1.139833 	Validation Loss: 1.307868 	 time: 0.3
Epoch: 3384 	Training Loss: 1.139835 	Validation Loss: 1.307800 	 time: 0.3
Epoch: 3385 	Training Loss: 1.139845 	Validation Loss: 1.307735 	 time: 0.3
Epoch: 3386 	Training Loss: 1.139837 	Validation Loss: 1.307779 	 time: 0.3
Epoch: 3387 	Training Loss: 1.139832 	Validation Loss: 1.308003 	 time: 0.3
Epoch: 3388 	Training Loss: 1.139805 	Validation Loss: 1.308065 	 time: 0.3
Epoch: 3389 	Training Loss: 1.139793 	Validation Loss: 1.308129 	 time: 0.3
Epoch: 3390 	Training Loss: 1.139789 	Validation Loss: 1.308149 	 time: 0.3
Epoch: 3391 	Training Loss: 1.139785 	Validation Loss: 1.308201 	 time: 0.3
Epoch: 3392 	Training Loss: 1.139785 	Validation Loss: 1.308269 	 time: 0.3
Epoch: 3393 	Training Loss: 1.139785 	Validation Loss: 1.308448 	 time: 0.3
Epoch: 3394 	Training Loss: 1.139788 	Validation Loss: 1.308479 	 time: 0.3
Epoch: 3395 	Training Loss: 1.139794 	Validation Loss: 1.308700 	 time: 0.3
Epoch: 3396 	Training Loss: 1.139827 	Validation Loss: 1.308560 	 time: 0.3
Epoch: 3397 	Training Loss: 1.139825 	Validation Loss: 1.308643 	 time: 0.3
Epoch: 3398 	Training Loss: 1.139849 	Validation Loss: 1.308459 	 time: 0.3
Epoch: 3399 	Training Loss: 1.139794 	Validation Loss: 1.308505 	 time: 0.3
Epoch: 3400 	Training Loss: 1.139814 	Validation Loss: 1.308738 	 time: 0.3
Epoch: 3401 	Training Loss: 1.139945 	Validation Loss: 1.309213 	 time: 0.3
Epoch: 3402 	Training Loss: 1.139809 	Validation Loss: 1.308757 	 time: 0.3
Epoch: 3403 	Training Loss: 1.139824 	Validation Loss: 1.309321 	 time: 0.3
Epoch: 3404 	Training Loss: 1.139988 	Validation Loss: 1.309017 	 time: 0.3
Epoch: 3405 	Training Loss: 1.139837 	Validation Loss: 1.309961 	 time: 0.3
Epoch: 3406 	Training Loss: 1.139953 	Validation Loss: 1.309470 	 time: 0.3
Epoch: 3407 	Training Loss: 1.140229 	Validation Loss: 1.308515 	 time: 0.3
Epoch: 3408 	Training Loss: 1.140149 	Validation Loss: 1.309666 	 time: 0.3
Epoch: 3409 	Training Loss: 1.140026 	Validation Loss: 1.308339 	 time: 0.3
Epoch: 3410 	Training Loss: 1.140061 	Validation Loss: 1.308394 	 time: 0.3
Epoch: 3411 	Training Loss: 1.139980 	Validation Loss: 1.307179 	 time: 0.3
Epoch: 3412 	Training Loss: 1.140007 	Validation Loss: 1.306455 	 time: 0.3
Epoch: 3413 	Training Loss: 1.140070 	Validation Loss: 1.306745 	 time: 0.3
Epoch: 3414 	Training Loss: 1.140079 	Validation Loss: 1.308695 	 time: 0.3
Epoch: 3415 	Training Loss: 1.139962 	Validation Loss: 1.312171 	 time: 0.3
Epoch: 3416 	Training Loss: 1.140051 	Validation Loss: 1.308405 	 time: 0.3
Epoch: 3417 	Training Loss: 1.140039 	Validation Loss: 1.307774 	 time: 0.3
Epoch: 3418 	Training Loss: 1.140149 	Validation Loss: 1.308743 	 time: 0.3
Epoch: 3419 	Training Loss: 1.140147 	Validation Loss: 1.311472 	 time: 0.3
Epoch: 3420 	Training Loss: 1.140138 	Validation Loss: 1.306489 	 time: 0.3
Epoch: 3421 	Training Loss: 1.140252 	Validation Loss: 1.306769 	 time: 0.3
Epoch: 3422 	Training Loss: 1.140142 	Validation Loss: 1.307351 	 time: 0.3
Epoch: 3423 	Training Loss: 1.140161 	Validation Loss: 1.309533 	 time: 0.3
Epoch: 3424 	Training Loss: 1.140255 	Validation Loss: 1.307703 	 time: 0.3
Epoch: 3425 	Training Loss: 1.140056 	Validation Loss: 1.307801 	 time: 0.3
Epoch: 3426 	Training Loss: 1.140143 	Validation Loss: 1.308954 	 time: 0.3
Epoch: 3427 	Training Loss: 1.140009 	Validation Loss: 1.310510 	 time: 0.3
Epoch: 3428 	Training Loss: 1.140094 	Validation Loss: 1.309477 	 time: 0.3
Epoch: 3429 	Training Loss: 1.140059 	Validation Loss: 1.309720 	 time: 0.3
Epoch: 3430 	Training Loss: 1.140034 	Validation Loss: 1.309584 	 time: 0.3
Epoch: 3431 	Training Loss: 1.139963 	Validation Loss: 1.310232 	 time: 0.3
Epoch: 3432 	Training Loss: 1.139952 	Validation Loss: 1.309110 	 time: 0.3
Epoch: 3433 	Training Loss: 1.139982 	Validation Loss: 1.309246 	 time: 0.3
Epoch: 3434 	Training Loss: 1.139958 	Validation Loss: 1.308767 	 time: 0.3
Epoch: 3435 	Training Loss: 1.139950 	Validation Loss: 1.308421 	 time: 0.3
Epoch: 3436 	Training Loss: 1.139936 	Validation Loss: 1.307982 	 time: 0.3
Epoch: 3437 	Training Loss: 1.139938 	Validation Loss: 1.307732 	 time: 0.3
Epoch: 3438 	Training Loss: 1.139924 	Validation Loss: 1.308220 	 time: 0.3
Epoch: 3439 	Training Loss: 1.139896 	Validation Loss: 1.308779 	 time: 0.3
Epoch: 3440 	Training Loss: 1.139874 	Validation Loss: 1.308588 	 time: 0.3
Epoch: 3441 	Training Loss: 1.139856 	Validation Loss: 1.308519 	 time: 0.3
Epoch: 3442 	Training Loss: 1.139835 	Validation Loss: 1.308056 	 time: 0.3
Epoch: 3443 	Training Loss: 1.139821 	Validation Loss: 1.307611 	 time: 0.3
Epoch: 3444 	Training Loss: 1.139814 	Validation Loss: 1.307396 	 time: 0.3
Epoch: 3445 	Training Loss: 1.139810 	Validation Loss: 1.307384 	 time: 0.3
Epoch: 3446 	Training Loss: 1.139782 	Validation Loss: 1.307774 	 time: 0.3
Epoch: 3447 	Training Loss: 1.139781 	Validation Loss: 1.307889 	 time: 0.3
Epoch: 3448 	Training Loss: 1.139762 	Validation Loss: 1.307962 	 time: 0.3
Epoch: 3449 	Training Loss: 1.139761 	Validation Loss: 1.307530 	 time: 0.3
Epoch: 3450 	Training Loss: 1.139753 	Validation Loss: 1.306947 	 time: 0.3
Epoch: 3451 	Training Loss: 1.139711 	Validation Loss: 1.306562 	 time: 0.3
Epoch: 3452 	Training Loss: 1.139733 	Validation Loss: 1.307366 	 time: 0.3
Epoch: 3453 	Training Loss: 1.139707 	Validation Loss: 1.307872 	 time: 0.3
Epoch: 3454 	Training Loss: 1.139697 	Validation Loss: 1.307757 	 time: 0.3
Epoch: 3455 	Training Loss: 1.139691 	Validation Loss: 1.307155 	 time: 0.3
Epoch: 3456 	Training Loss: 1.139689 	Validation Loss: 1.307121 	 time: 0.3
Epoch: 3457 	Training Loss: 1.139672 	Validation Loss: 1.307414 	 time: 0.3
Epoch: 3458 	Training Loss: 1.139673 	Validation Loss: 1.307797 	 time: 0.3
Epoch: 3459 	Training Loss: 1.139685 	Validation Loss: 1.307465 	 time: 0.3
Epoch: 3460 	Training Loss: 1.139686 	Validation Loss: 1.307216 	 time: 0.3
Epoch: 3461 	Training Loss: 1.139682 	Validation Loss: 1.306618 	 time: 0.3
Epoch: 3462 	Training Loss: 1.139678 	Validation Loss: 1.306917 	 time: 0.3
Epoch: 3463 	Training Loss: 1.139648 	Validation Loss: 1.307259 	 time: 0.3
Epoch: 3464 	Training Loss: 1.139656 	Validation Loss: 1.307141 	 time: 0.3
Epoch: 3465 	Training Loss: 1.139644 	Validation Loss: 1.306641 	 time: 0.3
Epoch: 3466 	Training Loss: 1.139663 	Validation Loss: 1.306964 	 time: 0.3
Epoch: 3467 	Training Loss: 1.139668 	Validation Loss: 1.306505 	 time: 0.3
Epoch: 3468 	Training Loss: 1.139656 	Validation Loss: 1.307103 	 time: 0.3
Epoch: 3469 	Training Loss: 1.139644 	Validation Loss: 1.306953 	 time: 0.3
Epoch: 3470 	Training Loss: 1.139628 	Validation Loss: 1.306832 	 time: 0.3
Epoch: 3471 	Training Loss: 1.139620 	Validation Loss: 1.306825 	 time: 0.3
Epoch: 3472 	Training Loss: 1.139631 	Validation Loss: 1.306523 	 time: 0.3
Epoch: 3473 	Training Loss: 1.139643 	Validation Loss: 1.307229 	 time: 0.3
Epoch: 3474 	Training Loss: 1.139676 	Validation Loss: 1.306306 	 time: 0.3
Epoch: 3475 	Training Loss: 1.139665 	Validation Loss: 1.306691 	 time: 0.3
Epoch: 3476 	Training Loss: 1.139655 	Validation Loss: 1.307012 	 time: 0.3
Epoch: 3477 	Training Loss: 1.139636 	Validation Loss: 1.307038 	 time: 0.3
Epoch: 3478 	Training Loss: 1.139631 	Validation Loss: 1.307143 	 time: 0.3
Epoch: 3479 	Training Loss: 1.139640 	Validation Loss: 1.306882 	 time: 0.3
Epoch: 3480 	Training Loss: 1.139647 	Validation Loss: 1.307277 	 time: 0.3
Epoch: 3481 	Training Loss: 1.139629 	Validation Loss: 1.307207 	 time: 0.3
Epoch: 3482 	Training Loss: 1.139653 	Validation Loss: 1.307092 	 time: 0.3
Epoch: 3483 	Training Loss: 1.139621 	Validation Loss: 1.306499 	 time: 0.3
Epoch: 3484 	Training Loss: 1.139641 	Validation Loss: 1.306717 	 time: 0.3
Epoch: 3485 	Training Loss: 1.139635 	Validation Loss: 1.306562 	 time: 0.3
Epoch: 3486 	Training Loss: 1.139614 	Validation Loss: 1.306710 	 time: 0.3
Epoch: 3487 	Training Loss: 1.139603 	Validation Loss: 1.306883 	 time: 0.3
Epoch: 3488 	Training Loss: 1.139625 	Validation Loss: 1.307581 	 time: 0.3
Epoch: 3489 	Training Loss: 1.139595 	Validation Loss: 1.307385 	 time: 0.3
Epoch: 3490 	Training Loss: 1.139570 	Validation Loss: 1.307248 	 time: 0.3
Epoch: 3491 	Training Loss: 1.139577 	Validation Loss: 1.306926 	 time: 0.3
Epoch: 3492 	Training Loss: 1.139584 	Validation Loss: 1.306974 	 time: 0.3
Epoch: 3493 	Training Loss: 1.139569 	Validation Loss: 1.307509 	 time: 0.3
Epoch: 3494 	Training Loss: 1.139571 	Validation Loss: 1.307406 	 time: 0.3
Epoch: 3495 	Training Loss: 1.139561 	Validation Loss: 1.307378 	 time: 0.3
Epoch: 3496 	Training Loss: 1.139553 	Validation Loss: 1.307007 	 time: 0.3
Epoch: 3497 	Training Loss: 1.139541 	Validation Loss: 1.306871 	 time: 0.3
Epoch: 3498 	Training Loss: 1.139528 	Validation Loss: 1.307383 	 time: 0.3
Epoch: 3499 	Training Loss: 1.139552 	Validation Loss: 1.307569 	 time: 0.3
Epoch: 3500 	Training Loss: 1.139527 	Validation Loss: 1.307711 	 time: 0.3
Epoch: 3501 	Training Loss: 1.139530 	Validation Loss: 1.307445 	 time: 0.3
Epoch: 3502 	Training Loss: 1.139521 	Validation Loss: 1.307359 	 time: 0.3
Epoch: 3503 	Training Loss: 1.139518 	Validation Loss: 1.307670 	 time: 0.3
Epoch: 3504 	Training Loss: 1.139514 	Validation Loss: 1.307961 	 time: 0.3
Epoch: 3505 	Training Loss: 1.139514 	Validation Loss: 1.308303 	 time: 0.3
Epoch: 3506 	Training Loss: 1.139524 	Validation Loss: 1.307869 	 time: 0.3
Epoch: 3507 	Training Loss: 1.139526 	Validation Loss: 1.308112 	 time: 0.3
Epoch: 3508 	Training Loss: 1.139551 	Validation Loss: 1.307562 	 time: 0.3
Epoch: 3509 	Training Loss: 1.139566 	Validation Loss: 1.308658 	 time: 0.3
Epoch: 3510 	Training Loss: 1.139678 	Validation Loss: 1.308483 	 time: 0.3
Epoch: 3511 	Training Loss: 1.139636 	Validation Loss: 1.308148 	 time: 0.3
Epoch: 3512 	Training Loss: 1.139697 	Validation Loss: 1.310199 	 time: 0.3
Epoch: 3513 	Training Loss: 1.140066 	Validation Loss: 1.307327 	 time: 0.3
Epoch: 3514 	Training Loss: 1.140021 	Validation Loss: 1.306556 	 time: 0.3
Epoch: 3515 	Training Loss: 1.139873 	Validation Loss: 1.308910 	 time: 0.3
Epoch: 3516 	Training Loss: 1.140037 	Validation Loss: 1.311201 	 time: 0.3
Epoch: 3517 	Training Loss: 1.140437 	Validation Loss: 1.307981 	 time: 0.3
Epoch: 3518 	Training Loss: 1.140171 	Validation Loss: 1.306643 	 time: 0.3
Epoch: 3519 	Training Loss: 1.139900 	Validation Loss: 1.308700 	 time: 0.3
Epoch: 3520 	Training Loss: 1.140262 	Validation Loss: 1.310293 	 time: 0.3
Epoch: 3521 	Training Loss: 1.139806 	Validation Loss: 1.311353 	 time: 0.3
Epoch: 3522 	Training Loss: 1.140211 	Validation Loss: 1.310198 	 time: 0.3
Epoch: 3523 	Training Loss: 1.139881 	Validation Loss: 1.309004 	 time: 0.3
Epoch: 3524 	Training Loss: 1.140058 	Validation Loss: 1.308390 	 time: 0.3
Epoch: 3525 	Training Loss: 1.140060 	Validation Loss: 1.310279 	 time: 0.3
Epoch: 3526 	Training Loss: 1.139814 	Validation Loss: 1.312258 	 time: 0.3
Epoch: 3527 	Training Loss: 1.140074 	Validation Loss: 1.308211 	 time: 0.3
Epoch: 3528 	Training Loss: 1.139821 	Validation Loss: 1.309404 	 time: 0.3
Epoch: 3529 	Training Loss: 1.140174 	Validation Loss: 1.309486 	 time: 0.3
Epoch: 3530 	Training Loss: 1.140670 	Validation Loss: 1.309464 	 time: 0.3
Epoch: 3531 	Training Loss: 1.140865 	Validation Loss: 1.317465 	 time: 0.3
Epoch: 3532 	Training Loss: 1.140690 	Validation Loss: 1.314578 	 time: 0.3
Epoch: 3533 	Training Loss: 1.140979 	Validation Loss: 1.312408 	 time: 0.3
Epoch: 3534 	Training Loss: 1.141206 	Validation Loss: 1.313381 	 time: 0.3
Epoch: 3535 	Training Loss: 1.144104 	Validation Loss: 1.321244 	 time: 0.3
Epoch: 3536 	Training Loss: 1.146266 	Validation Loss: 1.318465 	 time: 0.3
Epoch: 3537 	Training Loss: 1.146389 	Validation Loss: 1.321558 	 time: 0.3
Epoch: 3538 	Training Loss: 1.147858 	Validation Loss: 1.339255 	 time: 0.3
Epoch: 3539 	Training Loss: 1.154584 	Validation Loss: 1.332764 	 time: 0.3
Epoch: 3540 	Training Loss: 1.147744 	Validation Loss: 1.330438 	 time: 0.3
Epoch: 3541 	Training Loss: 1.155810 	Validation Loss: 1.351966 	 time: 0.3
Epoch: 3542 	Training Loss: 1.174667 	Validation Loss: 1.335533 	 time: 0.3
Epoch: 3543 	Training Loss: 1.170185 	Validation Loss: 1.332982 	 time: 0.3
Epoch: 3544 	Training Loss: 1.169445 	Validation Loss: 1.334039 	 time: 0.3
Epoch: 3545 	Training Loss: 1.174125 	Validation Loss: 1.325066 	 time: 0.3
Epoch: 3546 	Training Loss: 1.173131 	Validation Loss: 1.305596 	 time: 0.3
Validation loss decreased from 1.306028 to 1.305596. Model was saved
Epoch: 3547 	Training Loss: 1.158207 	Validation Loss: 1.346153 	 time: 0.3
Epoch: 3548 	Training Loss: 1.184689 	Validation Loss: 1.330349 	 time: 0.3
Epoch: 3549 	Training Loss: 1.163890 	Validation Loss: 1.347123 	 time: 0.3
Epoch: 3550 	Training Loss: 1.183634 	Validation Loss: 1.315271 	 time: 0.3
Epoch: 3551 	Training Loss: 1.162821 	Validation Loss: 1.318706 	 time: 0.3
Epoch: 3552 	Training Loss: 1.172598 	Validation Loss: 1.327764 	 time: 0.3
Epoch: 3553 	Training Loss: 1.161753 	Validation Loss: 1.354974 	 time: 0.3
Epoch: 3554 	Training Loss: 1.177106 	Validation Loss: 1.331047 	 time: 0.3
Epoch: 3555 	Training Loss: 1.155376 	Validation Loss: 1.313199 	 time: 0.3
Epoch: 3556 	Training Loss: 1.166160 	Validation Loss: 1.312320 	 time: 0.3
Epoch: 3557 	Training Loss: 1.161325 	Validation Loss: 1.325773 	 time: 0.3
Epoch: 3558 	Training Loss: 1.153061 	Validation Loss: 1.323355 	 time: 0.3
Epoch: 3559 	Training Loss: 1.155062 	Validation Loss: 1.315910 	 time: 0.3
Epoch: 3560 	Training Loss: 1.154101 	Validation Loss: 1.323886 	 time: 0.3
Epoch: 3561 	Training Loss: 1.151256 	Validation Loss: 1.324474 	 time: 0.3
Epoch: 3562 	Training Loss: 1.149560 	Validation Loss: 1.318485 	 time: 0.3
Epoch: 3563 	Training Loss: 1.148322 	Validation Loss: 1.320845 	 time: 0.3
Epoch: 3564 	Training Loss: 1.147005 	Validation Loss: 1.324519 	 time: 0.3
Epoch: 3565 	Training Loss: 1.146370 	Validation Loss: 1.324911 	 time: 0.3
Epoch: 3566 	Training Loss: 1.145857 	Validation Loss: 1.331510 	 time: 0.3
Epoch: 3567 	Training Loss: 1.146593 	Validation Loss: 1.314970 	 time: 0.3
Epoch: 3568 	Training Loss: 1.146619 	Validation Loss: 1.319798 	 time: 0.3
Epoch: 3569 	Training Loss: 1.144859 	Validation Loss: 1.321148 	 time: 0.3
Epoch: 3570 	Training Loss: 1.145353 	Validation Loss: 1.315389 	 time: 0.3
Epoch: 3571 	Training Loss: 1.144595 	Validation Loss: 1.318515 	 time: 0.3
Epoch: 3572 	Training Loss: 1.143708 	Validation Loss: 1.330717 	 time: 0.3
Epoch: 3573 	Training Loss: 1.144323 	Validation Loss: 1.324251 	 time: 0.3
Epoch: 3574 	Training Loss: 1.143118 	Validation Loss: 1.320579 	 time: 0.3
Epoch: 3575 	Training Loss: 1.143102 	Validation Loss: 1.317620 	 time: 0.3
Epoch: 3576 	Training Loss: 1.142471 	Validation Loss: 1.309640 	 time: 0.3
Epoch: 3577 	Training Loss: 1.142273 	Validation Loss: 1.309548 	 time: 0.3
Epoch: 3578 	Training Loss: 1.141941 	Validation Loss: 1.312234 	 time: 0.3
Epoch: 3579 	Training Loss: 1.141907 	Validation Loss: 1.313217 	 time: 0.3
Epoch: 3580 	Training Loss: 1.141784 	Validation Loss: 1.313497 	 time: 0.3
Epoch: 3581 	Training Loss: 1.141551 	Validation Loss: 1.313198 	 time: 0.3
Epoch: 3582 	Training Loss: 1.141292 	Validation Loss: 1.312261 	 time: 0.3
Epoch: 3583 	Training Loss: 1.141108 	Validation Loss: 1.310763 	 time: 0.3
Epoch: 3584 	Training Loss: 1.141021 	Validation Loss: 1.309367 	 time: 0.3
Epoch: 3585 	Training Loss: 1.140920 	Validation Loss: 1.307821 	 time: 0.3
Epoch: 3586 	Training Loss: 1.140828 	Validation Loss: 1.306758 	 time: 0.3
Epoch: 3587 	Training Loss: 1.140723 	Validation Loss: 1.305875 	 time: 0.3
Epoch: 3588 	Training Loss: 1.140482 	Validation Loss: 1.305301 	 time: 0.3
Validation loss decreased from 1.305596 to 1.305301. Model was saved
Epoch: 3589 	Training Loss: 1.140386 	Validation Loss: 1.305285 	 time: 0.3
Validation loss decreased from 1.305301 to 1.305285. Model was saved
Epoch: 3590 	Training Loss: 1.140255 	Validation Loss: 1.305108 	 time: 0.3
Validation loss decreased from 1.305285 to 1.305108. Model was saved
Epoch: 3591 	Training Loss: 1.140151 	Validation Loss: 1.306592 	 time: 0.3
Epoch: 3592 	Training Loss: 1.140145 	Validation Loss: 1.305608 	 time: 0.3
Epoch: 3593 	Training Loss: 1.140151 	Validation Loss: 1.305734 	 time: 0.3
Epoch: 3594 	Training Loss: 1.139874 	Validation Loss: 1.305619 	 time: 0.3
Epoch: 3595 	Training Loss: 1.139886 	Validation Loss: 1.304705 	 time: 0.3
Validation loss decreased from 1.305108 to 1.304705. Model was saved
Epoch: 3596 	Training Loss: 1.139914 	Validation Loss: 1.304316 	 time: 0.3
Validation loss decreased from 1.304705 to 1.304316. Model was saved
Epoch: 3597 	Training Loss: 1.139743 	Validation Loss: 1.303803 	 time: 0.3
Validation loss decreased from 1.304316 to 1.303803. Model was saved
Epoch: 3598 	Training Loss: 1.139821 	Validation Loss: 1.303333 	 time: 0.3
Validation loss decreased from 1.303803 to 1.303333. Model was saved
Epoch: 3599 	Training Loss: 1.139660 	Validation Loss: 1.303639 	 time: 0.3
Epoch: 3600 	Training Loss: 1.139745 	Validation Loss: 1.303666 	 time: 0.2
Epoch: 3601 	Training Loss: 1.139631 	Validation Loss: 1.304541 	 time: 0.3
Epoch: 3602 	Training Loss: 1.139693 	Validation Loss: 1.304178 	 time: 0.3
Epoch: 3603 	Training Loss: 1.139616 	Validation Loss: 1.304871 	 time: 0.3
Epoch: 3604 	Training Loss: 1.139556 	Validation Loss: 1.304995 	 time: 0.3
Epoch: 3605 	Training Loss: 1.139543 	Validation Loss: 1.304113 	 time: 0.3
Epoch: 3606 	Training Loss: 1.139497 	Validation Loss: 1.303497 	 time: 0.3
Epoch: 3607 	Training Loss: 1.139459 	Validation Loss: 1.303101 	 time: 0.3
Validation loss decreased from 1.303333 to 1.303101. Model was saved
Epoch: 3608 	Training Loss: 1.139456 	Validation Loss: 1.303221 	 time: 0.3
Epoch: 3609 	Training Loss: 1.139385 	Validation Loss: 1.303483 	 time: 0.3
Epoch: 3610 	Training Loss: 1.139342 	Validation Loss: 1.303643 	 time: 0.3
Epoch: 3611 	Training Loss: 1.139333 	Validation Loss: 1.303152 	 time: 0.3
Epoch: 3612 	Training Loss: 1.139287 	Validation Loss: 1.303064 	 time: 0.3
Validation loss decreased from 1.303101 to 1.303064. Model was saved
Epoch: 3613 	Training Loss: 1.139241 	Validation Loss: 1.303444 	 time: 0.3
Epoch: 3614 	Training Loss: 1.139250 	Validation Loss: 1.303879 	 time: 0.3
Epoch: 3615 	Training Loss: 1.139220 	Validation Loss: 1.304024 	 time: 0.3
Epoch: 3616 	Training Loss: 1.139178 	Validation Loss: 1.303102 	 time: 0.3
Epoch: 3617 	Training Loss: 1.139149 	Validation Loss: 1.302838 	 time: 0.3
Validation loss decreased from 1.303064 to 1.302838. Model was saved
Epoch: 3618 	Training Loss: 1.139133 	Validation Loss: 1.302882 	 time: 0.3
Epoch: 3619 	Training Loss: 1.139120 	Validation Loss: 1.303154 	 time: 0.3
Epoch: 3620 	Training Loss: 1.139105 	Validation Loss: 1.302185 	 time: 0.3
Validation loss decreased from 1.302838 to 1.302185. Model was saved
Epoch: 3621 	Training Loss: 1.139076 	Validation Loss: 1.301590 	 time: 0.3
Validation loss decreased from 1.302185 to 1.301590. Model was saved
Epoch: 3622 	Training Loss: 1.139068 	Validation Loss: 1.301456 	 time: 0.3
Validation loss decreased from 1.301590 to 1.301456. Model was saved
Epoch: 3623 	Training Loss: 1.139058 	Validation Loss: 1.301603 	 time: 0.3
Epoch: 3624 	Training Loss: 1.139041 	Validation Loss: 1.301781 	 time: 0.3
Epoch: 3625 	Training Loss: 1.139032 	Validation Loss: 1.301385 	 time: 0.3
Validation loss decreased from 1.301456 to 1.301385. Model was saved
Epoch: 3626 	Training Loss: 1.139032 	Validation Loss: 1.301182 	 time: 0.3
Validation loss decreased from 1.301385 to 1.301182. Model was saved
Epoch: 3627 	Training Loss: 1.139061 	Validation Loss: 1.301362 	 time: 0.3
Epoch: 3628 	Training Loss: 1.139008 	Validation Loss: 1.301416 	 time: 0.3
Epoch: 3629 	Training Loss: 1.139050 	Validation Loss: 1.301010 	 time: 0.3
Validation loss decreased from 1.301182 to 1.301010. Model was saved
Epoch: 3630 	Training Loss: 1.139167 	Validation Loss: 1.300983 	 time: 0.3
Validation loss decreased from 1.301010 to 1.300983. Model was saved
Epoch: 3631 	Training Loss: 1.139022 	Validation Loss: 1.301741 	 time: 0.3
Epoch: 3632 	Training Loss: 1.139204 	Validation Loss: 1.300343 	 time: 0.3
Validation loss decreased from 1.300983 to 1.300343. Model was saved
Epoch: 3633 	Training Loss: 1.139074 	Validation Loss: 1.300332 	 time: 0.3
Validation loss decreased from 1.300343 to 1.300332. Model was saved
Epoch: 3634 	Training Loss: 1.139131 	Validation Loss: 1.300667 	 time: 0.3
Epoch: 3635 	Training Loss: 1.139003 	Validation Loss: 1.300953 	 time: 0.3
Epoch: 3636 	Training Loss: 1.139175 	Validation Loss: 1.300279 	 time: 0.3
Validation loss decreased from 1.300332 to 1.300279. Model was saved
Epoch: 3637 	Training Loss: 1.139063 	Validation Loss: 1.300976 	 time: 0.3
Epoch: 3638 	Training Loss: 1.139143 	Validation Loss: 1.301171 	 time: 0.3
Epoch: 3639 	Training Loss: 1.139029 	Validation Loss: 1.302755 	 time: 0.3
Epoch: 3640 	Training Loss: 1.139074 	Validation Loss: 1.301127 	 time: 0.3
Epoch: 3641 	Training Loss: 1.138945 	Validation Loss: 1.300284 	 time: 0.3
Epoch: 3642 	Training Loss: 1.139019 	Validation Loss: 1.300740 	 time: 0.3
Epoch: 3643 	Training Loss: 1.138913 	Validation Loss: 1.300531 	 time: 0.3
Epoch: 3644 	Training Loss: 1.138861 	Validation Loss: 1.300676 	 time: 0.3
Epoch: 3645 	Training Loss: 1.138886 	Validation Loss: 1.300248 	 time: 0.3
Validation loss decreased from 1.300279 to 1.300248. Model was saved
Epoch: 3646 	Training Loss: 1.138834 	Validation Loss: 1.300451 	 time: 0.3
Epoch: 3647 	Training Loss: 1.138802 	Validation Loss: 1.300557 	 time: 0.3
Epoch: 3648 	Training Loss: 1.138809 	Validation Loss: 1.300552 	 time: 0.3
Epoch: 3649 	Training Loss: 1.138804 	Validation Loss: 1.300239 	 time: 0.3
Validation loss decreased from 1.300248 to 1.300239. Model was saved
Epoch: 3650 	Training Loss: 1.138797 	Validation Loss: 1.299775 	 time: 0.3
Validation loss decreased from 1.300239 to 1.299775. Model was saved
Epoch: 3651 	Training Loss: 1.138784 	Validation Loss: 1.299883 	 time: 0.3
Epoch: 3652 	Training Loss: 1.138766 	Validation Loss: 1.300208 	 time: 0.3
Epoch: 3653 	Training Loss: 1.138763 	Validation Loss: 1.300806 	 time: 0.3
Epoch: 3654 	Training Loss: 1.138809 	Validation Loss: 1.300762 	 time: 0.3
Epoch: 3655 	Training Loss: 1.138846 	Validation Loss: 1.300707 	 time: 0.3
Epoch: 3656 	Training Loss: 1.139003 	Validation Loss: 1.300741 	 time: 0.3
Epoch: 3657 	Training Loss: 1.138863 	Validation Loss: 1.300914 	 time: 0.3
Epoch: 3658 	Training Loss: 1.138954 	Validation Loss: 1.301101 	 time: 0.3
Epoch: 3659 	Training Loss: 1.138847 	Validation Loss: 1.300691 	 time: 0.3
Epoch: 3660 	Training Loss: 1.138880 	Validation Loss: 1.300404 	 time: 0.3
Epoch: 3661 	Training Loss: 1.138825 	Validation Loss: 1.300771 	 time: 0.3
Epoch: 3662 	Training Loss: 1.138831 	Validation Loss: 1.301061 	 time: 0.3
Epoch: 3663 	Training Loss: 1.138789 	Validation Loss: 1.301105 	 time: 0.3
Epoch: 3664 	Training Loss: 1.138783 	Validation Loss: 1.301040 	 time: 0.3
Epoch: 3665 	Training Loss: 1.138824 	Validation Loss: 1.300288 	 time: 0.3
Epoch: 3666 	Training Loss: 1.138853 	Validation Loss: 1.300249 	 time: 0.3
Epoch: 3667 	Training Loss: 1.138806 	Validation Loss: 1.300567 	 time: 0.3
Epoch: 3668 	Training Loss: 1.138812 	Validation Loss: 1.300731 	 time: 0.3
Epoch: 3669 	Training Loss: 1.138764 	Validation Loss: 1.300563 	 time: 0.3
Epoch: 3670 	Training Loss: 1.138772 	Validation Loss: 1.300094 	 time: 0.3
Epoch: 3671 	Training Loss: 1.138749 	Validation Loss: 1.300012 	 time: 0.3
Epoch: 3672 	Training Loss: 1.138735 	Validation Loss: 1.300314 	 time: 0.3
Epoch: 3673 	Training Loss: 1.138761 	Validation Loss: 1.300073 	 time: 0.3
Epoch: 3674 	Training Loss: 1.138718 	Validation Loss: 1.300209 	 time: 0.3
Epoch: 3675 	Training Loss: 1.138716 	Validation Loss: 1.300950 	 time: 0.3
Epoch: 3676 	Training Loss: 1.138746 	Validation Loss: 1.300677 	 time: 0.3
Epoch: 3677 	Training Loss: 1.138762 	Validation Loss: 1.300772 	 time: 0.3
Epoch: 3678 	Training Loss: 1.138748 	Validation Loss: 1.300895 	 time: 0.3
Epoch: 3679 	Training Loss: 1.138700 	Validation Loss: 1.300829 	 time: 0.3
Epoch: 3680 	Training Loss: 1.138699 	Validation Loss: 1.301056 	 time: 0.3
Epoch: 3681 	Training Loss: 1.138738 	Validation Loss: 1.300843 	 time: 0.3
Epoch: 3682 	Training Loss: 1.138747 	Validation Loss: 1.301186 	 time: 0.3
Epoch: 3683 	Training Loss: 1.138734 	Validation Loss: 1.301312 	 time: 0.3
Epoch: 3684 	Training Loss: 1.138734 	Validation Loss: 1.300645 	 time: 0.3
Epoch: 3685 	Training Loss: 1.138711 	Validation Loss: 1.300572 	 time: 0.3
Epoch: 3686 	Training Loss: 1.138706 	Validation Loss: 1.300521 	 time: 0.3
Epoch: 3687 	Training Loss: 1.138671 	Validation Loss: 1.300629 	 time: 0.3
Epoch: 3688 	Training Loss: 1.138686 	Validation Loss: 1.300653 	 time: 0.3
Epoch: 3689 	Training Loss: 1.138689 	Validation Loss: 1.300452 	 time: 0.3
Epoch: 3690 	Training Loss: 1.138667 	Validation Loss: 1.300518 	 time: 0.3
Epoch: 3691 	Training Loss: 1.138668 	Validation Loss: 1.300801 	 time: 0.3
Epoch: 3692 	Training Loss: 1.138668 	Validation Loss: 1.300613 	 time: 0.3
Epoch: 3693 	Training Loss: 1.138656 	Validation Loss: 1.300451 	 time: 0.3
Epoch: 3694 	Training Loss: 1.138658 	Validation Loss: 1.300244 	 time: 0.3
Epoch: 3695 	Training Loss: 1.138639 	Validation Loss: 1.300194 	 time: 0.3
Epoch: 3696 	Training Loss: 1.138646 	Validation Loss: 1.300147 	 time: 0.3
Epoch: 3697 	Training Loss: 1.138635 	Validation Loss: 1.300166 	 time: 0.3
Epoch: 3698 	Training Loss: 1.138629 	Validation Loss: 1.300240 	 time: 0.3
Epoch: 3699 	Training Loss: 1.138628 	Validation Loss: 1.300310 	 time: 0.3
Epoch: 3700 	Training Loss: 1.138625 	Validation Loss: 1.300198 	 time: 0.3
Epoch: 3701 	Training Loss: 1.138618 	Validation Loss: 1.300033 	 time: 0.3
Epoch: 3702 	Training Loss: 1.138615 	Validation Loss: 1.300040 	 time: 0.3
Epoch: 3703 	Training Loss: 1.138607 	Validation Loss: 1.300061 	 time: 0.3
Epoch: 3704 	Training Loss: 1.138603 	Validation Loss: 1.300062 	 time: 0.3
Epoch: 3705 	Training Loss: 1.138594 	Validation Loss: 1.300135 	 time: 0.3
Epoch: 3706 	Training Loss: 1.138571 	Validation Loss: 1.300351 	 time: 0.3
Epoch: 3707 	Training Loss: 1.138533 	Validation Loss: 1.300424 	 time: 0.3
Epoch: 3708 	Training Loss: 1.138520 	Validation Loss: 1.300421 	 time: 0.3
Epoch: 3709 	Training Loss: 1.138488 	Validation Loss: 1.300604 	 time: 0.3
Epoch: 3710 	Training Loss: 1.138393 	Validation Loss: 1.300890 	 time: 0.3
Epoch: 3711 	Training Loss: 1.138376 	Validation Loss: 1.300849 	 time: 0.3
Epoch: 3712 	Training Loss: 1.138377 	Validation Loss: 1.300724 	 time: 0.3
Epoch: 3713 	Training Loss: 1.138375 	Validation Loss: 1.300813 	 time: 0.3
Epoch: 3714 	Training Loss: 1.138369 	Validation Loss: 1.301011 	 time: 0.3
Epoch: 3715 	Training Loss: 1.138364 	Validation Loss: 1.301119 	 time: 0.3
Epoch: 3716 	Training Loss: 1.138347 	Validation Loss: 1.301084 	 time: 0.3
Epoch: 3717 	Training Loss: 1.138333 	Validation Loss: 1.301158 	 time: 0.3
Epoch: 3718 	Training Loss: 1.138320 	Validation Loss: 1.301276 	 time: 0.3
Epoch: 3719 	Training Loss: 1.138301 	Validation Loss: 1.301279 	 time: 0.3
Epoch: 3720 	Training Loss: 1.138283 	Validation Loss: 1.301346 	 time: 0.3
Epoch: 3721 	Training Loss: 1.138272 	Validation Loss: 1.301367 	 time: 0.3
Epoch: 3722 	Training Loss: 1.138259 	Validation Loss: 1.301410 	 time: 0.3
Epoch: 3723 	Training Loss: 1.138247 	Validation Loss: 1.301404 	 time: 0.3
Epoch: 3724 	Training Loss: 1.138232 	Validation Loss: 1.301464 	 time: 0.3
Epoch: 3725 	Training Loss: 1.138205 	Validation Loss: 1.301685 	 time: 0.3
Epoch: 3726 	Training Loss: 1.138160 	Validation Loss: 1.301641 	 time: 0.3
Epoch: 3727 	Training Loss: 1.138146 	Validation Loss: 1.301611 	 time: 0.3
Epoch: 3728 	Training Loss: 1.138146 	Validation Loss: 1.301474 	 time: 0.3
Epoch: 3729 	Training Loss: 1.138141 	Validation Loss: 1.301628 	 time: 0.3
Epoch: 3730 	Training Loss: 1.138135 	Validation Loss: 1.301521 	 time: 0.3
Epoch: 3731 	Training Loss: 1.138129 	Validation Loss: 1.301581 	 time: 0.3
Epoch: 3732 	Training Loss: 1.138124 	Validation Loss: 1.301453 	 time: 0.3
Epoch: 3733 	Training Loss: 1.138120 	Validation Loss: 1.301734 	 time: 0.3
Epoch: 3734 	Training Loss: 1.138117 	Validation Loss: 1.301370 	 time: 0.3
Epoch: 3735 	Training Loss: 1.138114 	Validation Loss: 1.301705 	 time: 0.3
Epoch: 3736 	Training Loss: 1.138112 	Validation Loss: 1.301321 	 time: 0.3
Epoch: 3737 	Training Loss: 1.138114 	Validation Loss: 1.301810 	 time: 0.3
Epoch: 3738 	Training Loss: 1.138105 	Validation Loss: 1.301307 	 time: 0.3
Epoch: 3739 	Training Loss: 1.138090 	Validation Loss: 1.301751 	 time: 0.3
Epoch: 3740 	Training Loss: 1.138074 	Validation Loss: 1.301417 	 time: 0.3
Epoch: 3741 	Training Loss: 1.138073 	Validation Loss: 1.301718 	 time: 0.3
Epoch: 3742 	Training Loss: 1.138065 	Validation Loss: 1.301412 	 time: 0.3
Epoch: 3743 	Training Loss: 1.138060 	Validation Loss: 1.301654 	 time: 0.3
Epoch: 3744 	Training Loss: 1.138054 	Validation Loss: 1.301670 	 time: 0.3
Epoch: 3745 	Training Loss: 1.138044 	Validation Loss: 1.301746 	 time: 0.3
Epoch: 3746 	Training Loss: 1.138036 	Validation Loss: 1.301725 	 time: 0.3
Epoch: 3747 	Training Loss: 1.138032 	Validation Loss: 1.302123 	 time: 0.3
Epoch: 3748 	Training Loss: 1.138022 	Validation Loss: 1.302269 	 time: 0.3
Epoch: 3749 	Training Loss: 1.137990 	Validation Loss: 1.302678 	 time: 0.3
Epoch: 3750 	Training Loss: 1.137965 	Validation Loss: 1.302341 	 time: 0.3
Epoch: 3751 	Training Loss: 1.137987 	Validation Loss: 1.303039 	 time: 0.3
Epoch: 3752 	Training Loss: 1.137986 	Validation Loss: 1.302341 	 time: 0.3
Epoch: 3753 	Training Loss: 1.137999 	Validation Loss: 1.302576 	 time: 0.3
Epoch: 3754 	Training Loss: 1.137957 	Validation Loss: 1.302306 	 time: 0.3
Epoch: 3755 	Training Loss: 1.137941 	Validation Loss: 1.302122 	 time: 0.3
Epoch: 3756 	Training Loss: 1.137951 	Validation Loss: 1.302998 	 time: 0.3
Epoch: 3757 	Training Loss: 1.137966 	Validation Loss: 1.302175 	 time: 0.3
Epoch: 3758 	Training Loss: 1.138092 	Validation Loss: 1.303204 	 time: 0.3
Epoch: 3759 	Training Loss: 1.138047 	Validation Loss: 1.303538 	 time: 0.3
Epoch: 3760 	Training Loss: 1.138003 	Validation Loss: 1.303334 	 time: 0.3
Epoch: 3761 	Training Loss: 1.137951 	Validation Loss: 1.302968 	 time: 0.3
Epoch: 3762 	Training Loss: 1.138006 	Validation Loss: 1.302493 	 time: 0.3
Epoch: 3763 	Training Loss: 1.137982 	Validation Loss: 1.304415 	 time: 0.3
Epoch: 3764 	Training Loss: 1.138010 	Validation Loss: 1.303092 	 time: 0.3
Epoch: 3765 	Training Loss: 1.137941 	Validation Loss: 1.301879 	 time: 0.3
Epoch: 3766 	Training Loss: 1.137963 	Validation Loss: 1.301848 	 time: 0.3
Epoch: 3767 	Training Loss: 1.137958 	Validation Loss: 1.302136 	 time: 0.3
Epoch: 3768 	Training Loss: 1.137956 	Validation Loss: 1.303034 	 time: 0.3
Epoch: 3769 	Training Loss: 1.137930 	Validation Loss: 1.302731 	 time: 0.3
Epoch: 3770 	Training Loss: 1.137932 	Validation Loss: 1.302597 	 time: 0.3
Epoch: 3771 	Training Loss: 1.137921 	Validation Loss: 1.302722 	 time: 0.3
Epoch: 3772 	Training Loss: 1.137949 	Validation Loss: 1.301845 	 time: 0.3
Epoch: 3773 	Training Loss: 1.137973 	Validation Loss: 1.302526 	 time: 0.3
Epoch: 3774 	Training Loss: 1.137978 	Validation Loss: 1.302521 	 time: 0.3
Epoch: 3775 	Training Loss: 1.137938 	Validation Loss: 1.302171 	 time: 0.3
Epoch: 3776 	Training Loss: 1.137906 	Validation Loss: 1.301875 	 time: 0.3
Epoch: 3777 	Training Loss: 1.138033 	Validation Loss: 1.301616 	 time: 0.3
Epoch: 3778 	Training Loss: 1.138320 	Validation Loss: 1.304340 	 time: 0.3
Epoch: 3779 	Training Loss: 1.138119 	Validation Loss: 1.306525 	 time: 0.3
Epoch: 3780 	Training Loss: 1.138425 	Validation Loss: 1.302543 	 time: 0.3
Epoch: 3781 	Training Loss: 1.138404 	Validation Loss: 1.300951 	 time: 0.3
Epoch: 3782 	Training Loss: 1.138166 	Validation Loss: 1.300124 	 time: 0.3
Epoch: 3783 	Training Loss: 1.138253 	Validation Loss: 1.301442 	 time: 0.3
Epoch: 3784 	Training Loss: 1.138251 	Validation Loss: 1.300976 	 time: 0.3
Epoch: 3785 	Training Loss: 1.138166 	Validation Loss: 1.300196 	 time: 0.3
Epoch: 3786 	Training Loss: 1.138189 	Validation Loss: 1.299910 	 time: 0.3
Epoch: 3787 	Training Loss: 1.138196 	Validation Loss: 1.302122 	 time: 0.3
Epoch: 3788 	Training Loss: 1.138323 	Validation Loss: 1.300823 	 time: 0.3
Epoch: 3789 	Training Loss: 1.138124 	Validation Loss: 1.301012 	 time: 0.3
Epoch: 3790 	Training Loss: 1.138364 	Validation Loss: 1.301649 	 time: 0.3
Epoch: 3791 	Training Loss: 1.138186 	Validation Loss: 1.302407 	 time: 0.3
Epoch: 3792 	Training Loss: 1.138418 	Validation Loss: 1.302430 	 time: 0.3
Epoch: 3793 	Training Loss: 1.138247 	Validation Loss: 1.302166 	 time: 0.3
Epoch: 3794 	Training Loss: 1.138260 	Validation Loss: 1.302025 	 time: 0.3
Epoch: 3795 	Training Loss: 1.138184 	Validation Loss: 1.302609 	 time: 0.3
Epoch: 3796 	Training Loss: 1.138182 	Validation Loss: 1.302089 	 time: 0.3
Epoch: 3797 	Training Loss: 1.138195 	Validation Loss: 1.303540 	 time: 0.3
Epoch: 3798 	Training Loss: 1.138049 	Validation Loss: 1.305115 	 time: 0.3
Epoch: 3799 	Training Loss: 1.138029 	Validation Loss: 1.303075 	 time: 0.3
Epoch: 3800 	Training Loss: 1.138121 	Validation Loss: 1.305756 	 time: 0.3
Epoch: 3801 	Training Loss: 1.137993 	Validation Loss: 1.305629 	 time: 0.3
Epoch: 3802 	Training Loss: 1.138004 	Validation Loss: 1.302431 	 time: 0.3
Epoch: 3803 	Training Loss: 1.137952 	Validation Loss: 1.301544 	 time: 0.3
Epoch: 3804 	Training Loss: 1.138018 	Validation Loss: 1.303463 	 time: 0.3
Epoch: 3805 	Training Loss: 1.137981 	Validation Loss: 1.303052 	 time: 0.3
Epoch: 3806 	Training Loss: 1.137953 	Validation Loss: 1.302740 	 time: 0.3
Epoch: 3807 	Training Loss: 1.137952 	Validation Loss: 1.302551 	 time: 0.3
Epoch: 3808 	Training Loss: 1.137914 	Validation Loss: 1.301480 	 time: 0.3
Epoch: 3809 	Training Loss: 1.137887 	Validation Loss: 1.301191 	 time: 0.3
Epoch: 3810 	Training Loss: 1.137871 	Validation Loss: 1.301430 	 time: 0.3
Epoch: 3811 	Training Loss: 1.137885 	Validation Loss: 1.301760 	 time: 0.3
Epoch: 3812 	Training Loss: 1.137859 	Validation Loss: 1.302137 	 time: 0.3
Epoch: 3813 	Training Loss: 1.137851 	Validation Loss: 1.302153 	 time: 0.3
Epoch: 3814 	Training Loss: 1.137840 	Validation Loss: 1.301865 	 time: 0.3
Epoch: 3815 	Training Loss: 1.137827 	Validation Loss: 1.301794 	 time: 0.3
Epoch: 3816 	Training Loss: 1.137833 	Validation Loss: 1.301935 	 time: 0.3
Epoch: 3817 	Training Loss: 1.137814 	Validation Loss: 1.301764 	 time: 0.3
Epoch: 3818 	Training Loss: 1.137806 	Validation Loss: 1.301457 	 time: 0.3
Epoch: 3819 	Training Loss: 1.137801 	Validation Loss: 1.301216 	 time: 0.3
Epoch: 3820 	Training Loss: 1.137794 	Validation Loss: 1.301352 	 time: 0.3
Epoch: 3821 	Training Loss: 1.137767 	Validation Loss: 1.301716 	 time: 0.3
Epoch: 3822 	Training Loss: 1.137732 	Validation Loss: 1.302090 	 time: 0.3
Epoch: 3823 	Training Loss: 1.137735 	Validation Loss: 1.302283 	 time: 0.3
Epoch: 3824 	Training Loss: 1.137719 	Validation Loss: 1.302212 	 time: 0.3
Epoch: 3825 	Training Loss: 1.137719 	Validation Loss: 1.302257 	 time: 0.3
Epoch: 3826 	Training Loss: 1.137713 	Validation Loss: 1.302156 	 time: 0.3
Epoch: 3827 	Training Loss: 1.137715 	Validation Loss: 1.302313 	 time: 0.3
Epoch: 3828 	Training Loss: 1.137703 	Validation Loss: 1.302275 	 time: 0.3
Epoch: 3829 	Training Loss: 1.137677 	Validation Loss: 1.302369 	 time: 0.3
Epoch: 3830 	Training Loss: 1.137671 	Validation Loss: 1.302669 	 time: 0.3
Epoch: 3831 	Training Loss: 1.137678 	Validation Loss: 1.302712 	 time: 0.3
Epoch: 3832 	Training Loss: 1.137676 	Validation Loss: 1.302926 	 time: 0.3
Epoch: 3833 	Training Loss: 1.137673 	Validation Loss: 1.302461 	 time: 0.3
Epoch: 3834 	Training Loss: 1.137649 	Validation Loss: 1.302029 	 time: 0.3
Epoch: 3835 	Training Loss: 1.137655 	Validation Loss: 1.302120 	 time: 0.3
Epoch: 3836 	Training Loss: 1.137672 	Validation Loss: 1.302368 	 time: 0.3
Epoch: 3837 	Training Loss: 1.137676 	Validation Loss: 1.302657 	 time: 0.3
Epoch: 3838 	Training Loss: 1.137699 	Validation Loss: 1.302433 	 time: 0.3
Epoch: 3839 	Training Loss: 1.137640 	Validation Loss: 1.302339 	 time: 0.3
Epoch: 3840 	Training Loss: 1.137645 	Validation Loss: 1.302544 	 time: 0.3
Epoch: 3841 	Training Loss: 1.137691 	Validation Loss: 1.302617 	 time: 0.3
Epoch: 3842 	Training Loss: 1.137652 	Validation Loss: 1.302497 	 time: 0.3
Epoch: 3843 	Training Loss: 1.137630 	Validation Loss: 1.302414 	 time: 0.3
Epoch: 3844 	Training Loss: 1.137662 	Validation Loss: 1.302432 	 time: 0.3
Epoch: 3845 	Training Loss: 1.137653 	Validation Loss: 1.302076 	 time: 0.3
Epoch: 3846 	Training Loss: 1.137656 	Validation Loss: 1.301942 	 time: 0.3
Epoch: 3847 	Training Loss: 1.137619 	Validation Loss: 1.302065 	 time: 0.3
Epoch: 3848 	Training Loss: 1.137599 	Validation Loss: 1.302172 	 time: 0.3
Epoch: 3849 	Training Loss: 1.137598 	Validation Loss: 1.302253 	 time: 0.3
Epoch: 3850 	Training Loss: 1.137598 	Validation Loss: 1.302090 	 time: 0.3
Epoch: 3851 	Training Loss: 1.137594 	Validation Loss: 1.302454 	 time: 0.3
Epoch: 3852 	Training Loss: 1.137569 	Validation Loss: 1.302692 	 time: 0.3
Epoch: 3853 	Training Loss: 1.137568 	Validation Loss: 1.302636 	 time: 0.3
Epoch: 3854 	Training Loss: 1.137560 	Validation Loss: 1.302575 	 time: 0.3
Epoch: 3855 	Training Loss: 1.137560 	Validation Loss: 1.302347 	 time: 0.3
Epoch: 3856 	Training Loss: 1.137531 	Validation Loss: 1.302561 	 time: 0.3
Epoch: 3857 	Training Loss: 1.137499 	Validation Loss: 1.302905 	 time: 0.3
Epoch: 3858 	Training Loss: 1.137487 	Validation Loss: 1.302915 	 time: 0.3
Epoch: 3859 	Training Loss: 1.137448 	Validation Loss: 1.302867 	 time: 0.3
Epoch: 3860 	Training Loss: 1.137451 	Validation Loss: 1.302624 	 time: 0.3
Epoch: 3861 	Training Loss: 1.137443 	Validation Loss: 1.302568 	 time: 0.3
Epoch: 3862 	Training Loss: 1.137441 	Validation Loss: 1.302565 	 time: 0.3
Epoch: 3863 	Training Loss: 1.137439 	Validation Loss: 1.302603 	 time: 0.3
Epoch: 3864 	Training Loss: 1.137422 	Validation Loss: 1.302630 	 time: 0.3
Epoch: 3865 	Training Loss: 1.137402 	Validation Loss: 1.302431 	 time: 0.3
Epoch: 3866 	Training Loss: 1.137363 	Validation Loss: 1.302351 	 time: 0.3
Epoch: 3867 	Training Loss: 1.137267 	Validation Loss: 1.302577 	 time: 0.3
Epoch: 3868 	Training Loss: 1.137227 	Validation Loss: 1.303223 	 time: 0.3
Epoch: 3869 	Training Loss: 1.137215 	Validation Loss: 1.303825 	 time: 0.3
Epoch: 3870 	Training Loss: 1.137104 	Validation Loss: 1.303971 	 time: 0.3
Epoch: 3871 	Training Loss: 1.137072 	Validation Loss: 1.303860 	 time: 0.3
Epoch: 3872 	Training Loss: 1.137070 	Validation Loss: 1.303836 	 time: 0.3
Epoch: 3873 	Training Loss: 1.137056 	Validation Loss: 1.303760 	 time: 0.3
Epoch: 3874 	Training Loss: 1.137050 	Validation Loss: 1.303795 	 time: 0.3
Epoch: 3875 	Training Loss: 1.137040 	Validation Loss: 1.303678 	 time: 0.3
Epoch: 3876 	Training Loss: 1.137050 	Validation Loss: 1.302695 	 time: 0.3
Epoch: 3877 	Training Loss: 1.137065 	Validation Loss: 1.302837 	 time: 0.3
Epoch: 3878 	Training Loss: 1.137079 	Validation Loss: 1.302219 	 time: 0.3
Epoch: 3879 	Training Loss: 1.137101 	Validation Loss: 1.302657 	 time: 0.3
Epoch: 3880 	Training Loss: 1.137020 	Validation Loss: 1.302228 	 time: 0.3
Epoch: 3881 	Training Loss: 1.136964 	Validation Loss: 1.301542 	 time: 0.3
Epoch: 3882 	Training Loss: 1.137074 	Validation Loss: 1.304739 	 time: 0.3
Epoch: 3883 	Training Loss: 1.137454 	Validation Loss: 1.303528 	 time: 0.3
Epoch: 3884 	Training Loss: 1.138516 	Validation Loss: 1.301932 	 time: 0.3
Epoch: 3885 	Training Loss: 1.137967 	Validation Loss: 1.300888 	 time: 0.3
Epoch: 3886 	Training Loss: 1.138098 	Validation Loss: 1.299929 	 time: 0.3
Epoch: 3887 	Training Loss: 1.138449 	Validation Loss: 1.302089 	 time: 0.3
Epoch: 3888 	Training Loss: 1.138812 	Validation Loss: 1.308428 	 time: 0.3
Epoch: 3889 	Training Loss: 1.139659 	Validation Loss: 1.295487 	 time: 0.3
Validation loss decreased from 1.299775 to 1.295487. Model was saved
Epoch: 3890 	Training Loss: 1.139176 	Validation Loss: 1.300108 	 time: 0.3
Epoch: 3891 	Training Loss: 1.138721 	Validation Loss: 1.309883 	 time: 0.3
Epoch: 3892 	Training Loss: 1.139659 	Validation Loss: 1.300726 	 time: 0.3
Epoch: 3893 	Training Loss: 1.139458 	Validation Loss: 1.305904 	 time: 0.3
Epoch: 3894 	Training Loss: 1.138728 	Validation Loss: 1.312503 	 time: 0.3
Epoch: 3895 	Training Loss: 1.139683 	Validation Loss: 1.310397 	 time: 0.3
Epoch: 3896 	Training Loss: 1.139212 	Validation Loss: 1.307752 	 time: 0.3
Epoch: 3897 	Training Loss: 1.138776 	Validation Loss: 1.306840 	 time: 0.3
Epoch: 3898 	Training Loss: 1.139046 	Validation Loss: 1.305741 	 time: 0.3
Epoch: 3899 	Training Loss: 1.138929 	Validation Loss: 1.306300 	 time: 0.3
Epoch: 3900 	Training Loss: 1.138798 	Validation Loss: 1.306454 	 time: 0.3
Epoch: 3901 	Training Loss: 1.138445 	Validation Loss: 1.308024 	 time: 0.3
Epoch: 3902 	Training Loss: 1.138474 	Validation Loss: 1.305147 	 time: 0.3
Epoch: 3903 	Training Loss: 1.138292 	Validation Loss: 1.302754 	 time: 0.3
Epoch: 3904 	Training Loss: 1.138274 	Validation Loss: 1.298224 	 time: 0.3
Epoch: 3905 	Training Loss: 1.138527 	Validation Loss: 1.299706 	 time: 0.3
Epoch: 3906 	Training Loss: 1.138217 	Validation Loss: 1.301651 	 time: 0.3
Epoch: 3907 	Training Loss: 1.138087 	Validation Loss: 1.297902 	 time: 0.3
Epoch: 3908 	Training Loss: 1.137883 	Validation Loss: 1.297132 	 time: 0.3
Epoch: 3909 	Training Loss: 1.137891 	Validation Loss: 1.300117 	 time: 0.3
Epoch: 3910 	Training Loss: 1.138010 	Validation Loss: 1.297960 	 time: 0.3
Epoch: 3911 	Training Loss: 1.137919 	Validation Loss: 1.298717 	 time: 0.3
Epoch: 3912 	Training Loss: 1.137750 	Validation Loss: 1.299434 	 time: 0.3
Epoch: 3913 	Training Loss: 1.137758 	Validation Loss: 1.300179 	 time: 0.3
Epoch: 3914 	Training Loss: 1.137765 	Validation Loss: 1.301951 	 time: 0.3
Epoch: 3915 	Training Loss: 1.137715 	Validation Loss: 1.302846 	 time: 0.3
Epoch: 3916 	Training Loss: 1.137732 	Validation Loss: 1.299907 	 time: 0.3
Epoch: 3917 	Training Loss: 1.137764 	Validation Loss: 1.300278 	 time: 0.3
Epoch: 3918 	Training Loss: 1.137611 	Validation Loss: 1.299912 	 time: 0.3
Epoch: 3919 	Training Loss: 1.137596 	Validation Loss: 1.299720 	 time: 0.3
Epoch: 3920 	Training Loss: 1.137546 	Validation Loss: 1.300213 	 time: 0.3
Epoch: 3921 	Training Loss: 1.137574 	Validation Loss: 1.299473 	 time: 0.3
Epoch: 3922 	Training Loss: 1.137651 	Validation Loss: 1.302021 	 time: 0.3
Epoch: 3923 	Training Loss: 1.137522 	Validation Loss: 1.303621 	 time: 0.3
Epoch: 3924 	Training Loss: 1.137497 	Validation Loss: 1.305335 	 time: 0.3
Epoch: 3925 	Training Loss: 1.137487 	Validation Loss: 1.305088 	 time: 0.3
Epoch: 3926 	Training Loss: 1.137474 	Validation Loss: 1.301848 	 time: 0.3
Epoch: 3927 	Training Loss: 1.137415 	Validation Loss: 1.301300 	 time: 0.3
Epoch: 3928 	Training Loss: 1.137357 	Validation Loss: 1.301363 	 time: 0.3
Epoch: 3929 	Training Loss: 1.137373 	Validation Loss: 1.301607 	 time: 0.3
Epoch: 3930 	Training Loss: 1.137327 	Validation Loss: 1.302302 	 time: 0.3
Epoch: 3931 	Training Loss: 1.137300 	Validation Loss: 1.302585 	 time: 0.3
Epoch: 3932 	Training Loss: 1.137296 	Validation Loss: 1.300773 	 time: 0.3
Epoch: 3933 	Training Loss: 1.137279 	Validation Loss: 1.300469 	 time: 0.3
Epoch: 3934 	Training Loss: 1.137261 	Validation Loss: 1.301310 	 time: 0.3
Epoch: 3935 	Training Loss: 1.137280 	Validation Loss: 1.300629 	 time: 0.3
Epoch: 3936 	Training Loss: 1.137295 	Validation Loss: 1.301764 	 time: 0.3
Epoch: 3937 	Training Loss: 1.137259 	Validation Loss: 1.302184 	 time: 0.3
Epoch: 3938 	Training Loss: 1.137292 	Validation Loss: 1.301857 	 time: 0.3
Epoch: 3939 	Training Loss: 1.137252 	Validation Loss: 1.301787 	 time: 0.3
Epoch: 3940 	Training Loss: 1.137191 	Validation Loss: 1.302325 	 time: 0.3
Epoch: 3941 	Training Loss: 1.137257 	Validation Loss: 1.301553 	 time: 0.3
Epoch: 3942 	Training Loss: 1.137258 	Validation Loss: 1.301847 	 time: 0.3
Epoch: 3943 	Training Loss: 1.137245 	Validation Loss: 1.302255 	 time: 0.3
Epoch: 3944 	Training Loss: 1.137283 	Validation Loss: 1.302078 	 time: 0.3
Epoch: 3945 	Training Loss: 1.137224 	Validation Loss: 1.301762 	 time: 0.3
Epoch: 3946 	Training Loss: 1.137314 	Validation Loss: 1.302621 	 time: 0.3
Epoch: 3947 	Training Loss: 1.137219 	Validation Loss: 1.304994 	 time: 0.3
Epoch: 3948 	Training Loss: 1.137243 	Validation Loss: 1.301674 	 time: 0.3
Epoch: 3949 	Training Loss: 1.137185 	Validation Loss: 1.300549 	 time: 0.3
Epoch: 3950 	Training Loss: 1.137182 	Validation Loss: 1.300095 	 time: 0.3
Epoch: 3951 	Training Loss: 1.137121 	Validation Loss: 1.300094 	 time: 0.3
Epoch: 3952 	Training Loss: 1.137177 	Validation Loss: 1.300694 	 time: 0.3
Epoch: 3953 	Training Loss: 1.137174 	Validation Loss: 1.301600 	 time: 0.3
Epoch: 3954 	Training Loss: 1.137161 	Validation Loss: 1.302161 	 time: 0.3
Epoch: 3955 	Training Loss: 1.137157 	Validation Loss: 1.302366 	 time: 0.3
Epoch: 3956 	Training Loss: 1.137137 	Validation Loss: 1.302133 	 time: 0.3
Epoch: 3957 	Training Loss: 1.137129 	Validation Loss: 1.301765 	 time: 0.3
Epoch: 3958 	Training Loss: 1.137126 	Validation Loss: 1.301670 	 time: 0.3
Epoch: 3959 	Training Loss: 1.137094 	Validation Loss: 1.301727 	 time: 0.3
Epoch: 3960 	Training Loss: 1.137056 	Validation Loss: 1.302004 	 time: 0.3
Epoch: 3961 	Training Loss: 1.137067 	Validation Loss: 1.301877 	 time: 0.3
Epoch: 3962 	Training Loss: 1.137046 	Validation Loss: 1.302139 	 time: 0.3
Epoch: 3963 	Training Loss: 1.137019 	Validation Loss: 1.302188 	 time: 0.3
Epoch: 3964 	Training Loss: 1.137021 	Validation Loss: 1.302042 	 time: 0.3
Epoch: 3965 	Training Loss: 1.137029 	Validation Loss: 1.302327 	 time: 0.3
Epoch: 3966 	Training Loss: 1.137024 	Validation Loss: 1.302196 	 time: 0.3
Epoch: 3967 	Training Loss: 1.137007 	Validation Loss: 1.302143 	 time: 0.3
Epoch: 3968 	Training Loss: 1.137012 	Validation Loss: 1.301962 	 time: 0.3
Epoch: 3969 	Training Loss: 1.137008 	Validation Loss: 1.301924 	 time: 0.3
Epoch: 3970 	Training Loss: 1.136995 	Validation Loss: 1.301821 	 time: 0.3
Epoch: 3971 	Training Loss: 1.136989 	Validation Loss: 1.302043 	 time: 0.3
Epoch: 3972 	Training Loss: 1.136986 	Validation Loss: 1.302573 	 time: 0.3
Epoch: 3973 	Training Loss: 1.136976 	Validation Loss: 1.302781 	 time: 0.3
Epoch: 3974 	Training Loss: 1.136966 	Validation Loss: 1.302747 	 time: 0.3
Epoch: 3975 	Training Loss: 1.136960 	Validation Loss: 1.302710 	 time: 0.3
Epoch: 3976 	Training Loss: 1.136956 	Validation Loss: 1.302680 	 time: 0.3
Epoch: 3977 	Training Loss: 1.136948 	Validation Loss: 1.302871 	 time: 0.3
Epoch: 3978 	Training Loss: 1.136938 	Validation Loss: 1.303109 	 time: 0.3
Epoch: 3979 	Training Loss: 1.136935 	Validation Loss: 1.302999 	 time: 0.3
Epoch: 3980 	Training Loss: 1.136929 	Validation Loss: 1.302817 	 time: 0.3
Epoch: 3981 	Training Loss: 1.136919 	Validation Loss: 1.302792 	 time: 0.3
Epoch: 3982 	Training Loss: 1.136912 	Validation Loss: 1.302783 	 time: 0.3
Epoch: 3983 	Training Loss: 1.136913 	Validation Loss: 1.303056 	 time: 0.3
Epoch: 3984 	Training Loss: 1.136912 	Validation Loss: 1.303343 	 time: 0.3
Epoch: 3985 	Training Loss: 1.136904 	Validation Loss: 1.303427 	 time: 0.3
Epoch: 3986 	Training Loss: 1.136906 	Validation Loss: 1.303683 	 time: 0.3
Epoch: 3987 	Training Loss: 1.136904 	Validation Loss: 1.303658 	 time: 0.3
Epoch: 3988 	Training Loss: 1.136898 	Validation Loss: 1.303409 	 time: 0.3
Epoch: 3989 	Training Loss: 1.136896 	Validation Loss: 1.303363 	 time: 0.3
Epoch: 3990 	Training Loss: 1.136895 	Validation Loss: 1.303180 	 time: 0.3
Epoch: 3991 	Training Loss: 1.136892 	Validation Loss: 1.303212 	 time: 0.3
Epoch: 3992 	Training Loss: 1.136886 	Validation Loss: 1.303269 	 time: 0.3
Epoch: 3993 	Training Loss: 1.136887 	Validation Loss: 1.303153 	 time: 0.3
Epoch: 3994 	Training Loss: 1.136889 	Validation Loss: 1.303400 	 time: 0.3
Epoch: 3995 	Training Loss: 1.136885 	Validation Loss: 1.303350 	 time: 0.3
Epoch: 3996 	Training Loss: 1.136876 	Validation Loss: 1.303193 	 time: 0.3
Epoch: 3997 	Training Loss: 1.136879 	Validation Loss: 1.303271 	 time: 0.3
Epoch: 3998 	Training Loss: 1.136879 	Validation Loss: 1.303097 	 time: 0.3
Epoch: 3999 	Training Loss: 1.136878 	Validation Loss: 1.303263 	 time: 0.3
Epoch: 4000 	Training Loss: 1.136872 	Validation Loss: 1.303314 	 time: 0.3
