Epoch: 1 	Training Loss: 1.794478 	Validation Loss: 1.807453 	 time: 0.3
Validation loss decreased from inf to 1.807453. Model was saved
Epoch: 2 	Training Loss: 1.806815 	Validation Loss: 1.799412 	 time: 0.2
Validation loss decreased from 1.807453 to 1.799412. Model was saved
Epoch: 3 	Training Loss: 1.798882 	Validation Loss: 1.793509 	 time: 0.2
Validation loss decreased from 1.799412 to 1.793509. Model was saved
Epoch: 4 	Training Loss: 1.793216 	Validation Loss: 1.790486 	 time: 0.2
Validation loss decreased from 1.793509 to 1.790486. Model was saved
Epoch: 5 	Training Loss: 1.790301 	Validation Loss: 1.788663 	 time: 0.3
Validation loss decreased from 1.790486 to 1.788663. Model was saved
Epoch: 6 	Training Loss: 1.788274 	Validation Loss: 1.786110 	 time: 0.3
Validation loss decreased from 1.788663 to 1.786110. Model was saved
Epoch: 7 	Training Loss: 1.785177 	Validation Loss: 1.782421 	 time: 0.3
Validation loss decreased from 1.786110 to 1.782421. Model was saved
Epoch: 8 	Training Loss: 1.780644 	Validation Loss: 1.777402 	 time: 0.3
Validation loss decreased from 1.782421 to 1.777402. Model was saved
Epoch: 9 	Training Loss: 1.774580 	Validation Loss: 1.770323 	 time: 0.3
Validation loss decreased from 1.777402 to 1.770323. Model was saved
Epoch: 10 	Training Loss: 1.766287 	Validation Loss: 1.761039 	 time: 0.3
Validation loss decreased from 1.770323 to 1.761039. Model was saved
Epoch: 11 	Training Loss: 1.755748 	Validation Loss: 1.750442 	 time: 0.3
Validation loss decreased from 1.761039 to 1.750442. Model was saved
Epoch: 12 	Training Loss: 1.743976 	Validation Loss: 1.739094 	 time: 0.3
Validation loss decreased from 1.750442 to 1.739094. Model was saved
Epoch: 13 	Training Loss: 1.731418 	Validation Loss: 1.726147 	 time: 0.3
Validation loss decreased from 1.739094 to 1.726147. Model was saved
Epoch: 14 	Training Loss: 1.716831 	Validation Loss: 1.712509 	 time: 0.3
Validation loss decreased from 1.726147 to 1.712509. Model was saved
Epoch: 15 	Training Loss: 1.700962 	Validation Loss: 1.700833 	 time: 0.3
Validation loss decreased from 1.712509 to 1.700833. Model was saved
Epoch: 16 	Training Loss: 1.686814 	Validation Loss: 1.689630 	 time: 0.3
Validation loss decreased from 1.700833 to 1.689630. Model was saved
Epoch: 17 	Training Loss: 1.673540 	Validation Loss: 1.677098 	 time: 0.3
Validation loss decreased from 1.689630 to 1.677098. Model was saved
Epoch: 18 	Training Loss: 1.659899 	Validation Loss: 1.664830 	 time: 0.3
Validation loss decreased from 1.677098 to 1.664830. Model was saved
Epoch: 19 	Training Loss: 1.647318 	Validation Loss: 1.653159 	 time: 0.3
Validation loss decreased from 1.664830 to 1.653159. Model was saved
Epoch: 20 	Training Loss: 1.634327 	Validation Loss: 1.643754 	 time: 0.3
Validation loss decreased from 1.653159 to 1.643754. Model was saved
Epoch: 21 	Training Loss: 1.621313 	Validation Loss: 1.635466 	 time: 0.3
Validation loss decreased from 1.643754 to 1.635466. Model was saved
Epoch: 22 	Training Loss: 1.609983 	Validation Loss: 1.624199 	 time: 0.3
Validation loss decreased from 1.635466 to 1.624199. Model was saved
Epoch: 23 	Training Loss: 1.598214 	Validation Loss: 1.611796 	 time: 0.3
Validation loss decreased from 1.624199 to 1.611796. Model was saved
Epoch: 24 	Training Loss: 1.586559 	Validation Loss: 1.601693 	 time: 0.3
Validation loss decreased from 1.611796 to 1.601693. Model was saved
Epoch: 25 	Training Loss: 1.576832 	Validation Loss: 1.592652 	 time: 0.3
Validation loss decreased from 1.601693 to 1.592652. Model was saved
Epoch: 26 	Training Loss: 1.567685 	Validation Loss: 1.584738 	 time: 0.3
Validation loss decreased from 1.592652 to 1.584738. Model was saved
Epoch: 27 	Training Loss: 1.559775 	Validation Loss: 1.576932 	 time: 0.3
Validation loss decreased from 1.584738 to 1.576932. Model was saved
Epoch: 28 	Training Loss: 1.551469 	Validation Loss: 1.570755 	 time: 0.3
Validation loss decreased from 1.576932 to 1.570755. Model was saved
Epoch: 29 	Training Loss: 1.543888 	Validation Loss: 1.565545 	 time: 0.3
Validation loss decreased from 1.570755 to 1.565545. Model was saved
Epoch: 30 	Training Loss: 1.537696 	Validation Loss: 1.558663 	 time: 0.3
Validation loss decreased from 1.565545 to 1.558663. Model was saved
Epoch: 31 	Training Loss: 1.530689 	Validation Loss: 1.552163 	 time: 0.3
Validation loss decreased from 1.558663 to 1.552163. Model was saved
Epoch: 32 	Training Loss: 1.524224 	Validation Loss: 1.547602 	 time: 0.3
Validation loss decreased from 1.552163 to 1.547602. Model was saved
Epoch: 33 	Training Loss: 1.518470 	Validation Loss: 1.544942 	 time: 0.3
Validation loss decreased from 1.547602 to 1.544942. Model was saved
Epoch: 34 	Training Loss: 1.513209 	Validation Loss: 1.542417 	 time: 0.3
Validation loss decreased from 1.544942 to 1.542417. Model was saved
Epoch: 35 	Training Loss: 1.508322 	Validation Loss: 1.538779 	 time: 0.3
Validation loss decreased from 1.542417 to 1.538779. Model was saved
Epoch: 36 	Training Loss: 1.504328 	Validation Loss: 1.535196 	 time: 0.3
Validation loss decreased from 1.538779 to 1.535196. Model was saved
Epoch: 37 	Training Loss: 1.500242 	Validation Loss: 1.533448 	 time: 0.3
Validation loss decreased from 1.535196 to 1.533448. Model was saved
Epoch: 38 	Training Loss: 1.496930 	Validation Loss: 1.532277 	 time: 0.3
Validation loss decreased from 1.533448 to 1.532277. Model was saved
Epoch: 39 	Training Loss: 1.493640 	Validation Loss: 1.530317 	 time: 0.3
Validation loss decreased from 1.532277 to 1.530317. Model was saved
Epoch: 40 	Training Loss: 1.490470 	Validation Loss: 1.528395 	 time: 0.3
Validation loss decreased from 1.530317 to 1.528395. Model was saved
Epoch: 41 	Training Loss: 1.487551 	Validation Loss: 1.527364 	 time: 0.3
Validation loss decreased from 1.528395 to 1.527364. Model was saved
Epoch: 42 	Training Loss: 1.484314 	Validation Loss: 1.527368 	 time: 0.3
Epoch: 43 	Training Loss: 1.481593 	Validation Loss: 1.527157 	 time: 0.3
Validation loss decreased from 1.527364 to 1.527157. Model was saved
Epoch: 44 	Training Loss: 1.478949 	Validation Loss: 1.526784 	 time: 0.3
Validation loss decreased from 1.527157 to 1.526784. Model was saved
Epoch: 45 	Training Loss: 1.476667 	Validation Loss: 1.526483 	 time: 0.3
Validation loss decreased from 1.526784 to 1.526483. Model was saved
Epoch: 46 	Training Loss: 1.474613 	Validation Loss: 1.526027 	 time: 0.3
Validation loss decreased from 1.526483 to 1.526027. Model was saved
Epoch: 47 	Training Loss: 1.472387 	Validation Loss: 1.525404 	 time: 0.3
Validation loss decreased from 1.526027 to 1.525404. Model was saved
Epoch: 48 	Training Loss: 1.470401 	Validation Loss: 1.524338 	 time: 0.3
Validation loss decreased from 1.525404 to 1.524338. Model was saved
Epoch: 49 	Training Loss: 1.468281 	Validation Loss: 1.523508 	 time: 0.3
Validation loss decreased from 1.524338 to 1.523508. Model was saved
Epoch: 50 	Training Loss: 1.466387 	Validation Loss: 1.523286 	 time: 0.3
Validation loss decreased from 1.523508 to 1.523286. Model was saved
Epoch: 51 	Training Loss: 1.464617 	Validation Loss: 1.523186 	 time: 0.3
Validation loss decreased from 1.523286 to 1.523186. Model was saved
Epoch: 52 	Training Loss: 1.462950 	Validation Loss: 1.523042 	 time: 0.3
Validation loss decreased from 1.523186 to 1.523042. Model was saved
Epoch: 53 	Training Loss: 1.461349 	Validation Loss: 1.522591 	 time: 0.3
Validation loss decreased from 1.523042 to 1.522591. Model was saved
Epoch: 54 	Training Loss: 1.459709 	Validation Loss: 1.521631 	 time: 0.3
Validation loss decreased from 1.522591 to 1.521631. Model was saved
Epoch: 55 	Training Loss: 1.458133 	Validation Loss: 1.520769 	 time: 0.3
Validation loss decreased from 1.521631 to 1.520769. Model was saved
Epoch: 56 	Training Loss: 1.456581 	Validation Loss: 1.520148 	 time: 0.3
Validation loss decreased from 1.520769 to 1.520148. Model was saved
Epoch: 57 	Training Loss: 1.455224 	Validation Loss: 1.519123 	 time: 0.3
Validation loss decreased from 1.520148 to 1.519123. Model was saved
Epoch: 58 	Training Loss: 1.453887 	Validation Loss: 1.518490 	 time: 0.3
Validation loss decreased from 1.519123 to 1.518490. Model was saved
Epoch: 59 	Training Loss: 1.452688 	Validation Loss: 1.518324 	 time: 0.3
Validation loss decreased from 1.518490 to 1.518324. Model was saved
Epoch: 60 	Training Loss: 1.451460 	Validation Loss: 1.517427 	 time: 0.3
Validation loss decreased from 1.518324 to 1.517427. Model was saved
Epoch: 61 	Training Loss: 1.450249 	Validation Loss: 1.516621 	 time: 0.3
Validation loss decreased from 1.517427 to 1.516621. Model was saved
Epoch: 62 	Training Loss: 1.448984 	Validation Loss: 1.516223 	 time: 0.3
Validation loss decreased from 1.516621 to 1.516223. Model was saved
Epoch: 63 	Training Loss: 1.447737 	Validation Loss: 1.515463 	 time: 0.2
Validation loss decreased from 1.516223 to 1.515463. Model was saved
Epoch: 64 	Training Loss: 1.446496 	Validation Loss: 1.515586 	 time: 0.3
Epoch: 65 	Training Loss: 1.445295 	Validation Loss: 1.515725 	 time: 0.3
Epoch: 66 	Training Loss: 1.444176 	Validation Loss: 1.515937 	 time: 0.3
Epoch: 67 	Training Loss: 1.443092 	Validation Loss: 1.516938 	 time: 0.3
Epoch: 68 	Training Loss: 1.442102 	Validation Loss: 1.516877 	 time: 0.3
Epoch: 69 	Training Loss: 1.441085 	Validation Loss: 1.517023 	 time: 0.3
Epoch: 70 	Training Loss: 1.440105 	Validation Loss: 1.517553 	 time: 0.3
Epoch: 71 	Training Loss: 1.439090 	Validation Loss: 1.517249 	 time: 0.3
Epoch: 72 	Training Loss: 1.438082 	Validation Loss: 1.517387 	 time: 0.3
Epoch: 73 	Training Loss: 1.437092 	Validation Loss: 1.517183 	 time: 0.3
Epoch: 74 	Training Loss: 1.436097 	Validation Loss: 1.517185 	 time: 0.3
Epoch: 75 	Training Loss: 1.435129 	Validation Loss: 1.517038 	 time: 0.3
Epoch: 76 	Training Loss: 1.434144 	Validation Loss: 1.516387 	 time: 0.3
Epoch: 77 	Training Loss: 1.433186 	Validation Loss: 1.516399 	 time: 0.3
Epoch: 78 	Training Loss: 1.432221 	Validation Loss: 1.515471 	 time: 0.2
Epoch: 79 	Training Loss: 1.431269 	Validation Loss: 1.514800 	 time: 0.2
Validation loss decreased from 1.515463 to 1.514800. Model was saved
Epoch: 80 	Training Loss: 1.430321 	Validation Loss: 1.513709 	 time: 0.3
Validation loss decreased from 1.514800 to 1.513709. Model was saved
Epoch: 81 	Training Loss: 1.429359 	Validation Loss: 1.512756 	 time: 0.3
Validation loss decreased from 1.513709 to 1.512756. Model was saved
Epoch: 82 	Training Loss: 1.428391 	Validation Loss: 1.511640 	 time: 0.3
Validation loss decreased from 1.512756 to 1.511640. Model was saved
Epoch: 83 	Training Loss: 1.427396 	Validation Loss: 1.510351 	 time: 0.3
Validation loss decreased from 1.511640 to 1.510351. Model was saved
Epoch: 84 	Training Loss: 1.426386 	Validation Loss: 1.509477 	 time: 0.3
Validation loss decreased from 1.510351 to 1.509477. Model was saved
Epoch: 85 	Training Loss: 1.425335 	Validation Loss: 1.507949 	 time: 0.3
Validation loss decreased from 1.509477 to 1.507949. Model was saved
Epoch: 86 	Training Loss: 1.424243 	Validation Loss: 1.506958 	 time: 0.3
Validation loss decreased from 1.507949 to 1.506958. Model was saved
Epoch: 87 	Training Loss: 1.423103 	Validation Loss: 1.505163 	 time: 0.3
Validation loss decreased from 1.506958 to 1.505163. Model was saved
Epoch: 88 	Training Loss: 1.421900 	Validation Loss: 1.504567 	 time: 0.3
Validation loss decreased from 1.505163 to 1.504567. Model was saved
Epoch: 89 	Training Loss: 1.420639 	Validation Loss: 1.502292 	 time: 0.3
Validation loss decreased from 1.504567 to 1.502292. Model was saved
Epoch: 90 	Training Loss: 1.419304 	Validation Loss: 1.501709 	 time: 0.3
Validation loss decreased from 1.502292 to 1.501709. Model was saved
Epoch: 91 	Training Loss: 1.417921 	Validation Loss: 1.498959 	 time: 0.3
Validation loss decreased from 1.501709 to 1.498959. Model was saved
Epoch: 92 	Training Loss: 1.416467 	Validation Loss: 1.498435 	 time: 0.3
Validation loss decreased from 1.498959 to 1.498435. Model was saved
Epoch: 93 	Training Loss: 1.414963 	Validation Loss: 1.495556 	 time: 0.3
Validation loss decreased from 1.498435 to 1.495556. Model was saved
Epoch: 94 	Training Loss: 1.413410 	Validation Loss: 1.495299 	 time: 0.3
Validation loss decreased from 1.495556 to 1.495299. Model was saved
Epoch: 95 	Training Loss: 1.411843 	Validation Loss: 1.493061 	 time: 0.3
Validation loss decreased from 1.495299 to 1.493061. Model was saved
Epoch: 96 	Training Loss: 1.410295 	Validation Loss: 1.492466 	 time: 0.3
Validation loss decreased from 1.493061 to 1.492466. Model was saved
Epoch: 97 	Training Loss: 1.408788 	Validation Loss: 1.490441 	 time: 0.2
Validation loss decreased from 1.492466 to 1.490441. Model was saved
Epoch: 98 	Training Loss: 1.407329 	Validation Loss: 1.489608 	 time: 0.2
Validation loss decreased from 1.490441 to 1.489608. Model was saved
Epoch: 99 	Training Loss: 1.405935 	Validation Loss: 1.487669 	 time: 0.2
Validation loss decreased from 1.489608 to 1.487669. Model was saved
Epoch: 100 	Training Loss: 1.404612 	Validation Loss: 1.486448 	 time: 0.2
Validation loss decreased from 1.487669 to 1.486448. Model was saved
Epoch: 101 	Training Loss: 1.403360 	Validation Loss: 1.485140 	 time: 0.2
Validation loss decreased from 1.486448 to 1.485140. Model was saved
Epoch: 102 	Training Loss: 1.402164 	Validation Loss: 1.484112 	 time: 0.2
Validation loss decreased from 1.485140 to 1.484112. Model was saved
Epoch: 103 	Training Loss: 1.401007 	Validation Loss: 1.483116 	 time: 0.2
Validation loss decreased from 1.484112 to 1.483116. Model was saved
Epoch: 104 	Training Loss: 1.399873 	Validation Loss: 1.482405 	 time: 0.3
Validation loss decreased from 1.483116 to 1.482405. Model was saved
Epoch: 105 	Training Loss: 1.398754 	Validation Loss: 1.482029 	 time: 0.2
Validation loss decreased from 1.482405 to 1.482029. Model was saved
Epoch: 106 	Training Loss: 1.397652 	Validation Loss: 1.481321 	 time: 0.3
Validation loss decreased from 1.482029 to 1.481321. Model was saved
Epoch: 107 	Training Loss: 1.396567 	Validation Loss: 1.481261 	 time: 0.3
Validation loss decreased from 1.481321 to 1.481261. Model was saved
Epoch: 108 	Training Loss: 1.395507 	Validation Loss: 1.480679 	 time: 0.2
Validation loss decreased from 1.481261 to 1.480679. Model was saved
Epoch: 109 	Training Loss: 1.394474 	Validation Loss: 1.480713 	 time: 0.3
Epoch: 110 	Training Loss: 1.393471 	Validation Loss: 1.479667 	 time: 0.2
Validation loss decreased from 1.480679 to 1.479667. Model was saved
Epoch: 111 	Training Loss: 1.392509 	Validation Loss: 1.480270 	 time: 0.3
Epoch: 112 	Training Loss: 1.391615 	Validation Loss: 1.478491 	 time: 0.3
Validation loss decreased from 1.479667 to 1.478491. Model was saved
Epoch: 113 	Training Loss: 1.390773 	Validation Loss: 1.479182 	 time: 0.3
Epoch: 114 	Training Loss: 1.389846 	Validation Loss: 1.477026 	 time: 0.3
Validation loss decreased from 1.478491 to 1.477026. Model was saved
Epoch: 115 	Training Loss: 1.388767 	Validation Loss: 1.476734 	 time: 0.3
Validation loss decreased from 1.477026 to 1.476734. Model was saved
Epoch: 116 	Training Loss: 1.387716 	Validation Loss: 1.475843 	 time: 0.3
Validation loss decreased from 1.476734 to 1.475843. Model was saved
Epoch: 117 	Training Loss: 1.386787 	Validation Loss: 1.474684 	 time: 0.3
Validation loss decreased from 1.475843 to 1.474684. Model was saved
Epoch: 118 	Training Loss: 1.385954 	Validation Loss: 1.474937 	 time: 0.3
Epoch: 119 	Training Loss: 1.385125 	Validation Loss: 1.473389 	 time: 0.3
Validation loss decreased from 1.474684 to 1.473389. Model was saved
Epoch: 120 	Training Loss: 1.384224 	Validation Loss: 1.473493 	 time: 0.3
Epoch: 121 	Training Loss: 1.383255 	Validation Loss: 1.472502 	 time: 0.3
Validation loss decreased from 1.473389 to 1.472502. Model was saved
Epoch: 122 	Training Loss: 1.382270 	Validation Loss: 1.472039 	 time: 0.3
Validation loss decreased from 1.472502 to 1.472039. Model was saved
Epoch: 123 	Training Loss: 1.381320 	Validation Loss: 1.471503 	 time: 0.3
Validation loss decreased from 1.472039 to 1.471503. Model was saved
Epoch: 124 	Training Loss: 1.380421 	Validation Loss: 1.470826 	 time: 0.3
Validation loss decreased from 1.471503 to 1.470826. Model was saved
Epoch: 125 	Training Loss: 1.379563 	Validation Loss: 1.470374 	 time: 0.3
Validation loss decreased from 1.470826 to 1.470374. Model was saved
Epoch: 126 	Training Loss: 1.378729 	Validation Loss: 1.469713 	 time: 0.3
Validation loss decreased from 1.470374 to 1.469713. Model was saved
Epoch: 127 	Training Loss: 1.377914 	Validation Loss: 1.469322 	 time: 0.3
Validation loss decreased from 1.469713 to 1.469322. Model was saved
Epoch: 128 	Training Loss: 1.377114 	Validation Loss: 1.468702 	 time: 0.3
Validation loss decreased from 1.469322 to 1.468702. Model was saved
Epoch: 129 	Training Loss: 1.376308 	Validation Loss: 1.468237 	 time: 0.3
Validation loss decreased from 1.468702 to 1.468237. Model was saved
Epoch: 130 	Training Loss: 1.375469 	Validation Loss: 1.467677 	 time: 0.3
Validation loss decreased from 1.468237 to 1.467677. Model was saved
Epoch: 131 	Training Loss: 1.374591 	Validation Loss: 1.467087 	 time: 0.3
Validation loss decreased from 1.467677 to 1.467087. Model was saved
Epoch: 132 	Training Loss: 1.373709 	Validation Loss: 1.466613 	 time: 0.3
Validation loss decreased from 1.467087 to 1.466613. Model was saved
Epoch: 133 	Training Loss: 1.372840 	Validation Loss: 1.466002 	 time: 0.3
Validation loss decreased from 1.466613 to 1.466002. Model was saved
Epoch: 134 	Training Loss: 1.371998 	Validation Loss: 1.465582 	 time: 0.3
Validation loss decreased from 1.466002 to 1.465582. Model was saved
Epoch: 135 	Training Loss: 1.371179 	Validation Loss: 1.465073 	 time: 0.3
Validation loss decreased from 1.465582 to 1.465073. Model was saved
Epoch: 136 	Training Loss: 1.370376 	Validation Loss: 1.464690 	 time: 0.3
Validation loss decreased from 1.465073 to 1.464690. Model was saved
Epoch: 137 	Training Loss: 1.369583 	Validation Loss: 1.464286 	 time: 0.3
Validation loss decreased from 1.464690 to 1.464286. Model was saved
Epoch: 138 	Training Loss: 1.368799 	Validation Loss: 1.464136 	 time: 0.3
Validation loss decreased from 1.464286 to 1.464136. Model was saved
Epoch: 139 	Training Loss: 1.368039 	Validation Loss: 1.463705 	 time: 0.3
Validation loss decreased from 1.464136 to 1.463705. Model was saved
Epoch: 140 	Training Loss: 1.367339 	Validation Loss: 1.464032 	 time: 0.3
Epoch: 141 	Training Loss: 1.366796 	Validation Loss: 1.463557 	 time: 0.3
Validation loss decreased from 1.463705 to 1.463557. Model was saved
Epoch: 142 	Training Loss: 1.366270 	Validation Loss: 1.463677 	 time: 0.3
Epoch: 143 	Training Loss: 1.365153 	Validation Loss: 1.463202 	 time: 0.3
Validation loss decreased from 1.463557 to 1.463202. Model was saved
Epoch: 144 	Training Loss: 1.364106 	Validation Loss: 1.462997 	 time: 0.3
Validation loss decreased from 1.463202 to 1.462997. Model was saved
Epoch: 145 	Training Loss: 1.363616 	Validation Loss: 1.463504 	 time: 0.2
Epoch: 146 	Training Loss: 1.363024 	Validation Loss: 1.462695 	 time: 0.3
Validation loss decreased from 1.462997 to 1.462695. Model was saved
Epoch: 147 	Training Loss: 1.362009 	Validation Loss: 1.462764 	 time: 0.3
Epoch: 148 	Training Loss: 1.361070 	Validation Loss: 1.462900 	 time: 0.3
Epoch: 149 	Training Loss: 1.360485 	Validation Loss: 1.462368 	 time: 0.3
Validation loss decreased from 1.462695 to 1.462368. Model was saved
Epoch: 150 	Training Loss: 1.359910 	Validation Loss: 1.462652 	 time: 0.3
Epoch: 151 	Training Loss: 1.359046 	Validation Loss: 1.462051 	 time: 0.3
Validation loss decreased from 1.462368 to 1.462051. Model was saved
Epoch: 152 	Training Loss: 1.358090 	Validation Loss: 1.462041 	 time: 0.3
Validation loss decreased from 1.462051 to 1.462041. Model was saved
Epoch: 153 	Training Loss: 1.357315 	Validation Loss: 1.462146 	 time: 0.3
Epoch: 154 	Training Loss: 1.356705 	Validation Loss: 1.461577 	 time: 0.3
Validation loss decreased from 1.462041 to 1.461577. Model was saved
Epoch: 155 	Training Loss: 1.356065 	Validation Loss: 1.462107 	 time: 0.3
Epoch: 156 	Training Loss: 1.355279 	Validation Loss: 1.461271 	 time: 0.3
Validation loss decreased from 1.461577 to 1.461271. Model was saved
Epoch: 157 	Training Loss: 1.354416 	Validation Loss: 1.461413 	 time: 0.3
Epoch: 158 	Training Loss: 1.353586 	Validation Loss: 1.461180 	 time: 0.3
Validation loss decreased from 1.461271 to 1.461180. Model was saved
Epoch: 159 	Training Loss: 1.352838 	Validation Loss: 1.460706 	 time: 0.3
Validation loss decreased from 1.461180 to 1.460706. Model was saved
Epoch: 160 	Training Loss: 1.352155 	Validation Loss: 1.461002 	 time: 0.3
Epoch: 161 	Training Loss: 1.351490 	Validation Loss: 1.460034 	 time: 0.2
Validation loss decreased from 1.460706 to 1.460034. Model was saved
Epoch: 162 	Training Loss: 1.350797 	Validation Loss: 1.460733 	 time: 0.3
Epoch: 163 	Training Loss: 1.350079 	Validation Loss: 1.459567 	 time: 0.3
Validation loss decreased from 1.460034 to 1.459567. Model was saved
Epoch: 164 	Training Loss: 1.349328 	Validation Loss: 1.460209 	 time: 0.2
Epoch: 165 	Training Loss: 1.348558 	Validation Loss: 1.459366 	 time: 0.2
Validation loss decreased from 1.459567 to 1.459366. Model was saved
Epoch: 166 	Training Loss: 1.347787 	Validation Loss: 1.459802 	 time: 0.2
Epoch: 167 	Training Loss: 1.347021 	Validation Loss: 1.459055 	 time: 0.2
Validation loss decreased from 1.459366 to 1.459055. Model was saved
Epoch: 168 	Training Loss: 1.346260 	Validation Loss: 1.459707 	 time: 0.3
Epoch: 169 	Training Loss: 1.345520 	Validation Loss: 1.458678 	 time: 0.3
Validation loss decreased from 1.459055 to 1.458678. Model was saved
Epoch: 170 	Training Loss: 1.344794 	Validation Loss: 1.459435 	 time: 0.2
Epoch: 171 	Training Loss: 1.344106 	Validation Loss: 1.457997 	 time: 0.3
Validation loss decreased from 1.458678 to 1.457997. Model was saved
Epoch: 172 	Training Loss: 1.343465 	Validation Loss: 1.458956 	 time: 0.2
Epoch: 173 	Training Loss: 1.342953 	Validation Loss: 1.456726 	 time: 0.3
Validation loss decreased from 1.457997 to 1.456726. Model was saved
Epoch: 174 	Training Loss: 1.342391 	Validation Loss: 1.458124 	 time: 0.2
Epoch: 175 	Training Loss: 1.341568 	Validation Loss: 1.456004 	 time: 0.3
Validation loss decreased from 1.456726 to 1.456004. Model was saved
Epoch: 176 	Training Loss: 1.340610 	Validation Loss: 1.455971 	 time: 0.2
Validation loss decreased from 1.456004 to 1.455971. Model was saved
Epoch: 177 	Training Loss: 1.339741 	Validation Loss: 1.455971 	 time: 0.2
Validation loss decreased from 1.455971 to 1.455971. Model was saved
Epoch: 178 	Training Loss: 1.339123 	Validation Loss: 1.454322 	 time: 0.2
Validation loss decreased from 1.455971 to 1.454322. Model was saved
Epoch: 179 	Training Loss: 1.338644 	Validation Loss: 1.455265 	 time: 0.2
Epoch: 180 	Training Loss: 1.338118 	Validation Loss: 1.453337 	 time: 0.2
Validation loss decreased from 1.454322 to 1.453337. Model was saved
Epoch: 181 	Training Loss: 1.337427 	Validation Loss: 1.454212 	 time: 0.2
Epoch: 182 	Training Loss: 1.336662 	Validation Loss: 1.452041 	 time: 0.3
Validation loss decreased from 1.453337 to 1.452041. Model was saved
Epoch: 183 	Training Loss: 1.335827 	Validation Loss: 1.452769 	 time: 0.3
Epoch: 184 	Training Loss: 1.334994 	Validation Loss: 1.451716 	 time: 0.2
Validation loss decreased from 1.452041 to 1.451716. Model was saved
Epoch: 185 	Training Loss: 1.334221 	Validation Loss: 1.451105 	 time: 0.3
Validation loss decreased from 1.451716 to 1.451105. Model was saved
Epoch: 186 	Training Loss: 1.333509 	Validation Loss: 1.451319 	 time: 0.2
Epoch: 187 	Training Loss: 1.332822 	Validation Loss: 1.450414 	 time: 0.3
Validation loss decreased from 1.451105 to 1.450414. Model was saved
Epoch: 188 	Training Loss: 1.332165 	Validation Loss: 1.450619 	 time: 0.3
Epoch: 189 	Training Loss: 1.331576 	Validation Loss: 1.449640 	 time: 0.3
Validation loss decreased from 1.450414 to 1.449640. Model was saved
Epoch: 190 	Training Loss: 1.331069 	Validation Loss: 1.450715 	 time: 0.3
Epoch: 191 	Training Loss: 1.330770 	Validation Loss: 1.448248 	 time: 0.3
Validation loss decreased from 1.449640 to 1.448248. Model was saved
Epoch: 192 	Training Loss: 1.330012 	Validation Loss: 1.449931 	 time: 0.3
Epoch: 193 	Training Loss: 1.328686 	Validation Loss: 1.449435 	 time: 0.3
Epoch: 194 	Training Loss: 1.327864 	Validation Loss: 1.447782 	 time: 0.2
Validation loss decreased from 1.448248 to 1.447782. Model was saved
Epoch: 195 	Training Loss: 1.327496 	Validation Loss: 1.449493 	 time: 0.3
Epoch: 196 	Training Loss: 1.326892 	Validation Loss: 1.447980 	 time: 0.2
Epoch: 197 	Training Loss: 1.326009 	Validation Loss: 1.448196 	 time: 0.2
Epoch: 198 	Training Loss: 1.324968 	Validation Loss: 1.448048 	 time: 0.2
Epoch: 199 	Training Loss: 1.324071 	Validation Loss: 1.447470 	 time: 0.2
Validation loss decreased from 1.447782 to 1.447470. Model was saved
Epoch: 200 	Training Loss: 1.323426 	Validation Loss: 1.447548 	 time: 0.3
Epoch: 201 	Training Loss: 1.322941 	Validation Loss: 1.446892 	 time: 0.3
Validation loss decreased from 1.447470 to 1.446892. Model was saved
Epoch: 202 	Training Loss: 1.322395 	Validation Loss: 1.447878 	 time: 0.3
Epoch: 203 	Training Loss: 1.321666 	Validation Loss: 1.446043 	 time: 0.3
Validation loss decreased from 1.446892 to 1.446043. Model was saved
Epoch: 204 	Training Loss: 1.320907 	Validation Loss: 1.447108 	 time: 0.3
Epoch: 205 	Training Loss: 1.320073 	Validation Loss: 1.445600 	 time: 0.2
Validation loss decreased from 1.446043 to 1.445600. Model was saved
Epoch: 206 	Training Loss: 1.319304 	Validation Loss: 1.445850 	 time: 0.2
Epoch: 207 	Training Loss: 1.318601 	Validation Loss: 1.444992 	 time: 0.2
Validation loss decreased from 1.445600 to 1.444992. Model was saved
Epoch: 208 	Training Loss: 1.317815 	Validation Loss: 1.444908 	 time: 0.2
Validation loss decreased from 1.444992 to 1.444908. Model was saved
Epoch: 209 	Training Loss: 1.317070 	Validation Loss: 1.443806 	 time: 0.3
Validation loss decreased from 1.444908 to 1.443806. Model was saved
Epoch: 210 	Training Loss: 1.316386 	Validation Loss: 1.443971 	 time: 0.2
Epoch: 211 	Training Loss: 1.315732 	Validation Loss: 1.443028 	 time: 0.2
Validation loss decreased from 1.443806 to 1.443028. Model was saved
Epoch: 212 	Training Loss: 1.315097 	Validation Loss: 1.443069 	 time: 0.3
Epoch: 213 	Training Loss: 1.314461 	Validation Loss: 1.442012 	 time: 0.2
Validation loss decreased from 1.443028 to 1.442012. Model was saved
Epoch: 214 	Training Loss: 1.313951 	Validation Loss: 1.443057 	 time: 0.3
Epoch: 215 	Training Loss: 1.313821 	Validation Loss: 1.440163 	 time: 0.3
Validation loss decreased from 1.442012 to 1.440163. Model was saved
Epoch: 216 	Training Loss: 1.313993 	Validation Loss: 1.442826 	 time: 0.3
Epoch: 217 	Training Loss: 1.312625 	Validation Loss: 1.441067 	 time: 0.3
Epoch: 218 	Training Loss: 1.311309 	Validation Loss: 1.439173 	 time: 0.2
Validation loss decreased from 1.440163 to 1.439173. Model was saved
Epoch: 219 	Training Loss: 1.311619 	Validation Loss: 1.441354 	 time: 0.3
Epoch: 220 	Training Loss: 1.310682 	Validation Loss: 1.439703 	 time: 0.3
Epoch: 221 	Training Loss: 1.309824 	Validation Loss: 1.438253 	 time: 0.3
Validation loss decreased from 1.439173 to 1.438253. Model was saved
Epoch: 222 	Training Loss: 1.309268 	Validation Loss: 1.440349 	 time: 0.3
Epoch: 223 	Training Loss: 1.308807 	Validation Loss: 1.438723 	 time: 0.3
Epoch: 224 	Training Loss: 1.308234 	Validation Loss: 1.437320 	 time: 0.3
Validation loss decreased from 1.438253 to 1.437320. Model was saved
Epoch: 225 	Training Loss: 1.307120 	Validation Loss: 1.439785 	 time: 0.3
Epoch: 226 	Training Loss: 1.307297 	Validation Loss: 1.438009 	 time: 0.2
Epoch: 227 	Training Loss: 1.306318 	Validation Loss: 1.436947 	 time: 0.2
Validation loss decreased from 1.437320 to 1.436947. Model was saved
Epoch: 228 	Training Loss: 1.305568 	Validation Loss: 1.438573 	 time: 0.3
Epoch: 229 	Training Loss: 1.305365 	Validation Loss: 1.437055 	 time: 0.2
Epoch: 230 	Training Loss: 1.304491 	Validation Loss: 1.436555 	 time: 0.3
Validation loss decreased from 1.436947 to 1.436555. Model was saved
Epoch: 231 	Training Loss: 1.303965 	Validation Loss: 1.436020 	 time: 0.2
Validation loss decreased from 1.436555 to 1.436020. Model was saved
Epoch: 232 	Training Loss: 1.303287 	Validation Loss: 1.435631 	 time: 0.2
Validation loss decreased from 1.436020 to 1.435631. Model was saved
Epoch: 233 	Training Loss: 1.303018 	Validation Loss: 1.436026 	 time: 0.3
Epoch: 234 	Training Loss: 1.302366 	Validation Loss: 1.433818 	 time: 0.2
Validation loss decreased from 1.435631 to 1.433818. Model was saved
Epoch: 235 	Training Loss: 1.301674 	Validation Loss: 1.433962 	 time: 0.2
Epoch: 236 	Training Loss: 1.301353 	Validation Loss: 1.435538 	 time: 0.2
Epoch: 237 	Training Loss: 1.300814 	Validation Loss: 1.433070 	 time: 0.2
Validation loss decreased from 1.433818 to 1.433070. Model was saved
Epoch: 238 	Training Loss: 1.300105 	Validation Loss: 1.432669 	 time: 0.3
Validation loss decreased from 1.433070 to 1.432669. Model was saved
Epoch: 239 	Training Loss: 1.299672 	Validation Loss: 1.434651 	 time: 0.3
Epoch: 240 	Training Loss: 1.299197 	Validation Loss: 1.433105 	 time: 0.3
Epoch: 241 	Training Loss: 1.298581 	Validation Loss: 1.432109 	 time: 0.2
Validation loss decreased from 1.432669 to 1.432109. Model was saved
Epoch: 242 	Training Loss: 1.298027 	Validation Loss: 1.433432 	 time: 0.2
Epoch: 243 	Training Loss: 1.297534 	Validation Loss: 1.433152 	 time: 0.3
Epoch: 244 	Training Loss: 1.297161 	Validation Loss: 1.432127 	 time: 0.3
Epoch: 245 	Training Loss: 1.296493 	Validation Loss: 1.432054 	 time: 0.3
Validation loss decreased from 1.432109 to 1.432054. Model was saved
Epoch: 246 	Training Loss: 1.295983 	Validation Loss: 1.432329 	 time: 0.3
Epoch: 247 	Training Loss: 1.295604 	Validation Loss: 1.431476 	 time: 0.3
Validation loss decreased from 1.432054 to 1.431476. Model was saved
Epoch: 248 	Training Loss: 1.295034 	Validation Loss: 1.430121 	 time: 0.3
Validation loss decreased from 1.431476 to 1.430121. Model was saved
Epoch: 249 	Training Loss: 1.294530 	Validation Loss: 1.430679 	 time: 0.3
Epoch: 250 	Training Loss: 1.293977 	Validation Loss: 1.430052 	 time: 0.2
Validation loss decreased from 1.430121 to 1.430052. Model was saved
Epoch: 251 	Training Loss: 1.293558 	Validation Loss: 1.428118 	 time: 0.3
Validation loss decreased from 1.430052 to 1.428118. Model was saved
Epoch: 252 	Training Loss: 1.293130 	Validation Loss: 1.428532 	 time: 0.2
Epoch: 253 	Training Loss: 1.292473 	Validation Loss: 1.428331 	 time: 0.3
Epoch: 254 	Training Loss: 1.292055 	Validation Loss: 1.426630 	 time: 0.3
Validation loss decreased from 1.428118 to 1.426630. Model was saved
Epoch: 255 	Training Loss: 1.291607 	Validation Loss: 1.426794 	 time: 0.3
Epoch: 256 	Training Loss: 1.291044 	Validation Loss: 1.426934 	 time: 0.2
Epoch: 257 	Training Loss: 1.290549 	Validation Loss: 1.425902 	 time: 0.2
Validation loss decreased from 1.426630 to 1.425902. Model was saved
Epoch: 258 	Training Loss: 1.289998 	Validation Loss: 1.425733 	 time: 0.3
Validation loss decreased from 1.425902 to 1.425733. Model was saved
Epoch: 259 	Training Loss: 1.289555 	Validation Loss: 1.425929 	 time: 0.2
Epoch: 260 	Training Loss: 1.289107 	Validation Loss: 1.425339 	 time: 0.3
Validation loss decreased from 1.425733 to 1.425339. Model was saved
Epoch: 261 	Training Loss: 1.288561 	Validation Loss: 1.424760 	 time: 0.3
Validation loss decreased from 1.425339 to 1.424760. Model was saved
Epoch: 262 	Training Loss: 1.288092 	Validation Loss: 1.424885 	 time: 0.3
Epoch: 263 	Training Loss: 1.287635 	Validation Loss: 1.424407 	 time: 0.3
Validation loss decreased from 1.424760 to 1.424407. Model was saved
Epoch: 264 	Training Loss: 1.287141 	Validation Loss: 1.423644 	 time: 0.3
Validation loss decreased from 1.424407 to 1.423644. Model was saved
Epoch: 265 	Training Loss: 1.286664 	Validation Loss: 1.423779 	 time: 0.3
Epoch: 266 	Training Loss: 1.286141 	Validation Loss: 1.423343 	 time: 0.3
Validation loss decreased from 1.423644 to 1.423343. Model was saved
Epoch: 267 	Training Loss: 1.285616 	Validation Loss: 1.422693 	 time: 0.3
Validation loss decreased from 1.423343 to 1.422693. Model was saved
Epoch: 268 	Training Loss: 1.285124 	Validation Loss: 1.422772 	 time: 0.3
Epoch: 269 	Training Loss: 1.284595 	Validation Loss: 1.422370 	 time: 0.2
Validation loss decreased from 1.422693 to 1.422370. Model was saved
Epoch: 270 	Training Loss: 1.284038 	Validation Loss: 1.421957 	 time: 0.3
Validation loss decreased from 1.422370 to 1.421957. Model was saved
Epoch: 271 	Training Loss: 1.283478 	Validation Loss: 1.421833 	 time: 0.3
Validation loss decreased from 1.421957 to 1.421833. Model was saved
Epoch: 272 	Training Loss: 1.282897 	Validation Loss: 1.421577 	 time: 0.3
Validation loss decreased from 1.421833 to 1.421577. Model was saved
Epoch: 273 	Training Loss: 1.282341 	Validation Loss: 1.421204 	 time: 0.3
Validation loss decreased from 1.421577 to 1.421204. Model was saved
Epoch: 274 	Training Loss: 1.281837 	Validation Loss: 1.420813 	 time: 0.3
Validation loss decreased from 1.421204 to 1.420813. Model was saved
Epoch: 275 	Training Loss: 1.281341 	Validation Loss: 1.420534 	 time: 0.3
Validation loss decreased from 1.420813 to 1.420534. Model was saved
Epoch: 276 	Training Loss: 1.280844 	Validation Loss: 1.419695 	 time: 0.3
Validation loss decreased from 1.420534 to 1.419695. Model was saved
Epoch: 277 	Training Loss: 1.280350 	Validation Loss: 1.419210 	 time: 0.3
Validation loss decreased from 1.419695 to 1.419210. Model was saved
Epoch: 278 	Training Loss: 1.279848 	Validation Loss: 1.418504 	 time: 0.3
Validation loss decreased from 1.419210 to 1.418504. Model was saved
Epoch: 279 	Training Loss: 1.279343 	Validation Loss: 1.417551 	 time: 0.3
Validation loss decreased from 1.418504 to 1.417551. Model was saved
Epoch: 280 	Training Loss: 1.278840 	Validation Loss: 1.416969 	 time: 0.3
Validation loss decreased from 1.417551 to 1.416969. Model was saved
Epoch: 281 	Training Loss: 1.278337 	Validation Loss: 1.416064 	 time: 0.3
Validation loss decreased from 1.416969 to 1.416064. Model was saved
Epoch: 282 	Training Loss: 1.277831 	Validation Loss: 1.415416 	 time: 0.3
Validation loss decreased from 1.416064 to 1.415416. Model was saved
Epoch: 283 	Training Loss: 1.277335 	Validation Loss: 1.414728 	 time: 0.2
Validation loss decreased from 1.415416 to 1.414728. Model was saved
Epoch: 284 	Training Loss: 1.276850 	Validation Loss: 1.414136 	 time: 0.3
Validation loss decreased from 1.414728 to 1.414136. Model was saved
Epoch: 285 	Training Loss: 1.276363 	Validation Loss: 1.413523 	 time: 0.3
Validation loss decreased from 1.414136 to 1.413523. Model was saved
Epoch: 286 	Training Loss: 1.275874 	Validation Loss: 1.413005 	 time: 0.3
Validation loss decreased from 1.413523 to 1.413005. Model was saved
Epoch: 287 	Training Loss: 1.275388 	Validation Loss: 1.412527 	 time: 0.3
Validation loss decreased from 1.413005 to 1.412527. Model was saved
Epoch: 288 	Training Loss: 1.274911 	Validation Loss: 1.412001 	 time: 0.3
Validation loss decreased from 1.412527 to 1.412001. Model was saved
Epoch: 289 	Training Loss: 1.274446 	Validation Loss: 1.411884 	 time: 0.3
Validation loss decreased from 1.412001 to 1.411884. Model was saved
Epoch: 290 	Training Loss: 1.273992 	Validation Loss: 1.411378 	 time: 0.3
Validation loss decreased from 1.411884 to 1.411378. Model was saved
Epoch: 291 	Training Loss: 1.273540 	Validation Loss: 1.411414 	 time: 0.2
Epoch: 292 	Training Loss: 1.273092 	Validation Loss: 1.411050 	 time: 0.3
Validation loss decreased from 1.411378 to 1.411050. Model was saved
Epoch: 293 	Training Loss: 1.272649 	Validation Loss: 1.410960 	 time: 0.3
Validation loss decreased from 1.411050 to 1.410960. Model was saved
Epoch: 294 	Training Loss: 1.272212 	Validation Loss: 1.410517 	 time: 0.2
Validation loss decreased from 1.410960 to 1.410517. Model was saved
Epoch: 295 	Training Loss: 1.271773 	Validation Loss: 1.410394 	 time: 0.3
Validation loss decreased from 1.410517 to 1.410394. Model was saved
Epoch: 296 	Training Loss: 1.271339 	Validation Loss: 1.409593 	 time: 0.2
Validation loss decreased from 1.410394 to 1.409593. Model was saved
Epoch: 297 	Training Loss: 1.270900 	Validation Loss: 1.409522 	 time: 0.2
Validation loss decreased from 1.409593 to 1.409522. Model was saved
Epoch: 298 	Training Loss: 1.270467 	Validation Loss: 1.408548 	 time: 0.2
Validation loss decreased from 1.409522 to 1.408548. Model was saved
Epoch: 299 	Training Loss: 1.270021 	Validation Loss: 1.408474 	 time: 0.2
Validation loss decreased from 1.408548 to 1.408474. Model was saved
Epoch: 300 	Training Loss: 1.269616 	Validation Loss: 1.407577 	 time: 0.2
Validation loss decreased from 1.408474 to 1.407577. Model was saved
Epoch: 301 	Training Loss: 1.269226 	Validation Loss: 1.407819 	 time: 0.3
Epoch: 302 	Training Loss: 1.268976 	Validation Loss: 1.406799 	 time: 0.3
Validation loss decreased from 1.407577 to 1.406799. Model was saved
Epoch: 303 	Training Loss: 1.268653 	Validation Loss: 1.407690 	 time: 0.3
Epoch: 304 	Training Loss: 1.268606 	Validation Loss: 1.406158 	 time: 0.3
Validation loss decreased from 1.406799 to 1.406158. Model was saved
Epoch: 305 	Training Loss: 1.268023 	Validation Loss: 1.407471 	 time: 0.3
Epoch: 306 	Training Loss: 1.267617 	Validation Loss: 1.405897 	 time: 0.3
Validation loss decreased from 1.406158 to 1.405897. Model was saved
Epoch: 307 	Training Loss: 1.266953 	Validation Loss: 1.406739 	 time: 0.3
Epoch: 308 	Training Loss: 1.266420 	Validation Loss: 1.406136 	 time: 0.2
Epoch: 309 	Training Loss: 1.265977 	Validation Loss: 1.405761 	 time: 0.3
Validation loss decreased from 1.405897 to 1.405761. Model was saved
Epoch: 310 	Training Loss: 1.265626 	Validation Loss: 1.406287 	 time: 0.3
Epoch: 311 	Training Loss: 1.265338 	Validation Loss: 1.405104 	 time: 0.2
Validation loss decreased from 1.405761 to 1.405104. Model was saved
Epoch: 312 	Training Loss: 1.265091 	Validation Loss: 1.406185 	 time: 0.3
Epoch: 313 	Training Loss: 1.264979 	Validation Loss: 1.404751 	 time: 0.3
Validation loss decreased from 1.405104 to 1.404751. Model was saved
Epoch: 314 	Training Loss: 1.264645 	Validation Loss: 1.406072 	 time: 0.3
Epoch: 315 	Training Loss: 1.264520 	Validation Loss: 1.404406 	 time: 0.3
Validation loss decreased from 1.404751 to 1.404406. Model was saved
Epoch: 316 	Training Loss: 1.263943 	Validation Loss: 1.405760 	 time: 0.3
Epoch: 317 	Training Loss: 1.263498 	Validation Loss: 1.404167 	 time: 0.3
Validation loss decreased from 1.404406 to 1.404167. Model was saved
Epoch: 318 	Training Loss: 1.262938 	Validation Loss: 1.404771 	 time: 0.3
Epoch: 319 	Training Loss: 1.262462 	Validation Loss: 1.403916 	 time: 0.3
Validation loss decreased from 1.404167 to 1.403916. Model was saved
Epoch: 320 	Training Loss: 1.262016 	Validation Loss: 1.403281 	 time: 0.3
Validation loss decreased from 1.403916 to 1.403281. Model was saved
Epoch: 321 	Training Loss: 1.261634 	Validation Loss: 1.403283 	 time: 0.3
Epoch: 322 	Training Loss: 1.261310 	Validation Loss: 1.402204 	 time: 0.3
Validation loss decreased from 1.403281 to 1.402204. Model was saved
Epoch: 323 	Training Loss: 1.261028 	Validation Loss: 1.402673 	 time: 0.3
Epoch: 324 	Training Loss: 1.260870 	Validation Loss: 1.401702 	 time: 0.3
Validation loss decreased from 1.402204 to 1.401702. Model was saved
Epoch: 325 	Training Loss: 1.260843 	Validation Loss: 1.402872 	 time: 0.3
Epoch: 326 	Training Loss: 1.261496 	Validation Loss: 1.400882 	 time: 0.3
Validation loss decreased from 1.401702 to 1.400882. Model was saved
Epoch: 327 	Training Loss: 1.260190 	Validation Loss: 1.403126 	 time: 0.3
Epoch: 328 	Training Loss: 1.259526 	Validation Loss: 1.402177 	 time: 0.2
Epoch: 329 	Training Loss: 1.259480 	Validation Loss: 1.400523 	 time: 0.2
Validation loss decreased from 1.400882 to 1.400523. Model was saved
Epoch: 330 	Training Loss: 1.259353 	Validation Loss: 1.402997 	 time: 0.3
Epoch: 331 	Training Loss: 1.258711 	Validation Loss: 1.401119 	 time: 0.2
Epoch: 332 	Training Loss: 1.257998 	Validation Loss: 1.399831 	 time: 0.3
Validation loss decreased from 1.400523 to 1.399831. Model was saved
Epoch: 333 	Training Loss: 1.258103 	Validation Loss: 1.401809 	 time: 0.3
Epoch: 334 	Training Loss: 1.257790 	Validation Loss: 1.399950 	 time: 0.3
Epoch: 335 	Training Loss: 1.257273 	Validation Loss: 1.399608 	 time: 0.3
Validation loss decreased from 1.399831 to 1.399608. Model was saved
Epoch: 336 	Training Loss: 1.257023 	Validation Loss: 1.400729 	 time: 0.3
Epoch: 337 	Training Loss: 1.256663 	Validation Loss: 1.399662 	 time: 0.2
Epoch: 338 	Training Loss: 1.256561 	Validation Loss: 1.399697 	 time: 0.3
Epoch: 339 	Training Loss: 1.256390 	Validation Loss: 1.399417 	 time: 0.3
Validation loss decreased from 1.399608 to 1.399417. Model was saved
Epoch: 340 	Training Loss: 1.255738 	Validation Loss: 1.399572 	 time: 0.3
Epoch: 341 	Training Loss: 1.255647 	Validation Loss: 1.399332 	 time: 0.3
Validation loss decreased from 1.399417 to 1.399332. Model was saved
Epoch: 342 	Training Loss: 1.255475 	Validation Loss: 1.398385 	 time: 0.3
Validation loss decreased from 1.399332 to 1.398385. Model was saved
Epoch: 343 	Training Loss: 1.254990 	Validation Loss: 1.399817 	 time: 0.3
Epoch: 344 	Training Loss: 1.254748 	Validation Loss: 1.398889 	 time: 0.2
Epoch: 345 	Training Loss: 1.254306 	Validation Loss: 1.397973 	 time: 0.3
Validation loss decreased from 1.398385 to 1.397973. Model was saved
Epoch: 346 	Training Loss: 1.254090 	Validation Loss: 1.399466 	 time: 0.3
Epoch: 347 	Training Loss: 1.253909 	Validation Loss: 1.397774 	 time: 0.2
Validation loss decreased from 1.397973 to 1.397774. Model was saved
Epoch: 348 	Training Loss: 1.253410 	Validation Loss: 1.397748 	 time: 0.3
Validation loss decreased from 1.397774 to 1.397748. Model was saved
Epoch: 349 	Training Loss: 1.253097 	Validation Loss: 1.398509 	 time: 0.3
Epoch: 350 	Training Loss: 1.252904 	Validation Loss: 1.397404 	 time: 0.3
Validation loss decreased from 1.397748 to 1.397404. Model was saved
Epoch: 351 	Training Loss: 1.252586 	Validation Loss: 1.397831 	 time: 0.3
Epoch: 352 	Training Loss: 1.252368 	Validation Loss: 1.397737 	 time: 0.3
Epoch: 353 	Training Loss: 1.252075 	Validation Loss: 1.397442 	 time: 0.3
Epoch: 354 	Training Loss: 1.251709 	Validation Loss: 1.397036 	 time: 0.3
Validation loss decreased from 1.397404 to 1.397036. Model was saved
Epoch: 355 	Training Loss: 1.251457 	Validation Loss: 1.396842 	 time: 0.3
Validation loss decreased from 1.397036 to 1.396842. Model was saved
Epoch: 356 	Training Loss: 1.251223 	Validation Loss: 1.396660 	 time: 0.2
Validation loss decreased from 1.396842 to 1.396660. Model was saved
Epoch: 357 	Training Loss: 1.250936 	Validation Loss: 1.395629 	 time: 0.3
Validation loss decreased from 1.396660 to 1.395629. Model was saved
Epoch: 358 	Training Loss: 1.250668 	Validation Loss: 1.396132 	 time: 0.3
Epoch: 359 	Training Loss: 1.250403 	Validation Loss: 1.395272 	 time: 0.3
Validation loss decreased from 1.395629 to 1.395272. Model was saved
Epoch: 360 	Training Loss: 1.250100 	Validation Loss: 1.394830 	 time: 0.2
Validation loss decreased from 1.395272 to 1.394830. Model was saved
Epoch: 361 	Training Loss: 1.249828 	Validation Loss: 1.395212 	 time: 0.2
Epoch: 362 	Training Loss: 1.249578 	Validation Loss: 1.394177 	 time: 0.3
Validation loss decreased from 1.394830 to 1.394177. Model was saved
Epoch: 363 	Training Loss: 1.249302 	Validation Loss: 1.394469 	 time: 0.3
Epoch: 364 	Training Loss: 1.249033 	Validation Loss: 1.394104 	 time: 0.3
Validation loss decreased from 1.394177 to 1.394104. Model was saved
Epoch: 365 	Training Loss: 1.248793 	Validation Loss: 1.394094 	 time: 0.3
Validation loss decreased from 1.394104 to 1.394094. Model was saved
Epoch: 366 	Training Loss: 1.248554 	Validation Loss: 1.393549 	 time: 0.3
Validation loss decreased from 1.394094 to 1.393549. Model was saved
Epoch: 367 	Training Loss: 1.248287 	Validation Loss: 1.394098 	 time: 0.3
Epoch: 368 	Training Loss: 1.248037 	Validation Loss: 1.393034 	 time: 0.3
Validation loss decreased from 1.393549 to 1.393034. Model was saved
Epoch: 369 	Training Loss: 1.247785 	Validation Loss: 1.393420 	 time: 0.3
Epoch: 370 	Training Loss: 1.247528 	Validation Loss: 1.392686 	 time: 0.3
Validation loss decreased from 1.393034 to 1.392686. Model was saved
Epoch: 371 	Training Loss: 1.247290 	Validation Loss: 1.392455 	 time: 0.3
Validation loss decreased from 1.392686 to 1.392455. Model was saved
Epoch: 372 	Training Loss: 1.247062 	Validation Loss: 1.391881 	 time: 0.3
Validation loss decreased from 1.392455 to 1.391881. Model was saved
Epoch: 373 	Training Loss: 1.246826 	Validation Loss: 1.392009 	 time: 0.3
Epoch: 374 	Training Loss: 1.246604 	Validation Loss: 1.391010 	 time: 0.3
Validation loss decreased from 1.391881 to 1.391010. Model was saved
Epoch: 375 	Training Loss: 1.246391 	Validation Loss: 1.391687 	 time: 0.3
Epoch: 376 	Training Loss: 1.246181 	Validation Loss: 1.390637 	 time: 0.3
Validation loss decreased from 1.391010 to 1.390637. Model was saved
Epoch: 377 	Training Loss: 1.245987 	Validation Loss: 1.391266 	 time: 0.3
Epoch: 378 	Training Loss: 1.245832 	Validation Loss: 1.390262 	 time: 0.3
Validation loss decreased from 1.390637 to 1.390262. Model was saved
Epoch: 379 	Training Loss: 1.245671 	Validation Loss: 1.391231 	 time: 0.3
Epoch: 380 	Training Loss: 1.245628 	Validation Loss: 1.389616 	 time: 0.2
Validation loss decreased from 1.390262 to 1.389616. Model was saved
Epoch: 381 	Training Loss: 1.245554 	Validation Loss: 1.391504 	 time: 0.3
Epoch: 382 	Training Loss: 1.245672 	Validation Loss: 1.389421 	 time: 0.3
Validation loss decreased from 1.389616 to 1.389421. Model was saved
Epoch: 383 	Training Loss: 1.245422 	Validation Loss: 1.391113 	 time: 0.3
Epoch: 384 	Training Loss: 1.245191 	Validation Loss: 1.389848 	 time: 0.3
Epoch: 385 	Training Loss: 1.244508 	Validation Loss: 1.390263 	 time: 0.2
Epoch: 386 	Training Loss: 1.243964 	Validation Loss: 1.390090 	 time: 0.2
Epoch: 387 	Training Loss: 1.243651 	Validation Loss: 1.389521 	 time: 0.2
Epoch: 388 	Training Loss: 1.243561 	Validation Loss: 1.389939 	 time: 0.3
Epoch: 389 	Training Loss: 1.243578 	Validation Loss: 1.389030 	 time: 0.3
Validation loss decreased from 1.389421 to 1.389030. Model was saved
Epoch: 390 	Training Loss: 1.243509 	Validation Loss: 1.389542 	 time: 0.3
Epoch: 391 	Training Loss: 1.243830 	Validation Loss: 1.388564 	 time: 0.3
Validation loss decreased from 1.389030 to 1.388564. Model was saved
Epoch: 392 	Training Loss: 1.243148 	Validation Loss: 1.388830 	 time: 0.3
Epoch: 393 	Training Loss: 1.242683 	Validation Loss: 1.387353 	 time: 0.3
Validation loss decreased from 1.388564 to 1.387353. Model was saved
Epoch: 394 	Training Loss: 1.242092 	Validation Loss: 1.386751 	 time: 0.3
Validation loss decreased from 1.387353 to 1.386751. Model was saved
Epoch: 395 	Training Loss: 1.241683 	Validation Loss: 1.386311 	 time: 0.3
Validation loss decreased from 1.386751 to 1.386311. Model was saved
Epoch: 396 	Training Loss: 1.241589 	Validation Loss: 1.384924 	 time: 0.3
Validation loss decreased from 1.386311 to 1.384924. Model was saved
Epoch: 397 	Training Loss: 1.241709 	Validation Loss: 1.385899 	 time: 0.3
Epoch: 398 	Training Loss: 1.241951 	Validation Loss: 1.384513 	 time: 0.2
Validation loss decreased from 1.384924 to 1.384513. Model was saved
Epoch: 399 	Training Loss: 1.241822 	Validation Loss: 1.385415 	 time: 0.3
Epoch: 400 	Training Loss: 1.241788 	Validation Loss: 1.384844 	 time: 0.2
Epoch: 401 	Training Loss: 1.240580 	Validation Loss: 1.384888 	 time: 0.3
Epoch: 402 	Training Loss: 1.240085 	Validation Loss: 1.384558 	 time: 0.3
Epoch: 403 	Training Loss: 1.240323 	Validation Loss: 1.383113 	 time: 0.3
Validation loss decreased from 1.384513 to 1.383113. Model was saved
Epoch: 404 	Training Loss: 1.240311 	Validation Loss: 1.383963 	 time: 0.3
Epoch: 405 	Training Loss: 1.240244 	Validation Loss: 1.382315 	 time: 0.3
Validation loss decreased from 1.383113 to 1.382315. Model was saved
Epoch: 406 	Training Loss: 1.239511 	Validation Loss: 1.382890 	 time: 0.3
Epoch: 407 	Training Loss: 1.238911 	Validation Loss: 1.382462 	 time: 0.3
Epoch: 408 	Training Loss: 1.238640 	Validation Loss: 1.381225 	 time: 0.3
Validation loss decreased from 1.382315 to 1.381225. Model was saved
Epoch: 409 	Training Loss: 1.238640 	Validation Loss: 1.381863 	 time: 0.3
Epoch: 410 	Training Loss: 1.238662 	Validation Loss: 1.380335 	 time: 0.3
Validation loss decreased from 1.381225 to 1.380335. Model was saved
Epoch: 411 	Training Loss: 1.238362 	Validation Loss: 1.380850 	 time: 0.3
Epoch: 412 	Training Loss: 1.238164 	Validation Loss: 1.380389 	 time: 0.3
Epoch: 413 	Training Loss: 1.237489 	Validation Loss: 1.380634 	 time: 0.3
Epoch: 414 	Training Loss: 1.236970 	Validation Loss: 1.380337 	 time: 0.3
Epoch: 415 	Training Loss: 1.236472 	Validation Loss: 1.380384 	 time: 0.3
Epoch: 416 	Training Loss: 1.236134 	Validation Loss: 1.380176 	 time: 0.3
Validation loss decreased from 1.380335 to 1.380176. Model was saved
Epoch: 417 	Training Loss: 1.235920 	Validation Loss: 1.379313 	 time: 0.3
Validation loss decreased from 1.380176 to 1.379313. Model was saved
Epoch: 418 	Training Loss: 1.235703 	Validation Loss: 1.380076 	 time: 0.2
Epoch: 419 	Training Loss: 1.235799 	Validation Loss: 1.377954 	 time: 0.3
Validation loss decreased from 1.379313 to 1.377954. Model was saved
Epoch: 420 	Training Loss: 1.235718 	Validation Loss: 1.379467 	 time: 0.3
Epoch: 421 	Training Loss: 1.235875 	Validation Loss: 1.377144 	 time: 0.3
Validation loss decreased from 1.377954 to 1.377144. Model was saved
Epoch: 422 	Training Loss: 1.234638 	Validation Loss: 1.377698 	 time: 0.3
Epoch: 423 	Training Loss: 1.233710 	Validation Loss: 1.377619 	 time: 0.3
Epoch: 424 	Training Loss: 1.233628 	Validation Loss: 1.375246 	 time: 0.2
Validation loss decreased from 1.377144 to 1.375246. Model was saved
Epoch: 425 	Training Loss: 1.233895 	Validation Loss: 1.377188 	 time: 0.3
Epoch: 426 	Training Loss: 1.234172 	Validation Loss: 1.374986 	 time: 0.2
Validation loss decreased from 1.375246 to 1.374986. Model was saved
Epoch: 427 	Training Loss: 1.233035 	Validation Loss: 1.376032 	 time: 0.3
Epoch: 428 	Training Loss: 1.232280 	Validation Loss: 1.376035 	 time: 0.3
Epoch: 429 	Training Loss: 1.231577 	Validation Loss: 1.374214 	 time: 0.3
Validation loss decreased from 1.374986 to 1.374214. Model was saved
Epoch: 430 	Training Loss: 1.231471 	Validation Loss: 1.375786 	 time: 0.3
Epoch: 431 	Training Loss: 1.231852 	Validation Loss: 1.373389 	 time: 0.3
Validation loss decreased from 1.374214 to 1.373389. Model was saved
Epoch: 432 	Training Loss: 1.231173 	Validation Loss: 1.374856 	 time: 0.2
Epoch: 433 	Training Loss: 1.230711 	Validation Loss: 1.373494 	 time: 0.3
Epoch: 434 	Training Loss: 1.229893 	Validation Loss: 1.373540 	 time: 0.3
Epoch: 435 	Training Loss: 1.229654 	Validation Loss: 1.374452 	 time: 0.3
Epoch: 436 	Training Loss: 1.229752 	Validation Loss: 1.372952 	 time: 0.3
Validation loss decreased from 1.373389 to 1.372952. Model was saved
Epoch: 437 	Training Loss: 1.229258 	Validation Loss: 1.374470 	 time: 0.3
Epoch: 438 	Training Loss: 1.228811 	Validation Loss: 1.373569 	 time: 0.3
Epoch: 439 	Training Loss: 1.228286 	Validation Loss: 1.373753 	 time: 0.3
Epoch: 440 	Training Loss: 1.227937 	Validation Loss: 1.374556 	 time: 0.2
Epoch: 441 	Training Loss: 1.227779 	Validation Loss: 1.373239 	 time: 0.2
Epoch: 442 	Training Loss: 1.227576 	Validation Loss: 1.374431 	 time: 0.3
Epoch: 443 	Training Loss: 1.227422 	Validation Loss: 1.373105 	 time: 0.2
Epoch: 444 	Training Loss: 1.226928 	Validation Loss: 1.373927 	 time: 0.2
Epoch: 445 	Training Loss: 1.226572 	Validation Loss: 1.372646 	 time: 0.2
Validation loss decreased from 1.372952 to 1.372646. Model was saved
Epoch: 446 	Training Loss: 1.226017 	Validation Loss: 1.372988 	 time: 0.3
Epoch: 447 	Training Loss: 1.225597 	Validation Loss: 1.371838 	 time: 0.3
Validation loss decreased from 1.372646 to 1.371838. Model was saved
Epoch: 448 	Training Loss: 1.225232 	Validation Loss: 1.371437 	 time: 0.3
Validation loss decreased from 1.371838 to 1.371437. Model was saved
Epoch: 449 	Training Loss: 1.224912 	Validation Loss: 1.370750 	 time: 0.3
Validation loss decreased from 1.371437 to 1.370750. Model was saved
Epoch: 450 	Training Loss: 1.224630 	Validation Loss: 1.369871 	 time: 0.3
Validation loss decreased from 1.370750 to 1.369871. Model was saved
Epoch: 451 	Training Loss: 1.224391 	Validation Loss: 1.370356 	 time: 0.3
Epoch: 452 	Training Loss: 1.224225 	Validation Loss: 1.369135 	 time: 0.3
Validation loss decreased from 1.369871 to 1.369135. Model was saved
Epoch: 453 	Training Loss: 1.224042 	Validation Loss: 1.370494 	 time: 0.3
Epoch: 454 	Training Loss: 1.224023 	Validation Loss: 1.369084 	 time: 0.2
Validation loss decreased from 1.369135 to 1.369084. Model was saved
Epoch: 455 	Training Loss: 1.223826 	Validation Loss: 1.370614 	 time: 0.2
Epoch: 456 	Training Loss: 1.223952 	Validation Loss: 1.368722 	 time: 0.2
Validation loss decreased from 1.369084 to 1.368722. Model was saved
Epoch: 457 	Training Loss: 1.223428 	Validation Loss: 1.369917 	 time: 0.3
Epoch: 458 	Training Loss: 1.223248 	Validation Loss: 1.368031 	 time: 0.3
Validation loss decreased from 1.368722 to 1.368031. Model was saved
Epoch: 459 	Training Loss: 1.222663 	Validation Loss: 1.368488 	 time: 0.3
Epoch: 460 	Training Loss: 1.222298 	Validation Loss: 1.367903 	 time: 0.3
Validation loss decreased from 1.368031 to 1.367903. Model was saved
Epoch: 461 	Training Loss: 1.222045 	Validation Loss: 1.367332 	 time: 0.3
Validation loss decreased from 1.367903 to 1.367332. Model was saved
Epoch: 462 	Training Loss: 1.221902 	Validation Loss: 1.368322 	 time: 0.3
Epoch: 463 	Training Loss: 1.221851 	Validation Loss: 1.366976 	 time: 0.2
Validation loss decreased from 1.367332 to 1.366976. Model was saved
Epoch: 464 	Training Loss: 1.221746 	Validation Loss: 1.368293 	 time: 0.3
Epoch: 465 	Training Loss: 1.221812 	Validation Loss: 1.366817 	 time: 0.3
Validation loss decreased from 1.366976 to 1.366817. Model was saved
Epoch: 466 	Training Loss: 1.221483 	Validation Loss: 1.368098 	 time: 0.3
Epoch: 467 	Training Loss: 1.221333 	Validation Loss: 1.366629 	 time: 0.2
Validation loss decreased from 1.366817 to 1.366629. Model was saved
Epoch: 468 	Training Loss: 1.220863 	Validation Loss: 1.367463 	 time: 0.3
Epoch: 469 	Training Loss: 1.220544 	Validation Loss: 1.366582 	 time: 0.3
Validation loss decreased from 1.366629 to 1.366582. Model was saved
Epoch: 470 	Training Loss: 1.220274 	Validation Loss: 1.366416 	 time: 0.3
Validation loss decreased from 1.366582 to 1.366416. Model was saved
Epoch: 471 	Training Loss: 1.220079 	Validation Loss: 1.366561 	 time: 0.3
Epoch: 472 	Training Loss: 1.219945 	Validation Loss: 1.365325 	 time: 0.2
Validation loss decreased from 1.366416 to 1.365325. Model was saved
Epoch: 473 	Training Loss: 1.219819 	Validation Loss: 1.366306 	 time: 0.2
Epoch: 474 	Training Loss: 1.219744 	Validation Loss: 1.364860 	 time: 0.2
Validation loss decreased from 1.365325 to 1.364860. Model was saved
Epoch: 475 	Training Loss: 1.219545 	Validation Loss: 1.365923 	 time: 0.3
Epoch: 476 	Training Loss: 1.219449 	Validation Loss: 1.364782 	 time: 0.2
Validation loss decreased from 1.364860 to 1.364782. Model was saved
Epoch: 477 	Training Loss: 1.219188 	Validation Loss: 1.365706 	 time: 0.2
Epoch: 478 	Training Loss: 1.219043 	Validation Loss: 1.364586 	 time: 0.2
Validation loss decreased from 1.364782 to 1.364586. Model was saved
Epoch: 479 	Training Loss: 1.218764 	Validation Loss: 1.365260 	 time: 0.2
Epoch: 480 	Training Loss: 1.218573 	Validation Loss: 1.364119 	 time: 0.2
Validation loss decreased from 1.364586 to 1.364119. Model was saved
Epoch: 481 	Training Loss: 1.218326 	Validation Loss: 1.364438 	 time: 0.3
Epoch: 482 	Training Loss: 1.218122 	Validation Loss: 1.363779 	 time: 0.3
Validation loss decreased from 1.364119 to 1.363779. Model was saved
Epoch: 483 	Training Loss: 1.217934 	Validation Loss: 1.363615 	 time: 0.2
Validation loss decreased from 1.363779 to 1.363615. Model was saved
Epoch: 484 	Training Loss: 1.217775 	Validation Loss: 1.363569 	 time: 0.2
Validation loss decreased from 1.363615 to 1.363569. Model was saved
Epoch: 485 	Training Loss: 1.217631 	Validation Loss: 1.363254 	 time: 0.2
Validation loss decreased from 1.363569 to 1.363254. Model was saved
Epoch: 486 	Training Loss: 1.217501 	Validation Loss: 1.363448 	 time: 0.3
Epoch: 487 	Training Loss: 1.217396 	Validation Loss: 1.363201 	 time: 0.2
Validation loss decreased from 1.363254 to 1.363201. Model was saved
Epoch: 488 	Training Loss: 1.217321 	Validation Loss: 1.363388 	 time: 0.2
Epoch: 489 	Training Loss: 1.217371 	Validation Loss: 1.363178 	 time: 0.2
Validation loss decreased from 1.363201 to 1.363178. Model was saved
Epoch: 490 	Training Loss: 1.217385 	Validation Loss: 1.363626 	 time: 0.2
Epoch: 491 	Training Loss: 1.217971 	Validation Loss: 1.363075 	 time: 0.2
Validation loss decreased from 1.363178 to 1.363075. Model was saved
Epoch: 492 	Training Loss: 1.217561 	Validation Loss: 1.364239 	 time: 0.2
Epoch: 493 	Training Loss: 1.217574 	Validation Loss: 1.362595 	 time: 0.2
Validation loss decreased from 1.363075 to 1.362595. Model was saved
Epoch: 494 	Training Loss: 1.216678 	Validation Loss: 1.363989 	 time: 0.2
Epoch: 495 	Training Loss: 1.216394 	Validation Loss: 1.363792 	 time: 0.3
Epoch: 496 	Training Loss: 1.216721 	Validation Loss: 1.362315 	 time: 0.2
Validation loss decreased from 1.362595 to 1.362315. Model was saved
Epoch: 497 	Training Loss: 1.216735 	Validation Loss: 1.364695 	 time: 0.2
Epoch: 498 	Training Loss: 1.216722 	Validation Loss: 1.362144 	 time: 0.2
Validation loss decreased from 1.362315 to 1.362144. Model was saved
Epoch: 499 	Training Loss: 1.216038 	Validation Loss: 1.362845 	 time: 0.2
Epoch: 500 	Training Loss: 1.215561 	Validation Loss: 1.363677 	 time: 0.2
Epoch: 501 	Training Loss: 1.215446 	Validation Loss: 1.362027 	 time: 0.2
Validation loss decreased from 1.362144 to 1.362027. Model was saved
Epoch: 502 	Training Loss: 1.215481 	Validation Loss: 1.363492 	 time: 0.2
Epoch: 503 	Training Loss: 1.215689 	Validation Loss: 1.363012 	 time: 0.2
Epoch: 504 	Training Loss: 1.215438 	Validation Loss: 1.363181 	 time: 0.2
Epoch: 505 	Training Loss: 1.215722 	Validation Loss: 1.362725 	 time: 0.2
Epoch: 506 	Training Loss: 1.214753 	Validation Loss: 1.363227 	 time: 0.2
Epoch: 507 	Training Loss: 1.214305 	Validation Loss: 1.361937 	 time: 0.3
Validation loss decreased from 1.362027 to 1.361937. Model was saved
Epoch: 508 	Training Loss: 1.214158 	Validation Loss: 1.361797 	 time: 0.3
Validation loss decreased from 1.361937 to 1.361797. Model was saved
Epoch: 509 	Training Loss: 1.214183 	Validation Loss: 1.362875 	 time: 0.3
Epoch: 510 	Training Loss: 1.214581 	Validation Loss: 1.359668 	 time: 0.2
Validation loss decreased from 1.361797 to 1.359668. Model was saved
Epoch: 511 	Training Loss: 1.214658 	Validation Loss: 1.361789 	 time: 0.3
Epoch: 512 	Training Loss: 1.214647 	Validation Loss: 1.360159 	 time: 0.3
Epoch: 513 	Training Loss: 1.213664 	Validation Loss: 1.359222 	 time: 0.3
Validation loss decreased from 1.359668 to 1.359222. Model was saved
Epoch: 514 	Training Loss: 1.213371 	Validation Loss: 1.360210 	 time: 0.3
Epoch: 515 	Training Loss: 1.213573 	Validation Loss: 1.358961 	 time: 0.3
Validation loss decreased from 1.359222 to 1.358961. Model was saved
Epoch: 516 	Training Loss: 1.213680 	Validation Loss: 1.359882 	 time: 0.3
Epoch: 517 	Training Loss: 1.213847 	Validation Loss: 1.358738 	 time: 0.3
Validation loss decreased from 1.358961 to 1.358738. Model was saved
Epoch: 518 	Training Loss: 1.212989 	Validation Loss: 1.358653 	 time: 0.3
Validation loss decreased from 1.358738 to 1.358653. Model was saved
Epoch: 519 	Training Loss: 1.212605 	Validation Loss: 1.357890 	 time: 0.3
Validation loss decreased from 1.358653 to 1.357890. Model was saved
Epoch: 520 	Training Loss: 1.212362 	Validation Loss: 1.357791 	 time: 0.3
Validation loss decreased from 1.357890 to 1.357791. Model was saved
Epoch: 521 	Training Loss: 1.212415 	Validation Loss: 1.358137 	 time: 0.3
Epoch: 522 	Training Loss: 1.212550 	Validation Loss: 1.356812 	 time: 0.3
Validation loss decreased from 1.357791 to 1.356812. Model was saved
Epoch: 523 	Training Loss: 1.212135 	Validation Loss: 1.357406 	 time: 0.3
Epoch: 524 	Training Loss: 1.211883 	Validation Loss: 1.356316 	 time: 0.3
Validation loss decreased from 1.356812 to 1.356316. Model was saved
Epoch: 525 	Training Loss: 1.211633 	Validation Loss: 1.355717 	 time: 0.3
Validation loss decreased from 1.356316 to 1.355717. Model was saved
Epoch: 526 	Training Loss: 1.211508 	Validation Loss: 1.355598 	 time: 0.3
Validation loss decreased from 1.355717 to 1.355598. Model was saved
Epoch: 527 	Training Loss: 1.211536 	Validation Loss: 1.354101 	 time: 0.3
Validation loss decreased from 1.355598 to 1.354101. Model was saved
Epoch: 528 	Training Loss: 1.211367 	Validation Loss: 1.354528 	 time: 0.3
Epoch: 529 	Training Loss: 1.211146 	Validation Loss: 1.353742 	 time: 0.3
Validation loss decreased from 1.354101 to 1.353742. Model was saved
Epoch: 530 	Training Loss: 1.210963 	Validation Loss: 1.353091 	 time: 0.3
Validation loss decreased from 1.353742 to 1.353091. Model was saved
Epoch: 531 	Training Loss: 1.210829 	Validation Loss: 1.353584 	 time: 0.2
Epoch: 532 	Training Loss: 1.210756 	Validation Loss: 1.352809 	 time: 0.3
Validation loss decreased from 1.353091 to 1.352809. Model was saved
Epoch: 533 	Training Loss: 1.210655 	Validation Loss: 1.352984 	 time: 0.3
Epoch: 534 	Training Loss: 1.210564 	Validation Loss: 1.353021 	 time: 0.3
Epoch: 535 	Training Loss: 1.210404 	Validation Loss: 1.352910 	 time: 0.3
Epoch: 536 	Training Loss: 1.210239 	Validation Loss: 1.352929 	 time: 0.3
Epoch: 537 	Training Loss: 1.210159 	Validation Loss: 1.353086 	 time: 0.3
Epoch: 538 	Training Loss: 1.210096 	Validation Loss: 1.353066 	 time: 0.3
Epoch: 539 	Training Loss: 1.210011 	Validation Loss: 1.352529 	 time: 0.3
Validation loss decreased from 1.352809 to 1.352529. Model was saved
Epoch: 540 	Training Loss: 1.209908 	Validation Loss: 1.352933 	 time: 0.3
Epoch: 541 	Training Loss: 1.209777 	Validation Loss: 1.352479 	 time: 0.2
Validation loss decreased from 1.352529 to 1.352479. Model was saved
Epoch: 542 	Training Loss: 1.209630 	Validation Loss: 1.352414 	 time: 0.2
Validation loss decreased from 1.352479 to 1.352414. Model was saved
Epoch: 543 	Training Loss: 1.209510 	Validation Loss: 1.352673 	 time: 0.3
Epoch: 544 	Training Loss: 1.209417 	Validation Loss: 1.352312 	 time: 0.3
Validation loss decreased from 1.352414 to 1.352312. Model was saved
Epoch: 545 	Training Loss: 1.209342 	Validation Loss: 1.352516 	 time: 0.3
Epoch: 546 	Training Loss: 1.209254 	Validation Loss: 1.352267 	 time: 0.3
Validation loss decreased from 1.352312 to 1.352267. Model was saved
Epoch: 547 	Training Loss: 1.209141 	Validation Loss: 1.352156 	 time: 0.3
Validation loss decreased from 1.352267 to 1.352156. Model was saved
Epoch: 548 	Training Loss: 1.209032 	Validation Loss: 1.352001 	 time: 0.3
Validation loss decreased from 1.352156 to 1.352001. Model was saved
Epoch: 549 	Training Loss: 1.208905 	Validation Loss: 1.352132 	 time: 0.3
Epoch: 550 	Training Loss: 1.208804 	Validation Loss: 1.351963 	 time: 0.3
Validation loss decreased from 1.352001 to 1.351963. Model was saved
Epoch: 551 	Training Loss: 1.208708 	Validation Loss: 1.352008 	 time: 0.3
Epoch: 552 	Training Loss: 1.208610 	Validation Loss: 1.352266 	 time: 0.3
Epoch: 553 	Training Loss: 1.208520 	Validation Loss: 1.351862 	 time: 0.3
Validation loss decreased from 1.351963 to 1.351862. Model was saved
Epoch: 554 	Training Loss: 1.208419 	Validation Loss: 1.352306 	 time: 0.3
Epoch: 555 	Training Loss: 1.208318 	Validation Loss: 1.351955 	 time: 0.3
Epoch: 556 	Training Loss: 1.208212 	Validation Loss: 1.352280 	 time: 0.3
Epoch: 557 	Training Loss: 1.208123 	Validation Loss: 1.352094 	 time: 0.3
Epoch: 558 	Training Loss: 1.208022 	Validation Loss: 1.352465 	 time: 0.3
Epoch: 559 	Training Loss: 1.207940 	Validation Loss: 1.352075 	 time: 0.2
Epoch: 560 	Training Loss: 1.207838 	Validation Loss: 1.352437 	 time: 0.3
Epoch: 561 	Training Loss: 1.207749 	Validation Loss: 1.351973 	 time: 0.3
Epoch: 562 	Training Loss: 1.207647 	Validation Loss: 1.352099 	 time: 0.3
Epoch: 563 	Training Loss: 1.207576 	Validation Loss: 1.351779 	 time: 0.3
Validation loss decreased from 1.351862 to 1.351779. Model was saved
Epoch: 564 	Training Loss: 1.207501 	Validation Loss: 1.351890 	 time: 0.3
Epoch: 565 	Training Loss: 1.207458 	Validation Loss: 1.351511 	 time: 0.3
Validation loss decreased from 1.351779 to 1.351511. Model was saved
Epoch: 566 	Training Loss: 1.207387 	Validation Loss: 1.351977 	 time: 0.2
Epoch: 567 	Training Loss: 1.207385 	Validation Loss: 1.351341 	 time: 0.3
Validation loss decreased from 1.351511 to 1.351341. Model was saved
Epoch: 568 	Training Loss: 1.207306 	Validation Loss: 1.352113 	 time: 0.3
Epoch: 569 	Training Loss: 1.207378 	Validation Loss: 1.351342 	 time: 0.3
Epoch: 570 	Training Loss: 1.207234 	Validation Loss: 1.352136 	 time: 0.2
Epoch: 571 	Training Loss: 1.207324 	Validation Loss: 1.351205 	 time: 0.2
Validation loss decreased from 1.351341 to 1.351205. Model was saved
Epoch: 572 	Training Loss: 1.207023 	Validation Loss: 1.351826 	 time: 0.3
Epoch: 573 	Training Loss: 1.206917 	Validation Loss: 1.350932 	 time: 0.3
Validation loss decreased from 1.351205 to 1.350932. Model was saved
Epoch: 574 	Training Loss: 1.206574 	Validation Loss: 1.351123 	 time: 0.3
Epoch: 575 	Training Loss: 1.206352 	Validation Loss: 1.350607 	 time: 0.3
Validation loss decreased from 1.350932 to 1.350607. Model was saved
Epoch: 576 	Training Loss: 1.206164 	Validation Loss: 1.350451 	 time: 0.3
Validation loss decreased from 1.350607 to 1.350451. Model was saved
Epoch: 577 	Training Loss: 1.206040 	Validation Loss: 1.350468 	 time: 0.3
Epoch: 578 	Training Loss: 1.205957 	Validation Loss: 1.349976 	 time: 0.3
Validation loss decreased from 1.350451 to 1.349976. Model was saved
Epoch: 579 	Training Loss: 1.205889 	Validation Loss: 1.350375 	 time: 0.3
Epoch: 580 	Training Loss: 1.205869 	Validation Loss: 1.349647 	 time: 0.2
Validation loss decreased from 1.349976 to 1.349647. Model was saved
Epoch: 581 	Training Loss: 1.205790 	Validation Loss: 1.350289 	 time: 0.3
Epoch: 582 	Training Loss: 1.205859 	Validation Loss: 1.349305 	 time: 0.3
Validation loss decreased from 1.349647 to 1.349305. Model was saved
Epoch: 583 	Training Loss: 1.205698 	Validation Loss: 1.350272 	 time: 0.3
Epoch: 584 	Training Loss: 1.205851 	Validation Loss: 1.348991 	 time: 0.3
Validation loss decreased from 1.349305 to 1.348991. Model was saved
Epoch: 585 	Training Loss: 1.205567 	Validation Loss: 1.350225 	 time: 0.3
Epoch: 586 	Training Loss: 1.205690 	Validation Loss: 1.348726 	 time: 0.2
Validation loss decreased from 1.348991 to 1.348726. Model was saved
Epoch: 587 	Training Loss: 1.205248 	Validation Loss: 1.349824 	 time: 0.3
Epoch: 588 	Training Loss: 1.205113 	Validation Loss: 1.348714 	 time: 0.3
Validation loss decreased from 1.348726 to 1.348714. Model was saved
Epoch: 589 	Training Loss: 1.204742 	Validation Loss: 1.349206 	 time: 0.3
Epoch: 590 	Training Loss: 1.204527 	Validation Loss: 1.349131 	 time: 0.3
Epoch: 591 	Training Loss: 1.204387 	Validation Loss: 1.349005 	 time: 0.2
Epoch: 592 	Training Loss: 1.204321 	Validation Loss: 1.349922 	 time: 0.3
Epoch: 593 	Training Loss: 1.204339 	Validation Loss: 1.349083 	 time: 0.3
Epoch: 594 	Training Loss: 1.204377 	Validation Loss: 1.350918 	 time: 0.3
Epoch: 595 	Training Loss: 1.204740 	Validation Loss: 1.349473 	 time: 0.3
Epoch: 596 	Training Loss: 1.204747 	Validation Loss: 1.352204 	 time: 0.3
Epoch: 597 	Training Loss: 1.205531 	Validation Loss: 1.349789 	 time: 0.3
Epoch: 598 	Training Loss: 1.204361 	Validation Loss: 1.350898 	 time: 0.3
Epoch: 599 	Training Loss: 1.203777 	Validation Loss: 1.350937 	 time: 0.3
Epoch: 600 	Training Loss: 1.203797 	Validation Loss: 1.350213 	 time: 0.3
Epoch: 601 	Training Loss: 1.203967 	Validation Loss: 1.352022 	 time: 0.3
Epoch: 602 	Training Loss: 1.204146 	Validation Loss: 1.349573 	 time: 0.3
Epoch: 603 	Training Loss: 1.203636 	Validation Loss: 1.350176 	 time: 0.3
Epoch: 604 	Training Loss: 1.203304 	Validation Loss: 1.350699 	 time: 0.3
Epoch: 605 	Training Loss: 1.203381 	Validation Loss: 1.349141 	 time: 0.3
Epoch: 606 	Training Loss: 1.203362 	Validation Loss: 1.350500 	 time: 0.2
Epoch: 607 	Training Loss: 1.203065 	Validation Loss: 1.349770 	 time: 0.3
Epoch: 608 	Training Loss: 1.202739 	Validation Loss: 1.349317 	 time: 0.3
Epoch: 609 	Training Loss: 1.202607 	Validation Loss: 1.350597 	 time: 0.3
Epoch: 610 	Training Loss: 1.202555 	Validation Loss: 1.349307 	 time: 0.3
Epoch: 611 	Training Loss: 1.202436 	Validation Loss: 1.349791 	 time: 0.3
Epoch: 612 	Training Loss: 1.202300 	Validation Loss: 1.349008 	 time: 0.3
Epoch: 613 	Training Loss: 1.202105 	Validation Loss: 1.348325 	 time: 0.3
Validation loss decreased from 1.348714 to 1.348325. Model was saved
Epoch: 614 	Training Loss: 1.202036 	Validation Loss: 1.348640 	 time: 0.3
Epoch: 615 	Training Loss: 1.202061 	Validation Loss: 1.347422 	 time: 0.3
Validation loss decreased from 1.348325 to 1.347422. Model was saved
Epoch: 616 	Training Loss: 1.201986 	Validation Loss: 1.348029 	 time: 0.3
Epoch: 617 	Training Loss: 1.201866 	Validation Loss: 1.347389 	 time: 0.3
Validation loss decreased from 1.347422 to 1.347389. Model was saved
Epoch: 618 	Training Loss: 1.201707 	Validation Loss: 1.347597 	 time: 0.3
Epoch: 619 	Training Loss: 1.201635 	Validation Loss: 1.348022 	 time: 0.3
Epoch: 620 	Training Loss: 1.201608 	Validation Loss: 1.347687 	 time: 0.3
Epoch: 621 	Training Loss: 1.201549 	Validation Loss: 1.348464 	 time: 0.3
Epoch: 622 	Training Loss: 1.201478 	Validation Loss: 1.347910 	 time: 0.3
Epoch: 623 	Training Loss: 1.201361 	Validation Loss: 1.348302 	 time: 0.3
Epoch: 624 	Training Loss: 1.201261 	Validation Loss: 1.348322 	 time: 0.3
Epoch: 625 	Training Loss: 1.201202 	Validation Loss: 1.348044 	 time: 0.3
Epoch: 626 	Training Loss: 1.201167 	Validation Loss: 1.348640 	 time: 0.3
Epoch: 627 	Training Loss: 1.201123 	Validation Loss: 1.348205 	 time: 0.3
Epoch: 628 	Training Loss: 1.201050 	Validation Loss: 1.348579 	 time: 0.3
Epoch: 629 	Training Loss: 1.200980 	Validation Loss: 1.348570 	 time: 0.3
Epoch: 630 	Training Loss: 1.200910 	Validation Loss: 1.348496 	 time: 0.3
Epoch: 631 	Training Loss: 1.200849 	Validation Loss: 1.348676 	 time: 0.3
Epoch: 632 	Training Loss: 1.200802 	Validation Loss: 1.348461 	 time: 0.3
Epoch: 633 	Training Loss: 1.200749 	Validation Loss: 1.348757 	 time: 0.3
Epoch: 634 	Training Loss: 1.200684 	Validation Loss: 1.348370 	 time: 0.3
Epoch: 635 	Training Loss: 1.200599 	Validation Loss: 1.348739 	 time: 0.3
Epoch: 636 	Training Loss: 1.200501 	Validation Loss: 1.348642 	 time: 0.3
Epoch: 637 	Training Loss: 1.200409 	Validation Loss: 1.348710 	 time: 0.3
Epoch: 638 	Training Loss: 1.200345 	Validation Loss: 1.348979 	 time: 0.2
Epoch: 639 	Training Loss: 1.200296 	Validation Loss: 1.348821 	 time: 0.3
Epoch: 640 	Training Loss: 1.200247 	Validation Loss: 1.349149 	 time: 0.2
Epoch: 641 	Training Loss: 1.200189 	Validation Loss: 1.348826 	 time: 0.3
Epoch: 642 	Training Loss: 1.200127 	Validation Loss: 1.349017 	 time: 0.3
Epoch: 643 	Training Loss: 1.200065 	Validation Loss: 1.348791 	 time: 0.3
Epoch: 644 	Training Loss: 1.199999 	Validation Loss: 1.348770 	 time: 0.3
Epoch: 645 	Training Loss: 1.199929 	Validation Loss: 1.348749 	 time: 0.3
Epoch: 646 	Training Loss: 1.199852 	Validation Loss: 1.348729 	 time: 0.3
Epoch: 647 	Training Loss: 1.199769 	Validation Loss: 1.348798 	 time: 0.3
Epoch: 648 	Training Loss: 1.199685 	Validation Loss: 1.348724 	 time: 0.3
Epoch: 649 	Training Loss: 1.199610 	Validation Loss: 1.349065 	 time: 0.3
Epoch: 650 	Training Loss: 1.199542 	Validation Loss: 1.348915 	 time: 0.3
Epoch: 651 	Training Loss: 1.199476 	Validation Loss: 1.349197 	 time: 0.3
Epoch: 652 	Training Loss: 1.199413 	Validation Loss: 1.349098 	 time: 0.3
Epoch: 653 	Training Loss: 1.199355 	Validation Loss: 1.349139 	 time: 0.3
Epoch: 654 	Training Loss: 1.199298 	Validation Loss: 1.348987 	 time: 0.3
Epoch: 655 	Training Loss: 1.199242 	Validation Loss: 1.348971 	 time: 0.2
Epoch: 656 	Training Loss: 1.199188 	Validation Loss: 1.348879 	 time: 0.2
Epoch: 657 	Training Loss: 1.199134 	Validation Loss: 1.348801 	 time: 0.3
Epoch: 658 	Training Loss: 1.199081 	Validation Loss: 1.348797 	 time: 0.3
Epoch: 659 	Training Loss: 1.199031 	Validation Loss: 1.348644 	 time: 0.3
Epoch: 660 	Training Loss: 1.198980 	Validation Loss: 1.348603 	 time: 0.2
Epoch: 661 	Training Loss: 1.198927 	Validation Loss: 1.348410 	 time: 0.2
Epoch: 662 	Training Loss: 1.198870 	Validation Loss: 1.348401 	 time: 0.2
Epoch: 663 	Training Loss: 1.198804 	Validation Loss: 1.348336 	 time: 0.3
Epoch: 664 	Training Loss: 1.198728 	Validation Loss: 1.348387 	 time: 0.3
Epoch: 665 	Training Loss: 1.198635 	Validation Loss: 1.348497 	 time: 0.3
Epoch: 666 	Training Loss: 1.198538 	Validation Loss: 1.348675 	 time: 0.2
Epoch: 667 	Training Loss: 1.198457 	Validation Loss: 1.348867 	 time: 0.2
Epoch: 668 	Training Loss: 1.198388 	Validation Loss: 1.349080 	 time: 0.2
Epoch: 669 	Training Loss: 1.198328 	Validation Loss: 1.349171 	 time: 0.3
Epoch: 670 	Training Loss: 1.198271 	Validation Loss: 1.349193 	 time: 0.3
Epoch: 671 	Training Loss: 1.198212 	Validation Loss: 1.349171 	 time: 0.3
Epoch: 672 	Training Loss: 1.198150 	Validation Loss: 1.349157 	 time: 0.3
Epoch: 673 	Training Loss: 1.198083 	Validation Loss: 1.349168 	 time: 0.3
Epoch: 674 	Training Loss: 1.198016 	Validation Loss: 1.349156 	 time: 0.3
Epoch: 675 	Training Loss: 1.197948 	Validation Loss: 1.349127 	 time: 0.3
Epoch: 676 	Training Loss: 1.197879 	Validation Loss: 1.349067 	 time: 0.2
Epoch: 677 	Training Loss: 1.197805 	Validation Loss: 1.348958 	 time: 0.3
Epoch: 678 	Training Loss: 1.197722 	Validation Loss: 1.348932 	 time: 0.3
Epoch: 679 	Training Loss: 1.197637 	Validation Loss: 1.348932 	 time: 0.3
Epoch: 680 	Training Loss: 1.197566 	Validation Loss: 1.348933 	 time: 0.3
Epoch: 681 	Training Loss: 1.197513 	Validation Loss: 1.348899 	 time: 0.3
Epoch: 682 	Training Loss: 1.197479 	Validation Loss: 1.348941 	 time: 0.3
Epoch: 683 	Training Loss: 1.197512 	Validation Loss: 1.348643 	 time: 0.3
Epoch: 684 	Training Loss: 1.197609 	Validation Loss: 1.349075 	 time: 0.3
Epoch: 685 	Training Loss: 1.198212 	Validation Loss: 1.348539 	 time: 0.2
Epoch: 686 	Training Loss: 1.198895 	Validation Loss: 1.349676 	 time: 0.3
Epoch: 687 	Training Loss: 1.199701 	Validation Loss: 1.348060 	 time: 0.3
Epoch: 688 	Training Loss: 1.197363 	Validation Loss: 1.350052 	 time: 0.3
Epoch: 689 	Training Loss: 1.200168 	Validation Loss: 1.350031 	 time: 0.2
Epoch: 690 	Training Loss: 1.199490 	Validation Loss: 1.349176 	 time: 0.2
Epoch: 691 	Training Loss: 1.199849 	Validation Loss: 1.348010 	 time: 0.2
Epoch: 692 	Training Loss: 1.197829 	Validation Loss: 1.351160 	 time: 0.3
Epoch: 693 	Training Loss: 1.199640 	Validation Loss: 1.349866 	 time: 0.2
Epoch: 694 	Training Loss: 1.198521 	Validation Loss: 1.348659 	 time: 0.2
Epoch: 695 	Training Loss: 1.199457 	Validation Loss: 1.347756 	 time: 0.2
Epoch: 696 	Training Loss: 1.197759 	Validation Loss: 1.349549 	 time: 0.2
Epoch: 697 	Training Loss: 1.198531 	Validation Loss: 1.351235 	 time: 0.3
Epoch: 698 	Training Loss: 1.198354 	Validation Loss: 1.347705 	 time: 0.3
Epoch: 699 	Training Loss: 1.197458 	Validation Loss: 1.349056 	 time: 0.2
Epoch: 700 	Training Loss: 1.198629 	Validation Loss: 1.347769 	 time: 0.2
Epoch: 701 	Training Loss: 1.196752 	Validation Loss: 1.351070 	 time: 0.2
Epoch: 702 	Training Loss: 1.198346 	Validation Loss: 1.348625 	 time: 0.2
Epoch: 703 	Training Loss: 1.196886 	Validation Loss: 1.347201 	 time: 0.2
Validation loss decreased from 1.347389 to 1.347201. Model was saved
Epoch: 704 	Training Loss: 1.197636 	Validation Loss: 1.346320 	 time: 0.3
Validation loss decreased from 1.347201 to 1.346320. Model was saved
Epoch: 705 	Training Loss: 1.196951 	Validation Loss: 1.347166 	 time: 0.2
Epoch: 706 	Training Loss: 1.197032 	Validation Loss: 1.347758 	 time: 0.2
Epoch: 707 	Training Loss: 1.196779 	Validation Loss: 1.347664 	 time: 0.2
Epoch: 708 	Training Loss: 1.196545 	Validation Loss: 1.346731 	 time: 0.2
Epoch: 709 	Training Loss: 1.196635 	Validation Loss: 1.346051 	 time: 0.3
Validation loss decreased from 1.346320 to 1.346051. Model was saved
Epoch: 710 	Training Loss: 1.196327 	Validation Loss: 1.346786 	 time: 0.3
Epoch: 711 	Training Loss: 1.196330 	Validation Loss: 1.347213 	 time: 0.2
Epoch: 712 	Training Loss: 1.196213 	Validation Loss: 1.347549 	 time: 0.2
Epoch: 713 	Training Loss: 1.196027 	Validation Loss: 1.346735 	 time: 0.3
Epoch: 714 	Training Loss: 1.196095 	Validation Loss: 1.345768 	 time: 0.3
Validation loss decreased from 1.346051 to 1.345768. Model was saved
Epoch: 715 	Training Loss: 1.195847 	Validation Loss: 1.346494 	 time: 0.3
Epoch: 716 	Training Loss: 1.195890 	Validation Loss: 1.347141 	 time: 0.3
Epoch: 717 	Training Loss: 1.195714 	Validation Loss: 1.347454 	 time: 0.2
Epoch: 718 	Training Loss: 1.195687 	Validation Loss: 1.346666 	 time: 0.2
Epoch: 719 	Training Loss: 1.195634 	Validation Loss: 1.345718 	 time: 0.3
Validation loss decreased from 1.345768 to 1.345718. Model was saved
Epoch: 720 	Training Loss: 1.195494 	Validation Loss: 1.345953 	 time: 0.3
Epoch: 721 	Training Loss: 1.195534 	Validation Loss: 1.346185 	 time: 0.3
Epoch: 722 	Training Loss: 1.195337 	Validation Loss: 1.346420 	 time: 0.3
Epoch: 723 	Training Loss: 1.195367 	Validation Loss: 1.346009 	 time: 0.3
Epoch: 724 	Training Loss: 1.195276 	Validation Loss: 1.345470 	 time: 0.3
Validation loss decreased from 1.345718 to 1.345470. Model was saved
Epoch: 725 	Training Loss: 1.195133 	Validation Loss: 1.345765 	 time: 0.3
Epoch: 726 	Training Loss: 1.195181 	Validation Loss: 1.345901 	 time: 0.2
Epoch: 727 	Training Loss: 1.195011 	Validation Loss: 1.346271 	 time: 0.2
Epoch: 728 	Training Loss: 1.195011 	Validation Loss: 1.346178 	 time: 0.3
Epoch: 729 	Training Loss: 1.194950 	Validation Loss: 1.345820 	 time: 0.3
Epoch: 730 	Training Loss: 1.194834 	Validation Loss: 1.345800 	 time: 0.2
Epoch: 731 	Training Loss: 1.194853 	Validation Loss: 1.345716 	 time: 0.3
Epoch: 732 	Training Loss: 1.194708 	Validation Loss: 1.345863 	 time: 0.3
Epoch: 733 	Training Loss: 1.194695 	Validation Loss: 1.345715 	 time: 0.3
Epoch: 734 	Training Loss: 1.194624 	Validation Loss: 1.345501 	 time: 0.2
Epoch: 735 	Training Loss: 1.194553 	Validation Loss: 1.345575 	 time: 0.2
Epoch: 736 	Training Loss: 1.194523 	Validation Loss: 1.345649 	 time: 0.3
Epoch: 737 	Training Loss: 1.194424 	Validation Loss: 1.345835 	 time: 0.3
Epoch: 738 	Training Loss: 1.194413 	Validation Loss: 1.345863 	 time: 0.2
Epoch: 739 	Training Loss: 1.194349 	Validation Loss: 1.345817 	 time: 0.2
Epoch: 740 	Training Loss: 1.194317 	Validation Loss: 1.345648 	 time: 0.2
Epoch: 741 	Training Loss: 1.194283 	Validation Loss: 1.345499 	 time: 0.3
Epoch: 742 	Training Loss: 1.194225 	Validation Loss: 1.345588 	 time: 0.3
Epoch: 743 	Training Loss: 1.194198 	Validation Loss: 1.345592 	 time: 0.2
Epoch: 744 	Training Loss: 1.194129 	Validation Loss: 1.345501 	 time: 0.3
Epoch: 745 	Training Loss: 1.194081 	Validation Loss: 1.345420 	 time: 0.3
Validation loss decreased from 1.345470 to 1.345420. Model was saved
Epoch: 746 	Training Loss: 1.194018 	Validation Loss: 1.345456 	 time: 0.3
Epoch: 747 	Training Loss: 1.193945 	Validation Loss: 1.345531 	 time: 0.3
Epoch: 748 	Training Loss: 1.193873 	Validation Loss: 1.345450 	 time: 0.3
Epoch: 749 	Training Loss: 1.193794 	Validation Loss: 1.345263 	 time: 0.3
Validation loss decreased from 1.345420 to 1.345263. Model was saved
Epoch: 750 	Training Loss: 1.193750 	Validation Loss: 1.345029 	 time: 0.3
Validation loss decreased from 1.345263 to 1.345029. Model was saved
Epoch: 751 	Training Loss: 1.193704 	Validation Loss: 1.344880 	 time: 0.3
Validation loss decreased from 1.345029 to 1.344880. Model was saved
Epoch: 752 	Training Loss: 1.193665 	Validation Loss: 1.344827 	 time: 0.3
Validation loss decreased from 1.344880 to 1.344827. Model was saved
Epoch: 753 	Training Loss: 1.193619 	Validation Loss: 1.344772 	 time: 0.3
Validation loss decreased from 1.344827 to 1.344772. Model was saved
Epoch: 754 	Training Loss: 1.193572 	Validation Loss: 1.344645 	 time: 0.3
Validation loss decreased from 1.344772 to 1.344645. Model was saved
Epoch: 755 	Training Loss: 1.193517 	Validation Loss: 1.344467 	 time: 0.3
Validation loss decreased from 1.344645 to 1.344467. Model was saved
Epoch: 756 	Training Loss: 1.193451 	Validation Loss: 1.344350 	 time: 0.3
Validation loss decreased from 1.344467 to 1.344350. Model was saved
Epoch: 757 	Training Loss: 1.193382 	Validation Loss: 1.344308 	 time: 0.3
Validation loss decreased from 1.344350 to 1.344308. Model was saved
Epoch: 758 	Training Loss: 1.193306 	Validation Loss: 1.344273 	 time: 0.3
Validation loss decreased from 1.344308 to 1.344273. Model was saved
Epoch: 759 	Training Loss: 1.193249 	Validation Loss: 1.344155 	 time: 0.3
Validation loss decreased from 1.344273 to 1.344155. Model was saved
Epoch: 760 	Training Loss: 1.193195 	Validation Loss: 1.344055 	 time: 0.2
Validation loss decreased from 1.344155 to 1.344055. Model was saved
Epoch: 761 	Training Loss: 1.193153 	Validation Loss: 1.344068 	 time: 0.2
Epoch: 762 	Training Loss: 1.193111 	Validation Loss: 1.344138 	 time: 0.3
Epoch: 763 	Training Loss: 1.193067 	Validation Loss: 1.344143 	 time: 0.3
Epoch: 764 	Training Loss: 1.193028 	Validation Loss: 1.344076 	 time: 0.2
Epoch: 765 	Training Loss: 1.192985 	Validation Loss: 1.344080 	 time: 0.3
Epoch: 766 	Training Loss: 1.192946 	Validation Loss: 1.344190 	 time: 0.3
Epoch: 767 	Training Loss: 1.192906 	Validation Loss: 1.344331 	 time: 0.3
Epoch: 768 	Training Loss: 1.192870 	Validation Loss: 1.344418 	 time: 0.3
Epoch: 769 	Training Loss: 1.192836 	Validation Loss: 1.344431 	 time: 0.2
Epoch: 770 	Training Loss: 1.192802 	Validation Loss: 1.344432 	 time: 0.3
Epoch: 771 	Training Loss: 1.192770 	Validation Loss: 1.344478 	 time: 0.3
Epoch: 772 	Training Loss: 1.192732 	Validation Loss: 1.344542 	 time: 0.3
Epoch: 773 	Training Loss: 1.192692 	Validation Loss: 1.344539 	 time: 0.3
Epoch: 774 	Training Loss: 1.192645 	Validation Loss: 1.344469 	 time: 0.3
Epoch: 775 	Training Loss: 1.192590 	Validation Loss: 1.344440 	 time: 0.2
Epoch: 776 	Training Loss: 1.192527 	Validation Loss: 1.344504 	 time: 0.3
Epoch: 777 	Training Loss: 1.192460 	Validation Loss: 1.344561 	 time: 0.3
Epoch: 778 	Training Loss: 1.192405 	Validation Loss: 1.344530 	 time: 0.3
Epoch: 779 	Training Loss: 1.192358 	Validation Loss: 1.344465 	 time: 0.3
Epoch: 780 	Training Loss: 1.192316 	Validation Loss: 1.344434 	 time: 0.3
Epoch: 781 	Training Loss: 1.192274 	Validation Loss: 1.344427 	 time: 0.3
Epoch: 782 	Training Loss: 1.192235 	Validation Loss: 1.344389 	 time: 0.3
Epoch: 783 	Training Loss: 1.192197 	Validation Loss: 1.344294 	 time: 0.3
Epoch: 784 	Training Loss: 1.192158 	Validation Loss: 1.344153 	 time: 0.3
Epoch: 785 	Training Loss: 1.192114 	Validation Loss: 1.343994 	 time: 0.3
Validation loss decreased from 1.344055 to 1.343994. Model was saved
Epoch: 786 	Training Loss: 1.192067 	Validation Loss: 1.343835 	 time: 0.3
Validation loss decreased from 1.343994 to 1.343835. Model was saved
Epoch: 787 	Training Loss: 1.192024 	Validation Loss: 1.343670 	 time: 0.3
Validation loss decreased from 1.343835 to 1.343670. Model was saved
Epoch: 788 	Training Loss: 1.191986 	Validation Loss: 1.343497 	 time: 0.3
Validation loss decreased from 1.343670 to 1.343497. Model was saved
Epoch: 789 	Training Loss: 1.191944 	Validation Loss: 1.343342 	 time: 0.3
Validation loss decreased from 1.343497 to 1.343342. Model was saved
Epoch: 790 	Training Loss: 1.191890 	Validation Loss: 1.343240 	 time: 0.3
Validation loss decreased from 1.343342 to 1.343240. Model was saved
Epoch: 791 	Training Loss: 1.191817 	Validation Loss: 1.343176 	 time: 0.2
Validation loss decreased from 1.343240 to 1.343176. Model was saved
Epoch: 792 	Training Loss: 1.191747 	Validation Loss: 1.343107 	 time: 0.3
Validation loss decreased from 1.343176 to 1.343107. Model was saved
Epoch: 793 	Training Loss: 1.191695 	Validation Loss: 1.343008 	 time: 0.3
Validation loss decreased from 1.343107 to 1.343008. Model was saved
Epoch: 794 	Training Loss: 1.191649 	Validation Loss: 1.342903 	 time: 0.3
Validation loss decreased from 1.343008 to 1.342903. Model was saved
Epoch: 795 	Training Loss: 1.191612 	Validation Loss: 1.342818 	 time: 0.3
Validation loss decreased from 1.342903 to 1.342818. Model was saved
Epoch: 796 	Training Loss: 1.191577 	Validation Loss: 1.342723 	 time: 0.3
Validation loss decreased from 1.342818 to 1.342723. Model was saved
Epoch: 797 	Training Loss: 1.191541 	Validation Loss: 1.342581 	 time: 0.2
Validation loss decreased from 1.342723 to 1.342581. Model was saved
Epoch: 798 	Training Loss: 1.191504 	Validation Loss: 1.342428 	 time: 0.3
Validation loss decreased from 1.342581 to 1.342428. Model was saved
Epoch: 799 	Training Loss: 1.191467 	Validation Loss: 1.342319 	 time: 0.3
Validation loss decreased from 1.342428 to 1.342319. Model was saved
Epoch: 800 	Training Loss: 1.191427 	Validation Loss: 1.342262 	 time: 0.3
Validation loss decreased from 1.342319 to 1.342262. Model was saved
Epoch: 801 	Training Loss: 1.191387 	Validation Loss: 1.342227 	 time: 0.3
Validation loss decreased from 1.342262 to 1.342227. Model was saved
Epoch: 802 	Training Loss: 1.191357 	Validation Loss: 1.342214 	 time: 0.3
Validation loss decreased from 1.342227 to 1.342214. Model was saved
Epoch: 803 	Training Loss: 1.191332 	Validation Loss: 1.342217 	 time: 0.3
Epoch: 804 	Training Loss: 1.191307 	Validation Loss: 1.342210 	 time: 0.3
Validation loss decreased from 1.342214 to 1.342210. Model was saved
Epoch: 805 	Training Loss: 1.191279 	Validation Loss: 1.342206 	 time: 0.3
Validation loss decreased from 1.342210 to 1.342206. Model was saved
Epoch: 806 	Training Loss: 1.191249 	Validation Loss: 1.342230 	 time: 0.3
Epoch: 807 	Training Loss: 1.191215 	Validation Loss: 1.342267 	 time: 0.3
Epoch: 808 	Training Loss: 1.191184 	Validation Loss: 1.342315 	 time: 0.3
Epoch: 809 	Training Loss: 1.191156 	Validation Loss: 1.342408 	 time: 0.3
Epoch: 810 	Training Loss: 1.191130 	Validation Loss: 1.342541 	 time: 0.3
Epoch: 811 	Training Loss: 1.191105 	Validation Loss: 1.342661 	 time: 0.2
Epoch: 812 	Training Loss: 1.191080 	Validation Loss: 1.342738 	 time: 0.2
Epoch: 813 	Training Loss: 1.191054 	Validation Loss: 1.342784 	 time: 0.2
Epoch: 814 	Training Loss: 1.191028 	Validation Loss: 1.342810 	 time: 0.2
Epoch: 815 	Training Loss: 1.191001 	Validation Loss: 1.342823 	 time: 0.3
Epoch: 816 	Training Loss: 1.190976 	Validation Loss: 1.342847 	 time: 0.3
Epoch: 817 	Training Loss: 1.190950 	Validation Loss: 1.342894 	 time: 0.3
Epoch: 818 	Training Loss: 1.190923 	Validation Loss: 1.342951 	 time: 0.2
Epoch: 819 	Training Loss: 1.190895 	Validation Loss: 1.343008 	 time: 0.3
Epoch: 820 	Training Loss: 1.190865 	Validation Loss: 1.343068 	 time: 0.3
Epoch: 821 	Training Loss: 1.190834 	Validation Loss: 1.343127 	 time: 0.3
Epoch: 822 	Training Loss: 1.190805 	Validation Loss: 1.343181 	 time: 0.2
Epoch: 823 	Training Loss: 1.190776 	Validation Loss: 1.343241 	 time: 0.3
Epoch: 824 	Training Loss: 1.190748 	Validation Loss: 1.343315 	 time: 0.3
Epoch: 825 	Training Loss: 1.190717 	Validation Loss: 1.343389 	 time: 0.3
Epoch: 826 	Training Loss: 1.190683 	Validation Loss: 1.343451 	 time: 0.3
Epoch: 827 	Training Loss: 1.190648 	Validation Loss: 1.343503 	 time: 0.3
Epoch: 828 	Training Loss: 1.190612 	Validation Loss: 1.343553 	 time: 0.2
Epoch: 829 	Training Loss: 1.190576 	Validation Loss: 1.343605 	 time: 0.3
Epoch: 830 	Training Loss: 1.190537 	Validation Loss: 1.343667 	 time: 0.3
Epoch: 831 	Training Loss: 1.190492 	Validation Loss: 1.343741 	 time: 0.2
Epoch: 832 	Training Loss: 1.190444 	Validation Loss: 1.343812 	 time: 0.3
Epoch: 833 	Training Loss: 1.190398 	Validation Loss: 1.343855 	 time: 0.3
Epoch: 834 	Training Loss: 1.190355 	Validation Loss: 1.343872 	 time: 0.2
Epoch: 835 	Training Loss: 1.190318 	Validation Loss: 1.343874 	 time: 0.3
Epoch: 836 	Training Loss: 1.190284 	Validation Loss: 1.343861 	 time: 0.2
Epoch: 837 	Training Loss: 1.190251 	Validation Loss: 1.343824 	 time: 0.2
Epoch: 838 	Training Loss: 1.190219 	Validation Loss: 1.343771 	 time: 0.2
Epoch: 839 	Training Loss: 1.190186 	Validation Loss: 1.343725 	 time: 0.3
Epoch: 840 	Training Loss: 1.190145 	Validation Loss: 1.343705 	 time: 0.3
Epoch: 841 	Training Loss: 1.190092 	Validation Loss: 1.343688 	 time: 0.2
Epoch: 842 	Training Loss: 1.190043 	Validation Loss: 1.343635 	 time: 0.3
Epoch: 843 	Training Loss: 1.190010 	Validation Loss: 1.343610 	 time: 0.3
Epoch: 844 	Training Loss: 1.189982 	Validation Loss: 1.343616 	 time: 0.3
Epoch: 845 	Training Loss: 1.189945 	Validation Loss: 1.343633 	 time: 0.3
Epoch: 846 	Training Loss: 1.189890 	Validation Loss: 1.343682 	 time: 0.3
Epoch: 847 	Training Loss: 1.189847 	Validation Loss: 1.343714 	 time: 0.3
Epoch: 848 	Training Loss: 1.189822 	Validation Loss: 1.343663 	 time: 0.3
Epoch: 849 	Training Loss: 1.189800 	Validation Loss: 1.343580 	 time: 0.3
Epoch: 850 	Training Loss: 1.189776 	Validation Loss: 1.343535 	 time: 0.3
Epoch: 851 	Training Loss: 1.189750 	Validation Loss: 1.343501 	 time: 0.3
Epoch: 852 	Training Loss: 1.189725 	Validation Loss: 1.343440 	 time: 0.3
Epoch: 853 	Training Loss: 1.189699 	Validation Loss: 1.343379 	 time: 0.2
Epoch: 854 	Training Loss: 1.189673 	Validation Loss: 1.343355 	 time: 0.3
Epoch: 855 	Training Loss: 1.189646 	Validation Loss: 1.343361 	 time: 0.3
Epoch: 856 	Training Loss: 1.189616 	Validation Loss: 1.343383 	 time: 0.3
Epoch: 857 	Training Loss: 1.189582 	Validation Loss: 1.343417 	 time: 0.3
Epoch: 858 	Training Loss: 1.189539 	Validation Loss: 1.343453 	 time: 0.3
Epoch: 859 	Training Loss: 1.189491 	Validation Loss: 1.343492 	 time: 0.3
Epoch: 860 	Training Loss: 1.189444 	Validation Loss: 1.343562 	 time: 0.3
Epoch: 861 	Training Loss: 1.189405 	Validation Loss: 1.343621 	 time: 0.3
Epoch: 862 	Training Loss: 1.189366 	Validation Loss: 1.343604 	 time: 0.3
Epoch: 863 	Training Loss: 1.189319 	Validation Loss: 1.343544 	 time: 0.3
Epoch: 864 	Training Loss: 1.189260 	Validation Loss: 1.343505 	 time: 0.3
Epoch: 865 	Training Loss: 1.189192 	Validation Loss: 1.343471 	 time: 0.3
Epoch: 866 	Training Loss: 1.189124 	Validation Loss: 1.343445 	 time: 0.3
Epoch: 867 	Training Loss: 1.189069 	Validation Loss: 1.343466 	 time: 0.3
Epoch: 868 	Training Loss: 1.189032 	Validation Loss: 1.343502 	 time: 0.3
Epoch: 869 	Training Loss: 1.188999 	Validation Loss: 1.343511 	 time: 0.2
Epoch: 870 	Training Loss: 1.188965 	Validation Loss: 1.343484 	 time: 0.3
Epoch: 871 	Training Loss: 1.188932 	Validation Loss: 1.343421 	 time: 0.3
Epoch: 872 	Training Loss: 1.188902 	Validation Loss: 1.343329 	 time: 0.3
Epoch: 873 	Training Loss: 1.188872 	Validation Loss: 1.343235 	 time: 0.3
Epoch: 874 	Training Loss: 1.188841 	Validation Loss: 1.343141 	 time: 0.3
Epoch: 875 	Training Loss: 1.188808 	Validation Loss: 1.343027 	 time: 0.3
Epoch: 876 	Training Loss: 1.188767 	Validation Loss: 1.342893 	 time: 0.3
Epoch: 877 	Training Loss: 1.188709 	Validation Loss: 1.342757 	 time: 0.3
Epoch: 878 	Training Loss: 1.188641 	Validation Loss: 1.342644 	 time: 0.3
Epoch: 879 	Training Loss: 1.188587 	Validation Loss: 1.342585 	 time: 0.3
Epoch: 880 	Training Loss: 1.188544 	Validation Loss: 1.342576 	 time: 0.2
Epoch: 881 	Training Loss: 1.188508 	Validation Loss: 1.342585 	 time: 0.3
Epoch: 882 	Training Loss: 1.188478 	Validation Loss: 1.342600 	 time: 0.3
Epoch: 883 	Training Loss: 1.188450 	Validation Loss: 1.342631 	 time: 0.3
Epoch: 884 	Training Loss: 1.188422 	Validation Loss: 1.342649 	 time: 0.3
Epoch: 885 	Training Loss: 1.188395 	Validation Loss: 1.342602 	 time: 0.3
Epoch: 886 	Training Loss: 1.188368 	Validation Loss: 1.342510 	 time: 0.3
Epoch: 887 	Training Loss: 1.188341 	Validation Loss: 1.342410 	 time: 0.3
Epoch: 888 	Training Loss: 1.188314 	Validation Loss: 1.342294 	 time: 0.3
Epoch: 889 	Training Loss: 1.188285 	Validation Loss: 1.342149 	 time: 0.2
Validation loss decreased from 1.342206 to 1.342149. Model was saved
Epoch: 890 	Training Loss: 1.188253 	Validation Loss: 1.341966 	 time: 0.3
Validation loss decreased from 1.342149 to 1.341966. Model was saved
Epoch: 891 	Training Loss: 1.188218 	Validation Loss: 1.341714 	 time: 0.3
Validation loss decreased from 1.341966 to 1.341714. Model was saved
Epoch: 892 	Training Loss: 1.188178 	Validation Loss: 1.341413 	 time: 0.3
Validation loss decreased from 1.341714 to 1.341413. Model was saved
Epoch: 893 	Training Loss: 1.188136 	Validation Loss: 1.341135 	 time: 0.3
Validation loss decreased from 1.341413 to 1.341135. Model was saved
Epoch: 894 	Training Loss: 1.188097 	Validation Loss: 1.340913 	 time: 0.3
Validation loss decreased from 1.341135 to 1.340913. Model was saved
Epoch: 895 	Training Loss: 1.188064 	Validation Loss: 1.340723 	 time: 0.3
Validation loss decreased from 1.340913 to 1.340723. Model was saved
Epoch: 896 	Training Loss: 1.188028 	Validation Loss: 1.340562 	 time: 0.3
Validation loss decreased from 1.340723 to 1.340562. Model was saved
Epoch: 897 	Training Loss: 1.187980 	Validation Loss: 1.340452 	 time: 0.2
Validation loss decreased from 1.340562 to 1.340452. Model was saved
Epoch: 898 	Training Loss: 1.187913 	Validation Loss: 1.340370 	 time: 0.3
Validation loss decreased from 1.340452 to 1.340370. Model was saved
Epoch: 899 	Training Loss: 1.187825 	Validation Loss: 1.340317 	 time: 0.3
Validation loss decreased from 1.340370 to 1.340317. Model was saved
Epoch: 900 	Training Loss: 1.187747 	Validation Loss: 1.340356 	 time: 0.2
Epoch: 901 	Training Loss: 1.187699 	Validation Loss: 1.340465 	 time: 0.3
Epoch: 902 	Training Loss: 1.187667 	Validation Loss: 1.340619 	 time: 0.3
Epoch: 903 	Training Loss: 1.187643 	Validation Loss: 1.340829 	 time: 0.3
Epoch: 904 	Training Loss: 1.187622 	Validation Loss: 1.341040 	 time: 0.3
Epoch: 905 	Training Loss: 1.187599 	Validation Loss: 1.341179 	 time: 0.2
Epoch: 906 	Training Loss: 1.187575 	Validation Loss: 1.341201 	 time: 0.3
Epoch: 907 	Training Loss: 1.187549 	Validation Loss: 1.341110 	 time: 0.3
Epoch: 908 	Training Loss: 1.187522 	Validation Loss: 1.340974 	 time: 0.3
Epoch: 909 	Training Loss: 1.187494 	Validation Loss: 1.340880 	 time: 0.3
Epoch: 910 	Training Loss: 1.187468 	Validation Loss: 1.340852 	 time: 0.3
Epoch: 911 	Training Loss: 1.187442 	Validation Loss: 1.340863 	 time: 0.3
Epoch: 912 	Training Loss: 1.187416 	Validation Loss: 1.340902 	 time: 0.3
Epoch: 913 	Training Loss: 1.187387 	Validation Loss: 1.340981 	 time: 0.3
Epoch: 914 	Training Loss: 1.187356 	Validation Loss: 1.341079 	 time: 0.3
Epoch: 915 	Training Loss: 1.187320 	Validation Loss: 1.341152 	 time: 0.3
Epoch: 916 	Training Loss: 1.187284 	Validation Loss: 1.341186 	 time: 0.3
Epoch: 917 	Training Loss: 1.187246 	Validation Loss: 1.341190 	 time: 0.2
Epoch: 918 	Training Loss: 1.187203 	Validation Loss: 1.341170 	 time: 0.2
Epoch: 919 	Training Loss: 1.187147 	Validation Loss: 1.341143 	 time: 0.3
Epoch: 920 	Training Loss: 1.187083 	Validation Loss: 1.341144 	 time: 0.3
Epoch: 921 	Training Loss: 1.187024 	Validation Loss: 1.341192 	 time: 0.3
Epoch: 922 	Training Loss: 1.186978 	Validation Loss: 1.341244 	 time: 0.3
Epoch: 923 	Training Loss: 1.186939 	Validation Loss: 1.341282 	 time: 0.3
Epoch: 924 	Training Loss: 1.186900 	Validation Loss: 1.341307 	 time: 0.3
Epoch: 925 	Training Loss: 1.186863 	Validation Loss: 1.341292 	 time: 0.3
Epoch: 926 	Training Loss: 1.186826 	Validation Loss: 1.341230 	 time: 0.3
Epoch: 927 	Training Loss: 1.186791 	Validation Loss: 1.341130 	 time: 0.3
Epoch: 928 	Training Loss: 1.186758 	Validation Loss: 1.340996 	 time: 0.3
Epoch: 929 	Training Loss: 1.186728 	Validation Loss: 1.340850 	 time: 0.3
Epoch: 930 	Training Loss: 1.186700 	Validation Loss: 1.340733 	 time: 0.3
Epoch: 931 	Training Loss: 1.186673 	Validation Loss: 1.340655 	 time: 0.3
Epoch: 932 	Training Loss: 1.186646 	Validation Loss: 1.340613 	 time: 0.3
Epoch: 933 	Training Loss: 1.186618 	Validation Loss: 1.340622 	 time: 0.3
Epoch: 934 	Training Loss: 1.186589 	Validation Loss: 1.340663 	 time: 0.3
Epoch: 935 	Training Loss: 1.186558 	Validation Loss: 1.340686 	 time: 0.3
Epoch: 936 	Training Loss: 1.186526 	Validation Loss: 1.340672 	 time: 0.3
Epoch: 937 	Training Loss: 1.186493 	Validation Loss: 1.340633 	 time: 0.3
Epoch: 938 	Training Loss: 1.186457 	Validation Loss: 1.340585 	 time: 0.3
Epoch: 939 	Training Loss: 1.186421 	Validation Loss: 1.340553 	 time: 0.3
Epoch: 940 	Training Loss: 1.186385 	Validation Loss: 1.340549 	 time: 0.3
Epoch: 941 	Training Loss: 1.186349 	Validation Loss: 1.340561 	 time: 0.3
Epoch: 942 	Training Loss: 1.186315 	Validation Loss: 1.340587 	 time: 0.3
Epoch: 943 	Training Loss: 1.186283 	Validation Loss: 1.340634 	 time: 0.3
Epoch: 944 	Training Loss: 1.186247 	Validation Loss: 1.340690 	 time: 0.3
Epoch: 945 	Training Loss: 1.186209 	Validation Loss: 1.340727 	 time: 0.3
Epoch: 946 	Training Loss: 1.186173 	Validation Loss: 1.340744 	 time: 0.3
Epoch: 947 	Training Loss: 1.186142 	Validation Loss: 1.340743 	 time: 0.3
Epoch: 948 	Training Loss: 1.186113 	Validation Loss: 1.340724 	 time: 0.2
Epoch: 949 	Training Loss: 1.186085 	Validation Loss: 1.340693 	 time: 0.3
Epoch: 950 	Training Loss: 1.186057 	Validation Loss: 1.340656 	 time: 0.3
Epoch: 951 	Training Loss: 1.186030 	Validation Loss: 1.340596 	 time: 0.3
Epoch: 952 	Training Loss: 1.186002 	Validation Loss: 1.340513 	 time: 0.3
Epoch: 953 	Training Loss: 1.185974 	Validation Loss: 1.340425 	 time: 0.3
Epoch: 954 	Training Loss: 1.185944 	Validation Loss: 1.340324 	 time: 0.2
Epoch: 955 	Training Loss: 1.185913 	Validation Loss: 1.340203 	 time: 0.2
Validation loss decreased from 1.340317 to 1.340203. Model was saved
Epoch: 956 	Training Loss: 1.185883 	Validation Loss: 1.340081 	 time: 0.2
Validation loss decreased from 1.340203 to 1.340081. Model was saved
Epoch: 957 	Training Loss: 1.185853 	Validation Loss: 1.339969 	 time: 0.2
Validation loss decreased from 1.340081 to 1.339969. Model was saved
Epoch: 958 	Training Loss: 1.185821 	Validation Loss: 1.339864 	 time: 0.3
Validation loss decreased from 1.339969 to 1.339864. Model was saved
Epoch: 959 	Training Loss: 1.185788 	Validation Loss: 1.339768 	 time: 0.3
Validation loss decreased from 1.339864 to 1.339768. Model was saved
Epoch: 960 	Training Loss: 1.185757 	Validation Loss: 1.339695 	 time: 0.3
Validation loss decreased from 1.339768 to 1.339695. Model was saved
Epoch: 961 	Training Loss: 1.185727 	Validation Loss: 1.339642 	 time: 0.3
Validation loss decreased from 1.339695 to 1.339642. Model was saved
Epoch: 962 	Training Loss: 1.185697 	Validation Loss: 1.339586 	 time: 0.3
Validation loss decreased from 1.339642 to 1.339586. Model was saved
Epoch: 963 	Training Loss: 1.185669 	Validation Loss: 1.339520 	 time: 0.3
Validation loss decreased from 1.339586 to 1.339520. Model was saved
Epoch: 964 	Training Loss: 1.185643 	Validation Loss: 1.339435 	 time: 0.3
Validation loss decreased from 1.339520 to 1.339435. Model was saved
Epoch: 965 	Training Loss: 1.185617 	Validation Loss: 1.339324 	 time: 0.3
Validation loss decreased from 1.339435 to 1.339324. Model was saved
Epoch: 966 	Training Loss: 1.185590 	Validation Loss: 1.339195 	 time: 0.3
Validation loss decreased from 1.339324 to 1.339195. Model was saved
Epoch: 967 	Training Loss: 1.185563 	Validation Loss: 1.339056 	 time: 0.3
Validation loss decreased from 1.339195 to 1.339056. Model was saved
Epoch: 968 	Training Loss: 1.185535 	Validation Loss: 1.338907 	 time: 0.3
Validation loss decreased from 1.339056 to 1.338907. Model was saved
Epoch: 969 	Training Loss: 1.185506 	Validation Loss: 1.338756 	 time: 0.3
Validation loss decreased from 1.338907 to 1.338756. Model was saved
Epoch: 970 	Training Loss: 1.185478 	Validation Loss: 1.338616 	 time: 0.2
Validation loss decreased from 1.338756 to 1.338616. Model was saved
Epoch: 971 	Training Loss: 1.185454 	Validation Loss: 1.338492 	 time: 0.2
Validation loss decreased from 1.338616 to 1.338492. Model was saved
Epoch: 972 	Training Loss: 1.185432 	Validation Loss: 1.338392 	 time: 0.3
Validation loss decreased from 1.338492 to 1.338392. Model was saved
Epoch: 973 	Training Loss: 1.185410 	Validation Loss: 1.338321 	 time: 0.3
Validation loss decreased from 1.338392 to 1.338321. Model was saved
Epoch: 974 	Training Loss: 1.185389 	Validation Loss: 1.338268 	 time: 0.3
Validation loss decreased from 1.338321 to 1.338268. Model was saved
Epoch: 975 	Training Loss: 1.185369 	Validation Loss: 1.338218 	 time: 0.3
Validation loss decreased from 1.338268 to 1.338218. Model was saved
Epoch: 976 	Training Loss: 1.185348 	Validation Loss: 1.338171 	 time: 0.3
Validation loss decreased from 1.338218 to 1.338171. Model was saved
Epoch: 977 	Training Loss: 1.185329 	Validation Loss: 1.338130 	 time: 0.3
Validation loss decreased from 1.338171 to 1.338130. Model was saved
Epoch: 978 	Training Loss: 1.185309 	Validation Loss: 1.338103 	 time: 0.2
Validation loss decreased from 1.338130 to 1.338103. Model was saved
Epoch: 979 	Training Loss: 1.185289 	Validation Loss: 1.338094 	 time: 0.3
Validation loss decreased from 1.338103 to 1.338094. Model was saved
Epoch: 980 	Training Loss: 1.185269 	Validation Loss: 1.338099 	 time: 0.3
Epoch: 981 	Training Loss: 1.185250 	Validation Loss: 1.338113 	 time: 0.3
Epoch: 982 	Training Loss: 1.185230 	Validation Loss: 1.338139 	 time: 0.3
Epoch: 983 	Training Loss: 1.185210 	Validation Loss: 1.338174 	 time: 0.3
Epoch: 984 	Training Loss: 1.185189 	Validation Loss: 1.338222 	 time: 0.3
Epoch: 985 	Training Loss: 1.185168 	Validation Loss: 1.338285 	 time: 0.3
Epoch: 986 	Training Loss: 1.185146 	Validation Loss: 1.338359 	 time: 0.3
Epoch: 987 	Training Loss: 1.185123 	Validation Loss: 1.338431 	 time: 0.3
Epoch: 988 	Training Loss: 1.185099 	Validation Loss: 1.338486 	 time: 0.3
Epoch: 989 	Training Loss: 1.185077 	Validation Loss: 1.338513 	 time: 0.3
Epoch: 990 	Training Loss: 1.185055 	Validation Loss: 1.338504 	 time: 0.3
Epoch: 991 	Training Loss: 1.185035 	Validation Loss: 1.338465 	 time: 0.3
Epoch: 992 	Training Loss: 1.185015 	Validation Loss: 1.338414 	 time: 0.3
Epoch: 993 	Training Loss: 1.184996 	Validation Loss: 1.338365 	 time: 0.3
Epoch: 994 	Training Loss: 1.184976 	Validation Loss: 1.338328 	 time: 0.3
Epoch: 995 	Training Loss: 1.184956 	Validation Loss: 1.338302 	 time: 0.2
Epoch: 996 	Training Loss: 1.184934 	Validation Loss: 1.338279 	 time: 0.3
Epoch: 997 	Training Loss: 1.184912 	Validation Loss: 1.338254 	 time: 0.3
Epoch: 998 	Training Loss: 1.184891 	Validation Loss: 1.338221 	 time: 0.3
Epoch: 999 	Training Loss: 1.184871 	Validation Loss: 1.338177 	 time: 0.3
Epoch: 1000 	Training Loss: 1.184853 	Validation Loss: 1.338121 	 time: 0.3
Epoch: 1001 	Training Loss: 1.184835 	Validation Loss: 1.338055 	 time: 0.3
Validation loss decreased from 1.338094 to 1.338055. Model was saved
Epoch: 1002 	Training Loss: 1.184816 	Validation Loss: 1.337980 	 time: 0.3
Validation loss decreased from 1.338055 to 1.337980. Model was saved
Epoch: 1003 	Training Loss: 1.184796 	Validation Loss: 1.337904 	 time: 0.3
Validation loss decreased from 1.337980 to 1.337904. Model was saved
Epoch: 1004 	Training Loss: 1.184775 	Validation Loss: 1.337833 	 time: 0.3
Validation loss decreased from 1.337904 to 1.337833. Model was saved
Epoch: 1005 	Training Loss: 1.184751 	Validation Loss: 1.337763 	 time: 0.2
Validation loss decreased from 1.337833 to 1.337763. Model was saved
Epoch: 1006 	Training Loss: 1.184728 	Validation Loss: 1.337708 	 time: 0.3
Validation loss decreased from 1.337763 to 1.337708. Model was saved
Epoch: 1007 	Training Loss: 1.184712 	Validation Loss: 1.337694 	 time: 0.3
Validation loss decreased from 1.337708 to 1.337694. Model was saved
Epoch: 1008 	Training Loss: 1.184695 	Validation Loss: 1.337714 	 time: 0.3
Epoch: 1009 	Training Loss: 1.184674 	Validation Loss: 1.337748 	 time: 0.3
Epoch: 1010 	Training Loss: 1.184652 	Validation Loss: 1.337803 	 time: 0.3
Epoch: 1011 	Training Loss: 1.184633 	Validation Loss: 1.337854 	 time: 0.3
Epoch: 1012 	Training Loss: 1.184615 	Validation Loss: 1.337862 	 time: 0.3
Epoch: 1013 	Training Loss: 1.184597 	Validation Loss: 1.337828 	 time: 0.3
Epoch: 1014 	Training Loss: 1.184577 	Validation Loss: 1.337761 	 time: 0.3
Epoch: 1015 	Training Loss: 1.184555 	Validation Loss: 1.337670 	 time: 0.3
Validation loss decreased from 1.337694 to 1.337670. Model was saved
Epoch: 1016 	Training Loss: 1.184534 	Validation Loss: 1.337593 	 time: 0.3
Validation loss decreased from 1.337670 to 1.337593. Model was saved
Epoch: 1017 	Training Loss: 1.184512 	Validation Loss: 1.337552 	 time: 0.3
Validation loss decreased from 1.337593 to 1.337552. Model was saved
Epoch: 1018 	Training Loss: 1.184486 	Validation Loss: 1.337546 	 time: 0.3
Validation loss decreased from 1.337552 to 1.337546. Model was saved
Epoch: 1019 	Training Loss: 1.184454 	Validation Loss: 1.337578 	 time: 0.3
Epoch: 1020 	Training Loss: 1.184417 	Validation Loss: 1.337640 	 time: 0.3
Epoch: 1021 	Training Loss: 1.184380 	Validation Loss: 1.337690 	 time: 0.3
Epoch: 1022 	Training Loss: 1.184347 	Validation Loss: 1.337689 	 time: 0.3
Epoch: 1023 	Training Loss: 1.184318 	Validation Loss: 1.337620 	 time: 0.3
Epoch: 1024 	Training Loss: 1.184287 	Validation Loss: 1.337505 	 time: 0.3
Validation loss decreased from 1.337546 to 1.337505. Model was saved
Epoch: 1025 	Training Loss: 1.184256 	Validation Loss: 1.337373 	 time: 0.3
Validation loss decreased from 1.337505 to 1.337373. Model was saved
Epoch: 1026 	Training Loss: 1.184221 	Validation Loss: 1.337228 	 time: 0.3
Validation loss decreased from 1.337373 to 1.337228. Model was saved
Epoch: 1027 	Training Loss: 1.184183 	Validation Loss: 1.337115 	 time: 0.2
Validation loss decreased from 1.337228 to 1.337115. Model was saved
Epoch: 1028 	Training Loss: 1.184155 	Validation Loss: 1.337088 	 time: 0.3
Validation loss decreased from 1.337115 to 1.337088. Model was saved
Epoch: 1029 	Training Loss: 1.184132 	Validation Loss: 1.337136 	 time: 0.3
Epoch: 1030 	Training Loss: 1.184111 	Validation Loss: 1.337247 	 time: 0.2
Epoch: 1031 	Training Loss: 1.184090 	Validation Loss: 1.337429 	 time: 0.3
Epoch: 1032 	Training Loss: 1.184070 	Validation Loss: 1.337637 	 time: 0.3
Epoch: 1033 	Training Loss: 1.184050 	Validation Loss: 1.337840 	 time: 0.3
Epoch: 1034 	Training Loss: 1.184031 	Validation Loss: 1.338058 	 time: 0.3
Epoch: 1035 	Training Loss: 1.184011 	Validation Loss: 1.338283 	 time: 0.3
Epoch: 1036 	Training Loss: 1.183990 	Validation Loss: 1.338485 	 time: 0.3
Epoch: 1037 	Training Loss: 1.183971 	Validation Loss: 1.338654 	 time: 0.3
Epoch: 1038 	Training Loss: 1.183951 	Validation Loss: 1.338762 	 time: 0.3
Epoch: 1039 	Training Loss: 1.183931 	Validation Loss: 1.338804 	 time: 0.2
Epoch: 1040 	Training Loss: 1.183911 	Validation Loss: 1.338804 	 time: 0.2
Epoch: 1041 	Training Loss: 1.183889 	Validation Loss: 1.338751 	 time: 0.2
Epoch: 1042 	Training Loss: 1.183864 	Validation Loss: 1.338651 	 time: 0.2
Epoch: 1043 	Training Loss: 1.183836 	Validation Loss: 1.338548 	 time: 0.2
Epoch: 1044 	Training Loss: 1.183802 	Validation Loss: 1.338453 	 time: 0.2
Epoch: 1045 	Training Loss: 1.183765 	Validation Loss: 1.338374 	 time: 0.2
Epoch: 1046 	Training Loss: 1.183725 	Validation Loss: 1.338328 	 time: 0.2
Epoch: 1047 	Training Loss: 1.183689 	Validation Loss: 1.338287 	 time: 0.3
Epoch: 1048 	Training Loss: 1.183655 	Validation Loss: 1.338231 	 time: 0.3
Epoch: 1049 	Training Loss: 1.183618 	Validation Loss: 1.338160 	 time: 0.3
Epoch: 1050 	Training Loss: 1.183570 	Validation Loss: 1.338070 	 time: 0.3
Epoch: 1051 	Training Loss: 1.183507 	Validation Loss: 1.337981 	 time: 0.3
Epoch: 1052 	Training Loss: 1.183448 	Validation Loss: 1.337917 	 time: 0.3
Epoch: 1053 	Training Loss: 1.183414 	Validation Loss: 1.337865 	 time: 0.3
Epoch: 1054 	Training Loss: 1.183387 	Validation Loss: 1.337797 	 time: 0.3
Epoch: 1055 	Training Loss: 1.183352 	Validation Loss: 1.337717 	 time: 0.3
Epoch: 1056 	Training Loss: 1.183307 	Validation Loss: 1.337627 	 time: 0.3
Epoch: 1057 	Training Loss: 1.183275 	Validation Loss: 1.337579 	 time: 0.3
Epoch: 1058 	Training Loss: 1.183257 	Validation Loss: 1.337614 	 time: 0.3
Epoch: 1059 	Training Loss: 1.183241 	Validation Loss: 1.337723 	 time: 0.3
Epoch: 1060 	Training Loss: 1.183225 	Validation Loss: 1.337847 	 time: 0.3
Epoch: 1061 	Training Loss: 1.183207 	Validation Loss: 1.337941 	 time: 0.3
Epoch: 1062 	Training Loss: 1.183187 	Validation Loss: 1.337987 	 time: 0.3
Epoch: 1063 	Training Loss: 1.183167 	Validation Loss: 1.337956 	 time: 0.3
Epoch: 1064 	Training Loss: 1.183146 	Validation Loss: 1.337860 	 time: 0.3
Epoch: 1065 	Training Loss: 1.183127 	Validation Loss: 1.337710 	 time: 0.3
Epoch: 1066 	Training Loss: 1.183109 	Validation Loss: 1.337529 	 time: 0.3
Epoch: 1067 	Training Loss: 1.183087 	Validation Loss: 1.337334 	 time: 0.2
Epoch: 1068 	Training Loss: 1.183064 	Validation Loss: 1.337138 	 time: 0.2
Epoch: 1069 	Training Loss: 1.183038 	Validation Loss: 1.336992 	 time: 0.2
Validation loss decreased from 1.337088 to 1.336992. Model was saved
Epoch: 1070 	Training Loss: 1.183008 	Validation Loss: 1.336921 	 time: 0.3
Validation loss decreased from 1.336992 to 1.336921. Model was saved
Epoch: 1071 	Training Loss: 1.182974 	Validation Loss: 1.336918 	 time: 0.2
Validation loss decreased from 1.336921 to 1.336918. Model was saved
Epoch: 1072 	Training Loss: 1.182941 	Validation Loss: 1.336968 	 time: 0.3
Epoch: 1073 	Training Loss: 1.182908 	Validation Loss: 1.337033 	 time: 0.2
Epoch: 1074 	Training Loss: 1.182876 	Validation Loss: 1.337077 	 time: 0.3
Epoch: 1075 	Training Loss: 1.182847 	Validation Loss: 1.337101 	 time: 0.3
Epoch: 1076 	Training Loss: 1.182822 	Validation Loss: 1.337088 	 time: 0.3
Epoch: 1077 	Training Loss: 1.182796 	Validation Loss: 1.336995 	 time: 0.2
Epoch: 1078 	Training Loss: 1.182769 	Validation Loss: 1.336803 	 time: 0.3
Validation loss decreased from 1.336918 to 1.336803. Model was saved
Epoch: 1079 	Training Loss: 1.182739 	Validation Loss: 1.336546 	 time: 0.2
Validation loss decreased from 1.336803 to 1.336546. Model was saved
Epoch: 1080 	Training Loss: 1.182713 	Validation Loss: 1.336261 	 time: 0.2
Validation loss decreased from 1.336546 to 1.336261. Model was saved
Epoch: 1081 	Training Loss: 1.182687 	Validation Loss: 1.335986 	 time: 0.2
Validation loss decreased from 1.336261 to 1.335986. Model was saved
Epoch: 1082 	Training Loss: 1.182657 	Validation Loss: 1.335752 	 time: 0.2
Validation loss decreased from 1.335986 to 1.335752. Model was saved
Epoch: 1083 	Training Loss: 1.182616 	Validation Loss: 1.335580 	 time: 0.3
Validation loss decreased from 1.335752 to 1.335580. Model was saved
Epoch: 1084 	Training Loss: 1.182568 	Validation Loss: 1.335465 	 time: 0.3
Validation loss decreased from 1.335580 to 1.335465. Model was saved
Epoch: 1085 	Training Loss: 1.182530 	Validation Loss: 1.335426 	 time: 0.3
Validation loss decreased from 1.335465 to 1.335426. Model was saved
Epoch: 1086 	Training Loss: 1.182494 	Validation Loss: 1.335474 	 time: 0.3
Epoch: 1087 	Training Loss: 1.182448 	Validation Loss: 1.335540 	 time: 0.3
Epoch: 1088 	Training Loss: 1.182402 	Validation Loss: 1.335556 	 time: 0.2
Epoch: 1089 	Training Loss: 1.182366 	Validation Loss: 1.335501 	 time: 0.3
Epoch: 1090 	Training Loss: 1.182338 	Validation Loss: 1.335410 	 time: 0.3
Validation loss decreased from 1.335426 to 1.335410. Model was saved
Epoch: 1091 	Training Loss: 1.182313 	Validation Loss: 1.335330 	 time: 0.3
Validation loss decreased from 1.335410 to 1.335330. Model was saved
Epoch: 1092 	Training Loss: 1.182291 	Validation Loss: 1.335268 	 time: 0.3
Validation loss decreased from 1.335330 to 1.335268. Model was saved
Epoch: 1093 	Training Loss: 1.182269 	Validation Loss: 1.335255 	 time: 0.3
Validation loss decreased from 1.335268 to 1.335255. Model was saved
Epoch: 1094 	Training Loss: 1.182248 	Validation Loss: 1.335275 	 time: 0.4
Epoch: 1095 	Training Loss: 1.182227 	Validation Loss: 1.335321 	 time: 0.3
Epoch: 1096 	Training Loss: 1.182206 	Validation Loss: 1.335417 	 time: 0.3
Epoch: 1097 	Training Loss: 1.182181 	Validation Loss: 1.335523 	 time: 0.3
Epoch: 1098 	Training Loss: 1.182155 	Validation Loss: 1.335593 	 time: 0.3
Epoch: 1099 	Training Loss: 1.182127 	Validation Loss: 1.335625 	 time: 0.3
Epoch: 1100 	Training Loss: 1.182096 	Validation Loss: 1.335625 	 time: 0.3
Epoch: 1101 	Training Loss: 1.182059 	Validation Loss: 1.335617 	 time: 0.3
Epoch: 1102 	Training Loss: 1.182018 	Validation Loss: 1.335639 	 time: 0.3
Epoch: 1103 	Training Loss: 1.181972 	Validation Loss: 1.335669 	 time: 0.3
Epoch: 1104 	Training Loss: 1.181925 	Validation Loss: 1.335684 	 time: 0.3
Epoch: 1105 	Training Loss: 1.181880 	Validation Loss: 1.335682 	 time: 0.3
Epoch: 1106 	Training Loss: 1.181840 	Validation Loss: 1.335702 	 time: 0.2
Epoch: 1107 	Training Loss: 1.181795 	Validation Loss: 1.335757 	 time: 0.3
Epoch: 1108 	Training Loss: 1.181745 	Validation Loss: 1.335839 	 time: 0.3
Epoch: 1109 	Training Loss: 1.181696 	Validation Loss: 1.335960 	 time: 0.3
Epoch: 1110 	Training Loss: 1.181652 	Validation Loss: 1.336095 	 time: 0.3
Epoch: 1111 	Training Loss: 1.181608 	Validation Loss: 1.336199 	 time: 0.3
Epoch: 1112 	Training Loss: 1.181563 	Validation Loss: 1.336252 	 time: 0.3
Epoch: 1113 	Training Loss: 1.181518 	Validation Loss: 1.336248 	 time: 0.3
Epoch: 1114 	Training Loss: 1.181474 	Validation Loss: 1.336177 	 time: 0.3
Epoch: 1115 	Training Loss: 1.181431 	Validation Loss: 1.336035 	 time: 0.2
Epoch: 1116 	Training Loss: 1.181388 	Validation Loss: 1.335840 	 time: 0.3
Epoch: 1117 	Training Loss: 1.181344 	Validation Loss: 1.335634 	 time: 0.2
Epoch: 1118 	Training Loss: 1.181301 	Validation Loss: 1.335471 	 time: 0.3
Epoch: 1119 	Training Loss: 1.181262 	Validation Loss: 1.335371 	 time: 0.2
Epoch: 1120 	Training Loss: 1.181229 	Validation Loss: 1.335307 	 time: 0.2
Epoch: 1121 	Training Loss: 1.181197 	Validation Loss: 1.335257 	 time: 0.3
Epoch: 1122 	Training Loss: 1.181166 	Validation Loss: 1.335231 	 time: 0.3
Validation loss decreased from 1.335255 to 1.335231. Model was saved
Epoch: 1123 	Training Loss: 1.181135 	Validation Loss: 1.335231 	 time: 0.3
Epoch: 1124 	Training Loss: 1.181106 	Validation Loss: 1.335259 	 time: 0.3
Epoch: 1125 	Training Loss: 1.181076 	Validation Loss: 1.335322 	 time: 0.3
Epoch: 1126 	Training Loss: 1.181044 	Validation Loss: 1.335400 	 time: 0.3
Epoch: 1127 	Training Loss: 1.181009 	Validation Loss: 1.335445 	 time: 0.3
Epoch: 1128 	Training Loss: 1.180968 	Validation Loss: 1.335438 	 time: 0.3
Epoch: 1129 	Training Loss: 1.180922 	Validation Loss: 1.335445 	 time: 0.3
Epoch: 1130 	Training Loss: 1.180886 	Validation Loss: 1.335507 	 time: 0.2
Epoch: 1131 	Training Loss: 1.180861 	Validation Loss: 1.335595 	 time: 0.3
Epoch: 1132 	Training Loss: 1.180838 	Validation Loss: 1.335679 	 time: 0.3
Epoch: 1133 	Training Loss: 1.180814 	Validation Loss: 1.335731 	 time: 0.3
Epoch: 1134 	Training Loss: 1.180787 	Validation Loss: 1.335750 	 time: 0.3
Epoch: 1135 	Training Loss: 1.180758 	Validation Loss: 1.335750 	 time: 0.3
Epoch: 1136 	Training Loss: 1.180725 	Validation Loss: 1.335749 	 time: 0.3
Epoch: 1137 	Training Loss: 1.180688 	Validation Loss: 1.335768 	 time: 0.3
Epoch: 1138 	Training Loss: 1.180649 	Validation Loss: 1.335811 	 time: 0.3
Epoch: 1139 	Training Loss: 1.180611 	Validation Loss: 1.335891 	 time: 0.3
Epoch: 1140 	Training Loss: 1.180577 	Validation Loss: 1.335995 	 time: 0.3
Epoch: 1141 	Training Loss: 1.180550 	Validation Loss: 1.336130 	 time: 0.3
Epoch: 1142 	Training Loss: 1.180525 	Validation Loss: 1.336270 	 time: 0.3
Epoch: 1143 	Training Loss: 1.180502 	Validation Loss: 1.336403 	 time: 0.3
Epoch: 1144 	Training Loss: 1.180478 	Validation Loss: 1.336532 	 time: 0.3
Epoch: 1145 	Training Loss: 1.180451 	Validation Loss: 1.336668 	 time: 0.3
Epoch: 1146 	Training Loss: 1.180419 	Validation Loss: 1.336815 	 time: 0.3
Epoch: 1147 	Training Loss: 1.180380 	Validation Loss: 1.336971 	 time: 0.2
Epoch: 1148 	Training Loss: 1.180338 	Validation Loss: 1.337125 	 time: 0.3
Epoch: 1149 	Training Loss: 1.180294 	Validation Loss: 1.337239 	 time: 0.3
Epoch: 1150 	Training Loss: 1.180247 	Validation Loss: 1.337294 	 time: 0.3
Epoch: 1151 	Training Loss: 1.180204 	Validation Loss: 1.337305 	 time: 0.3
Epoch: 1152 	Training Loss: 1.180173 	Validation Loss: 1.337301 	 time: 0.3
Epoch: 1153 	Training Loss: 1.180147 	Validation Loss: 1.337286 	 time: 0.3
Epoch: 1154 	Training Loss: 1.180122 	Validation Loss: 1.337244 	 time: 0.3
Epoch: 1155 	Training Loss: 1.180097 	Validation Loss: 1.337200 	 time: 0.3
Epoch: 1156 	Training Loss: 1.180071 	Validation Loss: 1.337167 	 time: 0.3
Epoch: 1157 	Training Loss: 1.180044 	Validation Loss: 1.337147 	 time: 0.3
Epoch: 1158 	Training Loss: 1.180017 	Validation Loss: 1.337104 	 time: 0.3
Epoch: 1159 	Training Loss: 1.179988 	Validation Loss: 1.337049 	 time: 0.3
Epoch: 1160 	Training Loss: 1.179957 	Validation Loss: 1.336974 	 time: 0.3
Epoch: 1161 	Training Loss: 1.179924 	Validation Loss: 1.336904 	 time: 0.3
Epoch: 1162 	Training Loss: 1.179895 	Validation Loss: 1.336832 	 time: 0.3
Epoch: 1163 	Training Loss: 1.179874 	Validation Loss: 1.336776 	 time: 0.3
Epoch: 1164 	Training Loss: 1.179854 	Validation Loss: 1.336681 	 time: 0.3
Epoch: 1165 	Training Loss: 1.179834 	Validation Loss: 1.336521 	 time: 0.3
Epoch: 1166 	Training Loss: 1.179815 	Validation Loss: 1.336325 	 time: 0.3
Epoch: 1167 	Training Loss: 1.179796 	Validation Loss: 1.336130 	 time: 0.3
Epoch: 1168 	Training Loss: 1.179778 	Validation Loss: 1.335948 	 time: 0.3
Epoch: 1169 	Training Loss: 1.179759 	Validation Loss: 1.335825 	 time: 0.3
Epoch: 1170 	Training Loss: 1.179740 	Validation Loss: 1.335731 	 time: 0.3
Epoch: 1171 	Training Loss: 1.179721 	Validation Loss: 1.335646 	 time: 0.3
Epoch: 1172 	Training Loss: 1.179704 	Validation Loss: 1.335594 	 time: 0.3
Epoch: 1173 	Training Loss: 1.179689 	Validation Loss: 1.335572 	 time: 0.3
Epoch: 1174 	Training Loss: 1.179674 	Validation Loss: 1.335562 	 time: 0.3
Epoch: 1175 	Training Loss: 1.179659 	Validation Loss: 1.335573 	 time: 0.3
Epoch: 1176 	Training Loss: 1.179645 	Validation Loss: 1.335604 	 time: 0.4
Epoch: 1177 	Training Loss: 1.179632 	Validation Loss: 1.335639 	 time: 0.3
Epoch: 1178 	Training Loss: 1.179618 	Validation Loss: 1.335693 	 time: 0.3
Epoch: 1179 	Training Loss: 1.179604 	Validation Loss: 1.335745 	 time: 0.3
Epoch: 1180 	Training Loss: 1.179588 	Validation Loss: 1.335786 	 time: 0.3
Epoch: 1181 	Training Loss: 1.179572 	Validation Loss: 1.335822 	 time: 0.3
Epoch: 1182 	Training Loss: 1.179554 	Validation Loss: 1.335872 	 time: 0.3
Epoch: 1183 	Training Loss: 1.179535 	Validation Loss: 1.335921 	 time: 0.3
Epoch: 1184 	Training Loss: 1.179516 	Validation Loss: 1.335998 	 time: 0.3
Epoch: 1185 	Training Loss: 1.179497 	Validation Loss: 1.336071 	 time: 0.3
Epoch: 1186 	Training Loss: 1.179475 	Validation Loss: 1.336145 	 time: 0.3
Epoch: 1187 	Training Loss: 1.179451 	Validation Loss: 1.336209 	 time: 0.3
Epoch: 1188 	Training Loss: 1.179422 	Validation Loss: 1.336307 	 time: 0.3
Epoch: 1189 	Training Loss: 1.179394 	Validation Loss: 1.336405 	 time: 0.3
Epoch: 1190 	Training Loss: 1.179369 	Validation Loss: 1.336551 	 time: 0.3
Epoch: 1191 	Training Loss: 1.179345 	Validation Loss: 1.336652 	 time: 0.3
Epoch: 1192 	Training Loss: 1.179321 	Validation Loss: 1.336805 	 time: 0.3
Epoch: 1193 	Training Loss: 1.179295 	Validation Loss: 1.336851 	 time: 0.3
Epoch: 1194 	Training Loss: 1.179268 	Validation Loss: 1.336969 	 time: 0.3
Epoch: 1195 	Training Loss: 1.179239 	Validation Loss: 1.337016 	 time: 0.3
Epoch: 1196 	Training Loss: 1.179208 	Validation Loss: 1.337074 	 time: 0.3
Epoch: 1197 	Training Loss: 1.179177 	Validation Loss: 1.337116 	 time: 0.3
Epoch: 1198 	Training Loss: 1.179148 	Validation Loss: 1.337150 	 time: 0.3
Epoch: 1199 	Training Loss: 1.179119 	Validation Loss: 1.337182 	 time: 0.3
Epoch: 1200 	Training Loss: 1.179091 	Validation Loss: 1.337201 	 time: 0.3
Epoch: 1201 	Training Loss: 1.179063 	Validation Loss: 1.337206 	 time: 0.3
Epoch: 1202 	Training Loss: 1.179036 	Validation Loss: 1.337164 	 time: 0.3
Epoch: 1203 	Training Loss: 1.179009 	Validation Loss: 1.337116 	 time: 0.3
Epoch: 1204 	Training Loss: 1.178984 	Validation Loss: 1.337038 	 time: 0.3
Epoch: 1205 	Training Loss: 1.178960 	Validation Loss: 1.336956 	 time: 0.3
Epoch: 1206 	Training Loss: 1.178939 	Validation Loss: 1.336878 	 time: 0.3
Epoch: 1207 	Training Loss: 1.178920 	Validation Loss: 1.336789 	 time: 0.3
Epoch: 1208 	Training Loss: 1.178903 	Validation Loss: 1.336713 	 time: 0.3
Epoch: 1209 	Training Loss: 1.178885 	Validation Loss: 1.336623 	 time: 0.3
Epoch: 1210 	Training Loss: 1.178867 	Validation Loss: 1.336578 	 time: 0.3
Epoch: 1211 	Training Loss: 1.178849 	Validation Loss: 1.336522 	 time: 0.3
Epoch: 1212 	Training Loss: 1.178832 	Validation Loss: 1.336503 	 time: 0.3
Epoch: 1213 	Training Loss: 1.178816 	Validation Loss: 1.336488 	 time: 0.3
Epoch: 1214 	Training Loss: 1.178798 	Validation Loss: 1.336495 	 time: 0.3
Epoch: 1215 	Training Loss: 1.178780 	Validation Loss: 1.336521 	 time: 0.3
Epoch: 1216 	Training Loss: 1.178763 	Validation Loss: 1.336564 	 time: 0.3
Epoch: 1217 	Training Loss: 1.178749 	Validation Loss: 1.336612 	 time: 0.3
Epoch: 1218 	Training Loss: 1.178733 	Validation Loss: 1.336645 	 time: 0.3
Epoch: 1219 	Training Loss: 1.178717 	Validation Loss: 1.336659 	 time: 0.3
Epoch: 1220 	Training Loss: 1.178700 	Validation Loss: 1.336678 	 time: 0.3
Epoch: 1221 	Training Loss: 1.178683 	Validation Loss: 1.336705 	 time: 0.3
Epoch: 1222 	Training Loss: 1.178666 	Validation Loss: 1.336751 	 time: 0.3
Epoch: 1223 	Training Loss: 1.178651 	Validation Loss: 1.336762 	 time: 0.3
Epoch: 1224 	Training Loss: 1.178636 	Validation Loss: 1.336774 	 time: 0.3
Epoch: 1225 	Training Loss: 1.178622 	Validation Loss: 1.336716 	 time: 0.3
Epoch: 1226 	Training Loss: 1.178609 	Validation Loss: 1.336699 	 time: 0.2
Epoch: 1227 	Training Loss: 1.178598 	Validation Loss: 1.336583 	 time: 0.3
Epoch: 1228 	Training Loss: 1.178584 	Validation Loss: 1.336579 	 time: 0.3
Epoch: 1229 	Training Loss: 1.178572 	Validation Loss: 1.336446 	 time: 0.3
Epoch: 1230 	Training Loss: 1.178558 	Validation Loss: 1.336405 	 time: 0.3
Epoch: 1231 	Training Loss: 1.178545 	Validation Loss: 1.336345 	 time: 0.3
Epoch: 1232 	Training Loss: 1.178533 	Validation Loss: 1.336347 	 time: 0.3
Epoch: 1233 	Training Loss: 1.178520 	Validation Loss: 1.336316 	 time: 0.3
Epoch: 1234 	Training Loss: 1.178508 	Validation Loss: 1.336296 	 time: 0.3
Epoch: 1235 	Training Loss: 1.178494 	Validation Loss: 1.336271 	 time: 0.3
Epoch: 1236 	Training Loss: 1.178478 	Validation Loss: 1.336270 	 time: 0.3
Epoch: 1237 	Training Loss: 1.178456 	Validation Loss: 1.336282 	 time: 0.3
Epoch: 1238 	Training Loss: 1.178438 	Validation Loss: 1.336243 	 time: 0.3
Epoch: 1239 	Training Loss: 1.178426 	Validation Loss: 1.336195 	 time: 0.3
Epoch: 1240 	Training Loss: 1.178416 	Validation Loss: 1.336111 	 time: 0.3
Epoch: 1241 	Training Loss: 1.178407 	Validation Loss: 1.336023 	 time: 0.3
Epoch: 1242 	Training Loss: 1.178400 	Validation Loss: 1.335943 	 time: 0.3
Epoch: 1243 	Training Loss: 1.178392 	Validation Loss: 1.335908 	 time: 0.3
Epoch: 1244 	Training Loss: 1.178388 	Validation Loss: 1.335819 	 time: 0.3
Epoch: 1245 	Training Loss: 1.178382 	Validation Loss: 1.335764 	 time: 0.3
Epoch: 1246 	Training Loss: 1.178380 	Validation Loss: 1.335623 	 time: 0.3
Epoch: 1247 	Training Loss: 1.178371 	Validation Loss: 1.335593 	 time: 0.3
Epoch: 1248 	Training Loss: 1.178363 	Validation Loss: 1.335453 	 time: 0.2
Epoch: 1249 	Training Loss: 1.178350 	Validation Loss: 1.335401 	 time: 0.2
Epoch: 1250 	Training Loss: 1.178339 	Validation Loss: 1.335345 	 time: 0.2
Epoch: 1251 	Training Loss: 1.178321 	Validation Loss: 1.335415 	 time: 0.2
Epoch: 1252 	Training Loss: 1.178304 	Validation Loss: 1.335401 	 time: 0.2
Epoch: 1253 	Training Loss: 1.178289 	Validation Loss: 1.335406 	 time: 0.2
Epoch: 1254 	Training Loss: 1.178272 	Validation Loss: 1.335418 	 time: 0.3
Epoch: 1255 	Training Loss: 1.178256 	Validation Loss: 1.335467 	 time: 0.2
Epoch: 1256 	Training Loss: 1.178237 	Validation Loss: 1.335484 	 time: 0.2
Epoch: 1257 	Training Loss: 1.178219 	Validation Loss: 1.335479 	 time: 0.3
Epoch: 1258 	Training Loss: 1.178202 	Validation Loss: 1.335454 	 time: 0.3
Epoch: 1259 	Training Loss: 1.178187 	Validation Loss: 1.335449 	 time: 0.3
Epoch: 1260 	Training Loss: 1.178169 	Validation Loss: 1.335417 	 time: 0.3
Epoch: 1261 	Training Loss: 1.178147 	Validation Loss: 1.335384 	 time: 0.3
Epoch: 1262 	Training Loss: 1.178109 	Validation Loss: 1.335365 	 time: 0.3
Epoch: 1263 	Training Loss: 1.178051 	Validation Loss: 1.335400 	 time: 0.3
Epoch: 1264 	Training Loss: 1.178002 	Validation Loss: 1.335353 	 time: 0.3
Epoch: 1265 	Training Loss: 1.177989 	Validation Loss: 1.335337 	 time: 0.3
Epoch: 1266 	Training Loss: 1.177987 	Validation Loss: 1.335317 	 time: 0.3
Epoch: 1267 	Training Loss: 1.177978 	Validation Loss: 1.335411 	 time: 0.3
Epoch: 1268 	Training Loss: 1.177980 	Validation Loss: 1.335467 	 time: 0.3
Epoch: 1269 	Training Loss: 1.177983 	Validation Loss: 1.335579 	 time: 0.3
Epoch: 1270 	Training Loss: 1.177997 	Validation Loss: 1.335491 	 time: 0.2
Epoch: 1271 	Training Loss: 1.177984 	Validation Loss: 1.335485 	 time: 0.3
Epoch: 1272 	Training Loss: 1.177952 	Validation Loss: 1.335291 	 time: 0.3
Epoch: 1273 	Training Loss: 1.177915 	Validation Loss: 1.335166 	 time: 0.3
Validation loss decreased from 1.335231 to 1.335166. Model was saved
Epoch: 1274 	Training Loss: 1.177879 	Validation Loss: 1.335068 	 time: 0.3
Validation loss decreased from 1.335166 to 1.335068. Model was saved
Epoch: 1275 	Training Loss: 1.177840 	Validation Loss: 1.335016 	 time: 0.3
Validation loss decreased from 1.335068 to 1.335016. Model was saved
Epoch: 1276 	Training Loss: 1.177813 	Validation Loss: 1.335026 	 time: 0.3
Epoch: 1277 	Training Loss: 1.177801 	Validation Loss: 1.335048 	 time: 0.3
Epoch: 1278 	Training Loss: 1.177803 	Validation Loss: 1.335025 	 time: 0.3
Epoch: 1279 	Training Loss: 1.177835 	Validation Loss: 1.334792 	 time: 0.3
Validation loss decreased from 1.335016 to 1.334792. Model was saved
Epoch: 1280 	Training Loss: 1.177927 	Validation Loss: 1.334918 	 time: 0.3
Epoch: 1281 	Training Loss: 1.178007 	Validation Loss: 1.334900 	 time: 0.3
Epoch: 1282 	Training Loss: 1.178074 	Validation Loss: 1.334958 	 time: 0.3
Epoch: 1283 	Training Loss: 1.178033 	Validation Loss: 1.334632 	 time: 0.3
Validation loss decreased from 1.334792 to 1.334632. Model was saved
Epoch: 1284 	Training Loss: 1.177841 	Validation Loss: 1.334724 	 time: 0.3
Epoch: 1285 	Training Loss: 1.177702 	Validation Loss: 1.334936 	 time: 0.3
Epoch: 1286 	Training Loss: 1.177814 	Validation Loss: 1.335065 	 time: 0.3
Epoch: 1287 	Training Loss: 1.178171 	Validation Loss: 1.335269 	 time: 0.3
Epoch: 1288 	Training Loss: 1.178258 	Validation Loss: 1.334937 	 time: 0.3
Epoch: 1289 	Training Loss: 1.178121 	Validation Loss: 1.334454 	 time: 0.3
Validation loss decreased from 1.334632 to 1.334454. Model was saved
Epoch: 1290 	Training Loss: 1.177721 	Validation Loss: 1.334846 	 time: 0.3
Epoch: 1291 	Training Loss: 1.177957 	Validation Loss: 1.335193 	 time: 0.3
Epoch: 1292 	Training Loss: 1.178530 	Validation Loss: 1.335488 	 time: 0.3
Epoch: 1293 	Training Loss: 1.178183 	Validation Loss: 1.335141 	 time: 0.3
Epoch: 1294 	Training Loss: 1.177712 	Validation Loss: 1.335696 	 time: 0.3
Epoch: 1295 	Training Loss: 1.177954 	Validation Loss: 1.335543 	 time: 0.3
Epoch: 1296 	Training Loss: 1.178110 	Validation Loss: 1.334907 	 time: 0.3
Epoch: 1297 	Training Loss: 1.178161 	Validation Loss: 1.334533 	 time: 0.3
Epoch: 1298 	Training Loss: 1.177641 	Validation Loss: 1.335171 	 time: 0.3
Epoch: 1299 	Training Loss: 1.178246 	Validation Loss: 1.336107 	 time: 0.3
Epoch: 1300 	Training Loss: 1.179603 	Validation Loss: 1.336054 	 time: 0.3
Epoch: 1301 	Training Loss: 1.178258 	Validation Loss: 1.337402 	 time: 0.3
Epoch: 1302 	Training Loss: 1.178664 	Validation Loss: 1.337806 	 time: 0.3
Epoch: 1303 	Training Loss: 1.180148 	Validation Loss: 1.336430 	 time: 0.3
Epoch: 1304 	Training Loss: 1.177981 	Validation Loss: 1.338925 	 time: 0.3
Epoch: 1305 	Training Loss: 1.181414 	Validation Loss: 1.335834 	 time: 0.3
Epoch: 1306 	Training Loss: 1.179690 	Validation Loss: 1.338600 	 time: 0.3
Epoch: 1307 	Training Loss: 1.183323 	Validation Loss: 1.335695 	 time: 0.3
Epoch: 1308 	Training Loss: 1.178259 	Validation Loss: 1.340332 	 time: 0.3
Epoch: 1309 	Training Loss: 1.185075 	Validation Loss: 1.336220 	 time: 0.3
Epoch: 1310 	Training Loss: 1.178347 	Validation Loss: 1.343170 	 time: 0.3
Epoch: 1311 	Training Loss: 1.183245 	Validation Loss: 1.339396 	 time: 0.3
Epoch: 1312 	Training Loss: 1.181078 	Validation Loss: 1.336085 	 time: 0.3
Epoch: 1313 	Training Loss: 1.178162 	Validation Loss: 1.339702 	 time: 0.3
Epoch: 1314 	Training Loss: 1.182787 	Validation Loss: 1.334989 	 time: 0.3
Epoch: 1315 	Training Loss: 1.177917 	Validation Loss: 1.337349 	 time: 0.3
Epoch: 1316 	Training Loss: 1.179538 	Validation Loss: 1.337646 	 time: 0.3
Epoch: 1317 	Training Loss: 1.179989 	Validation Loss: 1.335519 	 time: 0.3
Epoch: 1318 	Training Loss: 1.178280 	Validation Loss: 1.336422 	 time: 0.3
Epoch: 1319 	Training Loss: 1.178883 	Validation Loss: 1.335429 	 time: 0.3
Epoch: 1320 	Training Loss: 1.179262 	Validation Loss: 1.335762 	 time: 0.3
Epoch: 1321 	Training Loss: 1.177989 	Validation Loss: 1.338225 	 time: 0.3
Epoch: 1322 	Training Loss: 1.178381 	Validation Loss: 1.337744 	 time: 0.3
Epoch: 1323 	Training Loss: 1.178452 	Validation Loss: 1.337883 	 time: 0.3
Epoch: 1324 	Training Loss: 1.178117 	Validation Loss: 1.337369 	 time: 0.3
Epoch: 1325 	Training Loss: 1.177957 	Validation Loss: 1.335280 	 time: 0.3
Epoch: 1326 	Training Loss: 1.178188 	Validation Loss: 1.335026 	 time: 0.3
Epoch: 1327 	Training Loss: 1.177708 	Validation Loss: 1.336911 	 time: 0.3
Epoch: 1328 	Training Loss: 1.177857 	Validation Loss: 1.337275 	 time: 0.3
Epoch: 1329 	Training Loss: 1.177710 	Validation Loss: 1.337981 	 time: 0.2
Epoch: 1330 	Training Loss: 1.177725 	Validation Loss: 1.338180 	 time: 0.3
Epoch: 1331 	Training Loss: 1.177659 	Validation Loss: 1.337508 	 time: 0.2
Epoch: 1332 	Training Loss: 1.177645 	Validation Loss: 1.337386 	 time: 0.3
Epoch: 1333 	Training Loss: 1.177446 	Validation Loss: 1.338427 	 time: 0.3
Epoch: 1334 	Training Loss: 1.177470 	Validation Loss: 1.338329 	 time: 0.3
Epoch: 1335 	Training Loss: 1.177365 	Validation Loss: 1.338657 	 time: 0.3
Epoch: 1336 	Training Loss: 1.177356 	Validation Loss: 1.339297 	 time: 0.3
Epoch: 1337 	Training Loss: 1.177388 	Validation Loss: 1.338282 	 time: 0.3
Epoch: 1338 	Training Loss: 1.177328 	Validation Loss: 1.337297 	 time: 0.3
Epoch: 1339 	Training Loss: 1.177191 	Validation Loss: 1.337438 	 time: 0.3
Epoch: 1340 	Training Loss: 1.177249 	Validation Loss: 1.337389 	 time: 0.3
Epoch: 1341 	Training Loss: 1.177213 	Validation Loss: 1.337242 	 time: 0.3
Epoch: 1342 	Training Loss: 1.177120 	Validation Loss: 1.337510 	 time: 0.3
Epoch: 1343 	Training Loss: 1.177167 	Validation Loss: 1.337631 	 time: 0.3
Epoch: 1344 	Training Loss: 1.177150 	Validation Loss: 1.337496 	 time: 0.3
Epoch: 1345 	Training Loss: 1.177037 	Validation Loss: 1.337542 	 time: 0.3
Epoch: 1346 	Training Loss: 1.177062 	Validation Loss: 1.337360 	 time: 0.3
Epoch: 1347 	Training Loss: 1.177045 	Validation Loss: 1.337079 	 time: 0.3
Epoch: 1348 	Training Loss: 1.176963 	Validation Loss: 1.337101 	 time: 0.3
Epoch: 1349 	Training Loss: 1.176983 	Validation Loss: 1.336912 	 time: 0.3
Epoch: 1350 	Training Loss: 1.176980 	Validation Loss: 1.336481 	 time: 0.3
Epoch: 1351 	Training Loss: 1.176916 	Validation Loss: 1.336323 	 time: 0.3
Epoch: 1352 	Training Loss: 1.176916 	Validation Loss: 1.336387 	 time: 0.3
Epoch: 1353 	Training Loss: 1.176901 	Validation Loss: 1.336568 	 time: 0.3
Epoch: 1354 	Training Loss: 1.176842 	Validation Loss: 1.336972 	 time: 0.3
Epoch: 1355 	Training Loss: 1.176856 	Validation Loss: 1.337096 	 time: 0.3
Epoch: 1356 	Training Loss: 1.176859 	Validation Loss: 1.336948 	 time: 0.2
Epoch: 1357 	Training Loss: 1.176825 	Validation Loss: 1.336861 	 time: 0.3
Epoch: 1358 	Training Loss: 1.176826 	Validation Loss: 1.336725 	 time: 0.3
Epoch: 1359 	Training Loss: 1.176804 	Validation Loss: 1.336600 	 time: 0.3
Epoch: 1360 	Training Loss: 1.176785 	Validation Loss: 1.336602 	 time: 0.3
Epoch: 1361 	Training Loss: 1.176790 	Validation Loss: 1.336596 	 time: 0.3
Epoch: 1362 	Training Loss: 1.176780 	Validation Loss: 1.336582 	 time: 0.3
Epoch: 1363 	Training Loss: 1.176769 	Validation Loss: 1.336643 	 time: 0.3
Epoch: 1364 	Training Loss: 1.176756 	Validation Loss: 1.336747 	 time: 0.3
Epoch: 1365 	Training Loss: 1.176747 	Validation Loss: 1.336811 	 time: 0.3
Epoch: 1366 	Training Loss: 1.176742 	Validation Loss: 1.336783 	 time: 0.3
Epoch: 1367 	Training Loss: 1.176726 	Validation Loss: 1.336731 	 time: 0.3
Epoch: 1368 	Training Loss: 1.176728 	Validation Loss: 1.336700 	 time: 0.3
Epoch: 1369 	Training Loss: 1.176720 	Validation Loss: 1.336716 	 time: 0.3
Epoch: 1370 	Training Loss: 1.176704 	Validation Loss: 1.336775 	 time: 0.3
Epoch: 1371 	Training Loss: 1.176700 	Validation Loss: 1.336798 	 time: 0.3
Epoch: 1372 	Training Loss: 1.176687 	Validation Loss: 1.336791 	 time: 0.3
Epoch: 1373 	Training Loss: 1.176677 	Validation Loss: 1.336829 	 time: 0.3
Epoch: 1374 	Training Loss: 1.176673 	Validation Loss: 1.336929 	 time: 0.3
Epoch: 1375 	Training Loss: 1.176656 	Validation Loss: 1.337047 	 time: 0.3
Epoch: 1376 	Training Loss: 1.176641 	Validation Loss: 1.337119 	 time: 0.3
Epoch: 1377 	Training Loss: 1.176622 	Validation Loss: 1.337161 	 time: 0.3
Epoch: 1378 	Training Loss: 1.176593 	Validation Loss: 1.337245 	 time: 0.3
Epoch: 1379 	Training Loss: 1.176570 	Validation Loss: 1.337376 	 time: 0.3
Epoch: 1380 	Training Loss: 1.176557 	Validation Loss: 1.337494 	 time: 0.3
Epoch: 1381 	Training Loss: 1.176548 	Validation Loss: 1.337538 	 time: 0.3
Epoch: 1382 	Training Loss: 1.176539 	Validation Loss: 1.337543 	 time: 0.3
Epoch: 1383 	Training Loss: 1.176530 	Validation Loss: 1.337560 	 time: 0.3
Epoch: 1384 	Training Loss: 1.176520 	Validation Loss: 1.337605 	 time: 0.3
Epoch: 1385 	Training Loss: 1.176510 	Validation Loss: 1.337662 	 time: 0.3
Epoch: 1386 	Training Loss: 1.176500 	Validation Loss: 1.337694 	 time: 0.3
Epoch: 1387 	Training Loss: 1.176487 	Validation Loss: 1.337713 	 time: 0.3
Epoch: 1388 	Training Loss: 1.176474 	Validation Loss: 1.337731 	 time: 0.3
Epoch: 1389 	Training Loss: 1.176461 	Validation Loss: 1.337739 	 time: 0.3
Epoch: 1390 	Training Loss: 1.176444 	Validation Loss: 1.337731 	 time: 0.3
Epoch: 1391 	Training Loss: 1.176424 	Validation Loss: 1.337711 	 time: 0.3
Epoch: 1392 	Training Loss: 1.176397 	Validation Loss: 1.337688 	 time: 0.3
Epoch: 1393 	Training Loss: 1.176368 	Validation Loss: 1.337685 	 time: 0.3
Epoch: 1394 	Training Loss: 1.176349 	Validation Loss: 1.337716 	 time: 0.3
Epoch: 1395 	Training Loss: 1.176336 	Validation Loss: 1.337785 	 time: 0.3
Epoch: 1396 	Training Loss: 1.176324 	Validation Loss: 1.337873 	 time: 0.3
Epoch: 1397 	Training Loss: 1.176310 	Validation Loss: 1.337958 	 time: 0.3
Epoch: 1398 	Training Loss: 1.176295 	Validation Loss: 1.338038 	 time: 0.3
Epoch: 1399 	Training Loss: 1.176277 	Validation Loss: 1.338111 	 time: 0.2
Epoch: 1400 	Training Loss: 1.176256 	Validation Loss: 1.338167 	 time: 0.3
Epoch: 1401 	Training Loss: 1.176239 	Validation Loss: 1.338194 	 time: 0.3
Epoch: 1402 	Training Loss: 1.176225 	Validation Loss: 1.338200 	 time: 0.2
Epoch: 1403 	Training Loss: 1.176208 	Validation Loss: 1.338202 	 time: 0.2
Epoch: 1404 	Training Loss: 1.176188 	Validation Loss: 1.338199 	 time: 0.3
Epoch: 1405 	Training Loss: 1.176166 	Validation Loss: 1.338169 	 time: 0.3
Epoch: 1406 	Training Loss: 1.176145 	Validation Loss: 1.338110 	 time: 0.2
Epoch: 1407 	Training Loss: 1.176129 	Validation Loss: 1.338040 	 time: 0.3
Epoch: 1408 	Training Loss: 1.176117 	Validation Loss: 1.337976 	 time: 0.3
Epoch: 1409 	Training Loss: 1.176106 	Validation Loss: 1.337908 	 time: 0.3
Epoch: 1410 	Training Loss: 1.176096 	Validation Loss: 1.337822 	 time: 0.3
Epoch: 1411 	Training Loss: 1.176085 	Validation Loss: 1.337719 	 time: 0.3
Epoch: 1412 	Training Loss: 1.176075 	Validation Loss: 1.337619 	 time: 0.3
Epoch: 1413 	Training Loss: 1.176066 	Validation Loss: 1.337540 	 time: 0.3
Epoch: 1414 	Training Loss: 1.176058 	Validation Loss: 1.337483 	 time: 0.3
Epoch: 1415 	Training Loss: 1.176050 	Validation Loss: 1.337443 	 time: 0.2
Epoch: 1416 	Training Loss: 1.176042 	Validation Loss: 1.337412 	 time: 0.2
Epoch: 1417 	Training Loss: 1.176034 	Validation Loss: 1.337386 	 time: 0.3
Epoch: 1418 	Training Loss: 1.176025 	Validation Loss: 1.337366 	 time: 0.3
Epoch: 1419 	Training Loss: 1.176015 	Validation Loss: 1.337347 	 time: 0.2
Epoch: 1420 	Training Loss: 1.176002 	Validation Loss: 1.337320 	 time: 0.3
Epoch: 1421 	Training Loss: 1.175987 	Validation Loss: 1.337279 	 time: 0.3
Epoch: 1422 	Training Loss: 1.175971 	Validation Loss: 1.337230 	 time: 0.3
Epoch: 1423 	Training Loss: 1.175956 	Validation Loss: 1.337183 	 time: 0.3
Epoch: 1424 	Training Loss: 1.175942 	Validation Loss: 1.337141 	 time: 0.3
Epoch: 1425 	Training Loss: 1.175928 	Validation Loss: 1.337097 	 time: 0.2
Epoch: 1426 	Training Loss: 1.175910 	Validation Loss: 1.337049 	 time: 0.3
Epoch: 1427 	Training Loss: 1.175886 	Validation Loss: 1.337002 	 time: 0.3
Epoch: 1428 	Training Loss: 1.175860 	Validation Loss: 1.336966 	 time: 0.3
Epoch: 1429 	Training Loss: 1.175833 	Validation Loss: 1.336943 	 time: 0.3
Epoch: 1430 	Training Loss: 1.175800 	Validation Loss: 1.336919 	 time: 0.3
Epoch: 1431 	Training Loss: 1.175758 	Validation Loss: 1.336899 	 time: 0.3
Epoch: 1432 	Training Loss: 1.175719 	Validation Loss: 1.336913 	 time: 0.3
Epoch: 1433 	Training Loss: 1.175681 	Validation Loss: 1.336975 	 time: 0.3
Epoch: 1434 	Training Loss: 1.175644 	Validation Loss: 1.337046 	 time: 0.2
Epoch: 1435 	Training Loss: 1.175606 	Validation Loss: 1.337074 	 time: 0.3
Epoch: 1436 	Training Loss: 1.175571 	Validation Loss: 1.337075 	 time: 0.3
Epoch: 1437 	Training Loss: 1.175544 	Validation Loss: 1.337113 	 time: 0.3
Epoch: 1438 	Training Loss: 1.175520 	Validation Loss: 1.337175 	 time: 0.3
Epoch: 1439 	Training Loss: 1.175500 	Validation Loss: 1.337189 	 time: 0.3
Epoch: 1440 	Training Loss: 1.175481 	Validation Loss: 1.337129 	 time: 0.3
Epoch: 1441 	Training Loss: 1.175464 	Validation Loss: 1.337041 	 time: 0.3
Epoch: 1442 	Training Loss: 1.175449 	Validation Loss: 1.336982 	 time: 0.3
Epoch: 1443 	Training Loss: 1.175436 	Validation Loss: 1.336951 	 time: 0.3
Epoch: 1444 	Training Loss: 1.175424 	Validation Loss: 1.336915 	 time: 0.3
Epoch: 1445 	Training Loss: 1.175414 	Validation Loss: 1.336859 	 time: 0.3
Epoch: 1446 	Training Loss: 1.175404 	Validation Loss: 1.336803 	 time: 0.3
Epoch: 1447 	Training Loss: 1.175394 	Validation Loss: 1.336781 	 time: 0.3
Epoch: 1448 	Training Loss: 1.175385 	Validation Loss: 1.336795 	 time: 0.3
Epoch: 1449 	Training Loss: 1.175375 	Validation Loss: 1.336828 	 time: 0.3
Epoch: 1450 	Training Loss: 1.175365 	Validation Loss: 1.336855 	 time: 0.3
Epoch: 1451 	Training Loss: 1.175356 	Validation Loss: 1.336869 	 time: 0.3
Epoch: 1452 	Training Loss: 1.175347 	Validation Loss: 1.336884 	 time: 0.3
Epoch: 1453 	Training Loss: 1.175337 	Validation Loss: 1.336913 	 time: 0.3
Epoch: 1454 	Training Loss: 1.175328 	Validation Loss: 1.336950 	 time: 0.3
Epoch: 1455 	Training Loss: 1.175315 	Validation Loss: 1.336976 	 time: 0.3
Epoch: 1456 	Training Loss: 1.175302 	Validation Loss: 1.336984 	 time: 0.3
Epoch: 1457 	Training Loss: 1.175291 	Validation Loss: 1.336993 	 time: 0.3
Epoch: 1458 	Training Loss: 1.175285 	Validation Loss: 1.337017 	 time: 0.2
Epoch: 1459 	Training Loss: 1.175279 	Validation Loss: 1.337044 	 time: 0.3
Epoch: 1460 	Training Loss: 1.175271 	Validation Loss: 1.337058 	 time: 0.3
Epoch: 1461 	Training Loss: 1.175262 	Validation Loss: 1.337069 	 time: 0.3
Epoch: 1462 	Training Loss: 1.175253 	Validation Loss: 1.337086 	 time: 0.3
Epoch: 1463 	Training Loss: 1.175246 	Validation Loss: 1.337102 	 time: 0.3
Epoch: 1464 	Training Loss: 1.175239 	Validation Loss: 1.337108 	 time: 0.3
Epoch: 1465 	Training Loss: 1.175232 	Validation Loss: 1.337111 	 time: 0.3
Epoch: 1466 	Training Loss: 1.175226 	Validation Loss: 1.337124 	 time: 0.2
Epoch: 1467 	Training Loss: 1.175218 	Validation Loss: 1.337150 	 time: 0.3
Epoch: 1468 	Training Loss: 1.175210 	Validation Loss: 1.337184 	 time: 0.3
Epoch: 1469 	Training Loss: 1.175201 	Validation Loss: 1.337227 	 time: 0.3
Epoch: 1470 	Training Loss: 1.175193 	Validation Loss: 1.337288 	 time: 0.3
Epoch: 1471 	Training Loss: 1.175187 	Validation Loss: 1.337362 	 time: 0.3
Epoch: 1472 	Training Loss: 1.175181 	Validation Loss: 1.337432 	 time: 0.3
Epoch: 1473 	Training Loss: 1.175177 	Validation Loss: 1.337485 	 time: 0.3
Epoch: 1474 	Training Loss: 1.175172 	Validation Loss: 1.337529 	 time: 0.2
Epoch: 1475 	Training Loss: 1.175168 	Validation Loss: 1.337570 	 time: 0.3
Epoch: 1476 	Training Loss: 1.175163 	Validation Loss: 1.337605 	 time: 0.3
Epoch: 1477 	Training Loss: 1.175158 	Validation Loss: 1.337631 	 time: 0.3
Epoch: 1478 	Training Loss: 1.175153 	Validation Loss: 1.337646 	 time: 0.3
Epoch: 1479 	Training Loss: 1.175148 	Validation Loss: 1.337652 	 time: 0.3
Epoch: 1480 	Training Loss: 1.175144 	Validation Loss: 1.337649 	 time: 0.3
Epoch: 1481 	Training Loss: 1.175139 	Validation Loss: 1.337638 	 time: 0.3
Epoch: 1482 	Training Loss: 1.175134 	Validation Loss: 1.337627 	 time: 0.3
Epoch: 1483 	Training Loss: 1.175129 	Validation Loss: 1.337622 	 time: 0.2
Epoch: 1484 	Training Loss: 1.175124 	Validation Loss: 1.337628 	 time: 0.3
Epoch: 1485 	Training Loss: 1.175119 	Validation Loss: 1.337642 	 time: 0.3
Epoch: 1486 	Training Loss: 1.175114 	Validation Loss: 1.337660 	 time: 0.3
Epoch: 1487 	Training Loss: 1.175109 	Validation Loss: 1.337679 	 time: 0.3
Epoch: 1488 	Training Loss: 1.175104 	Validation Loss: 1.337695 	 time: 0.3
Epoch: 1489 	Training Loss: 1.175098 	Validation Loss: 1.337705 	 time: 0.3
Epoch: 1490 	Training Loss: 1.175093 	Validation Loss: 1.337713 	 time: 0.3
Epoch: 1491 	Training Loss: 1.175087 	Validation Loss: 1.337718 	 time: 0.2
Epoch: 1492 	Training Loss: 1.175083 	Validation Loss: 1.337722 	 time: 0.3
Epoch: 1493 	Training Loss: 1.175078 	Validation Loss: 1.337728 	 time: 0.3
Epoch: 1494 	Training Loss: 1.175072 	Validation Loss: 1.337742 	 time: 0.3
Epoch: 1495 	Training Loss: 1.175063 	Validation Loss: 1.337768 	 time: 0.3
Epoch: 1496 	Training Loss: 1.175045 	Validation Loss: 1.337807 	 time: 0.3
Epoch: 1497 	Training Loss: 1.175023 	Validation Loss: 1.337839 	 time: 0.3
Epoch: 1498 	Training Loss: 1.175013 	Validation Loss: 1.337852 	 time: 0.3
Epoch: 1499 	Training Loss: 1.175009 	Validation Loss: 1.337857 	 time: 0.3
Epoch: 1500 	Training Loss: 1.175005 	Validation Loss: 1.337869 	 time: 0.3
Epoch: 1501 	Training Loss: 1.175002 	Validation Loss: 1.337891 	 time: 0.3
Epoch: 1502 	Training Loss: 1.174998 	Validation Loss: 1.337911 	 time: 0.3
Epoch: 1503 	Training Loss: 1.174995 	Validation Loss: 1.337914 	 time: 0.3
Epoch: 1504 	Training Loss: 1.174991 	Validation Loss: 1.337903 	 time: 0.3
Epoch: 1505 	Training Loss: 1.174988 	Validation Loss: 1.337891 	 time: 0.3
Epoch: 1506 	Training Loss: 1.174984 	Validation Loss: 1.337885 	 time: 0.2
Epoch: 1507 	Training Loss: 1.174980 	Validation Loss: 1.337872 	 time: 0.3
Epoch: 1508 	Training Loss: 1.174977 	Validation Loss: 1.337842 	 time: 0.2
Epoch: 1509 	Training Loss: 1.174973 	Validation Loss: 1.337802 	 time: 0.3
Epoch: 1510 	Training Loss: 1.174969 	Validation Loss: 1.337768 	 time: 0.3
Epoch: 1511 	Training Loss: 1.174965 	Validation Loss: 1.337749 	 time: 0.3
Epoch: 1512 	Training Loss: 1.174961 	Validation Loss: 1.337740 	 time: 0.3
Epoch: 1513 	Training Loss: 1.174957 	Validation Loss: 1.337737 	 time: 0.3
Epoch: 1514 	Training Loss: 1.174953 	Validation Loss: 1.337739 	 time: 0.3
Epoch: 1515 	Training Loss: 1.174948 	Validation Loss: 1.337750 	 time: 0.3
Epoch: 1516 	Training Loss: 1.174943 	Validation Loss: 1.337765 	 time: 0.3
Epoch: 1517 	Training Loss: 1.174938 	Validation Loss: 1.337774 	 time: 0.3
Epoch: 1518 	Training Loss: 1.174933 	Validation Loss: 1.337772 	 time: 0.3
Epoch: 1519 	Training Loss: 1.174929 	Validation Loss: 1.337757 	 time: 0.3
Epoch: 1520 	Training Loss: 1.174925 	Validation Loss: 1.337738 	 time: 0.3
Epoch: 1521 	Training Loss: 1.174921 	Validation Loss: 1.337719 	 time: 0.3
Epoch: 1522 	Training Loss: 1.174918 	Validation Loss: 1.337700 	 time: 0.3
Epoch: 1523 	Training Loss: 1.174913 	Validation Loss: 1.337678 	 time: 0.3
Epoch: 1524 	Training Loss: 1.174909 	Validation Loss: 1.337651 	 time: 0.3
Epoch: 1525 	Training Loss: 1.174905 	Validation Loss: 1.337621 	 time: 0.3
Epoch: 1526 	Training Loss: 1.174901 	Validation Loss: 1.337595 	 time: 0.3
Epoch: 1527 	Training Loss: 1.174898 	Validation Loss: 1.337577 	 time: 0.3
Epoch: 1528 	Training Loss: 1.174894 	Validation Loss: 1.337567 	 time: 0.3
Epoch: 1529 	Training Loss: 1.174891 	Validation Loss: 1.337561 	 time: 0.3
Epoch: 1530 	Training Loss: 1.174888 	Validation Loss: 1.337558 	 time: 0.3
Epoch: 1531 	Training Loss: 1.174884 	Validation Loss: 1.337564 	 time: 0.3
Epoch: 1532 	Training Loss: 1.174881 	Validation Loss: 1.337582 	 time: 0.3
Epoch: 1533 	Training Loss: 1.174877 	Validation Loss: 1.337610 	 time: 0.2
Epoch: 1534 	Training Loss: 1.174874 	Validation Loss: 1.337642 	 time: 0.3
Epoch: 1535 	Training Loss: 1.174870 	Validation Loss: 1.337671 	 time: 0.3
Epoch: 1536 	Training Loss: 1.174867 	Validation Loss: 1.337697 	 time: 0.3
Epoch: 1537 	Training Loss: 1.174864 	Validation Loss: 1.337721 	 time: 0.2
Epoch: 1538 	Training Loss: 1.174861 	Validation Loss: 1.337744 	 time: 0.2
Epoch: 1539 	Training Loss: 1.174857 	Validation Loss: 1.337762 	 time: 0.3
Epoch: 1540 	Training Loss: 1.174854 	Validation Loss: 1.337773 	 time: 0.3
Epoch: 1541 	Training Loss: 1.174850 	Validation Loss: 1.337779 	 time: 0.3
Epoch: 1542 	Training Loss: 1.174847 	Validation Loss: 1.337782 	 time: 0.3
Epoch: 1543 	Training Loss: 1.174844 	Validation Loss: 1.337784 	 time: 0.3
Epoch: 1544 	Training Loss: 1.174840 	Validation Loss: 1.337785 	 time: 0.3
Epoch: 1545 	Training Loss: 1.174836 	Validation Loss: 1.337785 	 time: 0.3
Epoch: 1546 	Training Loss: 1.174832 	Validation Loss: 1.337786 	 time: 0.3
Epoch: 1547 	Training Loss: 1.174827 	Validation Loss: 1.337792 	 time: 0.3
Epoch: 1548 	Training Loss: 1.174820 	Validation Loss: 1.337801 	 time: 0.3
Epoch: 1549 	Training Loss: 1.174813 	Validation Loss: 1.337813 	 time: 0.2
Epoch: 1550 	Training Loss: 1.174806 	Validation Loss: 1.337819 	 time: 0.2
Epoch: 1551 	Training Loss: 1.174801 	Validation Loss: 1.337816 	 time: 0.3
Epoch: 1552 	Training Loss: 1.174797 	Validation Loss: 1.337806 	 time: 0.3
Epoch: 1553 	Training Loss: 1.174794 	Validation Loss: 1.337794 	 time: 0.3
Epoch: 1554 	Training Loss: 1.174791 	Validation Loss: 1.337783 	 time: 0.3
Epoch: 1555 	Training Loss: 1.174787 	Validation Loss: 1.337774 	 time: 0.3
Epoch: 1556 	Training Loss: 1.174784 	Validation Loss: 1.337766 	 time: 0.3
Epoch: 1557 	Training Loss: 1.174781 	Validation Loss: 1.337760 	 time: 0.2
Epoch: 1558 	Training Loss: 1.174778 	Validation Loss: 1.337756 	 time: 0.3
Epoch: 1559 	Training Loss: 1.174775 	Validation Loss: 1.337755 	 time: 0.2
Epoch: 1560 	Training Loss: 1.174772 	Validation Loss: 1.337752 	 time: 0.3
Epoch: 1561 	Training Loss: 1.174769 	Validation Loss: 1.337746 	 time: 0.3
Epoch: 1562 	Training Loss: 1.174765 	Validation Loss: 1.337736 	 time: 0.3
Epoch: 1563 	Training Loss: 1.174762 	Validation Loss: 1.337726 	 time: 0.3
Epoch: 1564 	Training Loss: 1.174758 	Validation Loss: 1.337716 	 time: 0.3
Epoch: 1565 	Training Loss: 1.174751 	Validation Loss: 1.337706 	 time: 0.3
Epoch: 1566 	Training Loss: 1.174739 	Validation Loss: 1.337692 	 time: 0.3
Epoch: 1567 	Training Loss: 1.174722 	Validation Loss: 1.337680 	 time: 0.3
Epoch: 1568 	Training Loss: 1.174710 	Validation Loss: 1.337672 	 time: 0.3
Epoch: 1569 	Training Loss: 1.174709 	Validation Loss: 1.337668 	 time: 0.3
Epoch: 1570 	Training Loss: 1.174708 	Validation Loss: 1.337669 	 time: 0.3
Epoch: 1571 	Training Loss: 1.174702 	Validation Loss: 1.337674 	 time: 0.3
Epoch: 1572 	Training Loss: 1.174695 	Validation Loss: 1.337687 	 time: 0.3
Epoch: 1573 	Training Loss: 1.174688 	Validation Loss: 1.337700 	 time: 0.3
Epoch: 1574 	Training Loss: 1.174683 	Validation Loss: 1.337700 	 time: 0.3
Epoch: 1575 	Training Loss: 1.174677 	Validation Loss: 1.337680 	 time: 0.3
Epoch: 1576 	Training Loss: 1.174671 	Validation Loss: 1.337645 	 time: 0.3
Epoch: 1577 	Training Loss: 1.174664 	Validation Loss: 1.337606 	 time: 0.3
Epoch: 1578 	Training Loss: 1.174657 	Validation Loss: 1.337572 	 time: 0.3
Epoch: 1579 	Training Loss: 1.174649 	Validation Loss: 1.337545 	 time: 0.3
Epoch: 1580 	Training Loss: 1.174642 	Validation Loss: 1.337527 	 time: 0.2
Epoch: 1581 	Training Loss: 1.174635 	Validation Loss: 1.337524 	 time: 0.2
Epoch: 1582 	Training Loss: 1.174630 	Validation Loss: 1.337554 	 time: 0.3
Epoch: 1583 	Training Loss: 1.174625 	Validation Loss: 1.337624 	 time: 0.3
Epoch: 1584 	Training Loss: 1.174621 	Validation Loss: 1.337722 	 time: 0.3
Epoch: 1585 	Training Loss: 1.174617 	Validation Loss: 1.337819 	 time: 0.3
Epoch: 1586 	Training Loss: 1.174612 	Validation Loss: 1.337891 	 time: 0.3
Epoch: 1587 	Training Loss: 1.174607 	Validation Loss: 1.337938 	 time: 0.3
Epoch: 1588 	Training Loss: 1.174602 	Validation Loss: 1.337975 	 time: 0.3
Epoch: 1589 	Training Loss: 1.174597 	Validation Loss: 1.338011 	 time: 0.2
Epoch: 1590 	Training Loss: 1.174592 	Validation Loss: 1.338040 	 time: 0.3
Epoch: 1591 	Training Loss: 1.174587 	Validation Loss: 1.338054 	 time: 0.3
Epoch: 1592 	Training Loss: 1.174583 	Validation Loss: 1.338050 	 time: 0.3
Epoch: 1593 	Training Loss: 1.174578 	Validation Loss: 1.338035 	 time: 0.3
Epoch: 1594 	Training Loss: 1.174573 	Validation Loss: 1.338017 	 time: 0.2
Epoch: 1595 	Training Loss: 1.174568 	Validation Loss: 1.337996 	 time: 0.3
Epoch: 1596 	Training Loss: 1.174562 	Validation Loss: 1.337968 	 time: 0.3
Epoch: 1597 	Training Loss: 1.174551 	Validation Loss: 1.337936 	 time: 0.3
Epoch: 1598 	Training Loss: 1.174536 	Validation Loss: 1.337917 	 time: 0.3
Epoch: 1599 	Training Loss: 1.174531 	Validation Loss: 1.337934 	 time: 0.3
Epoch: 1600 	Training Loss: 1.174517 	Validation Loss: 1.337972 	 time: 0.3
Epoch: 1601 	Training Loss: 1.174507 	Validation Loss: 1.338027 	 time: 0.3
Epoch: 1602 	Training Loss: 1.174500 	Validation Loss: 1.338090 	 time: 0.3
Epoch: 1603 	Training Loss: 1.174494 	Validation Loss: 1.338127 	 time: 0.3
Epoch: 1604 	Training Loss: 1.174489 	Validation Loss: 1.338127 	 time: 0.3
Epoch: 1605 	Training Loss: 1.174483 	Validation Loss: 1.338113 	 time: 0.3
Epoch: 1606 	Training Loss: 1.174478 	Validation Loss: 1.338095 	 time: 0.3
Epoch: 1607 	Training Loss: 1.174473 	Validation Loss: 1.338076 	 time: 0.3
Epoch: 1608 	Training Loss: 1.174467 	Validation Loss: 1.338055 	 time: 0.3
Epoch: 1609 	Training Loss: 1.174460 	Validation Loss: 1.338022 	 time: 0.3
Epoch: 1610 	Training Loss: 1.174453 	Validation Loss: 1.337968 	 time: 0.3
Epoch: 1611 	Training Loss: 1.174444 	Validation Loss: 1.337915 	 time: 0.3
Epoch: 1612 	Training Loss: 1.174436 	Validation Loss: 1.337889 	 time: 0.3
Epoch: 1613 	Training Loss: 1.174428 	Validation Loss: 1.337890 	 time: 0.3
Epoch: 1614 	Training Loss: 1.174421 	Validation Loss: 1.337896 	 time: 0.3
Epoch: 1615 	Training Loss: 1.174414 	Validation Loss: 1.337883 	 time: 0.2
Epoch: 1616 	Training Loss: 1.174407 	Validation Loss: 1.337844 	 time: 0.3
Epoch: 1617 	Training Loss: 1.174400 	Validation Loss: 1.337792 	 time: 0.3
Epoch: 1618 	Training Loss: 1.174393 	Validation Loss: 1.337754 	 time: 0.3
Epoch: 1619 	Training Loss: 1.174387 	Validation Loss: 1.337736 	 time: 0.2
Epoch: 1620 	Training Loss: 1.174380 	Validation Loss: 1.337720 	 time: 0.3
Epoch: 1621 	Training Loss: 1.174373 	Validation Loss: 1.337691 	 time: 0.2
Epoch: 1622 	Training Loss: 1.174366 	Validation Loss: 1.337652 	 time: 0.2
Epoch: 1623 	Training Loss: 1.174358 	Validation Loss: 1.337608 	 time: 0.3
Epoch: 1624 	Training Loss: 1.174349 	Validation Loss: 1.337558 	 time: 0.3
Epoch: 1625 	Training Loss: 1.174338 	Validation Loss: 1.337502 	 time: 0.3
Epoch: 1626 	Training Loss: 1.174328 	Validation Loss: 1.337438 	 time: 0.3
Epoch: 1627 	Training Loss: 1.174318 	Validation Loss: 1.337367 	 time: 0.3
Epoch: 1628 	Training Loss: 1.174309 	Validation Loss: 1.337297 	 time: 0.2
Epoch: 1629 	Training Loss: 1.174301 	Validation Loss: 1.337234 	 time: 0.3
Epoch: 1630 	Training Loss: 1.174294 	Validation Loss: 1.337176 	 time: 0.3
Epoch: 1631 	Training Loss: 1.174288 	Validation Loss: 1.337114 	 time: 0.3
Epoch: 1632 	Training Loss: 1.174282 	Validation Loss: 1.337039 	 time: 0.2
Epoch: 1633 	Training Loss: 1.174277 	Validation Loss: 1.336956 	 time: 0.3
Epoch: 1634 	Training Loss: 1.174271 	Validation Loss: 1.336881 	 time: 0.3
Epoch: 1635 	Training Loss: 1.174266 	Validation Loss: 1.336830 	 time: 0.3
Epoch: 1636 	Training Loss: 1.174262 	Validation Loss: 1.336813 	 time: 0.3
Epoch: 1637 	Training Loss: 1.174257 	Validation Loss: 1.336819 	 time: 0.3
Epoch: 1638 	Training Loss: 1.174253 	Validation Loss: 1.336830 	 time: 0.3
Epoch: 1639 	Training Loss: 1.174248 	Validation Loss: 1.336832 	 time: 0.3
Epoch: 1640 	Training Loss: 1.174244 	Validation Loss: 1.336824 	 time: 0.3
Epoch: 1641 	Training Loss: 1.174240 	Validation Loss: 1.336816 	 time: 0.3
Epoch: 1642 	Training Loss: 1.174235 	Validation Loss: 1.336814 	 time: 0.3
Epoch: 1643 	Training Loss: 1.174230 	Validation Loss: 1.336815 	 time: 0.3
Epoch: 1644 	Training Loss: 1.174225 	Validation Loss: 1.336814 	 time: 0.3
Epoch: 1645 	Training Loss: 1.174220 	Validation Loss: 1.336812 	 time: 0.3
Epoch: 1646 	Training Loss: 1.174214 	Validation Loss: 1.336818 	 time: 0.3
Epoch: 1647 	Training Loss: 1.174210 	Validation Loss: 1.336838 	 time: 0.3
Epoch: 1648 	Training Loss: 1.174206 	Validation Loss: 1.336866 	 time: 0.3
Epoch: 1649 	Training Loss: 1.174202 	Validation Loss: 1.336888 	 time: 0.3
Epoch: 1650 	Training Loss: 1.174198 	Validation Loss: 1.336900 	 time: 0.3
Epoch: 1651 	Training Loss: 1.174193 	Validation Loss: 1.336903 	 time: 0.3
Epoch: 1652 	Training Loss: 1.174189 	Validation Loss: 1.336904 	 time: 0.3
Epoch: 1653 	Training Loss: 1.174185 	Validation Loss: 1.336904 	 time: 0.3
Epoch: 1654 	Training Loss: 1.174181 	Validation Loss: 1.336900 	 time: 0.3
Epoch: 1655 	Training Loss: 1.174177 	Validation Loss: 1.336886 	 time: 0.3
Epoch: 1656 	Training Loss: 1.174173 	Validation Loss: 1.336867 	 time: 0.3
Epoch: 1657 	Training Loss: 1.174168 	Validation Loss: 1.336848 	 time: 0.2
Epoch: 1658 	Training Loss: 1.174164 	Validation Loss: 1.336836 	 time: 0.3
Epoch: 1659 	Training Loss: 1.174160 	Validation Loss: 1.336831 	 time: 0.3
Epoch: 1660 	Training Loss: 1.174157 	Validation Loss: 1.336828 	 time: 0.3
Epoch: 1661 	Training Loss: 1.174153 	Validation Loss: 1.336824 	 time: 0.3
Epoch: 1662 	Training Loss: 1.174149 	Validation Loss: 1.336820 	 time: 0.3
Epoch: 1663 	Training Loss: 1.174144 	Validation Loss: 1.336819 	 time: 0.3
Epoch: 1664 	Training Loss: 1.174140 	Validation Loss: 1.336820 	 time: 0.3
Epoch: 1665 	Training Loss: 1.174135 	Validation Loss: 1.336823 	 time: 0.3
Epoch: 1666 	Training Loss: 1.174131 	Validation Loss: 1.336822 	 time: 0.3
Epoch: 1667 	Training Loss: 1.174126 	Validation Loss: 1.336812 	 time: 0.3
Epoch: 1668 	Training Loss: 1.174121 	Validation Loss: 1.336794 	 time: 0.3
Epoch: 1669 	Training Loss: 1.174115 	Validation Loss: 1.336772 	 time: 0.3
Epoch: 1670 	Training Loss: 1.174109 	Validation Loss: 1.336752 	 time: 0.3
Epoch: 1671 	Training Loss: 1.174103 	Validation Loss: 1.336734 	 time: 0.3
Epoch: 1672 	Training Loss: 1.174097 	Validation Loss: 1.336712 	 time: 0.3
Epoch: 1673 	Training Loss: 1.174091 	Validation Loss: 1.336685 	 time: 0.3
Epoch: 1674 	Training Loss: 1.174085 	Validation Loss: 1.336657 	 time: 0.3
Epoch: 1675 	Training Loss: 1.174080 	Validation Loss: 1.336636 	 time: 0.3
Epoch: 1676 	Training Loss: 1.174074 	Validation Loss: 1.336627 	 time: 0.3
Epoch: 1677 	Training Loss: 1.174068 	Validation Loss: 1.336629 	 time: 0.3
Epoch: 1678 	Training Loss: 1.174061 	Validation Loss: 1.336638 	 time: 0.3
Epoch: 1679 	Training Loss: 1.174054 	Validation Loss: 1.336655 	 time: 0.3
Epoch: 1680 	Training Loss: 1.174046 	Validation Loss: 1.336680 	 time: 0.3
Epoch: 1681 	Training Loss: 1.174036 	Validation Loss: 1.336708 	 time: 0.3
Epoch: 1682 	Training Loss: 1.174027 	Validation Loss: 1.336726 	 time: 0.3
Epoch: 1683 	Training Loss: 1.174019 	Validation Loss: 1.336721 	 time: 0.3
Epoch: 1684 	Training Loss: 1.174013 	Validation Loss: 1.336689 	 time: 0.3
Epoch: 1685 	Training Loss: 1.174008 	Validation Loss: 1.336634 	 time: 0.3
Epoch: 1686 	Training Loss: 1.174004 	Validation Loss: 1.336565 	 time: 0.3
Epoch: 1687 	Training Loss: 1.173999 	Validation Loss: 1.336494 	 time: 0.3
Epoch: 1688 	Training Loss: 1.173995 	Validation Loss: 1.336431 	 time: 0.3
Epoch: 1689 	Training Loss: 1.173991 	Validation Loss: 1.336383 	 time: 0.3
Epoch: 1690 	Training Loss: 1.173988 	Validation Loss: 1.336354 	 time: 0.3
Epoch: 1691 	Training Loss: 1.173983 	Validation Loss: 1.336341 	 time: 0.3
Epoch: 1692 	Training Loss: 1.173979 	Validation Loss: 1.336337 	 time: 0.3
Epoch: 1693 	Training Loss: 1.173975 	Validation Loss: 1.336336 	 time: 0.3
Epoch: 1694 	Training Loss: 1.173971 	Validation Loss: 1.336334 	 time: 0.3
Epoch: 1695 	Training Loss: 1.173967 	Validation Loss: 1.336326 	 time: 0.3
Epoch: 1696 	Training Loss: 1.173963 	Validation Loss: 1.336314 	 time: 0.3
Epoch: 1697 	Training Loss: 1.173959 	Validation Loss: 1.336302 	 time: 0.3
Epoch: 1698 	Training Loss: 1.173954 	Validation Loss: 1.336289 	 time: 0.3
Epoch: 1699 	Training Loss: 1.173948 	Validation Loss: 1.336269 	 time: 0.3
Epoch: 1700 	Training Loss: 1.173940 	Validation Loss: 1.336224 	 time: 0.3
Epoch: 1701 	Training Loss: 1.173928 	Validation Loss: 1.336139 	 time: 0.3
Epoch: 1702 	Training Loss: 1.173911 	Validation Loss: 1.336012 	 time: 0.3
Epoch: 1703 	Training Loss: 1.173890 	Validation Loss: 1.335895 	 time: 0.3
Epoch: 1704 	Training Loss: 1.173873 	Validation Loss: 1.335866 	 time: 0.3
Epoch: 1705 	Training Loss: 1.173865 	Validation Loss: 1.335909 	 time: 0.3
Epoch: 1706 	Training Loss: 1.173861 	Validation Loss: 1.335960 	 time: 0.3
Epoch: 1707 	Training Loss: 1.173857 	Validation Loss: 1.335989 	 time: 0.3
Epoch: 1708 	Training Loss: 1.173854 	Validation Loss: 1.336013 	 time: 0.3
Epoch: 1709 	Training Loss: 1.173851 	Validation Loss: 1.336046 	 time: 0.3
Epoch: 1710 	Training Loss: 1.173847 	Validation Loss: 1.336073 	 time: 0.3
Epoch: 1711 	Training Loss: 1.173843 	Validation Loss: 1.336071 	 time: 0.3
Epoch: 1712 	Training Loss: 1.173838 	Validation Loss: 1.336042 	 time: 0.3
Epoch: 1713 	Training Loss: 1.173833 	Validation Loss: 1.336011 	 time: 0.3
Epoch: 1714 	Training Loss: 1.173828 	Validation Loss: 1.336005 	 time: 0.3
Epoch: 1715 	Training Loss: 1.173823 	Validation Loss: 1.336021 	 time: 0.3
Epoch: 1716 	Training Loss: 1.173817 	Validation Loss: 1.336034 	 time: 0.3
Epoch: 1717 	Training Loss: 1.173812 	Validation Loss: 1.336029 	 time: 0.3
Epoch: 1718 	Training Loss: 1.173806 	Validation Loss: 1.336012 	 time: 0.3
Epoch: 1719 	Training Loss: 1.173800 	Validation Loss: 1.335995 	 time: 0.3
Epoch: 1720 	Training Loss: 1.173795 	Validation Loss: 1.335976 	 time: 0.3
Epoch: 1721 	Training Loss: 1.173790 	Validation Loss: 1.335943 	 time: 0.3
Epoch: 1722 	Training Loss: 1.173785 	Validation Loss: 1.335895 	 time: 0.3
Epoch: 1723 	Training Loss: 1.173779 	Validation Loss: 1.335837 	 time: 0.3
Epoch: 1724 	Training Loss: 1.173773 	Validation Loss: 1.335780 	 time: 0.3
Epoch: 1725 	Training Loss: 1.173767 	Validation Loss: 1.335732 	 time: 0.3
Epoch: 1726 	Training Loss: 1.173760 	Validation Loss: 1.335699 	 time: 0.3
Epoch: 1727 	Training Loss: 1.173752 	Validation Loss: 1.335677 	 time: 0.3
Epoch: 1728 	Training Loss: 1.173743 	Validation Loss: 1.335657 	 time: 0.3
Epoch: 1729 	Training Loss: 1.173733 	Validation Loss: 1.335631 	 time: 0.3
Epoch: 1730 	Training Loss: 1.173723 	Validation Loss: 1.335604 	 time: 0.3
Epoch: 1731 	Training Loss: 1.173713 	Validation Loss: 1.335587 	 time: 0.3
Epoch: 1732 	Training Loss: 1.173703 	Validation Loss: 1.335587 	 time: 0.3
Epoch: 1733 	Training Loss: 1.173692 	Validation Loss: 1.335606 	 time: 0.3
Epoch: 1734 	Training Loss: 1.173680 	Validation Loss: 1.335638 	 time: 0.3
Epoch: 1735 	Training Loss: 1.173667 	Validation Loss: 1.335677 	 time: 0.3
Epoch: 1736 	Training Loss: 1.173655 	Validation Loss: 1.335726 	 time: 0.3
Epoch: 1737 	Training Loss: 1.173645 	Validation Loss: 1.335784 	 time: 0.3
Epoch: 1738 	Training Loss: 1.173634 	Validation Loss: 1.335847 	 time: 0.3
Epoch: 1739 	Training Loss: 1.173620 	Validation Loss: 1.335912 	 time: 0.3
Epoch: 1740 	Training Loss: 1.173602 	Validation Loss: 1.335975 	 time: 0.3
Epoch: 1741 	Training Loss: 1.173584 	Validation Loss: 1.336041 	 time: 0.3
Epoch: 1742 	Training Loss: 1.173574 	Validation Loss: 1.336113 	 time: 0.3
Epoch: 1743 	Training Loss: 1.173569 	Validation Loss: 1.336202 	 time: 0.3
Epoch: 1744 	Training Loss: 1.173563 	Validation Loss: 1.336286 	 time: 0.3
Epoch: 1745 	Training Loss: 1.173558 	Validation Loss: 1.336350 	 time: 0.3
Epoch: 1746 	Training Loss: 1.173553 	Validation Loss: 1.336387 	 time: 0.3
Epoch: 1747 	Training Loss: 1.173547 	Validation Loss: 1.336387 	 time: 0.2
Epoch: 1748 	Training Loss: 1.173542 	Validation Loss: 1.336358 	 time: 0.3
Epoch: 1749 	Training Loss: 1.173537 	Validation Loss: 1.336325 	 time: 0.2
Epoch: 1750 	Training Loss: 1.173532 	Validation Loss: 1.336294 	 time: 0.3
Epoch: 1751 	Training Loss: 1.173527 	Validation Loss: 1.336270 	 time: 0.3
Epoch: 1752 	Training Loss: 1.173522 	Validation Loss: 1.336252 	 time: 0.3
Epoch: 1753 	Training Loss: 1.173517 	Validation Loss: 1.336247 	 time: 0.3
Epoch: 1754 	Training Loss: 1.173513 	Validation Loss: 1.336270 	 time: 0.3
Epoch: 1755 	Training Loss: 1.173507 	Validation Loss: 1.336306 	 time: 0.3
Epoch: 1756 	Training Loss: 1.173501 	Validation Loss: 1.336338 	 time: 0.3
Epoch: 1757 	Training Loss: 1.173493 	Validation Loss: 1.336343 	 time: 0.2
Epoch: 1758 	Training Loss: 1.173477 	Validation Loss: 1.336299 	 time: 0.2
Epoch: 1759 	Training Loss: 1.173444 	Validation Loss: 1.336219 	 time: 0.2
Epoch: 1760 	Training Loss: 1.173417 	Validation Loss: 1.336142 	 time: 0.3
Epoch: 1761 	Training Loss: 1.173410 	Validation Loss: 1.336093 	 time: 0.3
Epoch: 1762 	Training Loss: 1.173407 	Validation Loss: 1.336065 	 time: 0.3
Epoch: 1763 	Training Loss: 1.173405 	Validation Loss: 1.336040 	 time: 0.3
Epoch: 1764 	Training Loss: 1.173401 	Validation Loss: 1.336033 	 time: 0.3
Epoch: 1765 	Training Loss: 1.173398 	Validation Loss: 1.336053 	 time: 0.3
Epoch: 1766 	Training Loss: 1.173394 	Validation Loss: 1.336086 	 time: 0.3
Epoch: 1767 	Training Loss: 1.173390 	Validation Loss: 1.336133 	 time: 0.3
Epoch: 1768 	Training Loss: 1.173386 	Validation Loss: 1.336189 	 time: 0.2
Epoch: 1769 	Training Loss: 1.173381 	Validation Loss: 1.336256 	 time: 0.3
Epoch: 1770 	Training Loss: 1.173377 	Validation Loss: 1.336338 	 time: 0.3
Epoch: 1771 	Training Loss: 1.173373 	Validation Loss: 1.336436 	 time: 0.3
Epoch: 1772 	Training Loss: 1.173368 	Validation Loss: 1.336550 	 time: 0.3
Epoch: 1773 	Training Loss: 1.173364 	Validation Loss: 1.336648 	 time: 0.2
Epoch: 1774 	Training Loss: 1.173360 	Validation Loss: 1.336705 	 time: 0.3
Epoch: 1775 	Training Loss: 1.173356 	Validation Loss: 1.336721 	 time: 0.2
Epoch: 1776 	Training Loss: 1.173352 	Validation Loss: 1.336711 	 time: 0.3
Epoch: 1777 	Training Loss: 1.173348 	Validation Loss: 1.336696 	 time: 0.3
Epoch: 1778 	Training Loss: 1.173345 	Validation Loss: 1.336677 	 time: 0.3
Epoch: 1779 	Training Loss: 1.173341 	Validation Loss: 1.336656 	 time: 0.3
Epoch: 1780 	Training Loss: 1.173338 	Validation Loss: 1.336646 	 time: 0.3
Epoch: 1781 	Training Loss: 1.173334 	Validation Loss: 1.336661 	 time: 0.3
Epoch: 1782 	Training Loss: 1.173331 	Validation Loss: 1.336705 	 time: 0.3
Epoch: 1783 	Training Loss: 1.173328 	Validation Loss: 1.336758 	 time: 0.3
Epoch: 1784 	Training Loss: 1.173324 	Validation Loss: 1.336803 	 time: 0.3
Epoch: 1785 	Training Loss: 1.173321 	Validation Loss: 1.336842 	 time: 0.3
Epoch: 1786 	Training Loss: 1.173317 	Validation Loss: 1.336872 	 time: 0.3
Epoch: 1787 	Training Loss: 1.173314 	Validation Loss: 1.336893 	 time: 0.3
Epoch: 1788 	Training Loss: 1.173310 	Validation Loss: 1.336895 	 time: 0.3
Epoch: 1789 	Training Loss: 1.173306 	Validation Loss: 1.336886 	 time: 0.3
Epoch: 1790 	Training Loss: 1.173301 	Validation Loss: 1.336877 	 time: 0.3
Epoch: 1791 	Training Loss: 1.173296 	Validation Loss: 1.336876 	 time: 0.3
Epoch: 1792 	Training Loss: 1.173290 	Validation Loss: 1.336883 	 time: 0.3
Epoch: 1793 	Training Loss: 1.173283 	Validation Loss: 1.336892 	 time: 0.3
Epoch: 1794 	Training Loss: 1.173277 	Validation Loss: 1.336908 	 time: 0.3
Epoch: 1795 	Training Loss: 1.173272 	Validation Loss: 1.336929 	 time: 0.3
Epoch: 1796 	Training Loss: 1.173269 	Validation Loss: 1.336953 	 time: 0.3
Epoch: 1797 	Training Loss: 1.173267 	Validation Loss: 1.336971 	 time: 0.2
Epoch: 1798 	Training Loss: 1.173265 	Validation Loss: 1.336982 	 time: 0.3
Epoch: 1799 	Training Loss: 1.173262 	Validation Loss: 1.336995 	 time: 0.2
Epoch: 1800 	Training Loss: 1.173259 	Validation Loss: 1.337009 	 time: 0.3
Epoch: 1801 	Training Loss: 1.173256 	Validation Loss: 1.337025 	 time: 0.3
Epoch: 1802 	Training Loss: 1.173252 	Validation Loss: 1.337038 	 time: 0.3
Epoch: 1803 	Training Loss: 1.173248 	Validation Loss: 1.337051 	 time: 0.3
Epoch: 1804 	Training Loss: 1.173244 	Validation Loss: 1.337066 	 time: 0.3
Epoch: 1805 	Training Loss: 1.173241 	Validation Loss: 1.337080 	 time: 0.2
Epoch: 1806 	Training Loss: 1.173238 	Validation Loss: 1.337091 	 time: 0.3
Epoch: 1807 	Training Loss: 1.173235 	Validation Loss: 1.337092 	 time: 0.3
Epoch: 1808 	Training Loss: 1.173232 	Validation Loss: 1.337086 	 time: 0.3
Epoch: 1809 	Training Loss: 1.173229 	Validation Loss: 1.337075 	 time: 0.2
Epoch: 1810 	Training Loss: 1.173225 	Validation Loss: 1.337059 	 time: 0.2
Epoch: 1811 	Training Loss: 1.173222 	Validation Loss: 1.337036 	 time: 0.3
Epoch: 1812 	Training Loss: 1.173218 	Validation Loss: 1.337008 	 time: 0.3
Epoch: 1813 	Training Loss: 1.173215 	Validation Loss: 1.336980 	 time: 0.3
Epoch: 1814 	Training Loss: 1.173211 	Validation Loss: 1.336952 	 time: 0.2
Epoch: 1815 	Training Loss: 1.173207 	Validation Loss: 1.336927 	 time: 0.3
Epoch: 1816 	Training Loss: 1.173203 	Validation Loss: 1.336905 	 time: 0.3
Epoch: 1817 	Training Loss: 1.173198 	Validation Loss: 1.336887 	 time: 0.3
Epoch: 1818 	Training Loss: 1.173189 	Validation Loss: 1.336878 	 time: 0.3
Epoch: 1819 	Training Loss: 1.173176 	Validation Loss: 1.336886 	 time: 0.3
Epoch: 1820 	Training Loss: 1.173160 	Validation Loss: 1.336913 	 time: 0.3
Epoch: 1821 	Training Loss: 1.173149 	Validation Loss: 1.336933 	 time: 0.2
Epoch: 1822 	Training Loss: 1.173143 	Validation Loss: 1.336950 	 time: 0.3
Epoch: 1823 	Training Loss: 1.173139 	Validation Loss: 1.336964 	 time: 0.2
Epoch: 1824 	Training Loss: 1.173136 	Validation Loss: 1.337003 	 time: 0.2
Epoch: 1825 	Training Loss: 1.173132 	Validation Loss: 1.337036 	 time: 0.2
Epoch: 1826 	Training Loss: 1.173129 	Validation Loss: 1.337052 	 time: 0.2
Epoch: 1827 	Training Loss: 1.173125 	Validation Loss: 1.337033 	 time: 0.2
Epoch: 1828 	Training Loss: 1.173121 	Validation Loss: 1.337013 	 time: 0.2
Epoch: 1829 	Training Loss: 1.173118 	Validation Loss: 1.336981 	 time: 0.2
Epoch: 1830 	Training Loss: 1.173113 	Validation Loss: 1.336948 	 time: 0.2
Epoch: 1831 	Training Loss: 1.173110 	Validation Loss: 1.336890 	 time: 0.2
Epoch: 1832 	Training Loss: 1.173106 	Validation Loss: 1.336855 	 time: 0.2
Epoch: 1833 	Training Loss: 1.173102 	Validation Loss: 1.336831 	 time: 0.3
Epoch: 1834 	Training Loss: 1.173099 	Validation Loss: 1.336857 	 time: 0.2
Epoch: 1835 	Training Loss: 1.173095 	Validation Loss: 1.336850 	 time: 0.2
Epoch: 1836 	Training Loss: 1.173092 	Validation Loss: 1.336880 	 time: 0.2
Epoch: 1837 	Training Loss: 1.173089 	Validation Loss: 1.336853 	 time: 0.2
Epoch: 1838 	Training Loss: 1.173086 	Validation Loss: 1.336889 	 time: 0.2
Epoch: 1839 	Training Loss: 1.173083 	Validation Loss: 1.336843 	 time: 0.2
Epoch: 1840 	Training Loss: 1.173080 	Validation Loss: 1.336872 	 time: 0.2
Epoch: 1841 	Training Loss: 1.173077 	Validation Loss: 1.336818 	 time: 0.2
Epoch: 1842 	Training Loss: 1.173073 	Validation Loss: 1.336844 	 time: 0.3
Epoch: 1843 	Training Loss: 1.173070 	Validation Loss: 1.336795 	 time: 0.2
Epoch: 1844 	Training Loss: 1.173066 	Validation Loss: 1.336806 	 time: 0.2
Epoch: 1845 	Training Loss: 1.173062 	Validation Loss: 1.336759 	 time: 0.2
Epoch: 1846 	Training Loss: 1.173059 	Validation Loss: 1.336743 	 time: 0.2
Epoch: 1847 	Training Loss: 1.173055 	Validation Loss: 1.336686 	 time: 0.3
Epoch: 1848 	Training Loss: 1.173050 	Validation Loss: 1.336650 	 time: 0.3
Epoch: 1849 	Training Loss: 1.173045 	Validation Loss: 1.336583 	 time: 0.3
Epoch: 1850 	Training Loss: 1.173040 	Validation Loss: 1.336537 	 time: 0.3
Epoch: 1851 	Training Loss: 1.173033 	Validation Loss: 1.336447 	 time: 0.3
Epoch: 1852 	Training Loss: 1.173024 	Validation Loss: 1.336363 	 time: 0.3
Epoch: 1853 	Training Loss: 1.173011 	Validation Loss: 1.336222 	 time: 0.3
Epoch: 1854 	Training Loss: 1.172993 	Validation Loss: 1.336120 	 time: 0.3
Epoch: 1855 	Training Loss: 1.172976 	Validation Loss: 1.335995 	 time: 0.3
Epoch: 1856 	Training Loss: 1.172963 	Validation Loss: 1.335959 	 time: 0.3
Epoch: 1857 	Training Loss: 1.172954 	Validation Loss: 1.335879 	 time: 0.3
Epoch: 1858 	Training Loss: 1.172948 	Validation Loss: 1.335956 	 time: 0.3
Epoch: 1859 	Training Loss: 1.172942 	Validation Loss: 1.335959 	 time: 0.3
Epoch: 1860 	Training Loss: 1.172932 	Validation Loss: 1.336189 	 time: 0.3
Epoch: 1861 	Training Loss: 1.172918 	Validation Loss: 1.336272 	 time: 0.3
Epoch: 1862 	Training Loss: 1.172897 	Validation Loss: 1.336479 	 time: 0.3
Epoch: 1863 	Training Loss: 1.172868 	Validation Loss: 1.336634 	 time: 0.3
Epoch: 1864 	Training Loss: 1.172835 	Validation Loss: 1.336743 	 time: 0.3
Epoch: 1865 	Training Loss: 1.172805 	Validation Loss: 1.336883 	 time: 0.3
Epoch: 1866 	Training Loss: 1.172791 	Validation Loss: 1.336773 	 time: 0.2
Epoch: 1867 	Training Loss: 1.172789 	Validation Loss: 1.336828 	 time: 0.3
Epoch: 1868 	Training Loss: 1.172789 	Validation Loss: 1.336603 	 time: 0.3
Epoch: 1869 	Training Loss: 1.172779 	Validation Loss: 1.336552 	 time: 0.3
Epoch: 1870 	Training Loss: 1.172766 	Validation Loss: 1.336383 	 time: 0.3
Epoch: 1871 	Training Loss: 1.172760 	Validation Loss: 1.336120 	 time: 0.3
Epoch: 1872 	Training Loss: 1.172759 	Validation Loss: 1.336044 	 time: 0.3
Epoch: 1873 	Training Loss: 1.172757 	Validation Loss: 1.335863 	 time: 0.2
Epoch: 1874 	Training Loss: 1.172749 	Validation Loss: 1.335872 	 time: 0.2
Epoch: 1875 	Training Loss: 1.172741 	Validation Loss: 1.335841 	 time: 0.3
Epoch: 1876 	Training Loss: 1.172735 	Validation Loss: 1.335807 	 time: 0.3
Epoch: 1877 	Training Loss: 1.172731 	Validation Loss: 1.335831 	 time: 0.3
Epoch: 1878 	Training Loss: 1.172727 	Validation Loss: 1.335718 	 time: 0.3
Epoch: 1879 	Training Loss: 1.172719 	Validation Loss: 1.335702 	 time: 0.3
Epoch: 1880 	Training Loss: 1.172706 	Validation Loss: 1.335588 	 time: 0.3
Epoch: 1881 	Training Loss: 1.172687 	Validation Loss: 1.335521 	 time: 0.3
Epoch: 1882 	Training Loss: 1.172671 	Validation Loss: 1.335470 	 time: 0.3
Epoch: 1883 	Training Loss: 1.172664 	Validation Loss: 1.335355 	 time: 0.3
Epoch: 1884 	Training Loss: 1.172661 	Validation Loss: 1.335293 	 time: 0.2
Epoch: 1885 	Training Loss: 1.172657 	Validation Loss: 1.335151 	 time: 0.3
Epoch: 1886 	Training Loss: 1.172652 	Validation Loss: 1.335109 	 time: 0.3
Epoch: 1887 	Training Loss: 1.172648 	Validation Loss: 1.335094 	 time: 0.3
Epoch: 1888 	Training Loss: 1.172644 	Validation Loss: 1.335143 	 time: 0.3
Epoch: 1889 	Training Loss: 1.172641 	Validation Loss: 1.335217 	 time: 0.3
Epoch: 1890 	Training Loss: 1.172638 	Validation Loss: 1.335229 	 time: 0.3
Epoch: 1891 	Training Loss: 1.172635 	Validation Loss: 1.335264 	 time: 0.3
Epoch: 1892 	Training Loss: 1.172632 	Validation Loss: 1.335242 	 time: 0.3
Epoch: 1893 	Training Loss: 1.172628 	Validation Loss: 1.335262 	 time: 0.2
Epoch: 1894 	Training Loss: 1.172623 	Validation Loss: 1.335259 	 time: 0.3
Epoch: 1895 	Training Loss: 1.172619 	Validation Loss: 1.335288 	 time: 0.3
Epoch: 1896 	Training Loss: 1.172615 	Validation Loss: 1.335325 	 time: 0.3
Epoch: 1897 	Training Loss: 1.172612 	Validation Loss: 1.335354 	 time: 0.3
Epoch: 1898 	Training Loss: 1.172609 	Validation Loss: 1.335404 	 time: 0.3
Epoch: 1899 	Training Loss: 1.172606 	Validation Loss: 1.335413 	 time: 0.3
Epoch: 1900 	Training Loss: 1.172602 	Validation Loss: 1.335466 	 time: 0.3
Epoch: 1901 	Training Loss: 1.172598 	Validation Loss: 1.335514 	 time: 0.3
Epoch: 1902 	Training Loss: 1.172593 	Validation Loss: 1.335620 	 time: 0.3
Epoch: 1903 	Training Loss: 1.172589 	Validation Loss: 1.335715 	 time: 0.3
Epoch: 1904 	Training Loss: 1.172583 	Validation Loss: 1.335807 	 time: 0.3
Epoch: 1905 	Training Loss: 1.172578 	Validation Loss: 1.335880 	 time: 0.3
Epoch: 1906 	Training Loss: 1.172572 	Validation Loss: 1.335935 	 time: 0.3
Epoch: 1907 	Training Loss: 1.172566 	Validation Loss: 1.336001 	 time: 0.3
Epoch: 1908 	Training Loss: 1.172559 	Validation Loss: 1.336052 	 time: 0.3
Epoch: 1909 	Training Loss: 1.172551 	Validation Loss: 1.336106 	 time: 0.3
Epoch: 1910 	Training Loss: 1.172542 	Validation Loss: 1.336133 	 time: 0.3
Epoch: 1911 	Training Loss: 1.172533 	Validation Loss: 1.336168 	 time: 0.3
Epoch: 1912 	Training Loss: 1.172527 	Validation Loss: 1.336152 	 time: 0.3
Epoch: 1913 	Training Loss: 1.172522 	Validation Loss: 1.336127 	 time: 0.3
Epoch: 1914 	Training Loss: 1.172517 	Validation Loss: 1.336073 	 time: 0.3
Epoch: 1915 	Training Loss: 1.172511 	Validation Loss: 1.336053 	 time: 0.3
Epoch: 1916 	Training Loss: 1.172505 	Validation Loss: 1.336010 	 time: 0.3
Epoch: 1917 	Training Loss: 1.172498 	Validation Loss: 1.335962 	 time: 0.3
Epoch: 1918 	Training Loss: 1.172488 	Validation Loss: 1.335921 	 time: 0.2
Epoch: 1919 	Training Loss: 1.172473 	Validation Loss: 1.335942 	 time: 0.3
Epoch: 1920 	Training Loss: 1.172454 	Validation Loss: 1.336002 	 time: 0.3
Epoch: 1921 	Training Loss: 1.172444 	Validation Loss: 1.336096 	 time: 0.3
Epoch: 1922 	Training Loss: 1.172439 	Validation Loss: 1.336200 	 time: 0.3
Epoch: 1923 	Training Loss: 1.172432 	Validation Loss: 1.336265 	 time: 0.3
Epoch: 1924 	Training Loss: 1.172420 	Validation Loss: 1.336250 	 time: 0.3
Epoch: 1925 	Training Loss: 1.172391 	Validation Loss: 1.336247 	 time: 0.3
Epoch: 1926 	Training Loss: 1.172323 	Validation Loss: 1.336381 	 time: 0.3
Epoch: 1927 	Training Loss: 1.172291 	Validation Loss: 1.336660 	 time: 0.3
Epoch: 1928 	Training Loss: 1.172288 	Validation Loss: 1.337045 	 time: 0.3
Epoch: 1929 	Training Loss: 1.172282 	Validation Loss: 1.337296 	 time: 0.3
Epoch: 1930 	Training Loss: 1.172275 	Validation Loss: 1.337123 	 time: 0.3
Epoch: 1931 	Training Loss: 1.172271 	Validation Loss: 1.336953 	 time: 0.3
Epoch: 1932 	Training Loss: 1.172267 	Validation Loss: 1.336320 	 time: 0.2
Epoch: 1933 	Training Loss: 1.172265 	Validation Loss: 1.335935 	 time: 0.3
Epoch: 1934 	Training Loss: 1.172255 	Validation Loss: 1.335391 	 time: 0.2
Epoch: 1935 	Training Loss: 1.172236 	Validation Loss: 1.335195 	 time: 0.3
Epoch: 1936 	Training Loss: 1.172223 	Validation Loss: 1.335231 	 time: 0.3
Epoch: 1937 	Training Loss: 1.172222 	Validation Loss: 1.335047 	 time: 0.2
Epoch: 1938 	Training Loss: 1.172231 	Validation Loss: 1.335355 	 time: 0.3
Epoch: 1939 	Training Loss: 1.172246 	Validation Loss: 1.334661 	 time: 0.3
Epoch: 1940 	Training Loss: 1.172235 	Validation Loss: 1.334860 	 time: 0.3
Epoch: 1941 	Training Loss: 1.172206 	Validation Loss: 1.334621 	 time: 0.3
Epoch: 1942 	Training Loss: 1.172184 	Validation Loss: 1.334564 	 time: 0.3
Epoch: 1943 	Training Loss: 1.172188 	Validation Loss: 1.334829 	 time: 0.3
Epoch: 1944 	Training Loss: 1.172200 	Validation Loss: 1.334783 	 time: 0.3
Epoch: 1945 	Training Loss: 1.172188 	Validation Loss: 1.335111 	 time: 0.3
Epoch: 1946 	Training Loss: 1.172168 	Validation Loss: 1.335319 	 time: 0.3
Epoch: 1947 	Training Loss: 1.172165 	Validation Loss: 1.335277 	 time: 0.3
Epoch: 1948 	Training Loss: 1.172170 	Validation Loss: 1.335382 	 time: 0.3
Epoch: 1949 	Training Loss: 1.172177 	Validation Loss: 1.335166 	 time: 0.3
Epoch: 1950 	Training Loss: 1.172167 	Validation Loss: 1.335347 	 time: 0.3
Epoch: 1951 	Training Loss: 1.172151 	Validation Loss: 1.335388 	 time: 0.3
Epoch: 1952 	Training Loss: 1.172142 	Validation Loss: 1.335428 	 time: 0.3
Epoch: 1953 	Training Loss: 1.172140 	Validation Loss: 1.335577 	 time: 0.3
Epoch: 1954 	Training Loss: 1.172144 	Validation Loss: 1.335588 	 time: 0.3
Epoch: 1955 	Training Loss: 1.172141 	Validation Loss: 1.335779 	 time: 0.3
Epoch: 1956 	Training Loss: 1.172133 	Validation Loss: 1.335755 	 time: 0.3
Epoch: 1957 	Training Loss: 1.172125 	Validation Loss: 1.335747 	 time: 0.3
Epoch: 1958 	Training Loss: 1.172118 	Validation Loss: 1.335765 	 time: 0.3
Epoch: 1959 	Training Loss: 1.172117 	Validation Loss: 1.335786 	 time: 0.3
Epoch: 1960 	Training Loss: 1.172116 	Validation Loss: 1.335974 	 time: 0.3
Epoch: 1961 	Training Loss: 1.172115 	Validation Loss: 1.335977 	 time: 0.3
Epoch: 1962 	Training Loss: 1.172110 	Validation Loss: 1.336050 	 time: 0.3
Epoch: 1963 	Training Loss: 1.172104 	Validation Loss: 1.335976 	 time: 0.3
Epoch: 1964 	Training Loss: 1.172099 	Validation Loss: 1.336012 	 time: 0.3
Epoch: 1965 	Training Loss: 1.172094 	Validation Loss: 1.336045 	 time: 0.3
Epoch: 1966 	Training Loss: 1.172091 	Validation Loss: 1.336024 	 time: 0.3
Epoch: 1967 	Training Loss: 1.172089 	Validation Loss: 1.336046 	 time: 0.3
Epoch: 1968 	Training Loss: 1.172088 	Validation Loss: 1.336013 	 time: 0.3
Epoch: 1969 	Training Loss: 1.172086 	Validation Loss: 1.336122 	 time: 0.3
Epoch: 1970 	Training Loss: 1.172081 	Validation Loss: 1.336123 	 time: 0.3
Epoch: 1971 	Training Loss: 1.172077 	Validation Loss: 1.336138 	 time: 0.3
Epoch: 1972 	Training Loss: 1.172072 	Validation Loss: 1.336107 	 time: 0.3
Epoch: 1973 	Training Loss: 1.172069 	Validation Loss: 1.336099 	 time: 0.3
Epoch: 1974 	Training Loss: 1.172066 	Validation Loss: 1.336135 	 time: 0.3
Epoch: 1975 	Training Loss: 1.172063 	Validation Loss: 1.336104 	 time: 0.3
Epoch: 1976 	Training Loss: 1.172061 	Validation Loss: 1.336104 	 time: 0.3
Epoch: 1977 	Training Loss: 1.172058 	Validation Loss: 1.336031 	 time: 0.3
Epoch: 1978 	Training Loss: 1.172053 	Validation Loss: 1.336038 	 time: 0.3
Epoch: 1979 	Training Loss: 1.172047 	Validation Loss: 1.335962 	 time: 0.3
Epoch: 1980 	Training Loss: 1.172036 	Validation Loss: 1.335887 	 time: 0.3
Epoch: 1981 	Training Loss: 1.172016 	Validation Loss: 1.335721 	 time: 0.3
Epoch: 1982 	Training Loss: 1.171989 	Validation Loss: 1.335584 	 time: 0.3
Epoch: 1983 	Training Loss: 1.171972 	Validation Loss: 1.335567 	 time: 0.3
Epoch: 1984 	Training Loss: 1.171967 	Validation Loss: 1.335590 	 time: 0.3
Epoch: 1985 	Training Loss: 1.171964 	Validation Loss: 1.335697 	 time: 0.3
Epoch: 1986 	Training Loss: 1.171960 	Validation Loss: 1.335790 	 time: 0.3
Epoch: 1987 	Training Loss: 1.171954 	Validation Loss: 1.335988 	 time: 0.3
Epoch: 1988 	Training Loss: 1.171936 	Validation Loss: 1.336023 	 time: 0.3
Epoch: 1989 	Training Loss: 1.171899 	Validation Loss: 1.336023 	 time: 0.3
Epoch: 1990 	Training Loss: 1.171850 	Validation Loss: 1.335877 	 time: 0.3
Epoch: 1991 	Training Loss: 1.171815 	Validation Loss: 1.335961 	 time: 0.3
Epoch: 1992 	Training Loss: 1.171805 	Validation Loss: 1.336145 	 time: 0.3
Epoch: 1993 	Training Loss: 1.171801 	Validation Loss: 1.336393 	 time: 0.3
Epoch: 1994 	Training Loss: 1.171798 	Validation Loss: 1.336667 	 time: 0.3
Epoch: 1995 	Training Loss: 1.171794 	Validation Loss: 1.336851 	 time: 0.3
Epoch: 1996 	Training Loss: 1.171792 	Validation Loss: 1.337083 	 time: 0.3
Epoch: 1997 	Training Loss: 1.171792 	Validation Loss: 1.337136 	 time: 0.3
Epoch: 1998 	Training Loss: 1.171792 	Validation Loss: 1.337210 	 time: 0.3
Epoch: 1999 	Training Loss: 1.171791 	Validation Loss: 1.337175 	 time: 0.3
Epoch: 2000 	Training Loss: 1.171782 	Validation Loss: 1.337231 	 time: 0.2
Epoch: 2001 	Training Loss: 1.171771 	Validation Loss: 1.337199 	 time: 0.3
Epoch: 2002 	Training Loss: 1.171764 	Validation Loss: 1.337131 	 time: 0.4
Epoch: 2003 	Training Loss: 1.171764 	Validation Loss: 1.337081 	 time: 0.3
Epoch: 2004 	Training Loss: 1.171771 	Validation Loss: 1.337011 	 time: 0.5
Epoch: 2005 	Training Loss: 1.171781 	Validation Loss: 1.337040 	 time: 0.3
Epoch: 2006 	Training Loss: 1.171791 	Validation Loss: 1.337031 	 time: 0.3
Epoch: 2007 	Training Loss: 1.171774 	Validation Loss: 1.337167 	 time: 0.3
Epoch: 2008 	Training Loss: 1.171750 	Validation Loss: 1.337258 	 time: 0.3
Epoch: 2009 	Training Loss: 1.171749 	Validation Loss: 1.337242 	 time: 0.3
Epoch: 2010 	Training Loss: 1.171764 	Validation Loss: 1.337301 	 time: 0.4
Epoch: 2011 	Training Loss: 1.171774 	Validation Loss: 1.337387 	 time: 0.3
Epoch: 2012 	Training Loss: 1.171762 	Validation Loss: 1.337478 	 time: 0.3
Epoch: 2013 	Training Loss: 1.171744 	Validation Loss: 1.337618 	 time: 0.3
Epoch: 2014 	Training Loss: 1.171728 	Validation Loss: 1.337745 	 time: 0.3
Epoch: 2015 	Training Loss: 1.171725 	Validation Loss: 1.337869 	 time: 0.2
Epoch: 2016 	Training Loss: 1.171736 	Validation Loss: 1.337689 	 time: 0.3
Epoch: 2017 	Training Loss: 1.171753 	Validation Loss: 1.337608 	 time: 0.3
Epoch: 2018 	Training Loss: 1.171776 	Validation Loss: 1.337513 	 time: 0.3
Epoch: 2019 	Training Loss: 1.171751 	Validation Loss: 1.337552 	 time: 0.3
Epoch: 2020 	Training Loss: 1.171715 	Validation Loss: 1.337729 	 time: 0.4
Epoch: 2021 	Training Loss: 1.171727 	Validation Loss: 1.337741 	 time: 0.3
Epoch: 2022 	Training Loss: 1.171748 	Validation Loss: 1.337806 	 time: 0.4
Epoch: 2023 	Training Loss: 1.171741 	Validation Loss: 1.337552 	 time: 0.3
Epoch: 2024 	Training Loss: 1.171707 	Validation Loss: 1.337542 	 time: 0.3
Epoch: 2025 	Training Loss: 1.171698 	Validation Loss: 1.337528 	 time: 0.3
Epoch: 2026 	Training Loss: 1.171716 	Validation Loss: 1.337428 	 time: 0.3
Epoch: 2027 	Training Loss: 1.171733 	Validation Loss: 1.337487 	 time: 0.3
Epoch: 2028 	Training Loss: 1.171741 	Validation Loss: 1.337353 	 time: 0.3
Epoch: 2029 	Training Loss: 1.171708 	Validation Loss: 1.337460 	 time: 0.3
Epoch: 2030 	Training Loss: 1.171683 	Validation Loss: 1.337476 	 time: 0.3
Epoch: 2031 	Training Loss: 1.171692 	Validation Loss: 1.337369 	 time: 0.3
Epoch: 2032 	Training Loss: 1.171708 	Validation Loss: 1.337322 	 time: 0.3
Epoch: 2033 	Training Loss: 1.171710 	Validation Loss: 1.337263 	 time: 0.3
Epoch: 2034 	Training Loss: 1.171685 	Validation Loss: 1.337437 	 time: 0.3
Epoch: 2035 	Training Loss: 1.171668 	Validation Loss: 1.337598 	 time: 0.3
Epoch: 2036 	Training Loss: 1.171676 	Validation Loss: 1.337519 	 time: 0.3
Epoch: 2037 	Training Loss: 1.171690 	Validation Loss: 1.337473 	 time: 0.3
Epoch: 2038 	Training Loss: 1.171716 	Validation Loss: 1.337252 	 time: 0.3
Epoch: 2039 	Training Loss: 1.171698 	Validation Loss: 1.337428 	 time: 0.3
Epoch: 2040 	Training Loss: 1.171665 	Validation Loss: 1.337580 	 time: 0.3
Epoch: 2041 	Training Loss: 1.171659 	Validation Loss: 1.337646 	 time: 0.3
Epoch: 2042 	Training Loss: 1.171668 	Validation Loss: 1.337757 	 time: 0.3
Epoch: 2043 	Training Loss: 1.171687 	Validation Loss: 1.337601 	 time: 0.3
Epoch: 2044 	Training Loss: 1.171669 	Validation Loss: 1.337723 	 time: 0.2
Epoch: 2045 	Training Loss: 1.171646 	Validation Loss: 1.337730 	 time: 0.3
Epoch: 2046 	Training Loss: 1.171640 	Validation Loss: 1.337629 	 time: 0.3
Epoch: 2047 	Training Loss: 1.171637 	Validation Loss: 1.337653 	 time: 0.2
Epoch: 2048 	Training Loss: 1.171649 	Validation Loss: 1.337626 	 time: 0.3
Epoch: 2049 	Training Loss: 1.171651 	Validation Loss: 1.337798 	 time: 0.2
Epoch: 2050 	Training Loss: 1.171653 	Validation Loss: 1.337741 	 time: 0.2
Epoch: 2051 	Training Loss: 1.171647 	Validation Loss: 1.337617 	 time: 0.3
Epoch: 2052 	Training Loss: 1.171625 	Validation Loss: 1.337520 	 time: 0.2
Epoch: 2053 	Training Loss: 1.171612 	Validation Loss: 1.337611 	 time: 0.3
Epoch: 2054 	Training Loss: 1.171603 	Validation Loss: 1.337748 	 time: 0.2
Epoch: 2055 	Training Loss: 1.171607 	Validation Loss: 1.337558 	 time: 0.3
Epoch: 2056 	Training Loss: 1.171615 	Validation Loss: 1.337333 	 time: 0.3
Epoch: 2057 	Training Loss: 1.171631 	Validation Loss: 1.337229 	 time: 0.3
Epoch: 2058 	Training Loss: 1.171629 	Validation Loss: 1.337427 	 time: 0.3
Epoch: 2059 	Training Loss: 1.171595 	Validation Loss: 1.337665 	 time: 0.3
Epoch: 2060 	Training Loss: 1.171577 	Validation Loss: 1.337743 	 time: 0.2
Epoch: 2061 	Training Loss: 1.171580 	Validation Loss: 1.337774 	 time: 0.3
Epoch: 2062 	Training Loss: 1.171608 	Validation Loss: 1.337641 	 time: 0.3
Epoch: 2063 	Training Loss: 1.171621 	Validation Loss: 1.337814 	 time: 0.3
Epoch: 2064 	Training Loss: 1.171611 	Validation Loss: 1.337924 	 time: 0.3
Epoch: 2065 	Training Loss: 1.171570 	Validation Loss: 1.338055 	 time: 0.3
Epoch: 2066 	Training Loss: 1.171540 	Validation Loss: 1.338315 	 time: 0.2
Epoch: 2067 	Training Loss: 1.171558 	Validation Loss: 1.338084 	 time: 0.3
Epoch: 2068 	Training Loss: 1.171578 	Validation Loss: 1.338181 	 time: 0.3
Epoch: 2069 	Training Loss: 1.171595 	Validation Loss: 1.337929 	 time: 0.3
Epoch: 2070 	Training Loss: 1.171567 	Validation Loss: 1.337853 	 time: 0.3
Epoch: 2071 	Training Loss: 1.171522 	Validation Loss: 1.337894 	 time: 0.3
Epoch: 2072 	Training Loss: 1.171520 	Validation Loss: 1.337972 	 time: 0.3
Epoch: 2073 	Training Loss: 1.171540 	Validation Loss: 1.338306 	 time: 0.3
Epoch: 2074 	Training Loss: 1.171550 	Validation Loss: 1.337874 	 time: 0.3
Epoch: 2075 	Training Loss: 1.171527 	Validation Loss: 1.337687 	 time: 0.3
Epoch: 2076 	Training Loss: 1.171499 	Validation Loss: 1.337664 	 time: 0.3
Epoch: 2077 	Training Loss: 1.171474 	Validation Loss: 1.337777 	 time: 0.3
Epoch: 2078 	Training Loss: 1.171455 	Validation Loss: 1.338088 	 time: 0.3
Epoch: 2079 	Training Loss: 1.171437 	Validation Loss: 1.338374 	 time: 0.3
Epoch: 2080 	Training Loss: 1.171430 	Validation Loss: 1.338587 	 time: 0.3
Epoch: 2081 	Training Loss: 1.171468 	Validation Loss: 1.338716 	 time: 0.3
Epoch: 2082 	Training Loss: 1.171461 	Validation Loss: 1.338584 	 time: 0.3
Epoch: 2083 	Training Loss: 1.171404 	Validation Loss: 1.338629 	 time: 0.3
Epoch: 2084 	Training Loss: 1.171390 	Validation Loss: 1.338820 	 time: 0.3
Epoch: 2085 	Training Loss: 1.171408 	Validation Loss: 1.339080 	 time: 0.2
Epoch: 2086 	Training Loss: 1.171425 	Validation Loss: 1.339142 	 time: 0.2
Epoch: 2087 	Training Loss: 1.171386 	Validation Loss: 1.339148 	 time: 0.2
Epoch: 2088 	Training Loss: 1.171364 	Validation Loss: 1.338980 	 time: 0.3
Epoch: 2089 	Training Loss: 1.171364 	Validation Loss: 1.338789 	 time: 0.3
Epoch: 2090 	Training Loss: 1.171371 	Validation Loss: 1.338855 	 time: 0.3
Epoch: 2091 	Training Loss: 1.171398 	Validation Loss: 1.338828 	 time: 0.3
Epoch: 2092 	Training Loss: 1.171401 	Validation Loss: 1.338904 	 time: 0.3
Epoch: 2093 	Training Loss: 1.171414 	Validation Loss: 1.338756 	 time: 0.3
Epoch: 2094 	Training Loss: 1.171380 	Validation Loss: 1.338730 	 time: 0.3
Epoch: 2095 	Training Loss: 1.171340 	Validation Loss: 1.338778 	 time: 0.2
Epoch: 2096 	Training Loss: 1.171368 	Validation Loss: 1.338788 	 time: 0.3
Epoch: 2097 	Training Loss: 1.171389 	Validation Loss: 1.338751 	 time: 0.3
Epoch: 2098 	Training Loss: 1.171370 	Validation Loss: 1.338613 	 time: 0.3
Epoch: 2099 	Training Loss: 1.171346 	Validation Loss: 1.338590 	 time: 0.2
Epoch: 2100 	Training Loss: 1.171324 	Validation Loss: 1.338644 	 time: 0.2
Epoch: 2101 	Training Loss: 1.171350 	Validation Loss: 1.338647 	 time: 0.2
Epoch: 2102 	Training Loss: 1.171374 	Validation Loss: 1.338907 	 time: 0.3
Epoch: 2103 	Training Loss: 1.171439 	Validation Loss: 1.338571 	 time: 0.3
Epoch: 2104 	Training Loss: 1.171423 	Validation Loss: 1.338460 	 time: 0.3
Epoch: 2105 	Training Loss: 1.171349 	Validation Loss: 1.338566 	 time: 0.3
Epoch: 2106 	Training Loss: 1.171346 	Validation Loss: 1.338751 	 time: 0.3
Epoch: 2107 	Training Loss: 1.171399 	Validation Loss: 1.339045 	 time: 0.3
Epoch: 2108 	Training Loss: 1.171360 	Validation Loss: 1.338953 	 time: 0.3
Epoch: 2109 	Training Loss: 1.171328 	Validation Loss: 1.338895 	 time: 0.3
Epoch: 2110 	Training Loss: 1.171365 	Validation Loss: 1.338729 	 time: 0.3
Epoch: 2111 	Training Loss: 1.171383 	Validation Loss: 1.338583 	 time: 0.3
Epoch: 2112 	Training Loss: 1.171335 	Validation Loss: 1.338743 	 time: 0.3
Epoch: 2113 	Training Loss: 1.171296 	Validation Loss: 1.339206 	 time: 0.3
Epoch: 2114 	Training Loss: 1.171339 	Validation Loss: 1.338935 	 time: 0.2
Epoch: 2115 	Training Loss: 1.171385 	Validation Loss: 1.338889 	 time: 0.3
Epoch: 2116 	Training Loss: 1.171460 	Validation Loss: 1.338541 	 time: 0.3
Epoch: 2117 	Training Loss: 1.171357 	Validation Loss: 1.338703 	 time: 0.2
Epoch: 2118 	Training Loss: 1.171328 	Validation Loss: 1.338784 	 time: 0.3
Epoch: 2119 	Training Loss: 1.171385 	Validation Loss: 1.338689 	 time: 0.3
Epoch: 2120 	Training Loss: 1.171335 	Validation Loss: 1.338827 	 time: 0.3
Epoch: 2121 	Training Loss: 1.171300 	Validation Loss: 1.339167 	 time: 0.3
Epoch: 2122 	Training Loss: 1.171359 	Validation Loss: 1.338785 	 time: 0.2
Epoch: 2123 	Training Loss: 1.171340 	Validation Loss: 1.338454 	 time: 0.3
Epoch: 2124 	Training Loss: 1.171286 	Validation Loss: 1.338127 	 time: 0.3
Epoch: 2125 	Training Loss: 1.171271 	Validation Loss: 1.338143 	 time: 0.3
Epoch: 2126 	Training Loss: 1.171300 	Validation Loss: 1.338656 	 time: 0.2
Epoch: 2127 	Training Loss: 1.171348 	Validation Loss: 1.338718 	 time: 0.3
Epoch: 2128 	Training Loss: 1.171308 	Validation Loss: 1.339099 	 time: 0.3
Epoch: 2129 	Training Loss: 1.171251 	Validation Loss: 1.338862 	 time: 0.3
Epoch: 2130 	Training Loss: 1.171238 	Validation Loss: 1.338517 	 time: 0.3
Epoch: 2131 	Training Loss: 1.171240 	Validation Loss: 1.338380 	 time: 0.2
Epoch: 2132 	Training Loss: 1.171245 	Validation Loss: 1.338238 	 time: 0.3
Epoch: 2133 	Training Loss: 1.171180 	Validation Loss: 1.338414 	 time: 0.3
Epoch: 2134 	Training Loss: 1.171141 	Validation Loss: 1.338422 	 time: 0.3
Epoch: 2135 	Training Loss: 1.171148 	Validation Loss: 1.338090 	 time: 0.3
Epoch: 2136 	Training Loss: 1.171143 	Validation Loss: 1.337921 	 time: 0.3
Epoch: 2137 	Training Loss: 1.171137 	Validation Loss: 1.337642 	 time: 0.3
Epoch: 2138 	Training Loss: 1.171132 	Validation Loss: 1.337863 	 time: 0.2
Epoch: 2139 	Training Loss: 1.171123 	Validation Loss: 1.337976 	 time: 0.3
Epoch: 2140 	Training Loss: 1.171112 	Validation Loss: 1.337983 	 time: 0.2
Epoch: 2141 	Training Loss: 1.171102 	Validation Loss: 1.338122 	 time: 0.2
Epoch: 2142 	Training Loss: 1.171104 	Validation Loss: 1.338203 	 time: 0.2
Epoch: 2143 	Training Loss: 1.171087 	Validation Loss: 1.338587 	 time: 0.3
Epoch: 2144 	Training Loss: 1.171061 	Validation Loss: 1.338815 	 time: 0.3
Epoch: 2145 	Training Loss: 1.171056 	Validation Loss: 1.338467 	 time: 0.3
Epoch: 2146 	Training Loss: 1.171050 	Validation Loss: 1.338484 	 time: 0.3
Epoch: 2147 	Training Loss: 1.171051 	Validation Loss: 1.338531 	 time: 0.3
Epoch: 2148 	Training Loss: 1.171046 	Validation Loss: 1.339163 	 time: 0.3
Epoch: 2149 	Training Loss: 1.171040 	Validation Loss: 1.339275 	 time: 0.3
Epoch: 2150 	Training Loss: 1.171034 	Validation Loss: 1.339368 	 time: 0.3
Epoch: 2151 	Training Loss: 1.171026 	Validation Loss: 1.339176 	 time: 0.2
Epoch: 2152 	Training Loss: 1.171025 	Validation Loss: 1.339164 	 time: 0.2
Epoch: 2153 	Training Loss: 1.171016 	Validation Loss: 1.339111 	 time: 0.3
Epoch: 2154 	Training Loss: 1.171011 	Validation Loss: 1.339029 	 time: 0.3
Epoch: 2155 	Training Loss: 1.171001 	Validation Loss: 1.338981 	 time: 0.3
Epoch: 2156 	Training Loss: 1.170992 	Validation Loss: 1.338927 	 time: 0.3
Epoch: 2157 	Training Loss: 1.170982 	Validation Loss: 1.339077 	 time: 0.2
Epoch: 2158 	Training Loss: 1.170971 	Validation Loss: 1.339131 	 time: 0.2
Epoch: 2159 	Training Loss: 1.170969 	Validation Loss: 1.339159 	 time: 0.3
Epoch: 2160 	Training Loss: 1.170965 	Validation Loss: 1.339176 	 time: 0.3
Epoch: 2161 	Training Loss: 1.170962 	Validation Loss: 1.339354 	 time: 0.2
Epoch: 2162 	Training Loss: 1.170957 	Validation Loss: 1.339431 	 time: 0.3
Epoch: 2163 	Training Loss: 1.170950 	Validation Loss: 1.339435 	 time: 0.3
Epoch: 2164 	Training Loss: 1.170938 	Validation Loss: 1.339226 	 time: 0.3
Epoch: 2165 	Training Loss: 1.170912 	Validation Loss: 1.339155 	 time: 0.3
Epoch: 2166 	Training Loss: 1.170882 	Validation Loss: 1.339298 	 time: 0.3
Epoch: 2167 	Training Loss: 1.170868 	Validation Loss: 1.339325 	 time: 0.3
Epoch: 2168 	Training Loss: 1.170872 	Validation Loss: 1.339234 	 time: 0.2
Epoch: 2169 	Training Loss: 1.170869 	Validation Loss: 1.338984 	 time: 0.3
Epoch: 2170 	Training Loss: 1.170858 	Validation Loss: 1.338819 	 time: 0.3
Epoch: 2171 	Training Loss: 1.170845 	Validation Loss: 1.338761 	 time: 0.3
Epoch: 2172 	Training Loss: 1.170833 	Validation Loss: 1.338771 	 time: 0.2
Epoch: 2173 	Training Loss: 1.170824 	Validation Loss: 1.338533 	 time: 0.2
Epoch: 2174 	Training Loss: 1.170820 	Validation Loss: 1.338380 	 time: 0.2
Epoch: 2175 	Training Loss: 1.170823 	Validation Loss: 1.338093 	 time: 0.3
Epoch: 2176 	Training Loss: 1.170825 	Validation Loss: 1.338137 	 time: 0.3
Epoch: 2177 	Training Loss: 1.170819 	Validation Loss: 1.338176 	 time: 0.3
Epoch: 2178 	Training Loss: 1.170802 	Validation Loss: 1.338315 	 time: 0.3
Epoch: 2179 	Training Loss: 1.170786 	Validation Loss: 1.338418 	 time: 0.3
