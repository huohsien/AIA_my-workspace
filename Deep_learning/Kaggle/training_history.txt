Epoch: 1 	Training Loss: 1.795299 	Validation Loss: 1.806810 	 time: 0.3
Validation loss decreased from inf to 1.806810. Model was saved
Epoch: 2 	Training Loss: 1.807209 	Validation Loss: 1.798295 	 time: 0.3
Validation loss decreased from 1.806810 to 1.798295. Model was saved
Epoch: 3 	Training Loss: 1.798316 	Validation Loss: 1.792770 	 time: 0.3
Validation loss decreased from 1.798295 to 1.792770. Model was saved
Epoch: 4 	Training Loss: 1.792565 	Validation Loss: 1.790676 	 time: 0.3
Validation loss decreased from 1.792770 to 1.790676. Model was saved
Epoch: 5 	Training Loss: 1.790480 	Validation Loss: 1.789946 	 time: 0.3
Validation loss decreased from 1.790676 to 1.789946. Model was saved
Epoch: 6 	Training Loss: 1.789439 	Validation Loss: 1.787473 	 time: 0.3
Validation loss decreased from 1.789946 to 1.787473. Model was saved
Epoch: 7 	Training Loss: 1.785989 	Validation Loss: 1.782933 	 time: 0.3
Validation loss decreased from 1.787473 to 1.782933. Model was saved
Epoch: 8 	Training Loss: 1.779660 	Validation Loss: 1.776436 	 time: 0.3
Validation loss decreased from 1.782933 to 1.776436. Model was saved
Epoch: 9 	Training Loss: 1.770630 	Validation Loss: 1.767937 	 time: 0.3
Validation loss decreased from 1.776436 to 1.767937. Model was saved
Epoch: 10 	Training Loss: 1.759010 	Validation Loss: 1.757856 	 time: 0.3
Validation loss decreased from 1.767937 to 1.757856. Model was saved
Epoch: 11 	Training Loss: 1.745602 	Validation Loss: 1.747272 	 time: 0.3
Validation loss decreased from 1.757856 to 1.747272. Model was saved
Epoch: 12 	Training Loss: 1.730883 	Validation Loss: 1.736231 	 time: 0.3
Validation loss decreased from 1.747272 to 1.736231. Model was saved
Epoch: 13 	Training Loss: 1.716202 	Validation Loss: 1.721169 	 time: 0.3
Validation loss decreased from 1.736231 to 1.721169. Model was saved
Epoch: 14 	Training Loss: 1.701086 	Validation Loss: 1.704300 	 time: 0.3
Validation loss decreased from 1.721169 to 1.704300. Model was saved
Epoch: 15 	Training Loss: 1.685466 	Validation Loss: 1.687870 	 time: 0.3
Validation loss decreased from 1.704300 to 1.687870. Model was saved
Epoch: 16 	Training Loss: 1.669297 	Validation Loss: 1.671100 	 time: 0.3
Validation loss decreased from 1.687870 to 1.671100. Model was saved
Epoch: 17 	Training Loss: 1.654983 	Validation Loss: 1.660550 	 time: 0.3
Validation loss decreased from 1.671100 to 1.660550. Model was saved
Epoch: 18 	Training Loss: 1.642433 	Validation Loss: 1.647553 	 time: 0.3
Validation loss decreased from 1.660550 to 1.647553. Model was saved
Epoch: 19 	Training Loss: 1.630486 	Validation Loss: 1.644902 	 time: 0.3
Validation loss decreased from 1.647553 to 1.644902. Model was saved
Epoch: 20 	Training Loss: 1.619696 	Validation Loss: 1.628953 	 time: 0.3
Validation loss decreased from 1.644902 to 1.628953. Model was saved
Epoch: 21 	Training Loss: 1.608265 	Validation Loss: 1.627533 	 time: 0.3
Validation loss decreased from 1.628953 to 1.627533. Model was saved
Epoch: 22 	Training Loss: 1.596754 	Validation Loss: 1.622840 	 time: 0.3
Validation loss decreased from 1.627533 to 1.622840. Model was saved
Epoch: 23 	Training Loss: 1.585880 	Validation Loss: 1.614680 	 time: 0.3
Validation loss decreased from 1.622840 to 1.614680. Model was saved
Epoch: 24 	Training Loss: 1.576568 	Validation Loss: 1.613874 	 time: 0.3
Validation loss decreased from 1.614680 to 1.613874. Model was saved
Epoch: 25 	Training Loss: 1.567080 	Validation Loss: 1.612541 	 time: 0.3
Validation loss decreased from 1.613874 to 1.612541. Model was saved
Epoch: 26 	Training Loss: 1.559493 	Validation Loss: 1.609119 	 time: 0.3
Validation loss decreased from 1.612541 to 1.609119. Model was saved
Epoch: 27 	Training Loss: 1.551202 	Validation Loss: 1.610368 	 time: 0.3
Epoch: 28 	Training Loss: 1.543727 	Validation Loss: 1.610784 	 time: 0.3
Epoch: 29 	Training Loss: 1.536145 	Validation Loss: 1.613350 	 time: 0.3
Epoch: 30 	Training Loss: 1.529707 	Validation Loss: 1.617874 	 time: 0.3
Epoch: 31 	Training Loss: 1.523461 	Validation Loss: 1.616695 	 time: 0.3
Epoch: 32 	Training Loss: 1.517934 	Validation Loss: 1.613504 	 time: 0.3
Epoch: 33 	Training Loss: 1.511755 	Validation Loss: 1.609482 	 time: 0.3
Epoch: 34 	Training Loss: 1.505627 	Validation Loss: 1.604311 	 time: 0.3
Validation loss decreased from 1.609119 to 1.604311. Model was saved
Epoch: 35 	Training Loss: 1.499655 	Validation Loss: 1.600660 	 time: 0.3
Validation loss decreased from 1.604311 to 1.600660. Model was saved
Epoch: 36 	Training Loss: 1.493467 	Validation Loss: 1.590854 	 time: 0.3
Validation loss decreased from 1.600660 to 1.590854. Model was saved
Epoch: 37 	Training Loss: 1.487679 	Validation Loss: 1.586532 	 time: 0.3
Validation loss decreased from 1.590854 to 1.586532. Model was saved
Epoch: 38 	Training Loss: 1.481940 	Validation Loss: 1.583608 	 time: 0.3
Validation loss decreased from 1.586532 to 1.583608. Model was saved
Epoch: 39 	Training Loss: 1.476807 	Validation Loss: 1.580520 	 time: 0.3
Validation loss decreased from 1.583608 to 1.580520. Model was saved
Epoch: 40 	Training Loss: 1.472238 	Validation Loss: 1.579334 	 time: 0.3
Validation loss decreased from 1.580520 to 1.579334. Model was saved
Epoch: 41 	Training Loss: 1.467669 	Validation Loss: 1.574899 	 time: 0.3
Validation loss decreased from 1.579334 to 1.574899. Model was saved
Epoch: 42 	Training Loss: 1.463710 	Validation Loss: 1.576049 	 time: 0.3
Epoch: 43 	Training Loss: 1.460091 	Validation Loss: 1.571629 	 time: 0.3
Validation loss decreased from 1.574899 to 1.571629. Model was saved
Epoch: 44 	Training Loss: 1.456041 	Validation Loss: 1.570233 	 time: 0.3
Validation loss decreased from 1.571629 to 1.570233. Model was saved
Epoch: 45 	Training Loss: 1.452231 	Validation Loss: 1.571057 	 time: 0.3
Epoch: 46 	Training Loss: 1.448894 	Validation Loss: 1.569851 	 time: 0.3
Validation loss decreased from 1.570233 to 1.569851. Model was saved
Epoch: 47 	Training Loss: 1.445635 	Validation Loss: 1.567017 	 time: 0.3
Validation loss decreased from 1.569851 to 1.567017. Model was saved
Epoch: 48 	Training Loss: 1.442919 	Validation Loss: 1.573205 	 time: 0.3
Epoch: 49 	Training Loss: 1.440358 	Validation Loss: 1.567857 	 time: 0.3
Epoch: 50 	Training Loss: 1.437500 	Validation Loss: 1.570124 	 time: 0.3
Epoch: 51 	Training Loss: 1.434551 	Validation Loss: 1.567681 	 time: 0.3
Epoch: 52 	Training Loss: 1.431177 	Validation Loss: 1.566923 	 time: 0.3
Validation loss decreased from 1.567017 to 1.566923. Model was saved
Epoch: 53 	Training Loss: 1.428586 	Validation Loss: 1.566187 	 time: 0.3
Validation loss decreased from 1.566923 to 1.566187. Model was saved
Epoch: 54 	Training Loss: 1.426853 	Validation Loss: 1.565190 	 time: 0.3
Validation loss decreased from 1.566187 to 1.565190. Model was saved
Epoch: 55 	Training Loss: 1.425133 	Validation Loss: 1.566019 	 time: 0.3
Epoch: 56 	Training Loss: 1.423128 	Validation Loss: 1.560683 	 time: 0.3
Validation loss decreased from 1.565190 to 1.560683. Model was saved
Epoch: 57 	Training Loss: 1.419791 	Validation Loss: 1.560603 	 time: 0.3
Validation loss decreased from 1.560683 to 1.560603. Model was saved
Epoch: 58 	Training Loss: 1.417188 	Validation Loss: 1.562787 	 time: 0.3
Epoch: 59 	Training Loss: 1.415810 	Validation Loss: 1.559800 	 time: 0.3
Validation loss decreased from 1.560603 to 1.559800. Model was saved
Epoch: 60 	Training Loss: 1.413776 	Validation Loss: 1.558909 	 time: 0.3
Validation loss decreased from 1.559800 to 1.558909. Model was saved
Epoch: 61 	Training Loss: 1.411403 	Validation Loss: 1.561274 	 time: 0.3
Epoch: 62 	Training Loss: 1.409386 	Validation Loss: 1.557564 	 time: 0.3
Validation loss decreased from 1.558909 to 1.557564. Model was saved
Epoch: 63 	Training Loss: 1.407493 	Validation Loss: 1.557992 	 time: 0.3
Epoch: 64 	Training Loss: 1.405792 	Validation Loss: 1.560091 	 time: 0.3
Epoch: 65 	Training Loss: 1.403969 	Validation Loss: 1.555915 	 time: 0.3
Validation loss decreased from 1.557564 to 1.555915. Model was saved
Epoch: 66 	Training Loss: 1.401739 	Validation Loss: 1.557403 	 time: 0.3
Epoch: 67 	Training Loss: 1.399667 	Validation Loss: 1.557265 	 time: 0.3
Epoch: 68 	Training Loss: 1.398026 	Validation Loss: 1.553614 	 time: 0.3
Validation loss decreased from 1.555915 to 1.553614. Model was saved
Epoch: 69 	Training Loss: 1.396621 	Validation Loss: 1.555204 	 time: 0.3
Epoch: 70 	Training Loss: 1.395306 	Validation Loss: 1.551829 	 time: 0.3
Validation loss decreased from 1.553614 to 1.551829. Model was saved
Epoch: 71 	Training Loss: 1.393747 	Validation Loss: 1.547151 	 time: 0.3
Validation loss decreased from 1.551829 to 1.547151. Model was saved
Epoch: 72 	Training Loss: 1.392130 	Validation Loss: 1.550389 	 time: 0.3
Epoch: 73 	Training Loss: 1.390237 	Validation Loss: 1.539104 	 time: 0.3
Validation loss decreased from 1.547151 to 1.539104. Model was saved
Epoch: 74 	Training Loss: 1.388004 	Validation Loss: 1.543290 	 time: 0.3
Epoch: 75 	Training Loss: 1.386045 	Validation Loss: 1.539804 	 time: 0.3
Epoch: 76 	Training Loss: 1.384812 	Validation Loss: 1.535577 	 time: 0.3
Validation loss decreased from 1.539104 to 1.535577. Model was saved
Epoch: 77 	Training Loss: 1.383753 	Validation Loss: 1.537966 	 time: 0.3
Epoch: 78 	Training Loss: 1.382169 	Validation Loss: 1.534457 	 time: 0.3
Validation loss decreased from 1.535577 to 1.534457. Model was saved
Epoch: 79 	Training Loss: 1.380126 	Validation Loss: 1.528578 	 time: 0.3
Validation loss decreased from 1.534457 to 1.528578. Model was saved
Epoch: 80 	Training Loss: 1.378492 	Validation Loss: 1.534455 	 time: 0.3
Epoch: 81 	Training Loss: 1.377208 	Validation Loss: 1.523868 	 time: 0.3
Validation loss decreased from 1.528578 to 1.523868. Model was saved
Epoch: 82 	Training Loss: 1.375641 	Validation Loss: 1.526604 	 time: 0.3
Epoch: 83 	Training Loss: 1.373900 	Validation Loss: 1.525370 	 time: 0.3
Epoch: 84 	Training Loss: 1.372426 	Validation Loss: 1.516702 	 time: 0.3
Validation loss decreased from 1.523868 to 1.516702. Model was saved
Epoch: 85 	Training Loss: 1.371461 	Validation Loss: 1.528006 	 time: 0.3
Epoch: 86 	Training Loss: 1.371208 	Validation Loss: 1.513682 	 time: 0.3
Validation loss decreased from 1.516702 to 1.513682. Model was saved
Epoch: 87 	Training Loss: 1.371368 	Validation Loss: 1.527130 	 time: 0.3
Epoch: 88 	Training Loss: 1.372265 	Validation Loss: 1.522999 	 time: 0.3
Epoch: 89 	Training Loss: 1.370170 	Validation Loss: 1.510402 	 time: 0.3
Validation loss decreased from 1.513682 to 1.510402. Model was saved
Epoch: 90 	Training Loss: 1.365080 	Validation Loss: 1.520281 	 time: 0.3
Epoch: 91 	Training Loss: 1.364320 	Validation Loss: 1.515854 	 time: 0.3
Epoch: 92 	Training Loss: 1.363814 	Validation Loss: 1.509509 	 time: 0.3
Validation loss decreased from 1.510402 to 1.509509. Model was saved
Epoch: 93 	Training Loss: 1.360073 	Validation Loss: 1.515277 	 time: 0.3
Epoch: 94 	Training Loss: 1.360136 	Validation Loss: 1.511678 	 time: 0.3
Epoch: 95 	Training Loss: 1.358581 	Validation Loss: 1.509878 	 time: 0.3
Epoch: 96 	Training Loss: 1.355673 	Validation Loss: 1.510440 	 time: 0.3
Epoch: 97 	Training Loss: 1.355914 	Validation Loss: 1.507880 	 time: 0.3
Validation loss decreased from 1.509509 to 1.507880. Model was saved
Epoch: 98 	Training Loss: 1.354151 	Validation Loss: 1.508876 	 time: 0.3
Epoch: 99 	Training Loss: 1.351938 	Validation Loss: 1.504266 	 time: 0.3
Validation loss decreased from 1.507880 to 1.504266. Model was saved
Epoch: 100 	Training Loss: 1.351460 	Validation Loss: 1.504529 	 time: 0.3
Epoch: 101 	Training Loss: 1.349964 	Validation Loss: 1.505386 	 time: 0.3
Epoch: 102 	Training Loss: 1.348422 	Validation Loss: 1.497755 	 time: 0.3
Validation loss decreased from 1.504266 to 1.497755. Model was saved
Epoch: 103 	Training Loss: 1.347585 	Validation Loss: 1.501585 	 time: 0.3
Epoch: 104 	Training Loss: 1.346005 	Validation Loss: 1.498462 	 time: 0.3
Epoch: 105 	Training Loss: 1.344260 	Validation Loss: 1.493964 	 time: 0.3
Validation loss decreased from 1.497755 to 1.493964. Model was saved
Epoch: 106 	Training Loss: 1.343487 	Validation Loss: 1.499664 	 time: 0.3
Epoch: 107 	Training Loss: 1.342534 	Validation Loss: 1.494267 	 time: 0.3
Epoch: 108 	Training Loss: 1.340885 	Validation Loss: 1.496097 	 time: 0.3
Epoch: 109 	Training Loss: 1.339840 	Validation Loss: 1.497934 	 time: 0.3
Epoch: 110 	Training Loss: 1.339496 	Validation Loss: 1.495094 	 time: 0.3
Epoch: 111 	Training Loss: 1.338387 	Validation Loss: 1.494368 	 time: 0.3
Epoch: 112 	Training Loss: 1.336827 	Validation Loss: 1.498418 	 time: 0.3
Epoch: 113 	Training Loss: 1.335652 	Validation Loss: 1.491631 	 time: 0.3
Validation loss decreased from 1.493964 to 1.491631. Model was saved
Epoch: 114 	Training Loss: 1.334511 	Validation Loss: 1.496305 	 time: 0.3
Epoch: 115 	Training Loss: 1.333325 	Validation Loss: 1.493929 	 time: 0.3
Epoch: 116 	Training Loss: 1.332078 	Validation Loss: 1.490548 	 time: 0.3
Validation loss decreased from 1.491631 to 1.490548. Model was saved
Epoch: 117 	Training Loss: 1.331089 	Validation Loss: 1.495152 	 time: 0.3
Epoch: 118 	Training Loss: 1.330525 	Validation Loss: 1.486490 	 time: 0.3
Validation loss decreased from 1.490548 to 1.486490. Model was saved
Epoch: 119 	Training Loss: 1.329758 	Validation Loss: 1.492813 	 time: 0.3
Epoch: 120 	Training Loss: 1.328477 	Validation Loss: 1.485574 	 time: 0.3
Validation loss decreased from 1.486490 to 1.485574. Model was saved
Epoch: 121 	Training Loss: 1.326962 	Validation Loss: 1.485359 	 time: 0.3
Validation loss decreased from 1.485574 to 1.485359. Model was saved
Epoch: 122 	Training Loss: 1.325963 	Validation Loss: 1.486405 	 time: 0.3
Epoch: 123 	Training Loss: 1.325211 	Validation Loss: 1.478954 	 time: 0.3
Validation loss decreased from 1.485359 to 1.478954. Model was saved
Epoch: 124 	Training Loss: 1.324265 	Validation Loss: 1.483976 	 time: 0.3
Epoch: 125 	Training Loss: 1.323103 	Validation Loss: 1.476610 	 time: 0.3
Validation loss decreased from 1.478954 to 1.476610. Model was saved
Epoch: 126 	Training Loss: 1.321753 	Validation Loss: 1.476176 	 time: 0.3
Validation loss decreased from 1.476610 to 1.476176. Model was saved
Epoch: 127 	Training Loss: 1.320570 	Validation Loss: 1.476068 	 time: 0.3
Validation loss decreased from 1.476176 to 1.476068. Model was saved
Epoch: 128 	Training Loss: 1.319529 	Validation Loss: 1.467170 	 time: 0.3
Validation loss decreased from 1.476068 to 1.467170. Model was saved
Epoch: 129 	Training Loss: 1.318701 	Validation Loss: 1.472369 	 time: 0.3
Epoch: 130 	Training Loss: 1.317987 	Validation Loss: 1.461058 	 time: 0.3
Validation loss decreased from 1.467170 to 1.461058. Model was saved
Epoch: 131 	Training Loss: 1.316831 	Validation Loss: 1.468293 	 time: 0.3
Epoch: 132 	Training Loss: 1.315849 	Validation Loss: 1.457379 	 time: 0.3
Validation loss decreased from 1.461058 to 1.457379. Model was saved
Epoch: 133 	Training Loss: 1.315252 	Validation Loss: 1.468477 	 time: 0.3
Epoch: 134 	Training Loss: 1.315856 	Validation Loss: 1.458286 	 time: 0.3
Epoch: 135 	Training Loss: 1.322774 	Validation Loss: 1.480567 	 time: 0.3
Epoch: 136 	Training Loss: 1.327418 	Validation Loss: 1.463740 	 time: 0.3
Epoch: 137 	Training Loss: 1.321285 	Validation Loss: 1.456445 	 time: 0.3
Validation loss decreased from 1.457379 to 1.456445. Model was saved
Epoch: 138 	Training Loss: 1.314848 	Validation Loss: 1.471434 	 time: 0.3
Epoch: 139 	Training Loss: 1.319041 	Validation Loss: 1.468453 	 time: 0.3
Epoch: 140 	Training Loss: 1.311981 	Validation Loss: 1.451196 	 time: 0.3
Validation loss decreased from 1.456445 to 1.451196. Model was saved
Epoch: 141 	Training Loss: 1.316404 	Validation Loss: 1.460829 	 time: 0.3
Epoch: 142 	Training Loss: 1.308969 	Validation Loss: 1.469072 	 time: 0.3
Epoch: 143 	Training Loss: 1.312481 	Validation Loss: 1.451791 	 time: 0.3
Epoch: 144 	Training Loss: 1.307690 	Validation Loss: 1.452656 	 time: 0.3
Epoch: 145 	Training Loss: 1.308929 	Validation Loss: 1.458933 	 time: 0.3
Epoch: 146 	Training Loss: 1.305023 	Validation Loss: 1.457648 	 time: 0.3
Epoch: 147 	Training Loss: 1.308072 	Validation Loss: 1.449606 	 time: 0.3
Validation loss decreased from 1.451196 to 1.449606. Model was saved
Epoch: 148 	Training Loss: 1.303493 	Validation Loss: 1.452685 	 time: 0.3
Epoch: 149 	Training Loss: 1.305652 	Validation Loss: 1.456189 	 time: 0.3
Epoch: 150 	Training Loss: 1.300757 	Validation Loss: 1.458549 	 time: 0.3
Epoch: 151 	Training Loss: 1.305355 	Validation Loss: 1.453841 	 time: 0.3
Epoch: 152 	Training Loss: 1.299373 	Validation Loss: 1.451288 	 time: 0.3
Epoch: 153 	Training Loss: 1.303097 	Validation Loss: 1.452457 	 time: 0.3
Epoch: 154 	Training Loss: 1.296564 	Validation Loss: 1.457685 	 time: 0.3
Epoch: 155 	Training Loss: 1.301832 	Validation Loss: 1.453019 	 time: 0.3
Epoch: 156 	Training Loss: 1.295813 	Validation Loss: 1.445725 	 time: 0.3
Validation loss decreased from 1.449606 to 1.445725. Model was saved
Epoch: 157 	Training Loss: 1.299292 	Validation Loss: 1.448942 	 time: 0.3
Epoch: 158 	Training Loss: 1.293980 	Validation Loss: 1.454648 	 time: 0.3
Epoch: 159 	Training Loss: 1.295426 	Validation Loss: 1.449592 	 time: 0.3
Epoch: 160 	Training Loss: 1.292725 	Validation Loss: 1.448640 	 time: 0.3
Epoch: 161 	Training Loss: 1.292558 	Validation Loss: 1.447060 	 time: 0.3
Epoch: 162 	Training Loss: 1.292225 	Validation Loss: 1.448978 	 time: 0.3
Epoch: 163 	Training Loss: 1.289550 	Validation Loss: 1.452374 	 time: 0.3
Epoch: 164 	Training Loss: 1.291020 	Validation Loss: 1.447155 	 time: 0.3
Epoch: 165 	Training Loss: 1.287804 	Validation Loss: 1.443762 	 time: 0.3
Validation loss decreased from 1.445725 to 1.443762. Model was saved
Epoch: 166 	Training Loss: 1.288974 	Validation Loss: 1.448096 	 time: 0.3
Epoch: 167 	Training Loss: 1.286373 	Validation Loss: 1.447495 	 time: 0.3
Epoch: 168 	Training Loss: 1.287256 	Validation Loss: 1.446291 	 time: 0.3
Epoch: 169 	Training Loss: 1.284779 	Validation Loss: 1.441299 	 time: 0.3
Validation loss decreased from 1.443762 to 1.441299. Model was saved
Epoch: 170 	Training Loss: 1.285040 	Validation Loss: 1.441110 	 time: 0.3
Validation loss decreased from 1.441299 to 1.441110. Model was saved
Epoch: 171 	Training Loss: 1.283037 	Validation Loss: 1.444267 	 time: 0.3
Epoch: 172 	Training Loss: 1.283206 	Validation Loss: 1.440742 	 time: 0.3
Validation loss decreased from 1.441110 to 1.440742. Model was saved
Epoch: 173 	Training Loss: 1.281651 	Validation Loss: 1.439073 	 time: 0.3
Validation loss decreased from 1.440742 to 1.439073. Model was saved
Epoch: 174 	Training Loss: 1.281251 	Validation Loss: 1.439003 	 time: 0.3
Validation loss decreased from 1.439073 to 1.439003. Model was saved
Epoch: 175 	Training Loss: 1.279808 	Validation Loss: 1.439603 	 time: 0.3
Epoch: 176 	Training Loss: 1.279393 	Validation Loss: 1.439940 	 time: 0.3
Epoch: 177 	Training Loss: 1.278324 	Validation Loss: 1.436795 	 time: 0.3
Validation loss decreased from 1.439003 to 1.436795. Model was saved
Epoch: 178 	Training Loss: 1.277784 	Validation Loss: 1.437384 	 time: 0.3
Epoch: 179 	Training Loss: 1.276604 	Validation Loss: 1.437255 	 time: 0.3
Epoch: 180 	Training Loss: 1.276200 	Validation Loss: 1.436373 	 time: 0.3
Validation loss decreased from 1.436795 to 1.436373. Model was saved
Epoch: 181 	Training Loss: 1.275450 	Validation Loss: 1.436379 	 time: 0.3
Epoch: 182 	Training Loss: 1.274889 	Validation Loss: 1.435377 	 time: 0.3
Validation loss decreased from 1.436373 to 1.435377. Model was saved
Epoch: 183 	Training Loss: 1.273715 	Validation Loss: 1.434663 	 time: 0.3
Validation loss decreased from 1.435377 to 1.434663. Model was saved
Epoch: 184 	Training Loss: 1.273347 	Validation Loss: 1.435915 	 time: 0.3
Epoch: 185 	Training Loss: 1.272756 	Validation Loss: 1.436174 	 time: 0.3
Epoch: 186 	Training Loss: 1.272395 	Validation Loss: 1.435375 	 time: 0.3
Epoch: 187 	Training Loss: 1.270910 	Validation Loss: 1.435906 	 time: 0.3
Epoch: 188 	Training Loss: 1.270476 	Validation Loss: 1.434107 	 time: 0.3
Validation loss decreased from 1.434663 to 1.434107. Model was saved
Epoch: 189 	Training Loss: 1.269832 	Validation Loss: 1.434869 	 time: 0.3
Epoch: 190 	Training Loss: 1.269375 	Validation Loss: 1.434181 	 time: 0.3
Epoch: 191 	Training Loss: 1.268269 	Validation Loss: 1.432127 	 time: 0.3
Validation loss decreased from 1.434107 to 1.432127. Model was saved
Epoch: 192 	Training Loss: 1.267792 	Validation Loss: 1.435439 	 time: 0.3
Epoch: 193 	Training Loss: 1.267463 	Validation Loss: 1.431446 	 time: 0.3
Validation loss decreased from 1.432127 to 1.431446. Model was saved
Epoch: 194 	Training Loss: 1.266689 	Validation Loss: 1.432339 	 time: 0.3
Epoch: 195 	Training Loss: 1.265985 	Validation Loss: 1.431921 	 time: 0.3
Epoch: 196 	Training Loss: 1.265302 	Validation Loss: 1.430225 	 time: 0.3
Validation loss decreased from 1.431446 to 1.430225. Model was saved
Epoch: 197 	Training Loss: 1.265051 	Validation Loss: 1.431487 	 time: 0.3
Epoch: 198 	Training Loss: 1.264464 	Validation Loss: 1.427851 	 time: 0.3
Validation loss decreased from 1.430225 to 1.427851. Model was saved
Epoch: 199 	Training Loss: 1.263761 	Validation Loss: 1.428910 	 time: 0.3
Epoch: 200 	Training Loss: 1.262992 	Validation Loss: 1.428972 	 time: 0.3
Epoch: 201 	Training Loss: 1.262348 	Validation Loss: 1.427536 	 time: 0.3
Validation loss decreased from 1.427851 to 1.427536. Model was saved
Epoch: 202 	Training Loss: 1.261897 	Validation Loss: 1.429793 	 time: 0.3
Epoch: 203 	Training Loss: 1.261295 	Validation Loss: 1.427467 	 time: 0.3
Validation loss decreased from 1.427536 to 1.427467. Model was saved
Epoch: 204 	Training Loss: 1.260673 	Validation Loss: 1.428162 	 time: 0.3
Epoch: 205 	Training Loss: 1.259930 	Validation Loss: 1.426958 	 time: 0.3
Validation loss decreased from 1.427467 to 1.426958. Model was saved
Epoch: 206 	Training Loss: 1.259273 	Validation Loss: 1.427191 	 time: 0.3
Epoch: 207 	Training Loss: 1.258791 	Validation Loss: 1.427292 	 time: 0.3
Epoch: 208 	Training Loss: 1.258318 	Validation Loss: 1.426397 	 time: 0.3
Validation loss decreased from 1.426958 to 1.426397. Model was saved
Epoch: 209 	Training Loss: 1.258065 	Validation Loss: 1.427200 	 time: 0.3
Epoch: 210 	Training Loss: 1.258121 	Validation Loss: 1.426588 	 time: 0.3
Epoch: 211 	Training Loss: 1.258411 	Validation Loss: 1.427357 	 time: 0.3
Epoch: 212 	Training Loss: 1.258614 	Validation Loss: 1.429694 	 time: 0.3
Epoch: 213 	Training Loss: 1.259707 	Validation Loss: 1.428325 	 time: 0.3
Epoch: 214 	Training Loss: 1.257613 	Validation Loss: 1.429887 	 time: 0.3
Epoch: 215 	Training Loss: 1.255530 	Validation Loss: 1.426582 	 time: 0.3
Epoch: 216 	Training Loss: 1.255752 	Validation Loss: 1.428723 	 time: 0.3
Epoch: 217 	Training Loss: 1.257782 	Validation Loss: 1.427999 	 time: 0.3
Epoch: 218 	Training Loss: 1.257673 	Validation Loss: 1.428616 	 time: 0.3
Epoch: 219 	Training Loss: 1.254077 	Validation Loss: 1.430486 	 time: 0.3
Epoch: 220 	Training Loss: 1.254339 	Validation Loss: 1.428000 	 time: 0.3
Epoch: 221 	Training Loss: 1.256150 	Validation Loss: 1.430138 	 time: 0.3
Epoch: 222 	Training Loss: 1.253610 	Validation Loss: 1.432329 	 time: 0.3
Epoch: 223 	Training Loss: 1.252633 	Validation Loss: 1.428137 	 time: 0.3
Epoch: 224 	Training Loss: 1.255675 	Validation Loss: 1.433502 	 time: 0.3
Epoch: 225 	Training Loss: 1.258267 	Validation Loss: 1.428946 	 time: 0.3
Epoch: 226 	Training Loss: 1.251479 	Validation Loss: 1.427730 	 time: 0.3
Epoch: 227 	Training Loss: 1.259489 	Validation Loss: 1.448490 	 time: 0.3
Epoch: 228 	Training Loss: 1.269477 	Validation Loss: 1.437240 	 time: 0.3
Epoch: 229 	Training Loss: 1.267515 	Validation Loss: 1.433065 	 time: 0.3
Epoch: 230 	Training Loss: 1.258309 	Validation Loss: 1.434155 	 time: 0.3
Epoch: 231 	Training Loss: 1.265040 	Validation Loss: 1.428817 	 time: 0.3
Epoch: 232 	Training Loss: 1.254625 	Validation Loss: 1.432975 	 time: 0.3
Epoch: 233 	Training Loss: 1.263067 	Validation Loss: 1.431439 	 time: 0.3
Epoch: 234 	Training Loss: 1.254410 	Validation Loss: 1.435929 	 time: 0.3
Epoch: 235 	Training Loss: 1.253747 	Validation Loss: 1.440866 	 time: 0.3
Epoch: 236 	Training Loss: 1.257726 	Validation Loss: 1.427934 	 time: 0.3
Epoch: 237 	Training Loss: 1.250366 	Validation Loss: 1.430030 	 time: 0.3
Epoch: 238 	Training Loss: 1.255294 	Validation Loss: 1.424494 	 time: 0.3
Validation loss decreased from 1.426397 to 1.424494. Model was saved
Epoch: 239 	Training Loss: 1.252369 	Validation Loss: 1.424083 	 time: 0.3
Validation loss decreased from 1.424494 to 1.424083. Model was saved
Epoch: 240 	Training Loss: 1.248736 	Validation Loss: 1.422559 	 time: 0.3
Validation loss decreased from 1.424083 to 1.422559. Model was saved
Epoch: 241 	Training Loss: 1.254825 	Validation Loss: 1.430308 	 time: 0.3
Epoch: 242 	Training Loss: 1.250707 	Validation Loss: 1.424994 	 time: 0.3
Epoch: 243 	Training Loss: 1.252356 	Validation Loss: 1.425294 	 time: 0.3
Epoch: 244 	Training Loss: 1.251287 	Validation Loss: 1.426025 	 time: 0.3
Epoch: 245 	Training Loss: 1.246777 	Validation Loss: 1.440413 	 time: 0.3
Epoch: 246 	Training Loss: 1.252175 	Validation Loss: 1.419041 	 time: 0.3
Validation loss decreased from 1.422559 to 1.419041. Model was saved
Epoch: 247 	Training Loss: 1.247223 	Validation Loss: 1.422450 	 time: 0.3
Epoch: 248 	Training Loss: 1.247726 	Validation Loss: 1.423739 	 time: 0.3
Epoch: 249 	Training Loss: 1.248456 	Validation Loss: 1.423657 	 time: 0.3
Epoch: 250 	Training Loss: 1.245734 	Validation Loss: 1.421916 	 time: 0.3
Epoch: 251 	Training Loss: 1.245010 	Validation Loss: 1.427709 	 time: 0.3
Epoch: 252 	Training Loss: 1.245521 	Validation Loss: 1.425468 	 time: 0.3
Epoch: 253 	Training Loss: 1.243421 	Validation Loss: 1.423678 	 time: 0.3
Epoch: 254 	Training Loss: 1.244357 	Validation Loss: 1.420918 	 time: 0.3
Epoch: 255 	Training Loss: 1.243008 	Validation Loss: 1.421702 	 time: 0.3
Epoch: 256 	Training Loss: 1.241816 	Validation Loss: 1.420837 	 time: 0.3
Epoch: 257 	Training Loss: 1.243506 	Validation Loss: 1.418487 	 time: 0.3
Validation loss decreased from 1.419041 to 1.418487. Model was saved
Epoch: 258 	Training Loss: 1.240890 	Validation Loss: 1.418052 	 time: 0.3
Validation loss decreased from 1.418487 to 1.418052. Model was saved
Epoch: 259 	Training Loss: 1.241176 	Validation Loss: 1.418013 	 time: 0.3
Validation loss decreased from 1.418052 to 1.418013. Model was saved
Epoch: 260 	Training Loss: 1.241404 	Validation Loss: 1.418666 	 time: 0.3
Epoch: 261 	Training Loss: 1.239806 	Validation Loss: 1.419445 	 time: 0.3
Epoch: 262 	Training Loss: 1.240394 	Validation Loss: 1.419876 	 time: 0.3
Epoch: 263 	Training Loss: 1.239575 	Validation Loss: 1.417949 	 time: 0.3
Validation loss decreased from 1.418013 to 1.417949. Model was saved
Epoch: 264 	Training Loss: 1.239149 	Validation Loss: 1.416734 	 time: 0.3
Validation loss decreased from 1.417949 to 1.416734. Model was saved
Epoch: 265 	Training Loss: 1.239258 	Validation Loss: 1.416962 	 time: 0.3
Epoch: 266 	Training Loss: 1.238300 	Validation Loss: 1.418931 	 time: 0.3
Epoch: 267 	Training Loss: 1.238403 	Validation Loss: 1.417518 	 time: 0.3
Epoch: 268 	Training Loss: 1.238080 	Validation Loss: 1.414923 	 time: 0.3
Validation loss decreased from 1.416734 to 1.414923. Model was saved
Epoch: 269 	Training Loss: 1.237459 	Validation Loss: 1.415004 	 time: 0.3
Epoch: 270 	Training Loss: 1.237552 	Validation Loss: 1.417158 	 time: 0.3
Epoch: 271 	Training Loss: 1.236989 	Validation Loss: 1.415006 	 time: 0.3
Epoch: 272 	Training Loss: 1.236855 	Validation Loss: 1.417600 	 time: 0.3
Epoch: 273 	Training Loss: 1.236533 	Validation Loss: 1.418042 	 time: 0.3
Epoch: 274 	Training Loss: 1.236289 	Validation Loss: 1.415122 	 time: 0.3
Epoch: 275 	Training Loss: 1.236070 	Validation Loss: 1.416315 	 time: 0.3
Epoch: 276 	Training Loss: 1.235622 	Validation Loss: 1.418616 	 time: 0.3
Epoch: 277 	Training Loss: 1.235621 	Validation Loss: 1.415152 	 time: 0.3
Epoch: 278 	Training Loss: 1.235150 	Validation Loss: 1.414991 	 time: 0.3
Epoch: 279 	Training Loss: 1.234922 	Validation Loss: 1.416616 	 time: 0.3
Epoch: 280 	Training Loss: 1.234789 	Validation Loss: 1.415486 	 time: 0.3
Epoch: 281 	Training Loss: 1.234426 	Validation Loss: 1.416992 	 time: 0.3
Epoch: 282 	Training Loss: 1.234228 	Validation Loss: 1.418323 	 time: 0.3
Epoch: 283 	Training Loss: 1.233931 	Validation Loss: 1.416830 	 time: 0.3
Epoch: 284 	Training Loss: 1.233750 	Validation Loss: 1.418105 	 time: 0.3
Epoch: 285 	Training Loss: 1.233497 	Validation Loss: 1.420132 	 time: 0.3
Epoch: 286 	Training Loss: 1.233289 	Validation Loss: 1.418768 	 time: 0.3
Epoch: 287 	Training Loss: 1.233024 	Validation Loss: 1.417858 	 time: 0.3
Epoch: 288 	Training Loss: 1.232772 	Validation Loss: 1.417777 	 time: 0.3
Epoch: 289 	Training Loss: 1.232610 	Validation Loss: 1.417583 	 time: 0.3
Epoch: 290 	Training Loss: 1.232329 	Validation Loss: 1.417935 	 time: 0.3
Epoch: 291 	Training Loss: 1.232134 	Validation Loss: 1.418545 	 time: 0.3
Epoch: 292 	Training Loss: 1.231910 	Validation Loss: 1.416977 	 time: 0.3
Epoch: 293 	Training Loss: 1.231711 	Validation Loss: 1.416485 	 time: 0.3
Epoch: 294 	Training Loss: 1.231491 	Validation Loss: 1.417616 	 time: 0.3
Epoch: 295 	Training Loss: 1.231302 	Validation Loss: 1.416289 	 time: 0.3
Epoch: 296 	Training Loss: 1.231097 	Validation Loss: 1.416923 	 time: 0.3
Epoch: 297 	Training Loss: 1.230876 	Validation Loss: 1.417073 	 time: 0.3
Epoch: 298 	Training Loss: 1.230689 	Validation Loss: 1.416229 	 time: 0.3
Epoch: 299 	Training Loss: 1.230494 	Validation Loss: 1.417253 	 time: 0.3
Epoch: 300 	Training Loss: 1.230329 	Validation Loss: 1.415757 	 time: 0.3
Epoch: 301 	Training Loss: 1.230124 	Validation Loss: 1.415827 	 time: 0.3
Epoch: 302 	Training Loss: 1.229907 	Validation Loss: 1.416617 	 time: 0.3
Epoch: 303 	Training Loss: 1.229642 	Validation Loss: 1.416776 	 time: 0.3
Epoch: 304 	Training Loss: 1.229380 	Validation Loss: 1.416395 	 time: 0.3
Epoch: 305 	Training Loss: 1.229165 	Validation Loss: 1.415903 	 time: 0.3
Epoch: 306 	Training Loss: 1.229023 	Validation Loss: 1.415536 	 time: 0.3
Epoch: 307 	Training Loss: 1.228869 	Validation Loss: 1.415470 	 time: 0.3
Epoch: 308 	Training Loss: 1.228706 	Validation Loss: 1.415235 	 time: 0.3
Epoch: 309 	Training Loss: 1.228547 	Validation Loss: 1.413634 	 time: 0.3
Validation loss decreased from 1.414923 to 1.413634. Model was saved
Epoch: 310 	Training Loss: 1.228389 	Validation Loss: 1.413673 	 time: 0.3
Epoch: 311 	Training Loss: 1.228225 	Validation Loss: 1.411829 	 time: 0.3
Validation loss decreased from 1.413634 to 1.411829. Model was saved
Epoch: 312 	Training Loss: 1.228054 	Validation Loss: 1.412732 	 time: 0.3
Epoch: 313 	Training Loss: 1.227881 	Validation Loss: 1.410382 	 time: 0.3
Validation loss decreased from 1.411829 to 1.410382. Model was saved
Epoch: 314 	Training Loss: 1.227728 	Validation Loss: 1.412056 	 time: 0.3
Epoch: 315 	Training Loss: 1.227638 	Validation Loss: 1.408323 	 time: 0.3
Validation loss decreased from 1.410382 to 1.408323. Model was saved
Epoch: 316 	Training Loss: 1.227613 	Validation Loss: 1.413089 	 time: 0.3
Epoch: 317 	Training Loss: 1.227854 	Validation Loss: 1.405826 	 time: 0.3
Validation loss decreased from 1.408323 to 1.405826. Model was saved
Epoch: 318 	Training Loss: 1.228218 	Validation Loss: 1.415809 	 time: 0.3
Epoch: 319 	Training Loss: 1.229422 	Validation Loss: 1.405887 	 time: 0.3
Epoch: 320 	Training Loss: 1.229015 	Validation Loss: 1.417526 	 time: 0.3
Epoch: 321 	Training Loss: 1.228958 	Validation Loss: 1.404561 	 time: 0.3
Validation loss decreased from 1.405826 to 1.404561. Model was saved
Epoch: 322 	Training Loss: 1.226614 	Validation Loss: 1.406126 	 time: 0.3
Epoch: 323 	Training Loss: 1.226781 	Validation Loss: 1.416661 	 time: 0.3
Epoch: 324 	Training Loss: 1.228683 	Validation Loss: 1.403997 	 time: 0.3
Validation loss decreased from 1.404561 to 1.403997. Model was saved
Epoch: 325 	Training Loss: 1.227579 	Validation Loss: 1.411696 	 time: 0.3
Epoch: 326 	Training Loss: 1.226263 	Validation Loss: 1.408262 	 time: 0.3
Epoch: 327 	Training Loss: 1.225679 	Validation Loss: 1.403740 	 time: 0.3
Validation loss decreased from 1.403997 to 1.403740. Model was saved
Epoch: 328 	Training Loss: 1.226699 	Validation Loss: 1.413986 	 time: 0.3
Epoch: 329 	Training Loss: 1.226825 	Validation Loss: 1.406574 	 time: 0.3
Epoch: 330 	Training Loss: 1.225132 	Validation Loss: 1.404414 	 time: 0.3
Epoch: 331 	Training Loss: 1.225343 	Validation Loss: 1.413394 	 time: 0.3
Epoch: 332 	Training Loss: 1.226305 	Validation Loss: 1.403852 	 time: 0.3
Epoch: 333 	Training Loss: 1.224983 	Validation Loss: 1.403497 	 time: 0.3
Validation loss decreased from 1.403740 to 1.403497. Model was saved
Epoch: 334 	Training Loss: 1.224489 	Validation Loss: 1.411109 	 time: 0.3
Epoch: 335 	Training Loss: 1.225130 	Validation Loss: 1.403502 	 time: 0.3
Epoch: 336 	Training Loss: 1.224534 	Validation Loss: 1.404232 	 time: 0.3
Epoch: 337 	Training Loss: 1.223859 	Validation Loss: 1.406577 	 time: 0.3
Epoch: 338 	Training Loss: 1.223914 	Validation Loss: 1.402074 	 time: 0.3
Validation loss decreased from 1.403497 to 1.402074. Model was saved
Epoch: 339 	Training Loss: 1.223878 	Validation Loss: 1.405268 	 time: 0.3
Epoch: 340 	Training Loss: 1.223538 	Validation Loss: 1.405438 	 time: 0.3
Epoch: 341 	Training Loss: 1.223178 	Validation Loss: 1.401608 	 time: 0.3
Validation loss decreased from 1.402074 to 1.401608. Model was saved
Epoch: 342 	Training Loss: 1.223134 	Validation Loss: 1.406313 	 time: 0.3
Epoch: 343 	Training Loss: 1.223115 	Validation Loss: 1.401289 	 time: 0.3
Validation loss decreased from 1.401608 to 1.401289. Model was saved
Epoch: 344 	Training Loss: 1.222857 	Validation Loss: 1.401260 	 time: 0.3
Validation loss decreased from 1.401289 to 1.401260. Model was saved
Epoch: 345 	Training Loss: 1.222600 	Validation Loss: 1.404280 	 time: 0.3
Epoch: 346 	Training Loss: 1.222442 	Validation Loss: 1.399462 	 time: 0.3
Validation loss decreased from 1.401260 to 1.399462. Model was saved
Epoch: 347 	Training Loss: 1.222312 	Validation Loss: 1.402763 	 time: 0.3
Epoch: 348 	Training Loss: 1.222183 	Validation Loss: 1.399674 	 time: 0.3
Epoch: 349 	Training Loss: 1.222084 	Validation Loss: 1.398655 	 time: 0.3
Validation loss decreased from 1.399462 to 1.398655. Model was saved
Epoch: 350 	Training Loss: 1.222013 	Validation Loss: 1.400157 	 time: 0.3
Epoch: 351 	Training Loss: 1.221808 	Validation Loss: 1.397024 	 time: 0.3
Validation loss decreased from 1.398655 to 1.397024. Model was saved
Epoch: 352 	Training Loss: 1.221534 	Validation Loss: 1.398604 	 time: 0.3
Epoch: 353 	Training Loss: 1.221268 	Validation Loss: 1.397621 	 time: 0.3
Epoch: 354 	Training Loss: 1.221119 	Validation Loss: 1.396551 	 time: 0.3
Validation loss decreased from 1.397024 to 1.396551. Model was saved
Epoch: 355 	Training Loss: 1.221088 	Validation Loss: 1.398903 	 time: 0.3
Epoch: 356 	Training Loss: 1.221029 	Validation Loss: 1.397331 	 time: 0.3
Epoch: 357 	Training Loss: 1.220873 	Validation Loss: 1.398933 	 time: 0.3
Epoch: 358 	Training Loss: 1.220634 	Validation Loss: 1.399711 	 time: 0.3
Epoch: 359 	Training Loss: 1.220430 	Validation Loss: 1.397637 	 time: 0.3
Epoch: 360 	Training Loss: 1.220291 	Validation Loss: 1.400615 	 time: 0.3
Epoch: 361 	Training Loss: 1.220165 	Validation Loss: 1.396762 	 time: 0.3
Epoch: 362 	Training Loss: 1.219956 	Validation Loss: 1.398798 	 time: 0.3
Epoch: 363 	Training Loss: 1.219737 	Validation Loss: 1.399140 	 time: 0.3
Epoch: 364 	Training Loss: 1.219574 	Validation Loss: 1.397426 	 time: 0.3
Epoch: 365 	Training Loss: 1.219471 	Validation Loss: 1.402251 	 time: 0.3
Epoch: 366 	Training Loss: 1.219429 	Validation Loss: 1.397374 	 time: 0.3
Epoch: 367 	Training Loss: 1.219428 	Validation Loss: 1.403966 	 time: 0.3
Epoch: 368 	Training Loss: 1.219816 	Validation Loss: 1.397723 	 time: 0.3
Epoch: 369 	Training Loss: 1.221006 	Validation Loss: 1.409314 	 time: 0.3
Epoch: 370 	Training Loss: 1.225201 	Validation Loss: 1.410090 	 time: 0.3
Epoch: 371 	Training Loss: 1.231547 	Validation Loss: 1.447387 	 time: 0.3
Epoch: 372 	Training Loss: 1.256684 	Validation Loss: 1.406998 	 time: 0.3
Epoch: 373 	Training Loss: 1.240482 	Validation Loss: 1.415551 	 time: 0.3
Epoch: 374 	Training Loss: 1.237897 	Validation Loss: 1.430852 	 time: 0.3
Epoch: 375 	Training Loss: 1.232213 	Validation Loss: 1.415057 	 time: 0.3
Epoch: 376 	Training Loss: 1.235408 	Validation Loss: 1.421429 	 time: 0.3
Epoch: 377 	Training Loss: 1.241480 	Validation Loss: 1.414632 	 time: 0.3
Epoch: 378 	Training Loss: 1.230700 	Validation Loss: 1.405656 	 time: 0.3
Epoch: 379 	Training Loss: 1.229212 	Validation Loss: 1.401499 	 time: 0.3
Epoch: 380 	Training Loss: 1.228958 	Validation Loss: 1.410660 	 time: 0.3
Epoch: 381 	Training Loss: 1.227232 	Validation Loss: 1.416551 	 time: 0.3
Epoch: 382 	Training Loss: 1.227397 	Validation Loss: 1.411179 	 time: 0.3
Epoch: 383 	Training Loss: 1.227034 	Validation Loss: 1.406097 	 time: 0.3
Epoch: 384 	Training Loss: 1.223206 	Validation Loss: 1.418245 	 time: 0.3
Epoch: 385 	Training Loss: 1.227320 	Validation Loss: 1.399081 	 time: 0.3
Epoch: 386 	Training Loss: 1.221189 	Validation Loss: 1.402157 	 time: 0.3
Epoch: 387 	Training Loss: 1.226302 	Validation Loss: 1.400895 	 time: 0.3
Epoch: 388 	Training Loss: 1.219970 	Validation Loss: 1.406236 	 time: 0.3
Epoch: 389 	Training Loss: 1.222987 	Validation Loss: 1.394416 	 time: 0.3
Validation loss decreased from 1.396551 to 1.394416. Model was saved
Epoch: 390 	Training Loss: 1.220985 	Validation Loss: 1.399804 	 time: 0.3
Epoch: 391 	Training Loss: 1.219752 	Validation Loss: 1.399385 	 time: 0.3
Epoch: 392 	Training Loss: 1.219577 	Validation Loss: 1.394671 	 time: 0.3
Epoch: 393 	Training Loss: 1.219642 	Validation Loss: 1.393129 	 time: 0.3
Validation loss decreased from 1.394416 to 1.393129. Model was saved
Epoch: 394 	Training Loss: 1.218822 	Validation Loss: 1.390806 	 time: 0.3
Validation loss decreased from 1.393129 to 1.390806. Model was saved
Epoch: 395 	Training Loss: 1.218530 	Validation Loss: 1.389366 	 time: 0.3
Validation loss decreased from 1.390806 to 1.389366. Model was saved
Epoch: 396 	Training Loss: 1.217386 	Validation Loss: 1.391631 	 time: 0.3
Epoch: 397 	Training Loss: 1.217572 	Validation Loss: 1.392580 	 time: 0.3
Epoch: 398 	Training Loss: 1.217819 	Validation Loss: 1.390437 	 time: 0.3
Epoch: 399 	Training Loss: 1.216623 	Validation Loss: 1.389647 	 time: 0.3
Epoch: 400 	Training Loss: 1.216693 	Validation Loss: 1.386096 	 time: 0.3
Validation loss decreased from 1.389366 to 1.386096. Model was saved
Epoch: 401 	Training Loss: 1.216044 	Validation Loss: 1.387022 	 time: 0.3
Epoch: 402 	Training Loss: 1.216304 	Validation Loss: 1.390171 	 time: 0.3
Epoch: 403 	Training Loss: 1.215832 	Validation Loss: 1.391934 	 time: 0.3
Epoch: 404 	Training Loss: 1.215615 	Validation Loss: 1.392038 	 time: 0.3
Epoch: 405 	Training Loss: 1.215378 	Validation Loss: 1.392682 	 time: 0.3
Epoch: 406 	Training Loss: 1.215236 	Validation Loss: 1.393371 	 time: 0.3
Epoch: 407 	Training Loss: 1.215110 	Validation Loss: 1.392735 	 time: 0.3
Epoch: 408 	Training Loss: 1.214799 	Validation Loss: 1.388610 	 time: 0.3
Epoch: 409 	Training Loss: 1.214529 	Validation Loss: 1.386708 	 time: 0.3
Epoch: 410 	Training Loss: 1.214315 	Validation Loss: 1.389146 	 time: 0.3
Epoch: 411 	Training Loss: 1.214251 	Validation Loss: 1.387911 	 time: 0.3
Epoch: 412 	Training Loss: 1.213902 	Validation Loss: 1.387317 	 time: 0.3
Epoch: 413 	Training Loss: 1.213842 	Validation Loss: 1.390179 	 time: 0.3
Epoch: 414 	Training Loss: 1.213560 	Validation Loss: 1.392177 	 time: 0.3
Epoch: 415 	Training Loss: 1.213522 	Validation Loss: 1.390175 	 time: 0.3
Epoch: 416 	Training Loss: 1.213338 	Validation Loss: 1.389305 	 time: 0.3
Epoch: 417 	Training Loss: 1.213143 	Validation Loss: 1.390962 	 time: 0.3
Epoch: 418 	Training Loss: 1.213005 	Validation Loss: 1.391864 	 time: 0.3
Epoch: 419 	Training Loss: 1.212896 	Validation Loss: 1.389461 	 time: 0.3
Epoch: 420 	Training Loss: 1.212774 	Validation Loss: 1.389497 	 time: 0.3
Epoch: 421 	Training Loss: 1.212574 	Validation Loss: 1.390843 	 time: 0.3
Epoch: 422 	Training Loss: 1.212471 	Validation Loss: 1.389017 	 time: 0.3
Epoch: 423 	Training Loss: 1.212368 	Validation Loss: 1.387390 	 time: 0.3
Epoch: 424 	Training Loss: 1.212263 	Validation Loss: 1.388579 	 time: 0.3
Epoch: 425 	Training Loss: 1.212108 	Validation Loss: 1.388551 	 time: 0.3
Epoch: 426 	Training Loss: 1.211997 	Validation Loss: 1.387847 	 time: 0.3
Epoch: 427 	Training Loss: 1.211902 	Validation Loss: 1.388667 	 time: 0.3
Epoch: 428 	Training Loss: 1.211769 	Validation Loss: 1.389055 	 time: 0.3
Epoch: 429 	Training Loss: 1.211676 	Validation Loss: 1.388108 	 time: 0.3
Epoch: 430 	Training Loss: 1.211569 	Validation Loss: 1.387224 	 time: 0.3
Epoch: 431 	Training Loss: 1.211462 	Validation Loss: 1.387156 	 time: 0.3
Epoch: 432 	Training Loss: 1.211356 	Validation Loss: 1.386426 	 time: 0.3
Epoch: 433 	Training Loss: 1.211238 	Validation Loss: 1.385332 	 time: 0.3
Validation loss decreased from 1.386096 to 1.385332. Model was saved
Epoch: 434 	Training Loss: 1.211153 	Validation Loss: 1.385224 	 time: 0.3
Validation loss decreased from 1.385332 to 1.385224. Model was saved
Epoch: 435 	Training Loss: 1.211056 	Validation Loss: 1.385517 	 time: 0.3
Epoch: 436 	Training Loss: 1.210971 	Validation Loss: 1.385660 	 time: 0.3
Epoch: 437 	Training Loss: 1.210896 	Validation Loss: 1.385943 	 time: 0.3
Epoch: 438 	Training Loss: 1.210811 	Validation Loss: 1.386662 	 time: 0.3
Epoch: 439 	Training Loss: 1.210734 	Validation Loss: 1.387104 	 time: 0.3
Epoch: 440 	Training Loss: 1.210647 	Validation Loss: 1.386970 	 time: 0.3
Epoch: 441 	Training Loss: 1.210558 	Validation Loss: 1.386758 	 time: 0.3
Epoch: 442 	Training Loss: 1.210472 	Validation Loss: 1.386255 	 time: 0.3
Epoch: 443 	Training Loss: 1.210397 	Validation Loss: 1.385441 	 time: 0.3
Epoch: 444 	Training Loss: 1.210317 	Validation Loss: 1.385460 	 time: 0.3
Epoch: 445 	Training Loss: 1.210235 	Validation Loss: 1.385936 	 time: 0.3
Epoch: 446 	Training Loss: 1.210155 	Validation Loss: 1.386003 	 time: 0.3
Epoch: 447 	Training Loss: 1.210057 	Validation Loss: 1.386367 	 time: 0.3
Epoch: 448 	Training Loss: 1.209963 	Validation Loss: 1.387519 	 time: 0.3
Epoch: 449 	Training Loss: 1.209865 	Validation Loss: 1.388434 	 time: 0.3
Epoch: 450 	Training Loss: 1.209766 	Validation Loss: 1.388955 	 time: 0.3
Epoch: 451 	Training Loss: 1.209666 	Validation Loss: 1.389397 	 time: 0.3
Epoch: 452 	Training Loss: 1.209565 	Validation Loss: 1.389479 	 time: 0.3
Epoch: 453 	Training Loss: 1.209463 	Validation Loss: 1.389456 	 time: 0.3
Epoch: 454 	Training Loss: 1.209360 	Validation Loss: 1.389457 	 time: 0.3
Epoch: 455 	Training Loss: 1.209260 	Validation Loss: 1.388989 	 time: 0.3
Epoch: 456 	Training Loss: 1.209161 	Validation Loss: 1.388449 	 time: 0.2
Epoch: 457 	Training Loss: 1.209056 	Validation Loss: 1.388303 	 time: 0.3
Epoch: 458 	Training Loss: 1.208948 	Validation Loss: 1.388278 	 time: 0.3
Epoch: 459 	Training Loss: 1.208852 	Validation Loss: 1.388414 	 time: 0.3
Epoch: 460 	Training Loss: 1.208762 	Validation Loss: 1.388709 	 time: 0.3
Epoch: 461 	Training Loss: 1.208679 	Validation Loss: 1.388965 	 time: 0.3
Epoch: 462 	Training Loss: 1.208593 	Validation Loss: 1.388980 	 time: 0.3
Epoch: 463 	Training Loss: 1.208500 	Validation Loss: 1.388579 	 time: 0.3
Epoch: 464 	Training Loss: 1.208402 	Validation Loss: 1.388331 	 time: 0.3
Epoch: 465 	Training Loss: 1.208308 	Validation Loss: 1.388366 	 time: 0.3
Epoch: 466 	Training Loss: 1.208220 	Validation Loss: 1.388486 	 time: 0.3
Epoch: 467 	Training Loss: 1.208131 	Validation Loss: 1.388335 	 time: 0.3
Epoch: 468 	Training Loss: 1.208036 	Validation Loss: 1.387829 	 time: 0.3
Epoch: 469 	Training Loss: 1.207939 	Validation Loss: 1.387686 	 time: 0.3
Epoch: 470 	Training Loss: 1.207844 	Validation Loss: 1.387944 	 time: 0.3
Epoch: 471 	Training Loss: 1.207752 	Validation Loss: 1.388096 	 time: 0.3
Epoch: 472 	Training Loss: 1.207666 	Validation Loss: 1.388205 	 time: 0.3
Epoch: 473 	Training Loss: 1.207585 	Validation Loss: 1.388301 	 time: 0.3
Epoch: 474 	Training Loss: 1.207510 	Validation Loss: 1.388268 	 time: 0.3
Epoch: 475 	Training Loss: 1.207437 	Validation Loss: 1.388181 	 time: 0.3
Epoch: 476 	Training Loss: 1.207366 	Validation Loss: 1.387824 	 time: 0.3
Epoch: 477 	Training Loss: 1.207293 	Validation Loss: 1.387282 	 time: 0.3
Epoch: 478 	Training Loss: 1.207221 	Validation Loss: 1.386886 	 time: 0.3
Epoch: 479 	Training Loss: 1.207149 	Validation Loss: 1.386460 	 time: 0.3
Epoch: 480 	Training Loss: 1.207076 	Validation Loss: 1.385888 	 time: 0.3
Epoch: 481 	Training Loss: 1.207005 	Validation Loss: 1.385476 	 time: 0.3
Epoch: 482 	Training Loss: 1.206934 	Validation Loss: 1.385348 	 time: 0.3
Epoch: 483 	Training Loss: 1.206863 	Validation Loss: 1.385317 	 time: 0.3
Epoch: 484 	Training Loss: 1.206791 	Validation Loss: 1.385310 	 time: 0.3
Epoch: 485 	Training Loss: 1.206718 	Validation Loss: 1.385315 	 time: 0.3
Epoch: 486 	Training Loss: 1.206643 	Validation Loss: 1.385322 	 time: 0.3
Epoch: 487 	Training Loss: 1.206563 	Validation Loss: 1.385304 	 time: 0.3
Epoch: 488 	Training Loss: 1.206477 	Validation Loss: 1.385072 	 time: 0.3
Validation loss decreased from 1.385224 to 1.385072. Model was saved
Epoch: 489 	Training Loss: 1.206380 	Validation Loss: 1.384632 	 time: 0.3
Validation loss decreased from 1.385072 to 1.384632. Model was saved
Epoch: 490 	Training Loss: 1.206279 	Validation Loss: 1.384289 	 time: 0.3
Validation loss decreased from 1.384632 to 1.384289. Model was saved
Epoch: 491 	Training Loss: 1.206204 	Validation Loss: 1.383950 	 time: 0.3
Validation loss decreased from 1.384289 to 1.383950. Model was saved
Epoch: 492 	Training Loss: 1.206136 	Validation Loss: 1.383541 	 time: 0.3
Validation loss decreased from 1.383950 to 1.383541. Model was saved
Epoch: 493 	Training Loss: 1.206067 	Validation Loss: 1.383212 	 time: 0.3
Validation loss decreased from 1.383541 to 1.383212. Model was saved
Epoch: 494 	Training Loss: 1.205995 	Validation Loss: 1.382936 	 time: 0.3
Validation loss decreased from 1.383212 to 1.382936. Model was saved
Epoch: 495 	Training Loss: 1.205916 	Validation Loss: 1.382638 	 time: 0.3
Validation loss decreased from 1.382936 to 1.382638. Model was saved
Epoch: 496 	Training Loss: 1.205842 	Validation Loss: 1.382373 	 time: 0.3
Validation loss decreased from 1.382638 to 1.382373. Model was saved
Epoch: 497 	Training Loss: 1.205778 	Validation Loss: 1.382169 	 time: 0.3
Validation loss decreased from 1.382373 to 1.382169. Model was saved
Epoch: 498 	Training Loss: 1.205713 	Validation Loss: 1.381946 	 time: 0.3
Validation loss decreased from 1.382169 to 1.381946. Model was saved
Epoch: 499 	Training Loss: 1.205643 	Validation Loss: 1.381848 	 time: 0.3
Validation loss decreased from 1.381946 to 1.381848. Model was saved
Epoch: 500 	Training Loss: 1.205570 	Validation Loss: 1.381617 	 time: 0.3
Validation loss decreased from 1.381848 to 1.381617. Model was saved
Epoch: 501 	Training Loss: 1.205498 	Validation Loss: 1.381261 	 time: 0.3
Validation loss decreased from 1.381617 to 1.381261. Model was saved
Epoch: 502 	Training Loss: 1.205431 	Validation Loss: 1.381078 	 time: 0.3
Validation loss decreased from 1.381261 to 1.381078. Model was saved
Epoch: 503 	Training Loss: 1.205369 	Validation Loss: 1.380989 	 time: 0.3
Validation loss decreased from 1.381078 to 1.380989. Model was saved
Epoch: 504 	Training Loss: 1.205307 	Validation Loss: 1.380896 	 time: 0.3
Validation loss decreased from 1.380989 to 1.380896. Model was saved
Epoch: 505 	Training Loss: 1.205244 	Validation Loss: 1.380673 	 time: 0.3
Validation loss decreased from 1.380896 to 1.380673. Model was saved
Epoch: 506 	Training Loss: 1.205180 	Validation Loss: 1.380334 	 time: 0.3
Validation loss decreased from 1.380673 to 1.380334. Model was saved
Epoch: 507 	Training Loss: 1.205117 	Validation Loss: 1.380002 	 time: 0.3
Validation loss decreased from 1.380334 to 1.380002. Model was saved
Epoch: 508 	Training Loss: 1.205064 	Validation Loss: 1.379697 	 time: 0.3
Validation loss decreased from 1.380002 to 1.379697. Model was saved
Epoch: 509 	Training Loss: 1.205014 	Validation Loss: 1.379518 	 time: 0.3
Validation loss decreased from 1.379697 to 1.379518. Model was saved
Epoch: 510 	Training Loss: 1.204964 	Validation Loss: 1.379248 	 time: 0.3
Validation loss decreased from 1.379518 to 1.379248. Model was saved
Epoch: 511 	Training Loss: 1.204915 	Validation Loss: 1.378989 	 time: 0.3
Validation loss decreased from 1.379248 to 1.378989. Model was saved
Epoch: 512 	Training Loss: 1.204865 	Validation Loss: 1.378798 	 time: 0.3
Validation loss decreased from 1.378989 to 1.378798. Model was saved
Epoch: 513 	Training Loss: 1.204817 	Validation Loss: 1.378600 	 time: 0.3
Validation loss decreased from 1.378798 to 1.378600. Model was saved
Epoch: 514 	Training Loss: 1.204770 	Validation Loss: 1.378462 	 time: 0.3
Validation loss decreased from 1.378600 to 1.378462. Model was saved
Epoch: 515 	Training Loss: 1.204724 	Validation Loss: 1.378206 	 time: 0.3
Validation loss decreased from 1.378462 to 1.378206. Model was saved
Epoch: 516 	Training Loss: 1.204680 	Validation Loss: 1.377921 	 time: 0.3
Validation loss decreased from 1.378206 to 1.377921. Model was saved
Epoch: 517 	Training Loss: 1.204637 	Validation Loss: 1.377664 	 time: 0.3
Validation loss decreased from 1.377921 to 1.377664. Model was saved
Epoch: 518 	Training Loss: 1.204595 	Validation Loss: 1.377454 	 time: 0.3
Validation loss decreased from 1.377664 to 1.377454. Model was saved
Epoch: 519 	Training Loss: 1.204554 	Validation Loss: 1.377308 	 time: 0.3
Validation loss decreased from 1.377454 to 1.377308. Model was saved
Epoch: 520 	Training Loss: 1.204514 	Validation Loss: 1.377130 	 time: 0.3
Validation loss decreased from 1.377308 to 1.377130. Model was saved
Epoch: 521 	Training Loss: 1.204475 	Validation Loss: 1.377040 	 time: 0.3
Validation loss decreased from 1.377130 to 1.377040. Model was saved
Epoch: 522 	Training Loss: 1.204436 	Validation Loss: 1.376987 	 time: 0.3
Validation loss decreased from 1.377040 to 1.376987. Model was saved
Epoch: 523 	Training Loss: 1.204398 	Validation Loss: 1.377000 	 time: 0.3
Epoch: 524 	Training Loss: 1.204361 	Validation Loss: 1.377041 	 time: 0.3
Epoch: 525 	Training Loss: 1.204324 	Validation Loss: 1.376985 	 time: 0.3
Validation loss decreased from 1.376987 to 1.376985. Model was saved
Epoch: 526 	Training Loss: 1.204287 	Validation Loss: 1.376932 	 time: 0.3
Validation loss decreased from 1.376985 to 1.376932. Model was saved
Epoch: 527 	Training Loss: 1.204249 	Validation Loss: 1.376848 	 time: 0.3
Validation loss decreased from 1.376932 to 1.376848. Model was saved
Epoch: 528 	Training Loss: 1.204210 	Validation Loss: 1.376782 	 time: 0.3
Validation loss decreased from 1.376848 to 1.376782. Model was saved
Epoch: 529 	Training Loss: 1.204169 	Validation Loss: 1.376708 	 time: 0.3
Validation loss decreased from 1.376782 to 1.376708. Model was saved
Epoch: 530 	Training Loss: 1.204123 	Validation Loss: 1.376564 	 time: 0.3
Validation loss decreased from 1.376708 to 1.376564. Model was saved
Epoch: 531 	Training Loss: 1.204069 	Validation Loss: 1.376407 	 time: 0.3
Validation loss decreased from 1.376564 to 1.376407. Model was saved
Epoch: 532 	Training Loss: 1.204003 	Validation Loss: 1.376163 	 time: 0.3
Validation loss decreased from 1.376407 to 1.376163. Model was saved
Epoch: 533 	Training Loss: 1.203944 	Validation Loss: 1.375937 	 time: 0.3
Validation loss decreased from 1.376163 to 1.375937. Model was saved
Epoch: 534 	Training Loss: 1.203900 	Validation Loss: 1.375806 	 time: 0.3
Validation loss decreased from 1.375937 to 1.375806. Model was saved
Epoch: 535 	Training Loss: 1.203860 	Validation Loss: 1.375612 	 time: 0.3
Validation loss decreased from 1.375806 to 1.375612. Model was saved
Epoch: 536 	Training Loss: 1.203821 	Validation Loss: 1.375373 	 time: 0.3
Validation loss decreased from 1.375612 to 1.375373. Model was saved
Epoch: 537 	Training Loss: 1.203781 	Validation Loss: 1.375151 	 time: 0.3
Validation loss decreased from 1.375373 to 1.375151. Model was saved
Epoch: 538 	Training Loss: 1.203737 	Validation Loss: 1.375001 	 time: 0.3
Validation loss decreased from 1.375151 to 1.375001. Model was saved
Epoch: 539 	Training Loss: 1.203686 	Validation Loss: 1.374862 	 time: 0.3
Validation loss decreased from 1.375001 to 1.374862. Model was saved
Epoch: 540 	Training Loss: 1.203626 	Validation Loss: 1.374683 	 time: 0.3
Validation loss decreased from 1.374862 to 1.374683. Model was saved
Epoch: 541 	Training Loss: 1.203559 	Validation Loss: 1.374462 	 time: 0.3
Validation loss decreased from 1.374683 to 1.374462. Model was saved
Epoch: 542 	Training Loss: 1.203488 	Validation Loss: 1.374238 	 time: 0.3
Validation loss decreased from 1.374462 to 1.374238. Model was saved
Epoch: 543 	Training Loss: 1.203407 	Validation Loss: 1.374157 	 time: 0.3
Validation loss decreased from 1.374238 to 1.374157. Model was saved
Epoch: 544 	Training Loss: 1.203336 	Validation Loss: 1.374116 	 time: 0.3
Validation loss decreased from 1.374157 to 1.374116. Model was saved
Epoch: 545 	Training Loss: 1.203287 	Validation Loss: 1.374068 	 time: 0.3
Validation loss decreased from 1.374116 to 1.374068. Model was saved
Epoch: 546 	Training Loss: 1.203247 	Validation Loss: 1.374179 	 time: 0.3
Epoch: 547 	Training Loss: 1.203209 	Validation Loss: 1.374181 	 time: 0.3
Epoch: 548 	Training Loss: 1.203171 	Validation Loss: 1.374443 	 time: 0.3
Epoch: 549 	Training Loss: 1.203133 	Validation Loss: 1.374465 	 time: 0.3
Epoch: 550 	Training Loss: 1.203092 	Validation Loss: 1.374563 	 time: 0.3
Epoch: 551 	Training Loss: 1.203049 	Validation Loss: 1.374310 	 time: 0.3
Epoch: 552 	Training Loss: 1.203005 	Validation Loss: 1.374220 	 time: 0.3
Epoch: 553 	Training Loss: 1.202960 	Validation Loss: 1.373869 	 time: 0.3
Validation loss decreased from 1.374068 to 1.373869. Model was saved
Epoch: 554 	Training Loss: 1.202911 	Validation Loss: 1.373780 	 time: 0.3
Validation loss decreased from 1.373869 to 1.373780. Model was saved
Epoch: 555 	Training Loss: 1.202855 	Validation Loss: 1.373500 	 time: 0.3
Validation loss decreased from 1.373780 to 1.373500. Model was saved
Epoch: 556 	Training Loss: 1.202797 	Validation Loss: 1.373571 	 time: 0.3
Epoch: 557 	Training Loss: 1.202754 	Validation Loss: 1.373504 	 time: 0.3
Epoch: 558 	Training Loss: 1.202725 	Validation Loss: 1.373901 	 time: 0.3
Epoch: 559 	Training Loss: 1.202686 	Validation Loss: 1.373953 	 time: 0.3
Epoch: 560 	Training Loss: 1.202634 	Validation Loss: 1.374360 	 time: 0.3
Epoch: 561 	Training Loss: 1.202570 	Validation Loss: 1.374222 	 time: 0.3
Epoch: 562 	Training Loss: 1.202502 	Validation Loss: 1.374750 	 time: 0.3
Epoch: 563 	Training Loss: 1.202441 	Validation Loss: 1.374465 	 time: 0.3
Epoch: 564 	Training Loss: 1.202401 	Validation Loss: 1.375438 	 time: 0.3
Epoch: 565 	Training Loss: 1.202387 	Validation Loss: 1.374370 	 time: 0.3
Epoch: 566 	Training Loss: 1.202456 	Validation Loss: 1.377343 	 time: 0.3
Epoch: 567 	Training Loss: 1.202801 	Validation Loss: 1.376756 	 time: 0.3
Epoch: 568 	Training Loss: 1.204154 	Validation Loss: 1.393703 	 time: 0.3
Epoch: 569 	Training Loss: 1.211066 	Validation Loss: 1.412503 	 time: 0.3
Epoch: 570 	Training Loss: 1.253685 	Validation Loss: 1.545348 	 time: 0.3
Epoch: 571 	Training Loss: 1.392727 	Validation Loss: 1.528801 	 time: 0.3
Epoch: 572 	Training Loss: 1.356725 	Validation Loss: 1.429579 	 time: 0.3
Epoch: 573 	Training Loss: 1.294954 	Validation Loss: 1.463549 	 time: 0.3
Epoch: 574 	Training Loss: 1.334074 	Validation Loss: 1.437943 	 time: 0.3
Epoch: 575 	Training Loss: 1.309897 	Validation Loss: 1.443126 	 time: 0.3
Epoch: 576 	Training Loss: 1.281616 	Validation Loss: 1.462406 	 time: 0.3
Epoch: 577 	Training Loss: 1.290307 	Validation Loss: 1.459493 	 time: 0.3
Epoch: 578 	Training Loss: 1.295528 	Validation Loss: 1.436949 	 time: 0.3
Epoch: 579 	Training Loss: 1.271944 	Validation Loss: 1.432773 	 time: 0.3
Epoch: 580 	Training Loss: 1.266021 	Validation Loss: 1.456877 	 time: 0.3
Epoch: 581 	Training Loss: 1.277267 	Validation Loss: 1.449860 	 time: 0.3
Epoch: 582 	Training Loss: 1.265095 	Validation Loss: 1.430890 	 time: 0.3
Epoch: 583 	Training Loss: 1.250132 	Validation Loss: 1.433738 	 time: 0.3
Epoch: 584 	Training Loss: 1.256004 	Validation Loss: 1.435504 	 time: 0.3
Epoch: 585 	Training Loss: 1.254097 	Validation Loss: 1.430562 	 time: 0.3
Epoch: 586 	Training Loss: 1.247342 	Validation Loss: 1.419748 	 time: 0.3
Epoch: 587 	Training Loss: 1.239793 	Validation Loss: 1.416326 	 time: 0.3
Epoch: 588 	Training Loss: 1.241870 	Validation Loss: 1.418387 	 time: 0.3
Epoch: 589 	Training Loss: 1.237865 	Validation Loss: 1.413864 	 time: 0.3
Epoch: 590 	Training Loss: 1.231416 	Validation Loss: 1.423958 	 time: 0.3
Epoch: 591 	Training Loss: 1.229349 	Validation Loss: 1.428880 	 time: 0.3
Epoch: 592 	Training Loss: 1.229787 	Validation Loss: 1.421335 	 time: 0.3
Epoch: 593 	Training Loss: 1.223293 	Validation Loss: 1.415236 	 time: 0.3
Epoch: 594 	Training Loss: 1.225418 	Validation Loss: 1.415800 	 time: 0.3
Epoch: 595 	Training Loss: 1.222811 	Validation Loss: 1.414634 	 time: 0.3
Epoch: 596 	Training Loss: 1.219020 	Validation Loss: 1.418837 	 time: 0.3
Epoch: 597 	Training Loss: 1.220166 	Validation Loss: 1.411972 	 time: 0.3
Epoch: 598 	Training Loss: 1.217363 	Validation Loss: 1.410342 	 time: 0.3
Epoch: 599 	Training Loss: 1.216612 	Validation Loss: 1.411483 	 time: 0.3
Epoch: 600 	Training Loss: 1.216451 	Validation Loss: 1.412918 	 time: 0.3
Epoch: 601 	Training Loss: 1.213709 	Validation Loss: 1.416651 	 time: 0.3
Epoch: 602 	Training Loss: 1.213751 	Validation Loss: 1.414127 	 time: 0.3
Epoch: 603 	Training Loss: 1.211775 	Validation Loss: 1.414757 	 time: 0.3
Epoch: 604 	Training Loss: 1.211695 	Validation Loss: 1.414591 	 time: 0.3
Epoch: 605 	Training Loss: 1.210026 	Validation Loss: 1.415771 	 time: 0.3
Epoch: 606 	Training Loss: 1.209457 	Validation Loss: 1.413792 	 time: 0.3
Epoch: 607 	Training Loss: 1.208908 	Validation Loss: 1.413820 	 time: 0.3
Epoch: 608 	Training Loss: 1.207907 	Validation Loss: 1.413037 	 time: 0.3
Epoch: 609 	Training Loss: 1.207318 	Validation Loss: 1.410496 	 time: 0.3
Epoch: 610 	Training Loss: 1.206855 	Validation Loss: 1.409669 	 time: 0.3
Epoch: 611 	Training Loss: 1.206450 	Validation Loss: 1.411902 	 time: 0.3
Epoch: 612 	Training Loss: 1.206164 	Validation Loss: 1.407608 	 time: 0.3
Epoch: 613 	Training Loss: 1.205366 	Validation Loss: 1.407991 	 time: 0.3
Epoch: 614 	Training Loss: 1.205267 	Validation Loss: 1.409722 	 time: 0.3
Epoch: 615 	Training Loss: 1.204752 	Validation Loss: 1.411552 	 time: 0.3
Epoch: 616 	Training Loss: 1.204468 	Validation Loss: 1.411306 	 time: 0.3
Epoch: 617 	Training Loss: 1.204048 	Validation Loss: 1.410273 	 time: 0.3
Epoch: 618 	Training Loss: 1.203746 	Validation Loss: 1.410157 	 time: 0.3
Epoch: 619 	Training Loss: 1.203434 	Validation Loss: 1.411047 	 time: 0.3
Epoch: 620 	Training Loss: 1.203210 	Validation Loss: 1.409558 	 time: 0.3
Epoch: 621 	Training Loss: 1.202820 	Validation Loss: 1.409220 	 time: 0.3
Epoch: 622 	Training Loss: 1.202654 	Validation Loss: 1.409380 	 time: 0.3
Epoch: 623 	Training Loss: 1.202240 	Validation Loss: 1.409649 	 time: 0.3
Epoch: 624 	Training Loss: 1.202066 	Validation Loss: 1.408421 	 time: 0.3
Epoch: 625 	Training Loss: 1.201742 	Validation Loss: 1.407788 	 time: 0.3
Epoch: 626 	Training Loss: 1.201530 	Validation Loss: 1.407809 	 time: 0.3
Epoch: 627 	Training Loss: 1.201330 	Validation Loss: 1.406817 	 time: 0.3
Epoch: 628 	Training Loss: 1.201136 	Validation Loss: 1.405424 	 time: 0.3
Epoch: 629 	Training Loss: 1.200942 	Validation Loss: 1.405070 	 time: 0.3
Epoch: 630 	Training Loss: 1.200812 	Validation Loss: 1.404990 	 time: 0.3
Epoch: 631 	Training Loss: 1.200625 	Validation Loss: 1.405247 	 time: 0.3
Epoch: 632 	Training Loss: 1.200520 	Validation Loss: 1.404785 	 time: 0.3
Epoch: 633 	Training Loss: 1.200332 	Validation Loss: 1.404516 	 time: 0.3
Epoch: 634 	Training Loss: 1.200217 	Validation Loss: 1.404424 	 time: 0.3
Epoch: 635 	Training Loss: 1.200094 	Validation Loss: 1.404504 	 time: 0.3
Epoch: 636 	Training Loss: 1.199982 	Validation Loss: 1.404616 	 time: 0.3
Epoch: 637 	Training Loss: 1.199887 	Validation Loss: 1.404743 	 time: 0.3
Epoch: 638 	Training Loss: 1.199783 	Validation Loss: 1.404860 	 time: 0.3
Epoch: 639 	Training Loss: 1.199665 	Validation Loss: 1.404681 	 time: 0.3
Epoch: 640 	Training Loss: 1.199560 	Validation Loss: 1.404095 	 time: 0.3
Epoch: 641 	Training Loss: 1.199440 	Validation Loss: 1.403668 	 time: 0.3
Epoch: 642 	Training Loss: 1.199361 	Validation Loss: 1.403361 	 time: 0.3
Epoch: 643 	Training Loss: 1.199268 	Validation Loss: 1.403241 	 time: 0.3
Epoch: 644 	Training Loss: 1.199180 	Validation Loss: 1.403326 	 time: 0.3
Epoch: 645 	Training Loss: 1.199103 	Validation Loss: 1.403425 	 time: 0.3
Epoch: 646 	Training Loss: 1.199024 	Validation Loss: 1.403434 	 time: 0.3
Epoch: 647 	Training Loss: 1.198949 	Validation Loss: 1.403004 	 time: 0.3
Epoch: 648 	Training Loss: 1.198860 	Validation Loss: 1.402398 	 time: 0.3
Epoch: 649 	Training Loss: 1.198771 	Validation Loss: 1.401891 	 time: 0.3
Epoch: 650 	Training Loss: 1.198701 	Validation Loss: 1.401463 	 time: 0.3
Epoch: 651 	Training Loss: 1.198632 	Validation Loss: 1.401298 	 time: 0.3
Epoch: 652 	Training Loss: 1.198570 	Validation Loss: 1.401219 	 time: 0.3
Epoch: 653 	Training Loss: 1.198514 	Validation Loss: 1.400967 	 time: 0.3
Epoch: 654 	Training Loss: 1.198456 	Validation Loss: 1.400535 	 time: 0.3
Epoch: 655 	Training Loss: 1.198403 	Validation Loss: 1.400090 	 time: 0.3
Epoch: 656 	Training Loss: 1.198343 	Validation Loss: 1.399700 	 time: 0.3
Epoch: 657 	Training Loss: 1.198287 	Validation Loss: 1.399104 	 time: 0.3
Epoch: 658 	Training Loss: 1.198227 	Validation Loss: 1.398342 	 time: 0.3
Epoch: 659 	Training Loss: 1.198171 	Validation Loss: 1.397682 	 time: 0.3
Epoch: 660 	Training Loss: 1.198114 	Validation Loss: 1.397048 	 time: 0.3
Epoch: 661 	Training Loss: 1.198051 	Validation Loss: 1.396253 	 time: 0.3
Epoch: 662 	Training Loss: 1.197991 	Validation Loss: 1.395596 	 time: 0.3
Epoch: 663 	Training Loss: 1.197931 	Validation Loss: 1.395537 	 time: 0.3
Epoch: 664 	Training Loss: 1.197868 	Validation Loss: 1.395948 	 time: 0.3
Epoch: 665 	Training Loss: 1.197807 	Validation Loss: 1.396481 	 time: 0.3
Epoch: 666 	Training Loss: 1.197747 	Validation Loss: 1.397184 	 time: 0.3
Epoch: 667 	Training Loss: 1.197696 	Validation Loss: 1.397932 	 time: 0.3
Epoch: 668 	Training Loss: 1.197649 	Validation Loss: 1.398367 	 time: 0.3
Epoch: 669 	Training Loss: 1.197609 	Validation Loss: 1.398443 	 time: 0.3
Epoch: 670 	Training Loss: 1.197563 	Validation Loss: 1.398430 	 time: 0.3
Epoch: 671 	Training Loss: 1.197498 	Validation Loss: 1.398535 	 time: 0.3
Epoch: 672 	Training Loss: 1.197404 	Validation Loss: 1.398704 	 time: 0.3
Epoch: 673 	Training Loss: 1.197302 	Validation Loss: 1.398680 	 time: 0.3
Epoch: 674 	Training Loss: 1.197239 	Validation Loss: 1.398576 	 time: 0.3
Epoch: 675 	Training Loss: 1.197189 	Validation Loss: 1.398355 	 time: 0.3
Epoch: 676 	Training Loss: 1.197140 	Validation Loss: 1.397952 	 time: 0.3
Epoch: 677 	Training Loss: 1.197093 	Validation Loss: 1.397378 	 time: 0.3
Epoch: 678 	Training Loss: 1.197045 	Validation Loss: 1.396924 	 time: 0.3
Epoch: 679 	Training Loss: 1.197001 	Validation Loss: 1.396734 	 time: 0.3
Epoch: 680 	Training Loss: 1.196956 	Validation Loss: 1.396540 	 time: 0.3
Epoch: 681 	Training Loss: 1.196911 	Validation Loss: 1.396180 	 time: 0.3
Epoch: 682 	Training Loss: 1.196866 	Validation Loss: 1.395801 	 time: 0.3
Epoch: 683 	Training Loss: 1.196825 	Validation Loss: 1.395535 	 time: 0.3
Epoch: 684 	Training Loss: 1.196788 	Validation Loss: 1.395249 	 time: 0.3
Epoch: 685 	Training Loss: 1.196753 	Validation Loss: 1.394882 	 time: 0.3
Epoch: 686 	Training Loss: 1.196719 	Validation Loss: 1.394558 	 time: 0.3
Epoch: 687 	Training Loss: 1.196683 	Validation Loss: 1.394261 	 time: 0.3
Epoch: 688 	Training Loss: 1.196647 	Validation Loss: 1.393830 	 time: 0.3
Epoch: 689 	Training Loss: 1.196610 	Validation Loss: 1.393275 	 time: 0.3
Epoch: 690 	Training Loss: 1.196574 	Validation Loss: 1.392748 	 time: 0.3
Epoch: 691 	Training Loss: 1.196537 	Validation Loss: 1.392236 	 time: 0.3
Epoch: 692 	Training Loss: 1.196499 	Validation Loss: 1.391641 	 time: 0.3
Epoch: 693 	Training Loss: 1.196460 	Validation Loss: 1.391090 	 time: 0.3
Epoch: 694 	Training Loss: 1.196417 	Validation Loss: 1.390712 	 time: 0.3
Epoch: 695 	Training Loss: 1.196369 	Validation Loss: 1.390428 	 time: 0.3
Epoch: 696 	Training Loss: 1.196309 	Validation Loss: 1.390178 	 time: 0.3
Epoch: 697 	Training Loss: 1.196236 	Validation Loss: 1.390034 	 time: 0.3
Epoch: 698 	Training Loss: 1.196199 	Validation Loss: 1.389973 	 time: 0.3
Epoch: 699 	Training Loss: 1.196164 	Validation Loss: 1.389907 	 time: 0.3
Epoch: 700 	Training Loss: 1.196130 	Validation Loss: 1.389841 	 time: 0.3
Epoch: 701 	Training Loss: 1.196097 	Validation Loss: 1.389815 	 time: 0.3
Epoch: 702 	Training Loss: 1.196064 	Validation Loss: 1.389780 	 time: 0.3
Epoch: 703 	Training Loss: 1.196028 	Validation Loss: 1.389666 	 time: 0.3
Epoch: 704 	Training Loss: 1.195990 	Validation Loss: 1.389507 	 time: 0.3
Epoch: 705 	Training Loss: 1.195952 	Validation Loss: 1.389351 	 time: 0.3
Epoch: 706 	Training Loss: 1.195914 	Validation Loss: 1.389180 	 time: 0.3
Epoch: 707 	Training Loss: 1.195879 	Validation Loss: 1.388979 	 time: 0.3
Epoch: 708 	Training Loss: 1.195845 	Validation Loss: 1.388801 	 time: 0.3
Epoch: 709 	Training Loss: 1.195813 	Validation Loss: 1.388678 	 time: 0.3
Epoch: 710 	Training Loss: 1.195781 	Validation Loss: 1.388570 	 time: 0.3
Epoch: 711 	Training Loss: 1.195750 	Validation Loss: 1.388448 	 time: 0.3
Epoch: 712 	Training Loss: 1.195718 	Validation Loss: 1.388347 	 time: 0.3
Epoch: 713 	Training Loss: 1.195687 	Validation Loss: 1.388272 	 time: 0.3
Epoch: 714 	Training Loss: 1.195656 	Validation Loss: 1.388186 	 time: 0.3
Epoch: 715 	Training Loss: 1.195624 	Validation Loss: 1.388094 	 time: 0.3
Epoch: 716 	Training Loss: 1.195592 	Validation Loss: 1.388042 	 time: 0.3
Epoch: 717 	Training Loss: 1.195559 	Validation Loss: 1.388034 	 time: 0.3
Epoch: 718 	Training Loss: 1.195525 	Validation Loss: 1.388035 	 time: 0.3
Epoch: 719 	Training Loss: 1.195490 	Validation Loss: 1.388036 	 time: 0.3
Epoch: 720 	Training Loss: 1.195453 	Validation Loss: 1.388050 	 time: 0.3
Epoch: 721 	Training Loss: 1.195411 	Validation Loss: 1.388073 	 time: 0.3
Epoch: 722 	Training Loss: 1.195365 	Validation Loss: 1.388111 	 time: 0.3
Epoch: 723 	Training Loss: 1.195301 	Validation Loss: 1.388262 	 time: 0.3
Epoch: 724 	Training Loss: 1.195193 	Validation Loss: 1.388495 	 time: 0.3
Epoch: 725 	Training Loss: 1.195141 	Validation Loss: 1.388599 	 time: 0.3
Epoch: 726 	Training Loss: 1.195111 	Validation Loss: 1.388608 	 time: 0.3
Epoch: 727 	Training Loss: 1.195084 	Validation Loss: 1.388527 	 time: 0.3
Epoch: 728 	Training Loss: 1.195051 	Validation Loss: 1.388293 	 time: 0.3
Epoch: 729 	Training Loss: 1.195016 	Validation Loss: 1.387995 	 time: 0.3
Epoch: 730 	Training Loss: 1.194990 	Validation Loss: 1.387680 	 time: 0.3
Epoch: 731 	Training Loss: 1.194958 	Validation Loss: 1.387115 	 time: 0.3
Epoch: 732 	Training Loss: 1.194929 	Validation Loss: 1.386521 	 time: 0.3
Epoch: 733 	Training Loss: 1.194904 	Validation Loss: 1.386102 	 time: 0.3
Epoch: 734 	Training Loss: 1.194875 	Validation Loss: 1.385697 	 time: 0.3
Epoch: 735 	Training Loss: 1.194843 	Validation Loss: 1.385269 	 time: 0.3
Epoch: 736 	Training Loss: 1.194811 	Validation Loss: 1.385013 	 time: 0.3
Epoch: 737 	Training Loss: 1.194777 	Validation Loss: 1.384825 	 time: 0.3
Epoch: 738 	Training Loss: 1.194741 	Validation Loss: 1.384530 	 time: 0.3
Epoch: 739 	Training Loss: 1.194704 	Validation Loss: 1.384299 	 time: 0.3
Epoch: 740 	Training Loss: 1.194666 	Validation Loss: 1.384215 	 time: 0.3
Epoch: 741 	Training Loss: 1.194622 	Validation Loss: 1.384139 	 time: 0.3
Epoch: 742 	Training Loss: 1.194577 	Validation Loss: 1.384043 	 time: 0.3
Epoch: 743 	Training Loss: 1.194534 	Validation Loss: 1.384011 	 time: 0.3
Epoch: 744 	Training Loss: 1.194490 	Validation Loss: 1.383918 	 time: 0.3
Epoch: 745 	Training Loss: 1.194446 	Validation Loss: 1.383672 	 time: 0.3
Epoch: 746 	Training Loss: 1.194400 	Validation Loss: 1.383337 	 time: 0.3
Epoch: 747 	Training Loss: 1.194350 	Validation Loss: 1.382947 	 time: 0.3
Epoch: 748 	Training Loss: 1.194306 	Validation Loss: 1.382450 	 time: 0.3
Epoch: 749 	Training Loss: 1.194275 	Validation Loss: 1.381847 	 time: 0.3
Epoch: 750 	Training Loss: 1.194249 	Validation Loss: 1.381264 	 time: 0.3
Epoch: 751 	Training Loss: 1.194222 	Validation Loss: 1.380847 	 time: 0.3
Epoch: 752 	Training Loss: 1.194189 	Validation Loss: 1.380620 	 time: 0.3
Epoch: 753 	Training Loss: 1.194154 	Validation Loss: 1.380532 	 time: 0.3
Epoch: 754 	Training Loss: 1.194116 	Validation Loss: 1.380556 	 time: 0.3
Epoch: 755 	Training Loss: 1.194077 	Validation Loss: 1.380653 	 time: 0.3
Epoch: 756 	Training Loss: 1.194031 	Validation Loss: 1.380742 	 time: 0.3
Epoch: 757 	Training Loss: 1.193987 	Validation Loss: 1.380825 	 time: 0.3
Epoch: 758 	Training Loss: 1.193955 	Validation Loss: 1.380946 	 time: 0.3
Epoch: 759 	Training Loss: 1.193921 	Validation Loss: 1.381077 	 time: 0.3
Epoch: 760 	Training Loss: 1.193877 	Validation Loss: 1.381119 	 time: 0.3
Epoch: 761 	Training Loss: 1.193829 	Validation Loss: 1.381040 	 time: 0.3
Epoch: 762 	Training Loss: 1.193787 	Validation Loss: 1.380889 	 time: 0.3
Epoch: 763 	Training Loss: 1.193749 	Validation Loss: 1.380640 	 time: 0.3
Epoch: 764 	Training Loss: 1.193713 	Validation Loss: 1.380293 	 time: 0.3
Epoch: 765 	Training Loss: 1.193679 	Validation Loss: 1.379997 	 time: 0.3
Epoch: 766 	Training Loss: 1.193650 	Validation Loss: 1.379829 	 time: 0.3
Epoch: 767 	Training Loss: 1.193623 	Validation Loss: 1.379671 	 time: 0.3
Epoch: 768 	Training Loss: 1.193597 	Validation Loss: 1.379472 	 time: 0.3
Epoch: 769 	Training Loss: 1.193570 	Validation Loss: 1.379264 	 time: 0.3
Epoch: 770 	Training Loss: 1.193542 	Validation Loss: 1.378987 	 time: 0.3
Epoch: 771 	Training Loss: 1.193514 	Validation Loss: 1.378598 	 time: 0.3
Epoch: 772 	Training Loss: 1.193483 	Validation Loss: 1.378152 	 time: 0.3
Epoch: 773 	Training Loss: 1.193451 	Validation Loss: 1.377646 	 time: 0.3
Epoch: 774 	Training Loss: 1.193414 	Validation Loss: 1.377019 	 time: 0.3
Epoch: 775 	Training Loss: 1.193380 	Validation Loss: 1.376321 	 time: 0.3
Epoch: 776 	Training Loss: 1.193351 	Validation Loss: 1.375634 	 time: 0.3
Epoch: 777 	Training Loss: 1.193323 	Validation Loss: 1.374948 	 time: 0.3
Epoch: 778 	Training Loss: 1.193295 	Validation Loss: 1.374302 	 time: 0.3
Epoch: 779 	Training Loss: 1.193266 	Validation Loss: 1.373796 	 time: 0.3
Epoch: 780 	Training Loss: 1.193236 	Validation Loss: 1.373430 	 time: 0.3
Validation loss decreased from 1.373500 to 1.373430. Model was saved
Epoch: 781 	Training Loss: 1.193205 	Validation Loss: 1.373111 	 time: 0.3
Validation loss decreased from 1.373430 to 1.373111. Model was saved
Epoch: 782 	Training Loss: 1.193171 	Validation Loss: 1.372815 	 time: 0.3
Validation loss decreased from 1.373111 to 1.372815. Model was saved
Epoch: 783 	Training Loss: 1.193134 	Validation Loss: 1.372569 	 time: 0.3
Validation loss decreased from 1.372815 to 1.372569. Model was saved
Epoch: 784 	Training Loss: 1.193095 	Validation Loss: 1.372339 	 time: 0.3
Validation loss decreased from 1.372569 to 1.372339. Model was saved
Epoch: 785 	Training Loss: 1.193057 	Validation Loss: 1.372086 	 time: 0.3
Validation loss decreased from 1.372339 to 1.372086. Model was saved
Epoch: 786 	Training Loss: 1.193023 	Validation Loss: 1.371852 	 time: 0.3
Validation loss decreased from 1.372086 to 1.371852. Model was saved
Epoch: 787 	Training Loss: 1.192986 	Validation Loss: 1.371655 	 time: 0.3
Validation loss decreased from 1.371852 to 1.371655. Model was saved
Epoch: 788 	Training Loss: 1.192943 	Validation Loss: 1.371436 	 time: 0.3
Validation loss decreased from 1.371655 to 1.371436. Model was saved
Epoch: 789 	Training Loss: 1.192885 	Validation Loss: 1.371165 	 time: 0.3
Validation loss decreased from 1.371436 to 1.371165. Model was saved
Epoch: 790 	Training Loss: 1.192814 	Validation Loss: 1.370867 	 time: 0.3
Validation loss decreased from 1.371165 to 1.370867. Model was saved
Epoch: 791 	Training Loss: 1.192743 	Validation Loss: 1.370561 	 time: 0.3
Validation loss decreased from 1.370867 to 1.370561. Model was saved
Epoch: 792 	Training Loss: 1.192678 	Validation Loss: 1.370255 	 time: 0.3
Validation loss decreased from 1.370561 to 1.370255. Model was saved
Epoch: 793 	Training Loss: 1.192629 	Validation Loss: 1.369975 	 time: 0.3
Validation loss decreased from 1.370255 to 1.369975. Model was saved
Epoch: 794 	Training Loss: 1.192585 	Validation Loss: 1.369732 	 time: 0.3
Validation loss decreased from 1.369975 to 1.369732. Model was saved
Epoch: 795 	Training Loss: 1.192534 	Validation Loss: 1.369526 	 time: 0.3
Validation loss decreased from 1.369732 to 1.369526. Model was saved
Epoch: 796 	Training Loss: 1.192469 	Validation Loss: 1.369355 	 time: 0.3
Validation loss decreased from 1.369526 to 1.369355. Model was saved
Epoch: 797 	Training Loss: 1.192393 	Validation Loss: 1.369186 	 time: 0.3
Validation loss decreased from 1.369355 to 1.369186. Model was saved
Epoch: 798 	Training Loss: 1.192318 	Validation Loss: 1.368975 	 time: 0.3
Validation loss decreased from 1.369186 to 1.368975. Model was saved
Epoch: 799 	Training Loss: 1.192251 	Validation Loss: 1.368789 	 time: 0.3
Validation loss decreased from 1.368975 to 1.368789. Model was saved
Epoch: 800 	Training Loss: 1.192231 	Validation Loss: 1.368662 	 time: 0.3
Validation loss decreased from 1.368789 to 1.368662. Model was saved
Epoch: 801 	Training Loss: 1.192171 	Validation Loss: 1.368495 	 time: 0.3
Validation loss decreased from 1.368662 to 1.368495. Model was saved
Epoch: 802 	Training Loss: 1.192115 	Validation Loss: 1.368332 	 time: 0.3
Validation loss decreased from 1.368495 to 1.368332. Model was saved
Epoch: 803 	Training Loss: 1.192062 	Validation Loss: 1.368162 	 time: 0.3
Validation loss decreased from 1.368332 to 1.368162. Model was saved
Epoch: 804 	Training Loss: 1.192014 	Validation Loss: 1.367936 	 time: 0.3
Validation loss decreased from 1.368162 to 1.367936. Model was saved
Epoch: 805 	Training Loss: 1.191969 	Validation Loss: 1.367694 	 time: 0.3
Validation loss decreased from 1.367936 to 1.367694. Model was saved
Epoch: 806 	Training Loss: 1.191919 	Validation Loss: 1.367446 	 time: 0.3
Validation loss decreased from 1.367694 to 1.367446. Model was saved
Epoch: 807 	Training Loss: 1.191865 	Validation Loss: 1.367222 	 time: 0.3
Validation loss decreased from 1.367446 to 1.367222. Model was saved
Epoch: 808 	Training Loss: 1.191809 	Validation Loss: 1.367123 	 time: 0.3
Validation loss decreased from 1.367222 to 1.367123. Model was saved
Epoch: 809 	Training Loss: 1.191760 	Validation Loss: 1.367222 	 time: 0.3
Epoch: 810 	Training Loss: 1.191713 	Validation Loss: 1.367462 	 time: 0.3
Epoch: 811 	Training Loss: 1.191663 	Validation Loss: 1.367708 	 time: 0.3
Epoch: 812 	Training Loss: 1.191607 	Validation Loss: 1.367852 	 time: 0.3
Epoch: 813 	Training Loss: 1.191541 	Validation Loss: 1.367846 	 time: 0.3
Epoch: 814 	Training Loss: 1.191469 	Validation Loss: 1.367737 	 time: 0.3
Epoch: 815 	Training Loss: 1.191393 	Validation Loss: 1.367632 	 time: 0.3
Epoch: 816 	Training Loss: 1.191309 	Validation Loss: 1.367567 	 time: 0.3
Epoch: 817 	Training Loss: 1.191253 	Validation Loss: 1.367559 	 time: 0.3
Epoch: 818 	Training Loss: 1.191215 	Validation Loss: 1.367586 	 time: 0.3
Epoch: 819 	Training Loss: 1.191159 	Validation Loss: 1.367643 	 time: 0.3
Epoch: 820 	Training Loss: 1.191055 	Validation Loss: 1.368048 	 time: 0.3
Epoch: 821 	Training Loss: 1.190996 	Validation Loss: 1.368414 	 time: 0.3
Epoch: 822 	Training Loss: 1.190955 	Validation Loss: 1.368395 	 time: 0.3
Epoch: 823 	Training Loss: 1.190886 	Validation Loss: 1.368499 	 time: 0.3
Epoch: 824 	Training Loss: 1.190828 	Validation Loss: 1.368545 	 time: 0.3
Epoch: 825 	Training Loss: 1.190777 	Validation Loss: 1.368244 	 time: 0.3
Epoch: 826 	Training Loss: 1.190713 	Validation Loss: 1.368109 	 time: 0.3
Epoch: 827 	Training Loss: 1.190643 	Validation Loss: 1.368154 	 time: 0.3
Epoch: 828 	Training Loss: 1.190589 	Validation Loss: 1.368015 	 time: 0.3
Epoch: 829 	Training Loss: 1.190544 	Validation Loss: 1.367867 	 time: 0.3
Epoch: 830 	Training Loss: 1.190479 	Validation Loss: 1.367844 	 time: 0.3
Epoch: 831 	Training Loss: 1.190399 	Validation Loss: 1.367697 	 time: 0.3
Epoch: 832 	Training Loss: 1.190333 	Validation Loss: 1.367534 	 time: 0.3
Epoch: 833 	Training Loss: 1.190282 	Validation Loss: 1.367559 	 time: 0.3
Epoch: 834 	Training Loss: 1.190232 	Validation Loss: 1.367643 	 time: 0.3
Epoch: 835 	Training Loss: 1.190177 	Validation Loss: 1.367681 	 time: 0.3
Epoch: 836 	Training Loss: 1.190110 	Validation Loss: 1.367718 	 time: 0.3
Epoch: 837 	Training Loss: 1.190029 	Validation Loss: 1.367700 	 time: 0.3
Epoch: 838 	Training Loss: 1.189938 	Validation Loss: 1.367485 	 time: 0.3
Epoch: 839 	Training Loss: 1.189830 	Validation Loss: 1.367198 	 time: 0.3
Epoch: 840 	Training Loss: 1.189723 	Validation Loss: 1.367008 	 time: 0.3
Validation loss decreased from 1.367123 to 1.367008. Model was saved
Epoch: 841 	Training Loss: 1.189630 	Validation Loss: 1.366736 	 time: 0.3
Validation loss decreased from 1.367008 to 1.366736. Model was saved
Epoch: 842 	Training Loss: 1.189531 	Validation Loss: 1.366419 	 time: 0.2
Validation loss decreased from 1.366736 to 1.366419. Model was saved
Epoch: 843 	Training Loss: 1.189411 	Validation Loss: 1.366030 	 time: 0.3
Validation loss decreased from 1.366419 to 1.366030. Model was saved
Epoch: 844 	Training Loss: 1.189251 	Validation Loss: 1.365469 	 time: 0.3
Validation loss decreased from 1.366030 to 1.365469. Model was saved
Epoch: 845 	Training Loss: 1.189036 	Validation Loss: 1.365125 	 time: 0.3
Validation loss decreased from 1.365469 to 1.365125. Model was saved
Epoch: 846 	Training Loss: 1.188828 	Validation Loss: 1.365213 	 time: 0.3
Epoch: 847 	Training Loss: 1.188581 	Validation Loss: 1.365257 	 time: 0.3
Epoch: 848 	Training Loss: 1.188195 	Validation Loss: 1.365315 	 time: 0.3
Epoch: 849 	Training Loss: 1.187698 	Validation Loss: 1.365514 	 time: 0.3
Epoch: 850 	Training Loss: 1.187171 	Validation Loss: 1.365552 	 time: 0.3
Epoch: 851 	Training Loss: 1.186668 	Validation Loss: 1.365203 	 time: 0.3
Epoch: 852 	Training Loss: 1.186184 	Validation Loss: 1.365102 	 time: 0.3
Validation loss decreased from 1.365125 to 1.365102. Model was saved
Epoch: 853 	Training Loss: 1.185605 	Validation Loss: 1.365113 	 time: 0.3
Epoch: 854 	Training Loss: 1.185074 	Validation Loss: 1.364676 	 time: 0.3
Validation loss decreased from 1.365102 to 1.364676. Model was saved
Epoch: 855 	Training Loss: 1.184628 	Validation Loss: 1.364445 	 time: 0.3
Validation loss decreased from 1.364676 to 1.364445. Model was saved
Epoch: 856 	Training Loss: 1.184231 	Validation Loss: 1.363906 	 time: 0.3
Validation loss decreased from 1.364445 to 1.363906. Model was saved
Epoch: 857 	Training Loss: 1.183873 	Validation Loss: 1.363425 	 time: 0.3
Validation loss decreased from 1.363906 to 1.363425. Model was saved
Epoch: 858 	Training Loss: 1.183519 	Validation Loss: 1.363154 	 time: 0.3
Validation loss decreased from 1.363425 to 1.363154. Model was saved
Epoch: 859 	Training Loss: 1.183185 	Validation Loss: 1.361998 	 time: 0.3
Validation loss decreased from 1.363154 to 1.361998. Model was saved
Epoch: 860 	Training Loss: 1.182835 	Validation Loss: 1.362195 	 time: 0.3
Epoch: 861 	Training Loss: 1.182524 	Validation Loss: 1.362492 	 time: 0.3
Epoch: 862 	Training Loss: 1.182307 	Validation Loss: 1.362877 	 time: 0.3
Epoch: 863 	Training Loss: 1.182131 	Validation Loss: 1.363273 	 time: 0.3
Epoch: 864 	Training Loss: 1.181941 	Validation Loss: 1.363480 	 time: 0.3
Epoch: 865 	Training Loss: 1.181737 	Validation Loss: 1.363687 	 time: 0.3
Epoch: 866 	Training Loss: 1.181506 	Validation Loss: 1.363703 	 time: 0.3
Epoch: 867 	Training Loss: 1.181274 	Validation Loss: 1.363672 	 time: 0.3
Epoch: 868 	Training Loss: 1.181003 	Validation Loss: 1.363754 	 time: 0.3
Epoch: 869 	Training Loss: 1.180739 	Validation Loss: 1.363812 	 time: 0.3
Epoch: 870 	Training Loss: 1.180504 	Validation Loss: 1.363856 	 time: 0.3
Epoch: 871 	Training Loss: 1.180259 	Validation Loss: 1.363669 	 time: 0.3
Epoch: 872 	Training Loss: 1.180029 	Validation Loss: 1.363596 	 time: 0.3
Epoch: 873 	Training Loss: 1.179834 	Validation Loss: 1.363464 	 time: 0.3
Epoch: 874 	Training Loss: 1.179649 	Validation Loss: 1.363173 	 time: 0.3
Epoch: 875 	Training Loss: 1.179519 	Validation Loss: 1.363132 	 time: 0.3
Epoch: 876 	Training Loss: 1.179396 	Validation Loss: 1.362883 	 time: 0.3
Epoch: 877 	Training Loss: 1.179259 	Validation Loss: 1.362835 	 time: 0.3
Epoch: 878 	Training Loss: 1.179114 	Validation Loss: 1.362721 	 time: 0.3
Epoch: 879 	Training Loss: 1.178963 	Validation Loss: 1.362942 	 time: 0.3
Epoch: 880 	Training Loss: 1.178849 	Validation Loss: 1.363272 	 time: 0.3
Epoch: 881 	Training Loss: 1.178747 	Validation Loss: 1.363742 	 time: 0.3
Epoch: 882 	Training Loss: 1.178640 	Validation Loss: 1.364074 	 time: 0.3
Epoch: 883 	Training Loss: 1.178516 	Validation Loss: 1.364143 	 time: 0.3
Epoch: 884 	Training Loss: 1.178367 	Validation Loss: 1.364417 	 time: 0.3
Epoch: 885 	Training Loss: 1.178208 	Validation Loss: 1.364364 	 time: 0.3
Epoch: 886 	Training Loss: 1.178076 	Validation Loss: 1.364545 	 time: 0.3
Epoch: 887 	Training Loss: 1.177926 	Validation Loss: 1.364573 	 time: 0.3
Epoch: 888 	Training Loss: 1.177828 	Validation Loss: 1.364924 	 time: 0.3
Epoch: 889 	Training Loss: 1.177758 	Validation Loss: 1.365464 	 time: 0.3
Epoch: 890 	Training Loss: 1.177692 	Validation Loss: 1.365884 	 time: 0.3
Epoch: 891 	Training Loss: 1.177620 	Validation Loss: 1.366149 	 time: 0.3
Epoch: 892 	Training Loss: 1.177538 	Validation Loss: 1.366360 	 time: 0.3
Epoch: 893 	Training Loss: 1.177446 	Validation Loss: 1.366351 	 time: 0.3
Epoch: 894 	Training Loss: 1.177343 	Validation Loss: 1.366186 	 time: 0.3
Epoch: 895 	Training Loss: 1.177240 	Validation Loss: 1.366214 	 time: 0.3
Epoch: 896 	Training Loss: 1.177128 	Validation Loss: 1.366460 	 time: 0.3
Epoch: 897 	Training Loss: 1.176983 	Validation Loss: 1.366715 	 time: 0.3
Epoch: 898 	Training Loss: 1.176836 	Validation Loss: 1.366928 	 time: 0.3
Epoch: 899 	Training Loss: 1.176744 	Validation Loss: 1.366879 	 time: 0.3
Epoch: 900 	Training Loss: 1.176650 	Validation Loss: 1.366823 	 time: 0.2
Epoch: 901 	Training Loss: 1.176548 	Validation Loss: 1.366599 	 time: 0.3
Epoch: 902 	Training Loss: 1.176446 	Validation Loss: 1.366364 	 time: 0.3
Epoch: 903 	Training Loss: 1.176358 	Validation Loss: 1.366133 	 time: 0.3
Epoch: 904 	Training Loss: 1.176293 	Validation Loss: 1.365739 	 time: 0.3
Epoch: 905 	Training Loss: 1.176235 	Validation Loss: 1.365337 	 time: 0.3
Epoch: 906 	Training Loss: 1.176173 	Validation Loss: 1.364796 	 time: 0.3
Epoch: 907 	Training Loss: 1.176114 	Validation Loss: 1.364256 	 time: 0.3
Epoch: 908 	Training Loss: 1.176053 	Validation Loss: 1.363727 	 time: 0.3
Epoch: 909 	Training Loss: 1.175986 	Validation Loss: 1.363222 	 time: 0.3
Epoch: 910 	Training Loss: 1.175903 	Validation Loss: 1.362953 	 time: 0.3
Epoch: 911 	Training Loss: 1.175819 	Validation Loss: 1.362931 	 time: 0.3
Epoch: 912 	Training Loss: 1.175739 	Validation Loss: 1.363245 	 time: 0.3
Epoch: 913 	Training Loss: 1.175660 	Validation Loss: 1.363603 	 time: 0.3
Epoch: 914 	Training Loss: 1.175575 	Validation Loss: 1.364033 	 time: 0.3
Epoch: 915 	Training Loss: 1.175490 	Validation Loss: 1.364137 	 time: 0.3
Epoch: 916 	Training Loss: 1.175418 	Validation Loss: 1.364094 	 time: 0.3
Epoch: 917 	Training Loss: 1.175349 	Validation Loss: 1.363664 	 time: 0.3
Epoch: 918 	Training Loss: 1.175288 	Validation Loss: 1.363310 	 time: 0.3
Epoch: 919 	Training Loss: 1.175242 	Validation Loss: 1.363255 	 time: 0.3
Epoch: 920 	Training Loss: 1.175194 	Validation Loss: 1.363327 	 time: 0.3
Epoch: 921 	Training Loss: 1.175138 	Validation Loss: 1.363456 	 time: 0.3
Epoch: 922 	Training Loss: 1.175069 	Validation Loss: 1.363608 	 time: 0.3
Epoch: 923 	Training Loss: 1.174965 	Validation Loss: 1.363977 	 time: 0.3
Epoch: 924 	Training Loss: 1.174870 	Validation Loss: 1.364252 	 time: 0.3
Epoch: 925 	Training Loss: 1.174809 	Validation Loss: 1.364514 	 time: 0.3
Epoch: 926 	Training Loss: 1.174737 	Validation Loss: 1.364346 	 time: 0.3
Epoch: 927 	Training Loss: 1.174635 	Validation Loss: 1.364190 	 time: 0.3
Epoch: 928 	Training Loss: 1.174562 	Validation Loss: 1.364342 	 time: 0.3
Epoch: 929 	Training Loss: 1.174503 	Validation Loss: 1.363883 	 time: 0.3
Epoch: 930 	Training Loss: 1.174440 	Validation Loss: 1.363991 	 time: 0.3
Epoch: 931 	Training Loss: 1.174378 	Validation Loss: 1.363647 	 time: 0.3
Epoch: 932 	Training Loss: 1.174331 	Validation Loss: 1.364272 	 time: 0.3
Epoch: 933 	Training Loss: 1.174262 	Validation Loss: 1.363233 	 time: 0.3
Epoch: 934 	Training Loss: 1.174177 	Validation Loss: 1.363995 	 time: 0.3
Epoch: 935 	Training Loss: 1.174084 	Validation Loss: 1.363666 	 time: 0.3
Epoch: 936 	Training Loss: 1.174006 	Validation Loss: 1.364019 	 time: 0.3
Epoch: 937 	Training Loss: 1.173973 	Validation Loss: 1.364232 	 time: 0.3
Epoch: 938 	Training Loss: 1.173894 	Validation Loss: 1.363345 	 time: 0.3
Epoch: 939 	Training Loss: 1.173842 	Validation Loss: 1.364835 	 time: 0.3
Epoch: 940 	Training Loss: 1.173756 	Validation Loss: 1.364431 	 time: 0.3
Epoch: 941 	Training Loss: 1.173648 	Validation Loss: 1.364957 	 time: 0.3
Epoch: 942 	Training Loss: 1.173577 	Validation Loss: 1.365975 	 time: 0.3
Epoch: 943 	Training Loss: 1.173523 	Validation Loss: 1.364766 	 time: 0.3
Epoch: 944 	Training Loss: 1.173508 	Validation Loss: 1.366766 	 time: 0.3
Epoch: 945 	Training Loss: 1.173406 	Validation Loss: 1.365282 	 time: 0.3
Epoch: 946 	Training Loss: 1.173269 	Validation Loss: 1.365732 	 time: 0.3
Epoch: 947 	Training Loss: 1.173187 	Validation Loss: 1.365491 	 time: 0.3
Epoch: 948 	Training Loss: 1.173110 	Validation Loss: 1.364739 	 time: 0.3
Epoch: 949 	Training Loss: 1.172998 	Validation Loss: 1.365510 	 time: 0.3
Epoch: 950 	Training Loss: 1.172848 	Validation Loss: 1.364100 	 time: 0.3
Epoch: 951 	Training Loss: 1.172718 	Validation Loss: 1.366099 	 time: 0.3
Epoch: 952 	Training Loss: 1.172609 	Validation Loss: 1.362221 	 time: 0.3
Epoch: 953 	Training Loss: 1.172554 	Validation Loss: 1.363531 	 time: 0.3
Epoch: 954 	Training Loss: 1.172437 	Validation Loss: 1.361721 	 time: 0.3
Validation loss decreased from 1.361998 to 1.361721. Model was saved
Epoch: 955 	Training Loss: 1.172318 	Validation Loss: 1.361025 	 time: 0.3
Validation loss decreased from 1.361721 to 1.361025. Model was saved
Epoch: 956 	Training Loss: 1.172243 	Validation Loss: 1.361024 	 time: 0.3
Validation loss decreased from 1.361025 to 1.361024. Model was saved
Epoch: 957 	Training Loss: 1.172192 	Validation Loss: 1.359244 	 time: 0.3
Validation loss decreased from 1.361024 to 1.359244. Model was saved
Epoch: 958 	Training Loss: 1.172122 	Validation Loss: 1.360396 	 time: 0.3
Epoch: 959 	Training Loss: 1.172032 	Validation Loss: 1.359365 	 time: 0.3
Epoch: 960 	Training Loss: 1.171971 	Validation Loss: 1.359989 	 time: 0.3
Epoch: 961 	Training Loss: 1.171911 	Validation Loss: 1.359768 	 time: 0.3
Epoch: 962 	Training Loss: 1.171866 	Validation Loss: 1.360082 	 time: 0.3
Epoch: 963 	Training Loss: 1.171814 	Validation Loss: 1.361189 	 time: 0.3
Epoch: 964 	Training Loss: 1.171760 	Validation Loss: 1.360856 	 time: 0.3
Epoch: 965 	Training Loss: 1.171703 	Validation Loss: 1.361330 	 time: 0.3
Epoch: 966 	Training Loss: 1.171663 	Validation Loss: 1.361401 	 time: 0.3
Epoch: 967 	Training Loss: 1.171628 	Validation Loss: 1.361202 	 time: 0.3
Epoch: 968 	Training Loss: 1.171600 	Validation Loss: 1.361926 	 time: 0.3
Epoch: 969 	Training Loss: 1.171560 	Validation Loss: 1.362133 	 time: 0.3
Epoch: 970 	Training Loss: 1.171526 	Validation Loss: 1.362473 	 time: 0.3
Epoch: 971 	Training Loss: 1.171478 	Validation Loss: 1.362635 	 time: 0.3
Epoch: 972 	Training Loss: 1.171428 	Validation Loss: 1.362743 	 time: 0.3
Epoch: 973 	Training Loss: 1.171390 	Validation Loss: 1.363017 	 time: 0.3
Epoch: 974 	Training Loss: 1.171354 	Validation Loss: 1.362972 	 time: 0.3
Epoch: 975 	Training Loss: 1.171321 	Validation Loss: 1.362951 	 time: 0.3
Epoch: 976 	Training Loss: 1.171287 	Validation Loss: 1.362794 	 time: 0.3
Epoch: 977 	Training Loss: 1.171262 	Validation Loss: 1.362757 	 time: 0.3
Epoch: 978 	Training Loss: 1.171230 	Validation Loss: 1.362958 	 time: 0.3
Epoch: 979 	Training Loss: 1.171203 	Validation Loss: 1.362828 	 time: 0.3
Epoch: 980 	Training Loss: 1.171180 	Validation Loss: 1.362567 	 time: 0.3
Epoch: 981 	Training Loss: 1.171159 	Validation Loss: 1.362485 	 time: 0.3
Epoch: 982 	Training Loss: 1.171136 	Validation Loss: 1.362731 	 time: 0.3
Epoch: 983 	Training Loss: 1.171110 	Validation Loss: 1.362905 	 time: 0.3
Epoch: 984 	Training Loss: 1.171088 	Validation Loss: 1.362769 	 time: 0.3
Epoch: 985 	Training Loss: 1.171064 	Validation Loss: 1.362559 	 time: 0.3
Epoch: 986 	Training Loss: 1.171039 	Validation Loss: 1.362371 	 time: 0.3
Epoch: 987 	Training Loss: 1.171014 	Validation Loss: 1.362211 	 time: 0.3
Epoch: 988 	Training Loss: 1.170988 	Validation Loss: 1.361905 	 time: 0.3
Epoch: 989 	Training Loss: 1.170964 	Validation Loss: 1.361630 	 time: 0.3
Epoch: 990 	Training Loss: 1.170941 	Validation Loss: 1.361427 	 time: 0.3
Epoch: 991 	Training Loss: 1.170916 	Validation Loss: 1.361389 	 time: 0.3
Epoch: 992 	Training Loss: 1.170892 	Validation Loss: 1.361489 	 time: 0.3
Epoch: 993 	Training Loss: 1.170869 	Validation Loss: 1.361450 	 time: 0.3
Epoch: 994 	Training Loss: 1.170843 	Validation Loss: 1.361366 	 time: 0.3
Epoch: 995 	Training Loss: 1.170817 	Validation Loss: 1.361316 	 time: 0.3
Epoch: 996 	Training Loss: 1.170776 	Validation Loss: 1.361095 	 time: 0.3
Epoch: 997 	Training Loss: 1.170727 	Validation Loss: 1.360935 	 time: 0.3
Epoch: 998 	Training Loss: 1.170682 	Validation Loss: 1.360467 	 time: 0.3
Epoch: 999 	Training Loss: 1.170625 	Validation Loss: 1.360670 	 time: 0.3
Epoch: 1000 	Training Loss: 1.170576 	Validation Loss: 1.360210 	 time: 0.3
Epoch: 1001 	Training Loss: 1.170522 	Validation Loss: 1.360769 	 time: 0.3
Epoch: 1002 	Training Loss: 1.170488 	Validation Loss: 1.360146 	 time: 0.3
Epoch: 1003 	Training Loss: 1.170467 	Validation Loss: 1.360635 	 time: 0.3
Epoch: 1004 	Training Loss: 1.170425 	Validation Loss: 1.360386 	 time: 0.3
Epoch: 1005 	Training Loss: 1.170370 	Validation Loss: 1.360463 	 time: 0.3
Epoch: 1006 	Training Loss: 1.170353 	Validation Loss: 1.360990 	 time: 0.3
Epoch: 1007 	Training Loss: 1.170340 	Validation Loss: 1.360651 	 time: 0.3
Epoch: 1008 	Training Loss: 1.170371 	Validation Loss: 1.361784 	 time: 0.3
Epoch: 1009 	Training Loss: 1.170384 	Validation Loss: 1.361193 	 time: 0.3
Epoch: 1010 	Training Loss: 1.170322 	Validation Loss: 1.361417 	 time: 0.3
Epoch: 1011 	Training Loss: 1.170216 	Validation Loss: 1.361342 	 time: 0.3
Epoch: 1012 	Training Loss: 1.170154 	Validation Loss: 1.361401 	 time: 0.3
Epoch: 1013 	Training Loss: 1.170175 	Validation Loss: 1.361650 	 time: 0.3
Epoch: 1014 	Training Loss: 1.170182 	Validation Loss: 1.360498 	 time: 0.3
Epoch: 1015 	Training Loss: 1.170149 	Validation Loss: 1.361405 	 time: 0.3
Epoch: 1016 	Training Loss: 1.169993 	Validation Loss: 1.361363 	 time: 0.3
Epoch: 1017 	Training Loss: 1.169885 	Validation Loss: 1.360840 	 time: 0.3
Epoch: 1018 	Training Loss: 1.169940 	Validation Loss: 1.361454 	 time: 0.3
Epoch: 1019 	Training Loss: 1.169955 	Validation Loss: 1.359820 	 time: 0.3
Epoch: 1020 	Training Loss: 1.169915 	Validation Loss: 1.359678 	 time: 0.3
Epoch: 1021 	Training Loss: 1.169827 	Validation Loss: 1.359562 	 time: 0.3
Epoch: 1022 	Training Loss: 1.169729 	Validation Loss: 1.359747 	 time: 0.3
Epoch: 1023 	Training Loss: 1.169756 	Validation Loss: 1.361277 	 time: 0.3
Epoch: 1024 	Training Loss: 1.169838 	Validation Loss: 1.360128 	 time: 0.3
Epoch: 1025 	Training Loss: 1.169795 	Validation Loss: 1.361845 	 time: 0.3
Epoch: 1026 	Training Loss: 1.169685 	Validation Loss: 1.362234 	 time: 0.3
Epoch: 1027 	Training Loss: 1.169546 	Validation Loss: 1.362175 	 time: 0.3
Epoch: 1028 	Training Loss: 1.169556 	Validation Loss: 1.363017 	 time: 0.3
Epoch: 1029 	Training Loss: 1.169514 	Validation Loss: 1.361664 	 time: 0.3
Epoch: 1030 	Training Loss: 1.169486 	Validation Loss: 1.361290 	 time: 0.3
Epoch: 1031 	Training Loss: 1.169436 	Validation Loss: 1.360765 	 time: 0.3
Epoch: 1032 	Training Loss: 1.169394 	Validation Loss: 1.361217 	 time: 0.3
Epoch: 1033 	Training Loss: 1.169307 	Validation Loss: 1.361942 	 time: 0.3
Epoch: 1034 	Training Loss: 1.169303 	Validation Loss: 1.361793 	 time: 0.3
Epoch: 1035 	Training Loss: 1.169256 	Validation Loss: 1.361816 	 time: 0.3
Epoch: 1036 	Training Loss: 1.169193 	Validation Loss: 1.361337 	 time: 0.3
Epoch: 1037 	Training Loss: 1.169144 	Validation Loss: 1.361527 	 time: 0.3
Epoch: 1038 	Training Loss: 1.169097 	Validation Loss: 1.362910 	 time: 0.3
Epoch: 1039 	Training Loss: 1.169091 	Validation Loss: 1.363073 	 time: 0.3
Epoch: 1040 	Training Loss: 1.169084 	Validation Loss: 1.363964 	 time: 0.3
Epoch: 1041 	Training Loss: 1.169048 	Validation Loss: 1.362614 	 time: 0.3
Epoch: 1042 	Training Loss: 1.169017 	Validation Loss: 1.363832 	 time: 0.3
Epoch: 1043 	Training Loss: 1.168976 	Validation Loss: 1.363770 	 time: 0.3
Epoch: 1044 	Training Loss: 1.168926 	Validation Loss: 1.362635 	 time: 0.3
Epoch: 1045 	Training Loss: 1.168931 	Validation Loss: 1.363979 	 time: 0.3
Epoch: 1046 	Training Loss: 1.168917 	Validation Loss: 1.361440 	 time: 0.3
Epoch: 1047 	Training Loss: 1.168855 	Validation Loss: 1.362198 	 time: 0.3
Epoch: 1048 	Training Loss: 1.168783 	Validation Loss: 1.361702 	 time: 0.3
Epoch: 1049 	Training Loss: 1.168760 	Validation Loss: 1.361619 	 time: 0.3
Epoch: 1050 	Training Loss: 1.168761 	Validation Loss: 1.362741 	 time: 0.3
Epoch: 1051 	Training Loss: 1.168754 	Validation Loss: 1.361369 	 time: 0.3
Epoch: 1052 	Training Loss: 1.168755 	Validation Loss: 1.362307 	 time: 0.3
Epoch: 1053 	Training Loss: 1.168705 	Validation Loss: 1.361657 	 time: 0.3
Epoch: 1054 	Training Loss: 1.168683 	Validation Loss: 1.362068 	 time: 0.3
Epoch: 1055 	Training Loss: 1.168656 	Validation Loss: 1.362853 	 time: 0.3
Epoch: 1056 	Training Loss: 1.168645 	Validation Loss: 1.361510 	 time: 0.3
Epoch: 1057 	Training Loss: 1.168644 	Validation Loss: 1.362679 	 time: 0.3
Epoch: 1058 	Training Loss: 1.168621 	Validation Loss: 1.361506 	 time: 0.3
Epoch: 1059 	Training Loss: 1.168571 	Validation Loss: 1.362171 	 time: 0.3
Epoch: 1060 	Training Loss: 1.168528 	Validation Loss: 1.362364 	 time: 0.3
Epoch: 1061 	Training Loss: 1.168506 	Validation Loss: 1.361966 	 time: 0.3
Epoch: 1062 	Training Loss: 1.168486 	Validation Loss: 1.362466 	 time: 0.3
Epoch: 1063 	Training Loss: 1.168471 	Validation Loss: 1.361715 	 time: 0.3
Epoch: 1064 	Training Loss: 1.168431 	Validation Loss: 1.361403 	 time: 0.3
Epoch: 1065 	Training Loss: 1.168419 	Validation Loss: 1.361756 	 time: 0.3
Epoch: 1066 	Training Loss: 1.168392 	Validation Loss: 1.361225 	 time: 0.3
Epoch: 1067 	Training Loss: 1.168378 	Validation Loss: 1.362110 	 time: 0.3
Epoch: 1068 	Training Loss: 1.168372 	Validation Loss: 1.361239 	 time: 0.3
Epoch: 1069 	Training Loss: 1.168352 	Validation Loss: 1.362141 	 time: 0.3
Epoch: 1070 	Training Loss: 1.168321 	Validation Loss: 1.361795 	 time: 0.3
Epoch: 1071 	Training Loss: 1.168280 	Validation Loss: 1.361612 	 time: 0.3
Epoch: 1072 	Training Loss: 1.168281 	Validation Loss: 1.362498 	 time: 0.3
Epoch: 1073 	Training Loss: 1.168279 	Validation Loss: 1.360808 	 time: 0.3
Epoch: 1074 	Training Loss: 1.168286 	Validation Loss: 1.362552 	 time: 0.3
Epoch: 1075 	Training Loss: 1.168256 	Validation Loss: 1.361186 	 time: 0.3
Epoch: 1076 	Training Loss: 1.168139 	Validation Loss: 1.361620 	 time: 0.3
Epoch: 1077 	Training Loss: 1.168084 	Validation Loss: 1.362611 	 time: 0.3
Epoch: 1078 	Training Loss: 1.168099 	Validation Loss: 1.360929 	 time: 0.3
Epoch: 1079 	Training Loss: 1.168114 	Validation Loss: 1.362262 	 time: 0.3
Epoch: 1080 	Training Loss: 1.168064 	Validation Loss: 1.360751 	 time: 0.3
Epoch: 1081 	Training Loss: 1.168020 	Validation Loss: 1.361825 	 time: 0.3
Epoch: 1082 	Training Loss: 1.167949 	Validation Loss: 1.361008 	 time: 0.3
Epoch: 1083 	Training Loss: 1.167893 	Validation Loss: 1.361811 	 time: 0.3
Epoch: 1084 	Training Loss: 1.167863 	Validation Loss: 1.361244 	 time: 0.3
Epoch: 1085 	Training Loss: 1.167821 	Validation Loss: 1.361851 	 time: 0.3
Epoch: 1086 	Training Loss: 1.167757 	Validation Loss: 1.361428 	 time: 0.3
Epoch: 1087 	Training Loss: 1.167739 	Validation Loss: 1.361833 	 time: 0.3
Epoch: 1088 	Training Loss: 1.167696 	Validation Loss: 1.361723 	 time: 0.3
Epoch: 1089 	Training Loss: 1.167648 	Validation Loss: 1.361119 	 time: 0.3
Epoch: 1090 	Training Loss: 1.167662 	Validation Loss: 1.362412 	 time: 0.3
Epoch: 1091 	Training Loss: 1.167681 	Validation Loss: 1.360533 	 time: 0.3
Epoch: 1092 	Training Loss: 1.167682 	Validation Loss: 1.362091 	 time: 0.3
Epoch: 1093 	Training Loss: 1.167652 	Validation Loss: 1.361093 	 time: 0.3
Epoch: 1094 	Training Loss: 1.167611 	Validation Loss: 1.361668 	 time: 0.3
Epoch: 1095 	Training Loss: 1.167475 	Validation Loss: 1.361581 	 time: 0.3
Epoch: 1096 	Training Loss: 1.167386 	Validation Loss: 1.360538 	 time: 0.3
Epoch: 1097 	Training Loss: 1.167406 	Validation Loss: 1.362034 	 time: 0.3
Epoch: 1098 	Training Loss: 1.167441 	Validation Loss: 1.360396 	 time: 0.3
Epoch: 1099 	Training Loss: 1.167431 	Validation Loss: 1.362157 	 time: 0.3
Epoch: 1100 	Training Loss: 1.167327 	Validation Loss: 1.361660 	 time: 0.3
Epoch: 1101 	Training Loss: 1.167280 	Validation Loss: 1.362104 	 time: 0.3
Epoch: 1102 	Training Loss: 1.167233 	Validation Loss: 1.362453 	 time: 0.3
Epoch: 1103 	Training Loss: 1.167195 	Validation Loss: 1.361801 	 time: 0.3
Epoch: 1104 	Training Loss: 1.167191 	Validation Loss: 1.363424 	 time: 0.3
Epoch: 1105 	Training Loss: 1.167205 	Validation Loss: 1.362473 	 time: 0.3
Epoch: 1106 	Training Loss: 1.167197 	Validation Loss: 1.363913 	 time: 0.3
Epoch: 1107 	Training Loss: 1.167145 	Validation Loss: 1.362759 	 time: 0.3
Epoch: 1108 	Training Loss: 1.167134 	Validation Loss: 1.363024 	 time: 0.3
Epoch: 1109 	Training Loss: 1.167102 	Validation Loss: 1.362692 	 time: 0.3
Epoch: 1110 	Training Loss: 1.167074 	Validation Loss: 1.362610 	 time: 0.3
Epoch: 1111 	Training Loss: 1.167073 	Validation Loss: 1.362905 	 time: 0.3
Epoch: 1112 	Training Loss: 1.167061 	Validation Loss: 1.362251 	 time: 0.3
Epoch: 1113 	Training Loss: 1.167058 	Validation Loss: 1.362608 	 time: 0.3
Epoch: 1114 	Training Loss: 1.167044 	Validation Loss: 1.361818 	 time: 0.3
Epoch: 1115 	Training Loss: 1.167028 	Validation Loss: 1.361970 	 time: 0.3
Epoch: 1116 	Training Loss: 1.167020 	Validation Loss: 1.361722 	 time: 0.3
Epoch: 1117 	Training Loss: 1.166997 	Validation Loss: 1.361958 	 time: 0.3
Epoch: 1118 	Training Loss: 1.166997 	Validation Loss: 1.362394 	 time: 0.3
Epoch: 1119 	Training Loss: 1.166987 	Validation Loss: 1.361838 	 time: 0.3
Epoch: 1120 	Training Loss: 1.166976 	Validation Loss: 1.362602 	 time: 0.3
Epoch: 1121 	Training Loss: 1.166951 	Validation Loss: 1.362003 	 time: 0.3
Epoch: 1122 	Training Loss: 1.166932 	Validation Loss: 1.361950 	 time: 0.3
Epoch: 1123 	Training Loss: 1.166907 	Validation Loss: 1.361988 	 time: 0.3
Epoch: 1124 	Training Loss: 1.166884 	Validation Loss: 1.362047 	 time: 0.3
Epoch: 1125 	Training Loss: 1.166873 	Validation Loss: 1.362440 	 time: 0.3
Epoch: 1126 	Training Loss: 1.166851 	Validation Loss: 1.362309 	 time: 0.3
Epoch: 1127 	Training Loss: 1.166829 	Validation Loss: 1.362543 	 time: 0.3
Epoch: 1128 	Training Loss: 1.166801 	Validation Loss: 1.362813 	 time: 0.3
Epoch: 1129 	Training Loss: 1.166795 	Validation Loss: 1.362721 	 time: 0.3
Epoch: 1130 	Training Loss: 1.166768 	Validation Loss: 1.362790 	 time: 0.3
Epoch: 1131 	Training Loss: 1.166761 	Validation Loss: 1.363170 	 time: 0.3
Epoch: 1132 	Training Loss: 1.166741 	Validation Loss: 1.363218 	 time: 0.3
Epoch: 1133 	Training Loss: 1.166729 	Validation Loss: 1.363431 	 time: 0.3
Epoch: 1134 	Training Loss: 1.166711 	Validation Loss: 1.363364 	 time: 0.3
Epoch: 1135 	Training Loss: 1.166687 	Validation Loss: 1.363702 	 time: 0.3
Epoch: 1136 	Training Loss: 1.166673 	Validation Loss: 1.363597 	 time: 0.3
Epoch: 1137 	Training Loss: 1.166641 	Validation Loss: 1.363691 	 time: 0.3
Epoch: 1138 	Training Loss: 1.166605 	Validation Loss: 1.363741 	 time: 0.3
Epoch: 1139 	Training Loss: 1.166572 	Validation Loss: 1.364038 	 time: 0.3
Epoch: 1140 	Training Loss: 1.166552 	Validation Loss: 1.363872 	 time: 0.3
Epoch: 1141 	Training Loss: 1.166540 	Validation Loss: 1.363906 	 time: 0.3
Epoch: 1142 	Training Loss: 1.166528 	Validation Loss: 1.364094 	 time: 0.3
Epoch: 1143 	Training Loss: 1.166514 	Validation Loss: 1.364113 	 time: 0.3
Epoch: 1144 	Training Loss: 1.166494 	Validation Loss: 1.363472 	 time: 0.3
Epoch: 1145 	Training Loss: 1.166494 	Validation Loss: 1.363827 	 time: 0.3
Epoch: 1146 	Training Loss: 1.166481 	Validation Loss: 1.363004 	 time: 0.3
Epoch: 1147 	Training Loss: 1.166477 	Validation Loss: 1.363666 	 time: 0.3
Epoch: 1148 	Training Loss: 1.166467 	Validation Loss: 1.362349 	 time: 0.3
Epoch: 1149 	Training Loss: 1.166463 	Validation Loss: 1.363134 	 time: 0.3
Epoch: 1150 	Training Loss: 1.166444 	Validation Loss: 1.362285 	 time: 0.3
Epoch: 1151 	Training Loss: 1.166426 	Validation Loss: 1.362624 	 time: 0.3
Epoch: 1152 	Training Loss: 1.166405 	Validation Loss: 1.362217 	 time: 0.3
Epoch: 1153 	Training Loss: 1.166396 	Validation Loss: 1.362341 	 time: 0.3
Epoch: 1154 	Training Loss: 1.166376 	Validation Loss: 1.362209 	 time: 0.3
Epoch: 1155 	Training Loss: 1.166361 	Validation Loss: 1.362166 	 time: 0.3
Epoch: 1156 	Training Loss: 1.166330 	Validation Loss: 1.362628 	 time: 0.3
Epoch: 1157 	Training Loss: 1.166307 	Validation Loss: 1.361773 	 time: 0.3
Epoch: 1158 	Training Loss: 1.166325 	Validation Loss: 1.362883 	 time: 0.3
Epoch: 1159 	Training Loss: 1.166377 	Validation Loss: 1.361600 	 time: 0.3
Epoch: 1160 	Training Loss: 1.166384 	Validation Loss: 1.363027 	 time: 0.3
Epoch: 1161 	Training Loss: 1.166320 	Validation Loss: 1.361971 	 time: 0.3
Epoch: 1162 	Training Loss: 1.166273 	Validation Loss: 1.362531 	 time: 0.3
Epoch: 1163 	Training Loss: 1.166250 	Validation Loss: 1.362108 	 time: 0.3
Epoch: 1164 	Training Loss: 1.166196 	Validation Loss: 1.361835 	 time: 0.3
Epoch: 1165 	Training Loss: 1.166195 	Validation Loss: 1.361909 	 time: 0.3
Epoch: 1166 	Training Loss: 1.166164 	Validation Loss: 1.362126 	 time: 0.3
Epoch: 1167 	Training Loss: 1.166191 	Validation Loss: 1.362768 	 time: 0.3
Epoch: 1168 	Training Loss: 1.166142 	Validation Loss: 1.361303 	 time: 0.3
Epoch: 1169 	Training Loss: 1.166227 	Validation Loss: 1.363209 	 time: 0.3
Epoch: 1170 	Training Loss: 1.166273 	Validation Loss: 1.360267 	 time: 0.3
Epoch: 1171 	Training Loss: 1.166329 	Validation Loss: 1.363035 	 time: 0.3
Epoch: 1172 	Training Loss: 1.166269 	Validation Loss: 1.361026 	 time: 0.3
Epoch: 1173 	Training Loss: 1.166165 	Validation Loss: 1.362046 	 time: 0.3
Epoch: 1174 	Training Loss: 1.166211 	Validation Loss: 1.361712 	 time: 0.3
Epoch: 1175 	Training Loss: 1.166108 	Validation Loss: 1.360468 	 time: 0.3
Epoch: 1176 	Training Loss: 1.166166 	Validation Loss: 1.363997 	 time: 0.3
Epoch: 1177 	Training Loss: 1.166270 	Validation Loss: 1.361465 	 time: 0.3
Epoch: 1178 	Training Loss: 1.166445 	Validation Loss: 1.364356 	 time: 0.3
Epoch: 1179 	Training Loss: 1.166378 	Validation Loss: 1.359696 	 time: 0.3
Epoch: 1180 	Training Loss: 1.166475 	Validation Loss: 1.363885 	 time: 0.3
Epoch: 1181 	Training Loss: 1.166381 	Validation Loss: 1.360441 	 time: 0.3
Epoch: 1182 	Training Loss: 1.166212 	Validation Loss: 1.361651 	 time: 0.3
Epoch: 1183 	Training Loss: 1.166073 	Validation Loss: 1.364338 	 time: 0.3
Epoch: 1184 	Training Loss: 1.166205 	Validation Loss: 1.360056 	 time: 0.3
Epoch: 1185 	Training Loss: 1.166328 	Validation Loss: 1.363977 	 time: 0.3
Epoch: 1186 	Training Loss: 1.166251 	Validation Loss: 1.361141 	 time: 0.3
Epoch: 1187 	Training Loss: 1.166064 	Validation Loss: 1.362126 	 time: 0.3
Epoch: 1188 	Training Loss: 1.166056 	Validation Loss: 1.363921 	 time: 0.3
Epoch: 1189 	Training Loss: 1.166140 	Validation Loss: 1.361747 	 time: 0.3
Epoch: 1190 	Training Loss: 1.166113 	Validation Loss: 1.363747 	 time: 0.3
Epoch: 1191 	Training Loss: 1.166020 	Validation Loss: 1.362054 	 time: 0.3
Epoch: 1192 	Training Loss: 1.166035 	Validation Loss: 1.362053 	 time: 0.3
Epoch: 1193 	Training Loss: 1.165911 	Validation Loss: 1.362832 	 time: 0.3
Epoch: 1194 	Training Loss: 1.165944 	Validation Loss: 1.361656 	 time: 0.3
Epoch: 1195 	Training Loss: 1.165906 	Validation Loss: 1.363137 	 time: 0.3
Epoch: 1196 	Training Loss: 1.165853 	Validation Loss: 1.362753 	 time: 0.3
Epoch: 1197 	Training Loss: 1.165851 	Validation Loss: 1.362904 	 time: 0.3
Epoch: 1198 	Training Loss: 1.165795 	Validation Loss: 1.363620 	 time: 0.3
Epoch: 1199 	Training Loss: 1.165761 	Validation Loss: 1.363268 	 time: 0.3
Epoch: 1200 	Training Loss: 1.165785 	Validation Loss: 1.364921 	 time: 0.3
Epoch: 1201 	Training Loss: 1.165756 	Validation Loss: 1.362896 	 time: 0.3
Epoch: 1202 	Training Loss: 1.165738 	Validation Loss: 1.365399 	 time: 0.3
Epoch: 1203 	Training Loss: 1.165666 	Validation Loss: 1.363474 	 time: 0.3
Epoch: 1204 	Training Loss: 1.165667 	Validation Loss: 1.364253 	 time: 0.3
Epoch: 1205 	Training Loss: 1.165541 	Validation Loss: 1.364265 	 time: 0.3
Epoch: 1206 	Training Loss: 1.165613 	Validation Loss: 1.363050 	 time: 0.3
Epoch: 1207 	Training Loss: 1.165532 	Validation Loss: 1.363791 	 time: 0.3
Epoch: 1208 	Training Loss: 1.165547 	Validation Loss: 1.363426 	 time: 0.3
Epoch: 1209 	Training Loss: 1.165580 	Validation Loss: 1.365044 	 time: 0.3
Epoch: 1210 	Training Loss: 1.165608 	Validation Loss: 1.363232 	 time: 0.3
Epoch: 1211 	Training Loss: 1.165589 	Validation Loss: 1.364150 	 time: 0.3
Epoch: 1212 	Training Loss: 1.165566 	Validation Loss: 1.363911 	 time: 0.3
Epoch: 1213 	Training Loss: 1.165540 	Validation Loss: 1.362832 	 time: 0.3
Epoch: 1214 	Training Loss: 1.165533 	Validation Loss: 1.365268 	 time: 0.3
Epoch: 1215 	Training Loss: 1.165502 	Validation Loss: 1.362194 	 time: 0.3
Epoch: 1216 	Training Loss: 1.165495 	Validation Loss: 1.363557 	 time: 0.3
Epoch: 1217 	Training Loss: 1.165390 	Validation Loss: 1.362475 	 time: 0.3
Epoch: 1218 	Training Loss: 1.165419 	Validation Loss: 1.362866 	 time: 0.3
Epoch: 1219 	Training Loss: 1.165349 	Validation Loss: 1.363026 	 time: 0.3
Epoch: 1220 	Training Loss: 1.165348 	Validation Loss: 1.362677 	 time: 0.3
Epoch: 1221 	Training Loss: 1.165344 	Validation Loss: 1.363034 	 time: 0.3
Epoch: 1222 	Training Loss: 1.165342 	Validation Loss: 1.361938 	 time: 0.3
Epoch: 1223 	Training Loss: 1.165319 	Validation Loss: 1.363099 	 time: 0.3
Epoch: 1224 	Training Loss: 1.165308 	Validation Loss: 1.363389 	 time: 0.3
Epoch: 1225 	Training Loss: 1.165289 	Validation Loss: 1.362631 	 time: 0.3
Epoch: 1226 	Training Loss: 1.165242 	Validation Loss: 1.363591 	 time: 0.3
Epoch: 1227 	Training Loss: 1.165246 	Validation Loss: 1.362673 	 time: 0.3
Epoch: 1228 	Training Loss: 1.165197 	Validation Loss: 1.363106 	 time: 0.3
Epoch: 1229 	Training Loss: 1.165165 	Validation Loss: 1.363809 	 time: 0.3
Epoch: 1230 	Training Loss: 1.165156 	Validation Loss: 1.361995 	 time: 0.3
Epoch: 1231 	Training Loss: 1.165131 	Validation Loss: 1.363841 	 time: 0.3
Epoch: 1232 	Training Loss: 1.165099 	Validation Loss: 1.362347 	 time: 0.3
Epoch: 1233 	Training Loss: 1.165078 	Validation Loss: 1.363350 	 time: 0.3
Epoch: 1234 	Training Loss: 1.165061 	Validation Loss: 1.362616 	 time: 0.3
Epoch: 1235 	Training Loss: 1.165029 	Validation Loss: 1.362636 	 time: 0.3
Epoch: 1236 	Training Loss: 1.165000 	Validation Loss: 1.362971 	 time: 0.3
Epoch: 1237 	Training Loss: 1.164975 	Validation Loss: 1.362046 	 time: 0.3
Epoch: 1238 	Training Loss: 1.164951 	Validation Loss: 1.363189 	 time: 0.3
Epoch: 1239 	Training Loss: 1.164943 	Validation Loss: 1.361180 	 time: 0.3
Epoch: 1240 	Training Loss: 1.164961 	Validation Loss: 1.363055 	 time: 0.3
Epoch: 1241 	Training Loss: 1.164902 	Validation Loss: 1.361335 	 time: 0.3
Epoch: 1242 	Training Loss: 1.164865 	Validation Loss: 1.362244 	 time: 0.3
Epoch: 1243 	Training Loss: 1.164843 	Validation Loss: 1.362240 	 time: 0.3
Epoch: 1244 	Training Loss: 1.164821 	Validation Loss: 1.362842 	 time: 0.3
Epoch: 1245 	Training Loss: 1.164802 	Validation Loss: 1.362600 	 time: 0.3
Epoch: 1246 	Training Loss: 1.164794 	Validation Loss: 1.363223 	 time: 0.3
Epoch: 1247 	Training Loss: 1.164758 	Validation Loss: 1.362801 	 time: 0.3
Epoch: 1248 	Training Loss: 1.164730 	Validation Loss: 1.363015 	 time: 0.3
Epoch: 1249 	Training Loss: 1.164717 	Validation Loss: 1.362167 	 time: 0.3
Epoch: 1250 	Training Loss: 1.164696 	Validation Loss: 1.364291 	 time: 0.3
Epoch: 1251 	Training Loss: 1.164722 	Validation Loss: 1.361627 	 time: 0.3
Epoch: 1252 	Training Loss: 1.164747 	Validation Loss: 1.364319 	 time: 0.3
Epoch: 1253 	Training Loss: 1.164713 	Validation Loss: 1.362158 	 time: 0.3
Epoch: 1254 	Training Loss: 1.164701 	Validation Loss: 1.363540 	 time: 0.3
Epoch: 1255 	Training Loss: 1.164661 	Validation Loss: 1.362153 	 time: 0.3
Epoch: 1256 	Training Loss: 1.164594 	Validation Loss: 1.362342 	 time: 0.3
Epoch: 1257 	Training Loss: 1.164596 	Validation Loss: 1.363252 	 time: 0.3
Epoch: 1258 	Training Loss: 1.164608 	Validation Loss: 1.362027 	 time: 0.3
Epoch: 1259 	Training Loss: 1.164648 	Validation Loss: 1.365433 	 time: 0.3
Epoch: 1260 	Training Loss: 1.164713 	Validation Loss: 1.361941 	 time: 0.3
Epoch: 1261 	Training Loss: 1.164785 	Validation Loss: 1.365583 	 time: 0.3
Epoch: 1262 	Training Loss: 1.164727 	Validation Loss: 1.362805 	 time: 0.3
Epoch: 1263 	Training Loss: 1.164607 	Validation Loss: 1.363085 	 time: 0.3
Epoch: 1264 	Training Loss: 1.164573 	Validation Loss: 1.365364 	 time: 0.3
Epoch: 1265 	Training Loss: 1.164632 	Validation Loss: 1.361291 	 time: 0.3
Epoch: 1266 	Training Loss: 1.164793 	Validation Loss: 1.368050 	 time: 0.3
Epoch: 1267 	Training Loss: 1.165274 	Validation Loss: 1.364771 	 time: 0.3
Epoch: 1268 	Training Loss: 1.166538 	Validation Loss: 1.388004 	 time: 0.3
Epoch: 1269 	Training Loss: 1.170614 	Validation Loss: 1.391983 	 time: 0.3
Epoch: 1270 	Training Loss: 1.214341 	Validation Loss: 1.565291 	 time: 0.3
Epoch: 1271 	Training Loss: 1.369822 	Validation Loss: 1.445051 	 time: 0.3
Epoch: 1272 	Training Loss: 1.246205 	Validation Loss: 1.413937 	 time: 0.3
Epoch: 1273 	Training Loss: 1.272964 	Validation Loss: 1.405446 	 time: 0.3
Epoch: 1274 	Training Loss: 1.259859 	Validation Loss: 1.418477 	 time: 0.3
Epoch: 1275 	Training Loss: 1.225802 	Validation Loss: 1.464329 	 time: 0.3
Epoch: 1276 	Training Loss: 1.265801 	Validation Loss: 1.431269 	 time: 0.3
Epoch: 1277 	Training Loss: 1.240956 	Validation Loss: 1.407870 	 time: 0.3
Epoch: 1278 	Training Loss: 1.228435 	Validation Loss: 1.405529 	 time: 0.3
Epoch: 1279 	Training Loss: 1.229854 	Validation Loss: 1.413734 	 time: 0.3
Epoch: 1280 	Training Loss: 1.219052 	Validation Loss: 1.427613 	 time: 0.3
Epoch: 1281 	Training Loss: 1.214920 	Validation Loss: 1.414876 	 time: 0.3
Epoch: 1282 	Training Loss: 1.213288 	Validation Loss: 1.409400 	 time: 0.3
Epoch: 1283 	Training Loss: 1.206748 	Validation Loss: 1.407126 	 time: 0.3
Epoch: 1284 	Training Loss: 1.203637 	Validation Loss: 1.405252 	 time: 0.3
Epoch: 1285 	Training Loss: 1.201295 	Validation Loss: 1.408509 	 time: 0.3
Epoch: 1286 	Training Loss: 1.194864 	Validation Loss: 1.409058 	 time: 0.3
Epoch: 1287 	Training Loss: 1.191690 	Validation Loss: 1.413102 	 time: 0.3
Epoch: 1288 	Training Loss: 1.189999 	Validation Loss: 1.407918 	 time: 0.3
Epoch: 1289 	Training Loss: 1.189646 	Validation Loss: 1.412341 	 time: 0.3
Epoch: 1290 	Training Loss: 1.185651 	Validation Loss: 1.412052 	 time: 0.3
Epoch: 1291 	Training Loss: 1.184115 	Validation Loss: 1.401049 	 time: 0.3
Epoch: 1292 	Training Loss: 1.182061 	Validation Loss: 1.402666 	 time: 0.3
Epoch: 1293 	Training Loss: 1.180835 	Validation Loss: 1.403545 	 time: 0.3
Epoch: 1294 	Training Loss: 1.178085 	Validation Loss: 1.401597 	 time: 0.3
Epoch: 1295 	Training Loss: 1.177001 	Validation Loss: 1.395511 	 time: 0.3
Epoch: 1296 	Training Loss: 1.175689 	Validation Loss: 1.388213 	 time: 0.3
Epoch: 1297 	Training Loss: 1.174825 	Validation Loss: 1.385093 	 time: 0.3
Epoch: 1298 	Training Loss: 1.173565 	Validation Loss: 1.387440 	 time: 0.3
Epoch: 1299 	Training Loss: 1.173438 	Validation Loss: 1.385883 	 time: 0.3
Epoch: 1300 	Training Loss: 1.172736 	Validation Loss: 1.385785 	 time: 0.3
Epoch: 1301 	Training Loss: 1.172046 	Validation Loss: 1.389319 	 time: 0.3
Epoch: 1302 	Training Loss: 1.171407 	Validation Loss: 1.388784 	 time: 0.3
Epoch: 1303 	Training Loss: 1.170996 	Validation Loss: 1.384157 	 time: 0.3
Epoch: 1304 	Training Loss: 1.170399 	Validation Loss: 1.383453 	 time: 0.3
Epoch: 1305 	Training Loss: 1.170079 	Validation Loss: 1.377919 	 time: 0.3
Epoch: 1306 	Training Loss: 1.169538 	Validation Loss: 1.373674 	 time: 0.3
Epoch: 1307 	Training Loss: 1.168997 	Validation Loss: 1.373663 	 time: 0.3
Epoch: 1308 	Training Loss: 1.168764 	Validation Loss: 1.374432 	 time: 0.3
Epoch: 1309 	Training Loss: 1.168369 	Validation Loss: 1.373190 	 time: 0.3
Epoch: 1310 	Training Loss: 1.168008 	Validation Loss: 1.371649 	 time: 0.3
Epoch: 1311 	Training Loss: 1.167878 	Validation Loss: 1.371232 	 time: 0.3
Epoch: 1312 	Training Loss: 1.167526 	Validation Loss: 1.370686 	 time: 0.3
Epoch: 1313 	Training Loss: 1.167243 	Validation Loss: 1.368903 	 time: 0.3
Epoch: 1314 	Training Loss: 1.166990 	Validation Loss: 1.368061 	 time: 0.3
Epoch: 1315 	Training Loss: 1.166718 	Validation Loss: 1.368109 	 time: 0.3
Epoch: 1316 	Training Loss: 1.166487 	Validation Loss: 1.367260 	 time: 0.3
Epoch: 1317 	Training Loss: 1.166201 	Validation Loss: 1.367053 	 time: 0.3
Epoch: 1318 	Training Loss: 1.165977 	Validation Loss: 1.366002 	 time: 0.3
Epoch: 1319 	Training Loss: 1.165746 	Validation Loss: 1.365592 	 time: 0.3
Epoch: 1320 	Training Loss: 1.165600 	Validation Loss: 1.366284 	 time: 0.3
Epoch: 1321 	Training Loss: 1.165401 	Validation Loss: 1.367055 	 time: 0.3
Epoch: 1322 	Training Loss: 1.165267 	Validation Loss: 1.367406 	 time: 0.3
Epoch: 1323 	Training Loss: 1.165125 	Validation Loss: 1.368451 	 time: 0.3
Epoch: 1324 	Training Loss: 1.164977 	Validation Loss: 1.369802 	 time: 0.3
Epoch: 1325 	Training Loss: 1.164876 	Validation Loss: 1.369726 	 time: 0.3
Epoch: 1326 	Training Loss: 1.164749 	Validation Loss: 1.369405 	 time: 0.3
Epoch: 1327 	Training Loss: 1.164649 	Validation Loss: 1.369474 	 time: 0.3
Epoch: 1328 	Training Loss: 1.164552 	Validation Loss: 1.369586 	 time: 0.3
Epoch: 1329 	Training Loss: 1.164461 	Validation Loss: 1.370117 	 time: 0.3
Epoch: 1330 	Training Loss: 1.164363 	Validation Loss: 1.370489 	 time: 0.3
Epoch: 1331 	Training Loss: 1.164210 	Validation Loss: 1.370133 	 time: 0.3
Epoch: 1332 	Training Loss: 1.164083 	Validation Loss: 1.370020 	 time: 0.3
Epoch: 1333 	Training Loss: 1.163991 	Validation Loss: 1.370201 	 time: 0.3
Epoch: 1334 	Training Loss: 1.163935 	Validation Loss: 1.370140 	 time: 0.3
Epoch: 1335 	Training Loss: 1.163878 	Validation Loss: 1.370015 	 time: 0.3
Epoch: 1336 	Training Loss: 1.163830 	Validation Loss: 1.370066 	 time: 0.3
Epoch: 1337 	Training Loss: 1.163787 	Validation Loss: 1.370121 	 time: 0.3
Epoch: 1338 	Training Loss: 1.163732 	Validation Loss: 1.370010 	 time: 0.3
Epoch: 1339 	Training Loss: 1.163673 	Validation Loss: 1.370016 	 time: 0.3
Epoch: 1340 	Training Loss: 1.163591 	Validation Loss: 1.370133 	 time: 0.3
Epoch: 1341 	Training Loss: 1.163535 	Validation Loss: 1.369584 	 time: 0.3
Epoch: 1342 	Training Loss: 1.163471 	Validation Loss: 1.369227 	 time: 0.3
Epoch: 1343 	Training Loss: 1.163420 	Validation Loss: 1.369250 	 time: 0.3
Epoch: 1344 	Training Loss: 1.163374 	Validation Loss: 1.368844 	 time: 0.3
Epoch: 1345 	Training Loss: 1.163324 	Validation Loss: 1.367865 	 time: 0.3
Epoch: 1346 	Training Loss: 1.163249 	Validation Loss: 1.367380 	 time: 0.3
Epoch: 1347 	Training Loss: 1.163164 	Validation Loss: 1.366879 	 time: 0.3
Epoch: 1348 	Training Loss: 1.163120 	Validation Loss: 1.366310 	 time: 0.3
Epoch: 1349 	Training Loss: 1.163064 	Validation Loss: 1.366697 	 time: 0.3
Epoch: 1350 	Training Loss: 1.163013 	Validation Loss: 1.367104 	 time: 0.3
Epoch: 1351 	Training Loss: 1.162968 	Validation Loss: 1.366734 	 time: 0.3
Epoch: 1352 	Training Loss: 1.162913 	Validation Loss: 1.366660 	 time: 0.3
Epoch: 1353 	Training Loss: 1.162859 	Validation Loss: 1.367094 	 time: 0.3
Epoch: 1354 	Training Loss: 1.162817 	Validation Loss: 1.366971 	 time: 0.3
Epoch: 1355 	Training Loss: 1.162786 	Validation Loss: 1.366623 	 time: 0.3
Epoch: 1356 	Training Loss: 1.162756 	Validation Loss: 1.366841 	 time: 0.3
Epoch: 1357 	Training Loss: 1.162724 	Validation Loss: 1.366984 	 time: 0.3
Epoch: 1358 	Training Loss: 1.162684 	Validation Loss: 1.366737 	 time: 0.3
Epoch: 1359 	Training Loss: 1.162638 	Validation Loss: 1.366902 	 time: 0.3
Epoch: 1360 	Training Loss: 1.162587 	Validation Loss: 1.367101 	 time: 0.3
Epoch: 1361 	Training Loss: 1.162553 	Validation Loss: 1.367051 	 time: 0.3
Epoch: 1362 	Training Loss: 1.162529 	Validation Loss: 1.367301 	 time: 0.3
Epoch: 1363 	Training Loss: 1.162513 	Validation Loss: 1.367788 	 time: 0.3
Epoch: 1364 	Training Loss: 1.162488 	Validation Loss: 1.367745 	 time: 0.3
Epoch: 1365 	Training Loss: 1.162457 	Validation Loss: 1.367771 	 time: 0.3
Epoch: 1366 	Training Loss: 1.162438 	Validation Loss: 1.368304 	 time: 0.3
Epoch: 1367 	Training Loss: 1.162420 	Validation Loss: 1.368482 	 time: 0.3
Epoch: 1368 	Training Loss: 1.162405 	Validation Loss: 1.368226 	 time: 0.3
Epoch: 1369 	Training Loss: 1.162382 	Validation Loss: 1.368366 	 time: 0.3
Epoch: 1370 	Training Loss: 1.162360 	Validation Loss: 1.368682 	 time: 0.3
Epoch: 1371 	Training Loss: 1.162345 	Validation Loss: 1.368556 	 time: 0.3
Epoch: 1372 	Training Loss: 1.162331 	Validation Loss: 1.368485 	 time: 0.3
Epoch: 1373 	Training Loss: 1.162311 	Validation Loss: 1.368639 	 time: 0.3
Epoch: 1374 	Training Loss: 1.162266 	Validation Loss: 1.368655 	 time: 0.3
Epoch: 1375 	Training Loss: 1.162237 	Validation Loss: 1.368725 	 time: 0.3
Epoch: 1376 	Training Loss: 1.162217 	Validation Loss: 1.369183 	 time: 0.3
Epoch: 1377 	Training Loss: 1.162191 	Validation Loss: 1.369439 	 time: 0.3
Epoch: 1378 	Training Loss: 1.162164 	Validation Loss: 1.369679 	 time: 0.3
Epoch: 1379 	Training Loss: 1.162140 	Validation Loss: 1.370106 	 time: 0.3
Epoch: 1380 	Training Loss: 1.162122 	Validation Loss: 1.370222 	 time: 0.3
Epoch: 1381 	Training Loss: 1.162109 	Validation Loss: 1.370190 	 time: 0.3
Epoch: 1382 	Training Loss: 1.162101 	Validation Loss: 1.370400 	 time: 0.3
Epoch: 1383 	Training Loss: 1.162091 	Validation Loss: 1.370466 	 time: 0.3
Epoch: 1384 	Training Loss: 1.162082 	Validation Loss: 1.370267 	 time: 0.3
Epoch: 1385 	Training Loss: 1.162074 	Validation Loss: 1.370237 	 time: 0.3
Epoch: 1386 	Training Loss: 1.162062 	Validation Loss: 1.370228 	 time: 0.3
Epoch: 1387 	Training Loss: 1.162045 	Validation Loss: 1.370017 	 time: 0.3
Epoch: 1388 	Training Loss: 1.162005 	Validation Loss: 1.369953 	 time: 0.3
Epoch: 1389 	Training Loss: 1.161990 	Validation Loss: 1.369850 	 time: 0.3
Epoch: 1390 	Training Loss: 1.161979 	Validation Loss: 1.369637 	 time: 0.3
Epoch: 1391 	Training Loss: 1.161970 	Validation Loss: 1.369596 	 time: 0.3
Epoch: 1392 	Training Loss: 1.161960 	Validation Loss: 1.369556 	 time: 0.3
Epoch: 1393 	Training Loss: 1.161951 	Validation Loss: 1.369433 	 time: 0.3
Epoch: 1394 	Training Loss: 1.161942 	Validation Loss: 1.369464 	 time: 0.3
Epoch: 1395 	Training Loss: 1.161932 	Validation Loss: 1.369474 	 time: 0.3
Epoch: 1396 	Training Loss: 1.161921 	Validation Loss: 1.369349 	 time: 0.3
Epoch: 1397 	Training Loss: 1.161905 	Validation Loss: 1.369285 	 time: 0.3
Epoch: 1398 	Training Loss: 1.161873 	Validation Loss: 1.369145 	 time: 0.3
Epoch: 1399 	Training Loss: 1.161840 	Validation Loss: 1.369189 	 time: 0.3
Epoch: 1400 	Training Loss: 1.161832 	Validation Loss: 1.369154 	 time: 0.3
Epoch: 1401 	Training Loss: 1.161808 	Validation Loss: 1.369213 	 time: 0.3
Epoch: 1402 	Training Loss: 1.161786 	Validation Loss: 1.369611 	 time: 0.3
Epoch: 1403 	Training Loss: 1.161773 	Validation Loss: 1.369845 	 time: 0.3
Epoch: 1404 	Training Loss: 1.161763 	Validation Loss: 1.370061 	 time: 0.3
Epoch: 1405 	Training Loss: 1.161756 	Validation Loss: 1.370451 	 time: 0.3
Epoch: 1406 	Training Loss: 1.161750 	Validation Loss: 1.370606 	 time: 0.3
Epoch: 1407 	Training Loss: 1.161742 	Validation Loss: 1.370713 	 time: 0.3
Epoch: 1408 	Training Loss: 1.161735 	Validation Loss: 1.370800 	 time: 0.3
Epoch: 1409 	Training Loss: 1.161727 	Validation Loss: 1.370582 	 time: 0.3
Epoch: 1410 	Training Loss: 1.161719 	Validation Loss: 1.370528 	 time: 0.3
Epoch: 1411 	Training Loss: 1.161711 	Validation Loss: 1.370656 	 time: 0.3
Epoch: 1412 	Training Loss: 1.161702 	Validation Loss: 1.370584 	 time: 0.3
Epoch: 1413 	Training Loss: 1.161694 	Validation Loss: 1.370593 	 time: 0.3
Epoch: 1414 	Training Loss: 1.161687 	Validation Loss: 1.370614 	 time: 0.3
Epoch: 1415 	Training Loss: 1.161679 	Validation Loss: 1.370435 	 time: 0.3
Epoch: 1416 	Training Loss: 1.161673 	Validation Loss: 1.370374 	 time: 0.3
Epoch: 1417 	Training Loss: 1.161665 	Validation Loss: 1.370339 	 time: 0.3
Epoch: 1418 	Training Loss: 1.161658 	Validation Loss: 1.370249 	 time: 0.3
Epoch: 1419 	Training Loss: 1.161651 	Validation Loss: 1.370169 	 time: 0.3
Epoch: 1420 	Training Loss: 1.161644 	Validation Loss: 1.370006 	 time: 0.3
Epoch: 1421 	Training Loss: 1.161637 	Validation Loss: 1.369961 	 time: 0.3
Epoch: 1422 	Training Loss: 1.161630 	Validation Loss: 1.370074 	 time: 0.3
Epoch: 1423 	Training Loss: 1.161622 	Validation Loss: 1.370105 	 time: 0.3
Epoch: 1424 	Training Loss: 1.161615 	Validation Loss: 1.370219 	 time: 0.3
Epoch: 1425 	Training Loss: 1.161607 	Validation Loss: 1.370362 	 time: 0.3
Epoch: 1426 	Training Loss: 1.161599 	Validation Loss: 1.370358 	 time: 0.3
Epoch: 1427 	Training Loss: 1.161593 	Validation Loss: 1.370385 	 time: 0.3
Epoch: 1428 	Training Loss: 1.161588 	Validation Loss: 1.370417 	 time: 0.3
Epoch: 1429 	Training Loss: 1.161581 	Validation Loss: 1.370431 	 time: 0.3
Epoch: 1430 	Training Loss: 1.161575 	Validation Loss: 1.370413 	 time: 0.3
Epoch: 1431 	Training Loss: 1.161569 	Validation Loss: 1.370364 	 time: 0.3
Epoch: 1432 	Training Loss: 1.161564 	Validation Loss: 1.370391 	 time: 0.3
Epoch: 1433 	Training Loss: 1.161558 	Validation Loss: 1.370382 	 time: 0.3
Epoch: 1434 	Training Loss: 1.161551 	Validation Loss: 1.370371 	 time: 0.3
Epoch: 1435 	Training Loss: 1.161546 	Validation Loss: 1.370407 	 time: 0.3
Epoch: 1436 	Training Loss: 1.161541 	Validation Loss: 1.370422 	 time: 0.3
Epoch: 1437 	Training Loss: 1.161535 	Validation Loss: 1.370466 	 time: 0.3
Epoch: 1438 	Training Loss: 1.161530 	Validation Loss: 1.370525 	 time: 0.3
Epoch: 1439 	Training Loss: 1.161525 	Validation Loss: 1.370620 	 time: 0.3
Epoch: 1440 	Training Loss: 1.161520 	Validation Loss: 1.370636 	 time: 0.3
Epoch: 1441 	Training Loss: 1.161514 	Validation Loss: 1.370636 	 time: 0.3
Epoch: 1442 	Training Loss: 1.161509 	Validation Loss: 1.370668 	 time: 0.3
Epoch: 1443 	Training Loss: 1.161504 	Validation Loss: 1.370696 	 time: 0.3
Epoch: 1444 	Training Loss: 1.161498 	Validation Loss: 1.370770 	 time: 0.3
Epoch: 1445 	Training Loss: 1.161493 	Validation Loss: 1.370866 	 time: 0.3
Epoch: 1446 	Training Loss: 1.161488 	Validation Loss: 1.371020 	 time: 0.3
Epoch: 1447 	Training Loss: 1.161482 	Validation Loss: 1.371073 	 time: 0.3
Epoch: 1448 	Training Loss: 1.161476 	Validation Loss: 1.371309 	 time: 0.3
Epoch: 1449 	Training Loss: 1.161470 	Validation Loss: 1.371244 	 time: 0.3
Epoch: 1450 	Training Loss: 1.161466 	Validation Loss: 1.371672 	 time: 0.3
Epoch: 1451 	Training Loss: 1.161466 	Validation Loss: 1.370945 	 time: 0.3
Epoch: 1452 	Training Loss: 1.161476 	Validation Loss: 1.372336 	 time: 0.3
Epoch: 1453 	Training Loss: 1.161508 	Validation Loss: 1.370264 	 time: 0.3
Epoch: 1454 	Training Loss: 1.161499 	Validation Loss: 1.372148 	 time: 0.3
Epoch: 1455 	Training Loss: 1.161461 	Validation Loss: 1.371129 	 time: 0.3
Epoch: 1456 	Training Loss: 1.161435 	Validation Loss: 1.370382 	 time: 0.3
Epoch: 1457 	Training Loss: 1.161443 	Validation Loss: 1.371996 	 time: 0.3
Epoch: 1458 	Training Loss: 1.161460 	Validation Loss: 1.370002 	 time: 0.3
Epoch: 1459 	Training Loss: 1.161407 	Validation Loss: 1.370641 	 time: 0.3
Epoch: 1460 	Training Loss: 1.161359 	Validation Loss: 1.371742 	 time: 0.3
Epoch: 1461 	Training Loss: 1.161403 	Validation Loss: 1.370090 	 time: 0.3
Epoch: 1462 	Training Loss: 1.161379 	Validation Loss: 1.370881 	 time: 0.3
Epoch: 1463 	Training Loss: 1.161346 	Validation Loss: 1.370453 	 time: 0.3
Epoch: 1464 	Training Loss: 1.161339 	Validation Loss: 1.370293 	 time: 0.3
Epoch: 1465 	Training Loss: 1.161324 	Validation Loss: 1.371861 	 time: 0.3
Epoch: 1466 	Training Loss: 1.161303 	Validation Loss: 1.370327 	 time: 0.3
Epoch: 1467 	Training Loss: 1.161302 	Validation Loss: 1.370647 	 time: 0.3
Epoch: 1468 	Training Loss: 1.161293 	Validation Loss: 1.372031 	 time: 0.3
Epoch: 1469 	Training Loss: 1.161301 	Validation Loss: 1.369135 	 time: 0.3
Epoch: 1470 	Training Loss: 1.161298 	Validation Loss: 1.370367 	 time: 0.3
Epoch: 1471 	Training Loss: 1.161266 	Validation Loss: 1.371301 	 time: 0.3
Epoch: 1472 	Training Loss: 1.161272 	Validation Loss: 1.369205 	 time: 0.3
Epoch: 1473 	Training Loss: 1.161273 	Validation Loss: 1.370491 	 time: 0.3
Epoch: 1474 	Training Loss: 1.161246 	Validation Loss: 1.369530 	 time: 0.3
Epoch: 1475 	Training Loss: 1.161226 	Validation Loss: 1.368313 	 time: 0.3
Epoch: 1476 	Training Loss: 1.161227 	Validation Loss: 1.369653 	 time: 0.3
Epoch: 1477 	Training Loss: 1.161227 	Validation Loss: 1.368002 	 time: 0.3
Epoch: 1478 	Training Loss: 1.161207 	Validation Loss: 1.367926 	 time: 0.3
Epoch: 1479 	Training Loss: 1.161197 	Validation Loss: 1.368954 	 time: 0.3
Epoch: 1480 	Training Loss: 1.161204 	Validation Loss: 1.367897 	 time: 0.3
Epoch: 1481 	Training Loss: 1.161197 	Validation Loss: 1.368551 	 time: 0.3
Epoch: 1482 	Training Loss: 1.161187 	Validation Loss: 1.368264 	 time: 0.3
Epoch: 1483 	Training Loss: 1.161178 	Validation Loss: 1.367724 	 time: 0.3
Epoch: 1484 	Training Loss: 1.161173 	Validation Loss: 1.368664 	 time: 0.3
Epoch: 1485 	Training Loss: 1.161167 	Validation Loss: 1.367728 	 time: 0.3
Epoch: 1486 	Training Loss: 1.161155 	Validation Loss: 1.367877 	 time: 0.3
Epoch: 1487 	Training Loss: 1.161151 	Validation Loss: 1.369043 	 time: 0.3
Epoch: 1488 	Training Loss: 1.161154 	Validation Loss: 1.368022 	 time: 0.3
Epoch: 1489 	Training Loss: 1.161150 	Validation Loss: 1.368470 	 time: 0.3
Epoch: 1490 	Training Loss: 1.161142 	Validation Loss: 1.368759 	 time: 0.3
Epoch: 1491 	Training Loss: 1.161136 	Validation Loss: 1.368079 	 time: 0.3
Epoch: 1492 	Training Loss: 1.161150 	Validation Loss: 1.369016 	 time: 0.3
Epoch: 1493 	Training Loss: 1.161146 	Validation Loss: 1.367854 	 time: 0.3
Epoch: 1494 	Training Loss: 1.161127 	Validation Loss: 1.368353 	 time: 0.3
Epoch: 1495 	Training Loss: 1.161117 	Validation Loss: 1.369757 	 time: 0.3
Epoch: 1496 	Training Loss: 1.161134 	Validation Loss: 1.367140 	 time: 0.3
Epoch: 1497 	Training Loss: 1.161127 	Validation Loss: 1.368445 	 time: 0.3
Epoch: 1498 	Training Loss: 1.161092 	Validation Loss: 1.369441 	 time: 0.3
Epoch: 1499 	Training Loss: 1.161098 	Validation Loss: 1.367708 	 time: 0.3
Epoch: 1500 	Training Loss: 1.161115 	Validation Loss: 1.369377 	 time: 0.3
Epoch: 1501 	Training Loss: 1.161121 	Validation Loss: 1.368678 	 time: 0.3
Epoch: 1502 	Training Loss: 1.161079 	Validation Loss: 1.368936 	 time: 0.3
Epoch: 1503 	Training Loss: 1.161115 	Validation Loss: 1.371040 	 time: 0.3
Epoch: 1504 	Training Loss: 1.161112 	Validation Loss: 1.368275 	 time: 0.3
Epoch: 1505 	Training Loss: 1.161094 	Validation Loss: 1.369381 	 time: 0.3
Epoch: 1506 	Training Loss: 1.161062 	Validation Loss: 1.371591 	 time: 0.3
Epoch: 1507 	Training Loss: 1.161100 	Validation Loss: 1.368734 	 time: 0.3
Epoch: 1508 	Training Loss: 1.161086 	Validation Loss: 1.370036 	 time: 0.3
Epoch: 1509 	Training Loss: 1.161068 	Validation Loss: 1.370791 	 time: 0.3
Epoch: 1510 	Training Loss: 1.161062 	Validation Loss: 1.369361 	 time: 0.3
Epoch: 1511 	Training Loss: 1.161072 	Validation Loss: 1.371034 	 time: 0.3
Epoch: 1512 	Training Loss: 1.161046 	Validation Loss: 1.369931 	 time: 0.3
Epoch: 1513 	Training Loss: 1.161038 	Validation Loss: 1.368922 	 time: 0.3
Epoch: 1514 	Training Loss: 1.161030 	Validation Loss: 1.371086 	 time: 0.3
Epoch: 1515 	Training Loss: 1.161031 	Validation Loss: 1.369511 	 time: 0.3
Epoch: 1516 	Training Loss: 1.160984 	Validation Loss: 1.369006 	 time: 0.3
Epoch: 1517 	Training Loss: 1.160979 	Validation Loss: 1.370630 	 time: 0.3
Epoch: 1518 	Training Loss: 1.160981 	Validation Loss: 1.369163 	 time: 0.3
Epoch: 1519 	Training Loss: 1.160969 	Validation Loss: 1.370017 	 time: 0.3
Epoch: 1520 	Training Loss: 1.160955 	Validation Loss: 1.370649 	 time: 0.3
Epoch: 1521 	Training Loss: 1.160922 	Validation Loss: 1.369924 	 time: 0.3
Epoch: 1522 	Training Loss: 1.160916 	Validation Loss: 1.370249 	 time: 0.3
Epoch: 1523 	Training Loss: 1.160891 	Validation Loss: 1.369609 	 time: 0.3
Epoch: 1524 	Training Loss: 1.160887 	Validation Loss: 1.369989 	 time: 0.3
Epoch: 1525 	Training Loss: 1.160869 	Validation Loss: 1.370253 	 time: 0.3
Epoch: 1526 	Training Loss: 1.160879 	Validation Loss: 1.368568 	 time: 0.3
Epoch: 1527 	Training Loss: 1.160874 	Validation Loss: 1.369515 	 time: 0.3
Epoch: 1528 	Training Loss: 1.160874 	Validation Loss: 1.368353 	 time: 0.3
Epoch: 1529 	Training Loss: 1.160854 	Validation Loss: 1.368423 	 time: 0.3
Epoch: 1530 	Training Loss: 1.160858 	Validation Loss: 1.369555 	 time: 0.3
Epoch: 1531 	Training Loss: 1.160864 	Validation Loss: 1.367563 	 time: 0.3
Epoch: 1532 	Training Loss: 1.160858 	Validation Loss: 1.368820 	 time: 0.3
Epoch: 1533 	Training Loss: 1.160838 	Validation Loss: 1.369261 	 time: 0.3
Epoch: 1534 	Training Loss: 1.160832 	Validation Loss: 1.367918 	 time: 0.3
Epoch: 1535 	Training Loss: 1.160834 	Validation Loss: 1.369177 	 time: 0.3
Epoch: 1536 	Training Loss: 1.160828 	Validation Loss: 1.368671 	 time: 0.3
Epoch: 1537 	Training Loss: 1.160826 	Validation Loss: 1.368898 	 time: 0.3
Epoch: 1538 	Training Loss: 1.160805 	Validation Loss: 1.368493 	 time: 0.3
Epoch: 1539 	Training Loss: 1.160801 	Validation Loss: 1.369035 	 time: 0.3
Epoch: 1540 	Training Loss: 1.160800 	Validation Loss: 1.369387 	 time: 0.3
Epoch: 1541 	Training Loss: 1.160802 	Validation Loss: 1.367785 	 time: 0.3
Epoch: 1542 	Training Loss: 1.160808 	Validation Loss: 1.369766 	 time: 0.3
Epoch: 1543 	Training Loss: 1.160791 	Validation Loss: 1.368817 	 time: 0.3
Epoch: 1544 	Training Loss: 1.160777 	Validation Loss: 1.368905 	 time: 0.3
Epoch: 1545 	Training Loss: 1.160764 	Validation Loss: 1.369114 	 time: 0.3
Epoch: 1546 	Training Loss: 1.160732 	Validation Loss: 1.369282 	 time: 0.3
Epoch: 1547 	Training Loss: 1.160738 	Validation Loss: 1.369220 	 time: 0.3
Epoch: 1548 	Training Loss: 1.160733 	Validation Loss: 1.368261 	 time: 0.3
Epoch: 1549 	Training Loss: 1.160746 	Validation Loss: 1.370530 	 time: 0.3
Epoch: 1550 	Training Loss: 1.160757 	Validation Loss: 1.368829 	 time: 0.3
Epoch: 1551 	Training Loss: 1.160762 	Validation Loss: 1.369997 	 time: 0.3
Epoch: 1552 	Training Loss: 1.160763 	Validation Loss: 1.369647 	 time: 0.3
Epoch: 1553 	Training Loss: 1.160711 	Validation Loss: 1.369525 	 time: 0.3
Epoch: 1554 	Training Loss: 1.160742 	Validation Loss: 1.370313 	 time: 0.3
Epoch: 1555 	Training Loss: 1.160740 	Validation Loss: 1.368096 	 time: 0.3
Epoch: 1556 	Training Loss: 1.160746 	Validation Loss: 1.370375 	 time: 0.3
Epoch: 1557 	Training Loss: 1.160719 	Validation Loss: 1.371428 	 time: 0.3
Epoch: 1558 	Training Loss: 1.160720 	Validation Loss: 1.367969 	 time: 0.3
Epoch: 1559 	Training Loss: 1.160768 	Validation Loss: 1.370327 	 time: 0.3
Epoch: 1560 	Training Loss: 1.160729 	Validation Loss: 1.369196 	 time: 0.3
Epoch: 1561 	Training Loss: 1.160702 	Validation Loss: 1.369301 	 time: 0.3
Epoch: 1562 	Training Loss: 1.160688 	Validation Loss: 1.370694 	 time: 0.3
Epoch: 1563 	Training Loss: 1.160689 	Validation Loss: 1.369214 	 time: 0.3
Epoch: 1564 	Training Loss: 1.160670 	Validation Loss: 1.368774 	 time: 0.3
Epoch: 1565 	Training Loss: 1.160652 	Validation Loss: 1.370394 	 time: 0.3
Epoch: 1566 	Training Loss: 1.160676 	Validation Loss: 1.367625 	 time: 0.3
Epoch: 1567 	Training Loss: 1.160653 	Validation Loss: 1.369408 	 time: 0.3
Epoch: 1568 	Training Loss: 1.160632 	Validation Loss: 1.368400 	 time: 0.3
Epoch: 1569 	Training Loss: 1.160597 	Validation Loss: 1.367465 	 time: 0.3
Epoch: 1570 	Training Loss: 1.160601 	Validation Loss: 1.367545 	 time: 0.3
Epoch: 1571 	Training Loss: 1.160580 	Validation Loss: 1.367481 	 time: 0.3
Epoch: 1572 	Training Loss: 1.160566 	Validation Loss: 1.367828 	 time: 0.3
Epoch: 1573 	Training Loss: 1.160554 	Validation Loss: 1.366935 	 time: 0.3
Epoch: 1574 	Training Loss: 1.160530 	Validation Loss: 1.367480 	 time: 0.3
Epoch: 1575 	Training Loss: 1.160536 	Validation Loss: 1.367466 	 time: 0.3
Epoch: 1576 	Training Loss: 1.160509 	Validation Loss: 1.367431 	 time: 0.3
Epoch: 1577 	Training Loss: 1.160517 	Validation Loss: 1.367758 	 time: 0.3
Epoch: 1578 	Training Loss: 1.160504 	Validation Loss: 1.367975 	 time: 0.3
Epoch: 1579 	Training Loss: 1.160493 	Validation Loss: 1.367685 	 time: 0.3
Epoch: 1580 	Training Loss: 1.160494 	Validation Loss: 1.368570 	 time: 0.3
Epoch: 1581 	Training Loss: 1.160477 	Validation Loss: 1.368522 	 time: 0.3
Epoch: 1582 	Training Loss: 1.160481 	Validation Loss: 1.368855 	 time: 0.3
Epoch: 1583 	Training Loss: 1.160477 	Validation Loss: 1.367954 	 time: 0.3
Epoch: 1584 	Training Loss: 1.160485 	Validation Loss: 1.369947 	 time: 0.3
Epoch: 1585 	Training Loss: 1.160517 	Validation Loss: 1.367759 	 time: 0.3
Epoch: 1586 	Training Loss: 1.160544 	Validation Loss: 1.369848 	 time: 0.3
Epoch: 1587 	Training Loss: 1.160453 	Validation Loss: 1.369530 	 time: 0.3
Epoch: 1588 	Training Loss: 1.160438 	Validation Loss: 1.367703 	 time: 0.3
Epoch: 1589 	Training Loss: 1.160509 	Validation Loss: 1.370915 	 time: 0.3
Epoch: 1590 	Training Loss: 1.160495 	Validation Loss: 1.367627 	 time: 0.3
Epoch: 1591 	Training Loss: 1.160467 	Validation Loss: 1.369783 	 time: 0.3
Epoch: 1592 	Training Loss: 1.160408 	Validation Loss: 1.369693 	 time: 0.3
Epoch: 1593 	Training Loss: 1.160393 	Validation Loss: 1.368183 	 time: 0.3
Epoch: 1594 	Training Loss: 1.160442 	Validation Loss: 1.370693 	 time: 0.3
Epoch: 1595 	Training Loss: 1.160440 	Validation Loss: 1.368528 	 time: 0.3
Epoch: 1596 	Training Loss: 1.160409 	Validation Loss: 1.369208 	 time: 0.3
Epoch: 1597 	Training Loss: 1.160359 	Validation Loss: 1.371904 	 time: 0.3
Epoch: 1598 	Training Loss: 1.160407 	Validation Loss: 1.368214 	 time: 0.3
Epoch: 1599 	Training Loss: 1.160399 	Validation Loss: 1.370898 	 time: 0.3
Epoch: 1600 	Training Loss: 1.160265 	Validation Loss: 1.371759 	 time: 0.3
Epoch: 1601 	Training Loss: 1.160269 	Validation Loss: 1.368774 	 time: 0.3
Epoch: 1602 	Training Loss: 1.160291 	Validation Loss: 1.371370 	 time: 0.3
Epoch: 1603 	Training Loss: 1.160201 	Validation Loss: 1.370066 	 time: 0.3
Epoch: 1604 	Training Loss: 1.160144 	Validation Loss: 1.369426 	 time: 0.3
Epoch: 1605 	Training Loss: 1.160135 	Validation Loss: 1.370933 	 time: 0.3
Epoch: 1606 	Training Loss: 1.160117 	Validation Loss: 1.369877 	 time: 0.3
Epoch: 1607 	Training Loss: 1.160063 	Validation Loss: 1.370066 	 time: 0.3
Epoch: 1608 	Training Loss: 1.160033 	Validation Loss: 1.369119 	 time: 0.3
Epoch: 1609 	Training Loss: 1.160038 	Validation Loss: 1.369728 	 time: 0.3
Epoch: 1610 	Training Loss: 1.160002 	Validation Loss: 1.368864 	 time: 0.3
Epoch: 1611 	Training Loss: 1.159996 	Validation Loss: 1.369573 	 time: 0.3
Epoch: 1612 	Training Loss: 1.159964 	Validation Loss: 1.368846 	 time: 0.3
Epoch: 1613 	Training Loss: 1.159956 	Validation Loss: 1.368618 	 time: 0.3
Epoch: 1614 	Training Loss: 1.159946 	Validation Loss: 1.368911 	 time: 0.3
Epoch: 1615 	Training Loss: 1.159938 	Validation Loss: 1.369505 	 time: 0.3
Epoch: 1616 	Training Loss: 1.159926 	Validation Loss: 1.369109 	 time: 0.3
Epoch: 1617 	Training Loss: 1.159919 	Validation Loss: 1.370977 	 time: 0.3
Epoch: 1618 	Training Loss: 1.159909 	Validation Loss: 1.370931 	 time: 0.3
Epoch: 1619 	Training Loss: 1.159877 	Validation Loss: 1.371963 	 time: 0.3
Epoch: 1620 	Training Loss: 1.159858 	Validation Loss: 1.372277 	 time: 0.3
Epoch: 1621 	Training Loss: 1.159849 	Validation Loss: 1.372863 	 time: 0.3
Epoch: 1622 	Training Loss: 1.159834 	Validation Loss: 1.373319 	 time: 0.3
Epoch: 1623 	Training Loss: 1.159836 	Validation Loss: 1.373129 	 time: 0.3
Epoch: 1624 	Training Loss: 1.159828 	Validation Loss: 1.374264 	 time: 0.3
Epoch: 1625 	Training Loss: 1.159822 	Validation Loss: 1.372505 	 time: 0.3
Epoch: 1626 	Training Loss: 1.159833 	Validation Loss: 1.375241 	 time: 0.3
Epoch: 1627 	Training Loss: 1.159841 	Validation Loss: 1.372747 	 time: 0.3
Epoch: 1628 	Training Loss: 1.159838 	Validation Loss: 1.374842 	 time: 0.3
Epoch: 1629 	Training Loss: 1.159807 	Validation Loss: 1.374120 	 time: 0.3
Epoch: 1630 	Training Loss: 1.159780 	Validation Loss: 1.374357 	 time: 0.3
Epoch: 1631 	Training Loss: 1.159789 	Validation Loss: 1.376858 	 time: 0.3
Epoch: 1632 	Training Loss: 1.159794 	Validation Loss: 1.374317 	 time: 0.3
Epoch: 1633 	Training Loss: 1.159828 	Validation Loss: 1.378405 	 time: 0.3
Epoch: 1634 	Training Loss: 1.159821 	Validation Loss: 1.375718 	 time: 0.3
Epoch: 1635 	Training Loss: 1.159768 	Validation Loss: 1.377517 	 time: 0.3
Epoch: 1636 	Training Loss: 1.159730 	Validation Loss: 1.378386 	 time: 0.3
Epoch: 1637 	Training Loss: 1.159720 	Validation Loss: 1.376891 	 time: 0.3
Epoch: 1638 	Training Loss: 1.159755 	Validation Loss: 1.379404 	 time: 0.3
Epoch: 1639 	Training Loss: 1.159784 	Validation Loss: 1.375962 	 time: 0.3
Epoch: 1640 	Training Loss: 1.159819 	Validation Loss: 1.379390 	 time: 0.3
Epoch: 1641 	Training Loss: 1.159780 	Validation Loss: 1.377569 	 time: 0.3
Epoch: 1642 	Training Loss: 1.159716 	Validation Loss: 1.377492 	 time: 0.3
Epoch: 1643 	Training Loss: 1.159720 	Validation Loss: 1.379700 	 time: 0.3
Epoch: 1644 	Training Loss: 1.159758 	Validation Loss: 1.375345 	 time: 0.3
Epoch: 1645 	Training Loss: 1.159874 	Validation Loss: 1.380094 	 time: 0.3
Epoch: 1646 	Training Loss: 1.159888 	Validation Loss: 1.376928 	 time: 0.3
Epoch: 1647 	Training Loss: 1.159833 	Validation Loss: 1.377832 	 time: 0.3
Epoch: 1648 	Training Loss: 1.159801 	Validation Loss: 1.379630 	 time: 0.3
Epoch: 1649 	Training Loss: 1.159848 	Validation Loss: 1.375538 	 time: 0.3
Epoch: 1650 	Training Loss: 1.159835 	Validation Loss: 1.378500 	 time: 0.3
Epoch: 1651 	Training Loss: 1.159710 	Validation Loss: 1.380720 	 time: 0.3
Epoch: 1652 	Training Loss: 1.159807 	Validation Loss: 1.375169 	 time: 0.3
Epoch: 1653 	Training Loss: 1.159895 	Validation Loss: 1.379372 	 time: 0.3
Epoch: 1654 	Training Loss: 1.159802 	Validation Loss: 1.378849 	 time: 0.3
Epoch: 1655 	Training Loss: 1.159750 	Validation Loss: 1.375059 	 time: 0.3
Epoch: 1656 	Training Loss: 1.159822 	Validation Loss: 1.378832 	 time: 0.3
Epoch: 1657 	Training Loss: 1.159783 	Validation Loss: 1.377307 	 time: 0.3
Epoch: 1658 	Training Loss: 1.159702 	Validation Loss: 1.377248 	 time: 0.3
Epoch: 1659 	Training Loss: 1.159683 	Validation Loss: 1.381257 	 time: 0.3
Epoch: 1660 	Training Loss: 1.159746 	Validation Loss: 1.376495 	 time: 0.3
Epoch: 1661 	Training Loss: 1.159766 	Validation Loss: 1.379965 	 time: 0.3
Epoch: 1662 	Training Loss: 1.159631 	Validation Loss: 1.381868 	 time: 0.3
Epoch: 1663 	Training Loss: 1.159737 	Validation Loss: 1.376226 	 time: 0.3
Epoch: 1664 	Training Loss: 1.159805 	Validation Loss: 1.380406 	 time: 0.3
Epoch: 1665 	Training Loss: 1.159759 	Validation Loss: 1.379690 	 time: 0.3
Epoch: 1666 	Training Loss: 1.159706 	Validation Loss: 1.376119 	 time: 0.3
Epoch: 1667 	Training Loss: 1.159763 	Validation Loss: 1.380036 	 time: 0.3
Epoch: 1668 	Training Loss: 1.159735 	Validation Loss: 1.378815 	 time: 0.3
Epoch: 1669 	Training Loss: 1.159633 	Validation Loss: 1.377368 	 time: 0.3
Epoch: 1670 	Training Loss: 1.159680 	Validation Loss: 1.380672 	 time: 0.3
Epoch: 1671 	Training Loss: 1.159662 	Validation Loss: 1.380309 	 time: 0.3
Epoch: 1672 	Training Loss: 1.159625 	Validation Loss: 1.378508 	 time: 0.3
Epoch: 1673 	Training Loss: 1.159662 	Validation Loss: 1.380811 	 time: 0.3
Epoch: 1674 	Training Loss: 1.159661 	Validation Loss: 1.377802 	 time: 0.3
Epoch: 1675 	Training Loss: 1.159630 	Validation Loss: 1.378489 	 time: 0.3
Epoch: 1676 	Training Loss: 1.159611 	Validation Loss: 1.379962 	 time: 0.3
Epoch: 1677 	Training Loss: 1.159661 	Validation Loss: 1.376791 	 time: 0.3
Epoch: 1678 	Training Loss: 1.159695 	Validation Loss: 1.379884 	 time: 0.3
Epoch: 1679 	Training Loss: 1.159617 	Validation Loss: 1.380137 	 time: 0.3
Epoch: 1680 	Training Loss: 1.159601 	Validation Loss: 1.377892 	 time: 0.3
Epoch: 1681 	Training Loss: 1.159606 	Validation Loss: 1.380185 	 time: 0.3
Epoch: 1682 	Training Loss: 1.159584 	Validation Loss: 1.378916 	 time: 0.3
Epoch: 1683 	Training Loss: 1.159557 	Validation Loss: 1.377972 	 time: 0.3
Epoch: 1684 	Training Loss: 1.159563 	Validation Loss: 1.379225 	 time: 0.3
Epoch: 1685 	Training Loss: 1.159530 	Validation Loss: 1.379238 	 time: 0.3
Epoch: 1686 	Training Loss: 1.159545 	Validation Loss: 1.378309 	 time: 0.3
Epoch: 1687 	Training Loss: 1.159508 	Validation Loss: 1.379087 	 time: 0.3
Epoch: 1688 	Training Loss: 1.159493 	Validation Loss: 1.377710 	 time: 0.3
Epoch: 1689 	Training Loss: 1.159480 	Validation Loss: 1.377859 	 time: 0.3
Epoch: 1690 	Training Loss: 1.159475 	Validation Loss: 1.377152 	 time: 0.3
Epoch: 1691 	Training Loss: 1.159470 	Validation Loss: 1.378076 	 time: 0.3
Epoch: 1692 	Training Loss: 1.159471 	Validation Loss: 1.376613 	 time: 0.3
Epoch: 1693 	Training Loss: 1.159449 	Validation Loss: 1.377354 	 time: 0.3
Epoch: 1694 	Training Loss: 1.159457 	Validation Loss: 1.376335 	 time: 0.3
Epoch: 1695 	Training Loss: 1.159443 	Validation Loss: 1.377845 	 time: 0.3
Epoch: 1696 	Training Loss: 1.159436 	Validation Loss: 1.377175 	 time: 0.3
Epoch: 1697 	Training Loss: 1.159428 	Validation Loss: 1.376821 	 time: 0.3
Epoch: 1698 	Training Loss: 1.159420 	Validation Loss: 1.377213 	 time: 0.3
Epoch: 1699 	Training Loss: 1.159420 	Validation Loss: 1.376692 	 time: 0.3
Epoch: 1700 	Training Loss: 1.159419 	Validation Loss: 1.378175 	 time: 0.3
Epoch: 1701 	Training Loss: 1.159413 	Validation Loss: 1.377500 	 time: 0.3
Epoch: 1702 	Training Loss: 1.159412 	Validation Loss: 1.379137 	 time: 0.3
Epoch: 1703 	Training Loss: 1.159411 	Validation Loss: 1.377388 	 time: 0.3
Epoch: 1704 	Training Loss: 1.159404 	Validation Loss: 1.379074 	 time: 0.3
Epoch: 1705 	Training Loss: 1.159390 	Validation Loss: 1.378281 	 time: 0.3
Epoch: 1706 	Training Loss: 1.159378 	Validation Loss: 1.379042 	 time: 0.3
Epoch: 1707 	Training Loss: 1.159363 	Validation Loss: 1.379434 	 time: 0.3
Epoch: 1708 	Training Loss: 1.159355 	Validation Loss: 1.379076 	 time: 0.3
Epoch: 1709 	Training Loss: 1.159352 	Validation Loss: 1.379630 	 time: 0.3
Epoch: 1710 	Training Loss: 1.159352 	Validation Loss: 1.378721 	 time: 0.3
Epoch: 1711 	Training Loss: 1.159350 	Validation Loss: 1.380313 	 time: 0.3
Epoch: 1712 	Training Loss: 1.159350 	Validation Loss: 1.378252 	 time: 0.3
Epoch: 1713 	Training Loss: 1.159353 	Validation Loss: 1.380321 	 time: 0.3
Epoch: 1714 	Training Loss: 1.159350 	Validation Loss: 1.378353 	 time: 0.3
Epoch: 1715 	Training Loss: 1.159328 	Validation Loss: 1.379535 	 time: 0.3
Epoch: 1716 	Training Loss: 1.159321 	Validation Loss: 1.377630 	 time: 0.3
Epoch: 1717 	Training Loss: 1.159315 	Validation Loss: 1.378638 	 time: 0.3
Epoch: 1718 	Training Loss: 1.159310 	Validation Loss: 1.377472 	 time: 0.3
Epoch: 1719 	Training Loss: 1.159297 	Validation Loss: 1.378098 	 time: 0.3
Epoch: 1720 	Training Loss: 1.159292 	Validation Loss: 1.376996 	 time: 0.3
Epoch: 1721 	Training Loss: 1.159291 	Validation Loss: 1.377824 	 time: 0.3
Epoch: 1722 	Training Loss: 1.159282 	Validation Loss: 1.376669 	 time: 0.3
Epoch: 1723 	Training Loss: 1.159280 	Validation Loss: 1.377730 	 time: 0.3
Epoch: 1724 	Training Loss: 1.159268 	Validation Loss: 1.375934 	 time: 0.3
Epoch: 1725 	Training Loss: 1.159260 	Validation Loss: 1.377826 	 time: 0.3
Epoch: 1726 	Training Loss: 1.159241 	Validation Loss: 1.374524 	 time: 0.3
Epoch: 1727 	Training Loss: 1.159236 	Validation Loss: 1.375771 	 time: 0.3
Epoch: 1728 	Training Loss: 1.159240 	Validation Loss: 1.372193 	 time: 0.3
Epoch: 1729 	Training Loss: 1.159259 	Validation Loss: 1.374866 	 time: 0.3
Epoch: 1730 	Training Loss: 1.159250 	Validation Loss: 1.371727 	 time: 0.3
Epoch: 1731 	Training Loss: 1.159235 	Validation Loss: 1.373921 	 time: 0.3
Epoch: 1732 	Training Loss: 1.159176 	Validation Loss: 1.374205 	 time: 0.3
Epoch: 1733 	Training Loss: 1.159169 	Validation Loss: 1.371991 	 time: 0.3
Epoch: 1734 	Training Loss: 1.159215 	Validation Loss: 1.374819 	 time: 0.3
Epoch: 1735 	Training Loss: 1.159260 	Validation Loss: 1.370815 	 time: 0.3
Epoch: 1736 	Training Loss: 1.159275 	Validation Loss: 1.375124 	 time: 0.3
Epoch: 1737 	Training Loss: 1.159240 	Validation Loss: 1.370914 	 time: 0.3
Epoch: 1738 	Training Loss: 1.159229 	Validation Loss: 1.371529 	 time: 0.3
Epoch: 1739 	Training Loss: 1.159178 	Validation Loss: 1.372908 	 time: 0.3
Epoch: 1740 	Training Loss: 1.159198 	Validation Loss: 1.371902 	 time: 0.3
Epoch: 1741 	Training Loss: 1.159186 	Validation Loss: 1.374240 	 time: 0.3
Epoch: 1742 	Training Loss: 1.159198 	Validation Loss: 1.370450 	 time: 0.3
Epoch: 1743 	Training Loss: 1.159189 	Validation Loss: 1.373409 	 time: 0.3
Epoch: 1744 	Training Loss: 1.159155 	Validation Loss: 1.371858 	 time: 0.3
Epoch: 1745 	Training Loss: 1.159116 	Validation Loss: 1.372420 	 time: 0.3
Epoch: 1746 	Training Loss: 1.159126 	Validation Loss: 1.371731 	 time: 0.3
Epoch: 1747 	Training Loss: 1.159104 	Validation Loss: 1.372079 	 time: 0.3
Epoch: 1748 	Training Loss: 1.159106 	Validation Loss: 1.371865 	 time: 0.3
Epoch: 1749 	Training Loss: 1.159104 	Validation Loss: 1.372931 	 time: 0.3
Epoch: 1750 	Training Loss: 1.159094 	Validation Loss: 1.371293 	 time: 0.3
Epoch: 1751 	Training Loss: 1.159108 	Validation Loss: 1.374109 	 time: 0.3
Epoch: 1752 	Training Loss: 1.159147 	Validation Loss: 1.370540 	 time: 0.3
Epoch: 1753 	Training Loss: 1.159178 	Validation Loss: 1.373915 	 time: 0.3
Epoch: 1754 	Training Loss: 1.159165 	Validation Loss: 1.369671 	 time: 0.3
Epoch: 1755 	Training Loss: 1.159131 	Validation Loss: 1.371786 	 time: 0.3
Epoch: 1756 	Training Loss: 1.159076 	Validation Loss: 1.370799 	 time: 0.3
Epoch: 1757 	Training Loss: 1.159060 	Validation Loss: 1.368294 	 time: 0.3
Epoch: 1758 	Training Loss: 1.159089 	Validation Loss: 1.371949 	 time: 0.3
Epoch: 1759 	Training Loss: 1.159101 	Validation Loss: 1.368548 	 time: 0.3
Epoch: 1760 	Training Loss: 1.159126 	Validation Loss: 1.372179 	 time: 0.3
Epoch: 1761 	Training Loss: 1.159111 	Validation Loss: 1.369605 	 time: 0.3
Epoch: 1762 	Training Loss: 1.159051 	Validation Loss: 1.370514 	 time: 0.3
Epoch: 1763 	Training Loss: 1.159052 	Validation Loss: 1.372137 	 time: 0.3
Epoch: 1764 	Training Loss: 1.159082 	Validation Loss: 1.369225 	 time: 0.3
Epoch: 1765 	Training Loss: 1.159098 	Validation Loss: 1.373301 	 time: 0.3
Epoch: 1766 	Training Loss: 1.159103 	Validation Loss: 1.370083 	 time: 0.3
Epoch: 1767 	Training Loss: 1.159066 	Validation Loss: 1.371567 	 time: 0.3
Epoch: 1768 	Training Loss: 1.159033 	Validation Loss: 1.372321 	 time: 0.3
Epoch: 1769 	Training Loss: 1.159041 	Validation Loss: 1.368967 	 time: 0.3
Epoch: 1770 	Training Loss: 1.159114 	Validation Loss: 1.374619 	 time: 0.2
Epoch: 1771 	Training Loss: 1.159172 	Validation Loss: 1.368522 	 time: 0.3
Epoch: 1772 	Training Loss: 1.159178 	Validation Loss: 1.373154 	 time: 0.3
Epoch: 1773 	Training Loss: 1.159093 	Validation Loss: 1.371084 	 time: 0.3
Epoch: 1774 	Training Loss: 1.159042 	Validation Loss: 1.369717 	 time: 0.3
Epoch: 1775 	Training Loss: 1.159052 	Validation Loss: 1.373092 	 time: 0.3
Epoch: 1776 	Training Loss: 1.159093 	Validation Loss: 1.369790 	 time: 0.3
Epoch: 1777 	Training Loss: 1.159085 	Validation Loss: 1.372986 	 time: 0.3
Epoch: 1778 	Training Loss: 1.159039 	Validation Loss: 1.373073 	 time: 0.3
Epoch: 1779 	Training Loss: 1.159025 	Validation Loss: 1.369694 	 time: 0.3
Epoch: 1780 	Training Loss: 1.159065 	Validation Loss: 1.373546 	 time: 0.3
Epoch: 1781 	Training Loss: 1.159114 	Validation Loss: 1.369233 	 time: 0.3
Epoch: 1782 	Training Loss: 1.159096 	Validation Loss: 1.373197 	 time: 0.3
Epoch: 1783 	Training Loss: 1.159065 	Validation Loss: 1.370848 	 time: 0.3
Epoch: 1784 	Training Loss: 1.159044 	Validation Loss: 1.371095 	 time: 0.3
Epoch: 1785 	Training Loss: 1.159035 	Validation Loss: 1.373810 	 time: 0.3
Epoch: 1786 	Training Loss: 1.159042 	Validation Loss: 1.370840 	 time: 0.3
Epoch: 1787 	Training Loss: 1.159042 	Validation Loss: 1.372167 	 time: 0.3
Epoch: 1788 	Training Loss: 1.159014 	Validation Loss: 1.372945 	 time: 0.3
Epoch: 1789 	Training Loss: 1.159028 	Validation Loss: 1.370236 	 time: 0.3
Epoch: 1790 	Training Loss: 1.159067 	Validation Loss: 1.374287 	 time: 0.3
Epoch: 1791 	Training Loss: 1.159107 	Validation Loss: 1.369692 	 time: 0.3
Epoch: 1792 	Training Loss: 1.159099 	Validation Loss: 1.373030 	 time: 0.3
Epoch: 1793 	Training Loss: 1.159078 	Validation Loss: 1.371341 	 time: 0.3
Epoch: 1794 	Training Loss: 1.159107 	Validation Loss: 1.372132 	 time: 0.3
Epoch: 1795 	Training Loss: 1.159045 	Validation Loss: 1.373382 	 time: 0.3
Epoch: 1796 	Training Loss: 1.159054 	Validation Loss: 1.371556 	 time: 0.3
Epoch: 1797 	Training Loss: 1.159049 	Validation Loss: 1.374106 	 time: 0.3
Epoch: 1798 	Training Loss: 1.159145 	Validation Loss: 1.368446 	 time: 0.3
Epoch: 1799 	Training Loss: 1.159106 	Validation Loss: 1.372173 	 time: 0.3
Epoch: 1800 	Training Loss: 1.159126 	Validation Loss: 1.371622 	 time: 0.3
Epoch: 1801 	Training Loss: 1.159045 	Validation Loss: 1.372695 	 time: 0.3
Epoch: 1802 	Training Loss: 1.159103 	Validation Loss: 1.372332 	 time: 0.3
Epoch: 1803 	Training Loss: 1.159025 	Validation Loss: 1.371050 	 time: 0.3
Epoch: 1804 	Training Loss: 1.159080 	Validation Loss: 1.372882 	 time: 0.3
Epoch: 1805 	Training Loss: 1.159127 	Validation Loss: 1.372379 	 time: 0.3
Epoch: 1806 	Training Loss: 1.159072 	Validation Loss: 1.373519 	 time: 0.3
Epoch: 1807 	Training Loss: 1.159057 	Validation Loss: 1.372866 	 time: 0.3
Epoch: 1808 	Training Loss: 1.158998 	Validation Loss: 1.373251 	 time: 0.3
Epoch: 1809 	Training Loss: 1.159022 	Validation Loss: 1.372849 	 time: 0.3
Epoch: 1810 	Training Loss: 1.158998 	Validation Loss: 1.373475 	 time: 0.3
Epoch: 1811 	Training Loss: 1.158990 	Validation Loss: 1.375656 	 time: 0.3
Epoch: 1812 	Training Loss: 1.159016 	Validation Loss: 1.372318 	 time: 0.3
Epoch: 1813 	Training Loss: 1.159002 	Validation Loss: 1.374745 	 time: 0.3
Epoch: 1814 	Training Loss: 1.158990 	Validation Loss: 1.373542 	 time: 0.3
Epoch: 1815 	Training Loss: 1.158961 	Validation Loss: 1.373630 	 time: 0.3
Epoch: 1816 	Training Loss: 1.158969 	Validation Loss: 1.372034 	 time: 0.3
Epoch: 1817 	Training Loss: 1.158958 	Validation Loss: 1.373401 	 time: 0.3
Epoch: 1818 	Training Loss: 1.158959 	Validation Loss: 1.372595 	 time: 0.3
Epoch: 1819 	Training Loss: 1.158947 	Validation Loss: 1.373052 	 time: 0.3
Epoch: 1820 	Training Loss: 1.158961 	Validation Loss: 1.372385 	 time: 0.3
Epoch: 1821 	Training Loss: 1.158941 	Validation Loss: 1.373640 	 time: 0.3
Epoch: 1822 	Training Loss: 1.158956 	Validation Loss: 1.371005 	 time: 0.3
Epoch: 1823 	Training Loss: 1.158957 	Validation Loss: 1.374072 	 time: 0.3
Epoch: 1824 	Training Loss: 1.159005 	Validation Loss: 1.369617 	 time: 0.3
Epoch: 1825 	Training Loss: 1.159080 	Validation Loss: 1.374144 	 time: 0.3
Epoch: 1826 	Training Loss: 1.159084 	Validation Loss: 1.370597 	 time: 0.3
Epoch: 1827 	Training Loss: 1.159025 	Validation Loss: 1.370867 	 time: 0.3
Epoch: 1828 	Training Loss: 1.158998 	Validation Loss: 1.373998 	 time: 0.3
Epoch: 1829 	Training Loss: 1.159067 	Validation Loss: 1.368379 	 time: 0.3
Epoch: 1830 	Training Loss: 1.159071 	Validation Loss: 1.371226 	 time: 0.3
Epoch: 1831 	Training Loss: 1.159003 	Validation Loss: 1.371226 	 time: 0.3
Epoch: 1832 	Training Loss: 1.158976 	Validation Loss: 1.368853 	 time: 0.3
Epoch: 1833 	Training Loss: 1.159012 	Validation Loss: 1.371623 	 time: 0.3
Epoch: 1834 	Training Loss: 1.159053 	Validation Loss: 1.367559 	 time: 0.3
Epoch: 1835 	Training Loss: 1.159013 	Validation Loss: 1.370719 	 time: 0.3
Epoch: 1836 	Training Loss: 1.158953 	Validation Loss: 1.371097 	 time: 0.3
Epoch: 1837 	Training Loss: 1.158926 	Validation Loss: 1.368736 	 time: 0.3
Epoch: 1838 	Training Loss: 1.158950 	Validation Loss: 1.370100 	 time: 0.3
Epoch: 1839 	Training Loss: 1.158893 	Validation Loss: 1.368639 	 time: 0.3
Epoch: 1840 	Training Loss: 1.158947 	Validation Loss: 1.367833 	 time: 0.3
Epoch: 1841 	Training Loss: 1.158885 	Validation Loss: 1.368955 	 time: 0.3
Epoch: 1842 	Training Loss: 1.158911 	Validation Loss: 1.367541 	 time: 0.3
Epoch: 1843 	Training Loss: 1.158872 	Validation Loss: 1.369057 	 time: 0.3
Epoch: 1844 	Training Loss: 1.158848 	Validation Loss: 1.367605 	 time: 0.3
Epoch: 1845 	Training Loss: 1.158881 	Validation Loss: 1.371137 	 time: 0.3
Epoch: 1846 	Training Loss: 1.158858 	Validation Loss: 1.368305 	 time: 0.3
Epoch: 1847 	Training Loss: 1.158872 	Validation Loss: 1.370684 	 time: 0.3
Epoch: 1848 	Training Loss: 1.158847 	Validation Loss: 1.368432 	 time: 0.3
Epoch: 1849 	Training Loss: 1.158812 	Validation Loss: 1.368244 	 time: 0.3
Epoch: 1850 	Training Loss: 1.158825 	Validation Loss: 1.371275 	 time: 0.3
Epoch: 1851 	Training Loss: 1.158818 	Validation Loss: 1.369440 	 time: 0.3
Epoch: 1852 	Training Loss: 1.158823 	Validation Loss: 1.371558 	 time: 0.3
Epoch: 1853 	Training Loss: 1.158778 	Validation Loss: 1.372704 	 time: 0.3
Epoch: 1854 	Training Loss: 1.158799 	Validation Loss: 1.370444 	 time: 0.3
Epoch: 1855 	Training Loss: 1.158786 	Validation Loss: 1.373055 	 time: 0.3
Epoch: 1856 	Training Loss: 1.158800 	Validation Loss: 1.370906 	 time: 0.3
Epoch: 1857 	Training Loss: 1.158777 	Validation Loss: 1.372988 	 time: 0.3
Epoch: 1858 	Training Loss: 1.158787 	Validation Loss: 1.371391 	 time: 0.3
Epoch: 1859 	Training Loss: 1.158753 	Validation Loss: 1.371883 	 time: 0.3
Epoch: 1860 	Training Loss: 1.158766 	Validation Loss: 1.372119 	 time: 0.3
Epoch: 1861 	Training Loss: 1.158738 	Validation Loss: 1.370593 	 time: 0.3
Epoch: 1862 	Training Loss: 1.158747 	Validation Loss: 1.372047 	 time: 0.3
Epoch: 1863 	Training Loss: 1.158734 	Validation Loss: 1.372027 	 time: 0.3
Epoch: 1864 	Training Loss: 1.158721 	Validation Loss: 1.371068 	 time: 0.3
Epoch: 1865 	Training Loss: 1.158728 	Validation Loss: 1.372380 	 time: 0.3
Epoch: 1866 	Training Loss: 1.158725 	Validation Loss: 1.370109 	 time: 0.3
Epoch: 1867 	Training Loss: 1.158726 	Validation Loss: 1.371696 	 time: 0.3
Epoch: 1868 	Training Loss: 1.158730 	Validation Loss: 1.369845 	 time: 0.3
Epoch: 1869 	Training Loss: 1.158721 	Validation Loss: 1.371576 	 time: 0.3
Epoch: 1870 	Training Loss: 1.158685 	Validation Loss: 1.370282 	 time: 0.3
Epoch: 1871 	Training Loss: 1.158659 	Validation Loss: 1.369947 	 time: 0.3
Epoch: 1872 	Training Loss: 1.158623 	Validation Loss: 1.371755 	 time: 0.3
Epoch: 1873 	Training Loss: 1.158637 	Validation Loss: 1.367065 	 time: 0.3
Epoch: 1874 	Training Loss: 1.158690 	Validation Loss: 1.371256 	 time: 0.3
Epoch: 1875 	Training Loss: 1.158637 	Validation Loss: 1.369019 	 time: 0.3
Epoch: 1876 	Training Loss: 1.158539 	Validation Loss: 1.369591 	 time: 0.3
Epoch: 1877 	Training Loss: 1.158498 	Validation Loss: 1.371606 	 time: 0.3
Epoch: 1878 	Training Loss: 1.158484 	Validation Loss: 1.366574 	 time: 0.3
Epoch: 1879 	Training Loss: 1.158616 	Validation Loss: 1.373086 	 time: 0.3
Epoch: 1880 	Training Loss: 1.158776 	Validation Loss: 1.365729 	 time: 0.3
Epoch: 1881 	Training Loss: 1.158723 	Validation Loss: 1.367900 	 time: 0.3
Epoch: 1882 	Training Loss: 1.158548 	Validation Loss: 1.369212 	 time: 0.3
Epoch: 1883 	Training Loss: 1.158629 	Validation Loss: 1.365626 	 time: 0.3
Epoch: 1884 	Training Loss: 1.158700 	Validation Loss: 1.367705 	 time: 0.3
Epoch: 1885 	Training Loss: 1.158626 	Validation Loss: 1.366814 	 time: 0.3
Epoch: 1886 	Training Loss: 1.158477 	Validation Loss: 1.366685 	 time: 0.3
Epoch: 1887 	Training Loss: 1.158414 	Validation Loss: 1.366789 	 time: 0.3
Epoch: 1888 	Training Loss: 1.158420 	Validation Loss: 1.367572 	 time: 0.3
Epoch: 1889 	Training Loss: 1.158398 	Validation Loss: 1.369336 	 time: 0.3
Epoch: 1890 	Training Loss: 1.158411 	Validation Loss: 1.369666 	 time: 0.3
Epoch: 1891 	Training Loss: 1.158340 	Validation Loss: 1.368414 	 time: 0.3
Epoch: 1892 	Training Loss: 1.158381 	Validation Loss: 1.368614 	 time: 0.3
Epoch: 1893 	Training Loss: 1.158311 	Validation Loss: 1.367016 	 time: 0.3
Epoch: 1894 	Training Loss: 1.158353 	Validation Loss: 1.370188 	 time: 0.3
Epoch: 1895 	Training Loss: 1.158340 	Validation Loss: 1.366537 	 time: 0.3
Epoch: 1896 	Training Loss: 1.158332 	Validation Loss: 1.370350 	 time: 0.3
Epoch: 1897 	Training Loss: 1.158349 	Validation Loss: 1.367235 	 time: 0.3
Epoch: 1898 	Training Loss: 1.158311 	Validation Loss: 1.368400 	 time: 0.3
Epoch: 1899 	Training Loss: 1.158335 	Validation Loss: 1.368099 	 time: 0.3
Epoch: 1900 	Training Loss: 1.158283 	Validation Loss: 1.366814 	 time: 0.3
Epoch: 1901 	Training Loss: 1.158277 	Validation Loss: 1.368486 	 time: 0.3
Epoch: 1902 	Training Loss: 1.158274 	Validation Loss: 1.367743 	 time: 0.3
Epoch: 1903 	Training Loss: 1.158239 	Validation Loss: 1.367995 	 time: 0.3
Epoch: 1904 	Training Loss: 1.158234 	Validation Loss: 1.369118 	 time: 0.3
Epoch: 1905 	Training Loss: 1.158226 	Validation Loss: 1.369011 	 time: 0.3
Epoch: 1906 	Training Loss: 1.158213 	Validation Loss: 1.368672 	 time: 0.3
Epoch: 1907 	Training Loss: 1.158178 	Validation Loss: 1.369552 	 time: 0.3
Epoch: 1908 	Training Loss: 1.158194 	Validation Loss: 1.369812 	 time: 0.3
Epoch: 1909 	Training Loss: 1.158174 	Validation Loss: 1.368564 	 time: 0.3
Epoch: 1910 	Training Loss: 1.158160 	Validation Loss: 1.367883 	 time: 0.3
Epoch: 1911 	Training Loss: 1.158153 	Validation Loss: 1.368475 	 time: 0.3
Epoch: 1912 	Training Loss: 1.158125 	Validation Loss: 1.368731 	 time: 0.3
Epoch: 1913 	Training Loss: 1.158137 	Validation Loss: 1.367662 	 time: 0.3
Epoch: 1914 	Training Loss: 1.158098 	Validation Loss: 1.369496 	 time: 0.3
Epoch: 1915 	Training Loss: 1.158093 	Validation Loss: 1.368698 	 time: 0.3
Epoch: 1916 	Training Loss: 1.158034 	Validation Loss: 1.371211 	 time: 0.3
Epoch: 1917 	Training Loss: 1.158038 	Validation Loss: 1.369618 	 time: 0.3
Epoch: 1918 	Training Loss: 1.157987 	Validation Loss: 1.370648 	 time: 0.3
Epoch: 1919 	Training Loss: 1.157995 	Validation Loss: 1.371118 	 time: 0.3
Epoch: 1920 	Training Loss: 1.157942 	Validation Loss: 1.372342 	 time: 0.3
Epoch: 1921 	Training Loss: 1.157963 	Validation Loss: 1.372582 	 time: 0.3
Epoch: 1922 	Training Loss: 1.157928 	Validation Loss: 1.373001 	 time: 0.3
Epoch: 1923 	Training Loss: 1.157937 	Validation Loss: 1.374260 	 time: 0.3
Epoch: 1924 	Training Loss: 1.157914 	Validation Loss: 1.373221 	 time: 0.3
Epoch: 1925 	Training Loss: 1.157928 	Validation Loss: 1.375266 	 time: 0.3
Epoch: 1926 	Training Loss: 1.157947 	Validation Loss: 1.371361 	 time: 0.3
Epoch: 1927 	Training Loss: 1.157985 	Validation Loss: 1.374574 	 time: 0.3
Epoch: 1928 	Training Loss: 1.157959 	Validation Loss: 1.371881 	 time: 0.3
Epoch: 1929 	Training Loss: 1.157976 	Validation Loss: 1.373439 	 time: 0.3
Epoch: 1930 	Training Loss: 1.157930 	Validation Loss: 1.375681 	 time: 0.3
Epoch: 1931 	Training Loss: 1.157945 	Validation Loss: 1.371823 	 time: 0.3
Epoch: 1932 	Training Loss: 1.157994 	Validation Loss: 1.375884 	 time: 0.3
Epoch: 1933 	Training Loss: 1.158039 	Validation Loss: 1.370742 	 time: 0.3
Epoch: 1934 	Training Loss: 1.158013 	Validation Loss: 1.373437 	 time: 0.3
Epoch: 1935 	Training Loss: 1.157953 	Validation Loss: 1.374401 	 time: 0.3
Epoch: 1936 	Training Loss: 1.157928 	Validation Loss: 1.373835 	 time: 0.3
Epoch: 1937 	Training Loss: 1.157948 	Validation Loss: 1.380200 	 time: 0.3
Epoch: 1938 	Training Loss: 1.158037 	Validation Loss: 1.370339 	 time: 0.3
Epoch: 1939 	Training Loss: 1.158193 	Validation Loss: 1.378716 	 time: 0.3
Epoch: 1940 	Training Loss: 1.158172 	Validation Loss: 1.370804 	 time: 0.3
Epoch: 1941 	Training Loss: 1.158077 	Validation Loss: 1.371938 	 time: 0.3
Epoch: 1942 	Training Loss: 1.157952 	Validation Loss: 1.377117 	 time: 0.3
Epoch: 1943 	Training Loss: 1.158020 	Validation Loss: 1.369567 	 time: 0.3
Epoch: 1944 	Training Loss: 1.158228 	Validation Loss: 1.378361 	 time: 0.3
Epoch: 1945 	Training Loss: 1.158119 	Validation Loss: 1.373946 	 time: 0.3
Epoch: 1946 	Training Loss: 1.158015 	Validation Loss: 1.372693 	 time: 0.3
Epoch: 1947 	Training Loss: 1.158111 	Validation Loss: 1.381362 	 time: 0.3
Epoch: 1948 	Training Loss: 1.158093 	Validation Loss: 1.374367 	 time: 0.3
Epoch: 1949 	Training Loss: 1.157985 	Validation Loss: 1.374033 	 time: 0.3
Epoch: 1950 	Training Loss: 1.157969 	Validation Loss: 1.378685 	 time: 0.3
Epoch: 1951 	Training Loss: 1.158027 	Validation Loss: 1.375472 	 time: 0.3
Epoch: 1952 	Training Loss: 1.157956 	Validation Loss: 1.378043 	 time: 0.3
Epoch: 1953 	Training Loss: 1.157934 	Validation Loss: 1.381496 	 time: 0.3
Epoch: 1954 	Training Loss: 1.157959 	Validation Loss: 1.379250 	 time: 0.3
Epoch: 1955 	Training Loss: 1.157896 	Validation Loss: 1.381102 	 time: 0.3
Epoch: 1956 	Training Loss: 1.157896 	Validation Loss: 1.380934 	 time: 0.3
Epoch: 1957 	Training Loss: 1.157928 	Validation Loss: 1.378242 	 time: 0.3
Epoch: 1958 	Training Loss: 1.157979 	Validation Loss: 1.381786 	 time: 0.3
Epoch: 1959 	Training Loss: 1.157890 	Validation Loss: 1.381761 	 time: 0.3
Epoch: 1960 	Training Loss: 1.157864 	Validation Loss: 1.383807 	 time: 0.3
Epoch: 1961 	Training Loss: 1.157943 	Validation Loss: 1.384402 	 time: 0.3
Epoch: 1962 	Training Loss: 1.157949 	Validation Loss: 1.377278 	 time: 0.3
Epoch: 1963 	Training Loss: 1.158002 	Validation Loss: 1.384756 	 time: 0.3
Epoch: 1964 	Training Loss: 1.158085 	Validation Loss: 1.375291 	 time: 0.3
Epoch: 1965 	Training Loss: 1.158283 	Validation Loss: 1.383040 	 time: 0.3
Epoch: 1966 	Training Loss: 1.158172 	Validation Loss: 1.377619 	 time: 0.3
Epoch: 1967 	Training Loss: 1.157980 	Validation Loss: 1.374492 	 time: 0.3
Epoch: 1968 	Training Loss: 1.158083 	Validation Loss: 1.380235 	 time: 0.3
Epoch: 1969 	Training Loss: 1.158023 	Validation Loss: 1.375772 	 time: 0.3
Epoch: 1970 	Training Loss: 1.157940 	Validation Loss: 1.376168 	 time: 0.3
Epoch: 1971 	Training Loss: 1.157930 	Validation Loss: 1.381431 	 time: 0.3
Epoch: 1972 	Training Loss: 1.157930 	Validation Loss: 1.377630 	 time: 0.3
Epoch: 1973 	Training Loss: 1.157895 	Validation Loss: 1.374896 	 time: 0.3
Epoch: 1974 	Training Loss: 1.157882 	Validation Loss: 1.378167 	 time: 0.3
Epoch: 1975 	Training Loss: 1.157865 	Validation Loss: 1.378856 	 time: 0.3
Epoch: 1976 	Training Loss: 1.157858 	Validation Loss: 1.377889 	 time: 0.3
Epoch: 1977 	Training Loss: 1.157827 	Validation Loss: 1.383145 	 time: 0.3
Epoch: 1978 	Training Loss: 1.157800 	Validation Loss: 1.382508 	 time: 0.3
Epoch: 1979 	Training Loss: 1.157769 	Validation Loss: 1.381141 	 time: 0.3
Epoch: 1980 	Training Loss: 1.157766 	Validation Loss: 1.383051 	 time: 0.3
Epoch: 1981 	Training Loss: 1.157721 	Validation Loss: 1.380785 	 time: 0.3
Epoch: 1982 	Training Loss: 1.157649 	Validation Loss: 1.382139 	 time: 0.3
Epoch: 1983 	Training Loss: 1.157658 	Validation Loss: 1.381517 	 time: 0.3
Epoch: 1984 	Training Loss: 1.157586 	Validation Loss: 1.380041 	 time: 0.3
Epoch: 1985 	Training Loss: 1.157625 	Validation Loss: 1.380229 	 time: 0.3
Epoch: 1986 	Training Loss: 1.157562 	Validation Loss: 1.380410 	 time: 0.3
Epoch: 1987 	Training Loss: 1.157522 	Validation Loss: 1.378550 	 time: 0.3
Epoch: 1988 	Training Loss: 1.157537 	Validation Loss: 1.380194 	 time: 0.3
Epoch: 1989 	Training Loss: 1.157560 	Validation Loss: 1.374792 	 time: 0.3
Epoch: 1990 	Training Loss: 1.157652 	Validation Loss: 1.378865 	 time: 0.3
Epoch: 1991 	Training Loss: 1.157637 	Validation Loss: 1.373263 	 time: 0.3
Epoch: 1992 	Training Loss: 1.157635 	Validation Loss: 1.375438 	 time: 0.3
Epoch: 1993 	Training Loss: 1.157572 	Validation Loss: 1.380166 	 time: 0.3
Epoch: 1994 	Training Loss: 1.157575 	Validation Loss: 1.377070 	 time: 0.3
Epoch: 1995 	Training Loss: 1.157554 	Validation Loss: 1.377191 	 time: 0.3
Epoch: 1996 	Training Loss: 1.157558 	Validation Loss: 1.376191 	 time: 0.3
Epoch: 1997 	Training Loss: 1.157497 	Validation Loss: 1.376547 	 time: 0.3
Epoch: 1998 	Training Loss: 1.157537 	Validation Loss: 1.376970 	 time: 0.3
Epoch: 1999 	Training Loss: 1.157484 	Validation Loss: 1.377927 	 time: 0.3
Epoch: 2000 	Training Loss: 1.157478 	Validation Loss: 1.380484 	 time: 0.3
Epoch: 2001 	Training Loss: 1.157474 	Validation Loss: 1.379127 	 time: 0.3
Epoch: 2002 	Training Loss: 1.157427 	Validation Loss: 1.376696 	 time: 0.3
Epoch: 2003 	Training Loss: 1.157438 	Validation Loss: 1.377421 	 time: 0.3
Epoch: 2004 	Training Loss: 1.157432 	Validation Loss: 1.374758 	 time: 0.3
Epoch: 2005 	Training Loss: 1.157427 	Validation Loss: 1.376741 	 time: 0.3
Epoch: 2006 	Training Loss: 1.157394 	Validation Loss: 1.376980 	 time: 0.3
Epoch: 2007 	Training Loss: 1.157367 	Validation Loss: 1.376500 	 time: 0.3
Epoch: 2008 	Training Loss: 1.157380 	Validation Loss: 1.377739 	 time: 0.3
Epoch: 2009 	Training Loss: 1.157343 	Validation Loss: 1.377097 	 time: 0.3
Epoch: 2010 	Training Loss: 1.157340 	Validation Loss: 1.378344 	 time: 0.3
Epoch: 2011 	Training Loss: 1.157335 	Validation Loss: 1.375862 	 time: 0.3
Epoch: 2012 	Training Loss: 1.157321 	Validation Loss: 1.377913 	 time: 0.3
Epoch: 2013 	Training Loss: 1.157357 	Validation Loss: 1.374296 	 time: 0.3
Epoch: 2014 	Training Loss: 1.157367 	Validation Loss: 1.377585 	 time: 0.3
Epoch: 2015 	Training Loss: 1.157339 	Validation Loss: 1.375200 	 time: 0.3
Epoch: 2016 	Training Loss: 1.157313 	Validation Loss: 1.375324 	 time: 0.3
Epoch: 2017 	Training Loss: 1.157279 	Validation Loss: 1.375378 	 time: 0.3
Epoch: 2018 	Training Loss: 1.157288 	Validation Loss: 1.374070 	 time: 0.3
Epoch: 2019 	Training Loss: 1.157289 	Validation Loss: 1.377545 	 time: 0.3
Epoch: 2020 	Training Loss: 1.157358 	Validation Loss: 1.371971 	 time: 0.3
Epoch: 2021 	Training Loss: 1.157517 	Validation Loss: 1.377764 	 time: 0.3
Epoch: 2022 	Training Loss: 1.157535 	Validation Loss: 1.372370 	 time: 0.3
Epoch: 2023 	Training Loss: 1.157604 	Validation Loss: 1.376282 	 time: 0.3
Epoch: 2024 	Training Loss: 1.157447 	Validation Loss: 1.375314 	 time: 0.3
Epoch: 2025 	Training Loss: 1.157409 	Validation Loss: 1.373435 	 time: 0.3
Epoch: 2026 	Training Loss: 1.157400 	Validation Loss: 1.376061 	 time: 0.3
Epoch: 2027 	Training Loss: 1.157350 	Validation Loss: 1.374415 	 time: 0.3
Epoch: 2028 	Training Loss: 1.157357 	Validation Loss: 1.373964 	 time: 0.3
Epoch: 2029 	Training Loss: 1.157287 	Validation Loss: 1.375380 	 time: 0.3
Epoch: 2030 	Training Loss: 1.157310 	Validation Loss: 1.375694 	 time: 0.3
Epoch: 2031 	Training Loss: 1.157311 	Validation Loss: 1.376113 	 time: 0.3
Epoch: 2032 	Training Loss: 1.157312 	Validation Loss: 1.376267 	 time: 0.3
Epoch: 2033 	Training Loss: 1.157274 	Validation Loss: 1.376291 	 time: 0.3
Epoch: 2034 	Training Loss: 1.157319 	Validation Loss: 1.373311 	 time: 0.3
Epoch: 2035 	Training Loss: 1.157311 	Validation Loss: 1.374057 	 time: 0.3
Epoch: 2036 	Training Loss: 1.157321 	Validation Loss: 1.374343 	 time: 0.3
Epoch: 2037 	Training Loss: 1.157264 	Validation Loss: 1.375148 	 time: 0.3
Epoch: 2038 	Training Loss: 1.157279 	Validation Loss: 1.376722 	 time: 0.3
Epoch: 2039 	Training Loss: 1.157272 	Validation Loss: 1.376054 	 time: 0.3
Epoch: 2040 	Training Loss: 1.157196 	Validation Loss: 1.375954 	 time: 0.3
Epoch: 2041 	Training Loss: 1.157213 	Validation Loss: 1.375746 	 time: 0.3
Epoch: 2042 	Training Loss: 1.157191 	Validation Loss: 1.377286 	 time: 0.3
Epoch: 2043 	Training Loss: 1.157210 	Validation Loss: 1.373696 	 time: 0.3
Epoch: 2044 	Training Loss: 1.157185 	Validation Loss: 1.374666 	 time: 0.3
Epoch: 2045 	Training Loss: 1.157215 	Validation Loss: 1.373170 	 time: 0.3
Epoch: 2046 	Training Loss: 1.157206 	Validation Loss: 1.373009 	 time: 0.3
Epoch: 2047 	Training Loss: 1.157150 	Validation Loss: 1.375196 	 time: 0.3
Epoch: 2048 	Training Loss: 1.157185 	Validation Loss: 1.372256 	 time: 0.3
Epoch: 2049 	Training Loss: 1.157212 	Validation Loss: 1.375046 	 time: 0.3
Epoch: 2050 	Training Loss: 1.157264 	Validation Loss: 1.371055 	 time: 0.3
Epoch: 2051 	Training Loss: 1.157295 	Validation Loss: 1.376569 	 time: 0.3
Epoch: 2052 	Training Loss: 1.157264 	Validation Loss: 1.372094 	 time: 0.3
Epoch: 2053 	Training Loss: 1.157311 	Validation Loss: 1.373695 	 time: 0.3
Epoch: 2054 	Training Loss: 1.157196 	Validation Loss: 1.374494 	 time: 0.3
Epoch: 2055 	Training Loss: 1.157245 	Validation Loss: 1.370008 	 time: 0.3
Epoch: 2056 	Training Loss: 1.157456 	Validation Loss: 1.375457 	 time: 0.3
Epoch: 2057 	Training Loss: 1.157630 	Validation Loss: 1.371650 	 time: 0.3
Epoch: 2058 	Training Loss: 1.157340 	Validation Loss: 1.374420 	 time: 0.3
Epoch: 2059 	Training Loss: 1.157216 	Validation Loss: 1.374971 	 time: 0.3
Epoch: 2060 	Training Loss: 1.157314 	Validation Loss: 1.372503 	 time: 0.3
Epoch: 2061 	Training Loss: 1.157308 	Validation Loss: 1.374700 	 time: 0.3
Epoch: 2062 	Training Loss: 1.157223 	Validation Loss: 1.376575 	 time: 0.3
Epoch: 2063 	Training Loss: 1.157340 	Validation Loss: 1.371226 	 time: 0.3
Epoch: 2064 	Training Loss: 1.157367 	Validation Loss: 1.374703 	 time: 0.3
Epoch: 2065 	Training Loss: 1.157330 	Validation Loss: 1.375822 	 time: 0.3
Epoch: 2066 	Training Loss: 1.157252 	Validation Loss: 1.372412 	 time: 0.3
Epoch: 2067 	Training Loss: 1.157501 	Validation Loss: 1.380436 	 time: 0.3
Epoch: 2068 	Training Loss: 1.157566 	Validation Loss: 1.375651 	 time: 0.3
Epoch: 2069 	Training Loss: 1.157260 	Validation Loss: 1.375008 	 time: 0.3
Epoch: 2070 	Training Loss: 1.157454 	Validation Loss: 1.381718 	 time: 0.3
Epoch: 2071 	Training Loss: 1.157329 	Validation Loss: 1.379512 	 time: 0.3
Epoch: 2072 	Training Loss: 1.157331 	Validation Loss: 1.376668 	 time: 0.3
Epoch: 2073 	Training Loss: 1.157356 	Validation Loss: 1.378209 	 time: 0.3
Epoch: 2074 	Training Loss: 1.157344 	Validation Loss: 1.376808 	 time: 0.3
Epoch: 2075 	Training Loss: 1.157225 	Validation Loss: 1.378827 	 time: 0.3
Epoch: 2076 	Training Loss: 1.157287 	Validation Loss: 1.381653 	 time: 0.3
Epoch: 2077 	Training Loss: 1.157441 	Validation Loss: 1.379505 	 time: 0.3
Epoch: 2078 	Training Loss: 1.157260 	Validation Loss: 1.376736 	 time: 0.3
Epoch: 2079 	Training Loss: 1.157465 	Validation Loss: 1.377390 	 time: 0.3
Epoch: 2080 	Training Loss: 1.157621 	Validation Loss: 1.378382 	 time: 0.3
Epoch: 2081 	Training Loss: 1.157296 	Validation Loss: 1.376371 	 time: 0.3
Epoch: 2082 	Training Loss: 1.157679 	Validation Loss: 1.376083 	 time: 0.3
Epoch: 2083 	Training Loss: 1.157776 	Validation Loss: 1.378957 	 time: 0.3
Epoch: 2084 	Training Loss: 1.157549 	Validation Loss: 1.376190 	 time: 0.3
Epoch: 2085 	Training Loss: 1.158170 	Validation Loss: 1.382044 	 time: 0.3
Epoch: 2086 	Training Loss: 1.157443 	Validation Loss: 1.387218 	 time: 0.3
Epoch: 2087 	Training Loss: 1.157899 	Validation Loss: 1.375466 	 time: 0.3
Epoch: 2088 	Training Loss: 1.158439 	Validation Loss: 1.380487 	 time: 0.3
Epoch: 2089 	Training Loss: 1.157532 	Validation Loss: 1.383093 	 time: 0.3
Epoch: 2090 	Training Loss: 1.157607 	Validation Loss: 1.374287 	 time: 0.3
Epoch: 2091 	Training Loss: 1.158383 	Validation Loss: 1.380275 	 time: 0.3
Epoch: 2092 	Training Loss: 1.157687 	Validation Loss: 1.376772 	 time: 0.3
Epoch: 2093 	Training Loss: 1.157520 	Validation Loss: 1.369887 	 time: 0.3
Epoch: 2094 	Training Loss: 1.157766 	Validation Loss: 1.378232 	 time: 0.3
Epoch: 2095 	Training Loss: 1.157549 	Validation Loss: 1.381660 	 time: 0.3
Epoch: 2096 	Training Loss: 1.157487 	Validation Loss: 1.377357 	 time: 0.3
Epoch: 2097 	Training Loss: 1.157706 	Validation Loss: 1.383098 	 time: 0.3
Epoch: 2098 	Training Loss: 1.157712 	Validation Loss: 1.377075 	 time: 0.3
Epoch: 2099 	Training Loss: 1.157994 	Validation Loss: 1.372327 	 time: 0.3
Epoch: 2100 	Training Loss: 1.157922 	Validation Loss: 1.384373 	 time: 0.3
Epoch: 2101 	Training Loss: 1.158235 	Validation Loss: 1.376235 	 time: 0.3
Epoch: 2102 	Training Loss: 1.160428 	Validation Loss: 1.406910 	 time: 0.3
Epoch: 2103 	Training Loss: 1.164914 	Validation Loss: 1.403234 	 time: 0.3
Epoch: 2104 	Training Loss: 1.191061 	Validation Loss: 1.462483 	 time: 0.3
Epoch: 2105 	Training Loss: 1.220092 	Validation Loss: 1.413865 	 time: 0.3
Epoch: 2106 	Training Loss: 1.229667 	Validation Loss: 1.439193 	 time: 0.3
Epoch: 2107 	Training Loss: 1.245114 	Validation Loss: 1.437395 	 time: 0.3
Epoch: 2108 	Training Loss: 1.224710 	Validation Loss: 1.452802 	 time: 0.3
Epoch: 2109 	Training Loss: 1.253135 	Validation Loss: 1.413889 	 time: 0.3
Epoch: 2110 	Training Loss: 1.221126 	Validation Loss: 1.398576 	 time: 0.3
Epoch: 2111 	Training Loss: 1.231103 	Validation Loss: 1.416193 	 time: 0.3
Epoch: 2112 	Training Loss: 1.214221 	Validation Loss: 1.431636 	 time: 0.3
Epoch: 2113 	Training Loss: 1.219764 	Validation Loss: 1.421903 	 time: 0.3
Epoch: 2114 	Training Loss: 1.202448 	Validation Loss: 1.419029 	 time: 0.3
Epoch: 2115 	Training Loss: 1.200324 	Validation Loss: 1.420453 	 time: 0.3
Epoch: 2116 	Training Loss: 1.214990 	Validation Loss: 1.409648 	 time: 0.3
Epoch: 2117 	Training Loss: 1.191181 	Validation Loss: 1.407250 	 time: 0.3
Epoch: 2118 	Training Loss: 1.198220 	Validation Loss: 1.406560 	 time: 0.3
Epoch: 2119 	Training Loss: 1.193868 	Validation Loss: 1.402855 	 time: 0.3
Epoch: 2120 	Training Loss: 1.184564 	Validation Loss: 1.413965 	 time: 0.3
Epoch: 2121 	Training Loss: 1.196055 	Validation Loss: 1.399093 	 time: 0.3
Epoch: 2122 	Training Loss: 1.180187 	Validation Loss: 1.421817 	 time: 0.3
Epoch: 2123 	Training Loss: 1.181761 	Validation Loss: 1.416440 	 time: 0.3
Epoch: 2124 	Training Loss: 1.178522 	Validation Loss: 1.397957 	 time: 0.3
Epoch: 2125 	Training Loss: 1.177035 	Validation Loss: 1.397513 	 time: 0.3
Epoch: 2126 	Training Loss: 1.175683 	Validation Loss: 1.397774 	 time: 0.3
Epoch: 2127 	Training Loss: 1.172434 	Validation Loss: 1.398800 	 time: 0.3
Epoch: 2128 	Training Loss: 1.171684 	Validation Loss: 1.393292 	 time: 0.3
Epoch: 2129 	Training Loss: 1.168793 	Validation Loss: 1.398213 	 time: 0.3
Epoch: 2130 	Training Loss: 1.168815 	Validation Loss: 1.385999 	 time: 0.3
Epoch: 2131 	Training Loss: 1.167714 	Validation Loss: 1.385958 	 time: 0.3
Epoch: 2132 	Training Loss: 1.166139 	Validation Loss: 1.388497 	 time: 0.3
Epoch: 2133 	Training Loss: 1.165532 	Validation Loss: 1.383504 	 time: 0.3
Epoch: 2134 	Training Loss: 1.163902 	Validation Loss: 1.379926 	 time: 0.3
Epoch: 2135 	Training Loss: 1.164150 	Validation Loss: 1.377652 	 time: 0.3
Epoch: 2136 	Training Loss: 1.162937 	Validation Loss: 1.379009 	 time: 0.3
Epoch: 2137 	Training Loss: 1.162040 	Validation Loss: 1.373703 	 time: 0.3
Epoch: 2138 	Training Loss: 1.161822 	Validation Loss: 1.366029 	 time: 0.3
Epoch: 2139 	Training Loss: 1.161032 	Validation Loss: 1.369272 	 time: 0.3
Epoch: 2140 	Training Loss: 1.160507 	Validation Loss: 1.371147 	 time: 0.3
Epoch: 2141 	Training Loss: 1.160287 	Validation Loss: 1.368808 	 time: 0.3
Epoch: 2142 	Training Loss: 1.159850 	Validation Loss: 1.366623 	 time: 0.3
Epoch: 2143 	Training Loss: 1.159553 	Validation Loss: 1.362821 	 time: 0.3
Epoch: 2144 	Training Loss: 1.159171 	Validation Loss: 1.364537 	 time: 0.3
Epoch: 2145 	Training Loss: 1.158955 	Validation Loss: 1.366401 	 time: 0.3
Epoch: 2146 	Training Loss: 1.158643 	Validation Loss: 1.363778 	 time: 0.3
Epoch: 2147 	Training Loss: 1.158334 	Validation Loss: 1.358330 	 time: 0.3
Validation loss decreased from 1.359244 to 1.358330. Model was saved
Epoch: 2148 	Training Loss: 1.158065 	Validation Loss: 1.357869 	 time: 0.2
Validation loss decreased from 1.358330 to 1.357869. Model was saved
Epoch: 2149 	Training Loss: 1.157811 	Validation Loss: 1.356641 	 time: 0.3
Validation loss decreased from 1.357869 to 1.356641. Model was saved
Epoch: 2150 	Training Loss: 1.157604 	Validation Loss: 1.356866 	 time: 0.3
Epoch: 2151 	Training Loss: 1.157403 	Validation Loss: 1.357722 	 time: 0.3
Epoch: 2152 	Training Loss: 1.157264 	Validation Loss: 1.357050 	 time: 0.3
Epoch: 2153 	Training Loss: 1.157100 	Validation Loss: 1.359194 	 time: 0.3
Epoch: 2154 	Training Loss: 1.156911 	Validation Loss: 1.360564 	 time: 0.3
Epoch: 2155 	Training Loss: 1.156853 	Validation Loss: 1.360353 	 time: 0.3
Epoch: 2156 	Training Loss: 1.156694 	Validation Loss: 1.359819 	 time: 0.3
Epoch: 2157 	Training Loss: 1.156609 	Validation Loss: 1.360407 	 time: 0.2
Epoch: 2158 	Training Loss: 1.156536 	Validation Loss: 1.361465 	 time: 0.3
Epoch: 2159 	Training Loss: 1.156423 	Validation Loss: 1.362559 	 time: 0.3
Epoch: 2160 	Training Loss: 1.156335 	Validation Loss: 1.364001 	 time: 0.3
Epoch: 2161 	Training Loss: 1.156275 	Validation Loss: 1.364249 	 time: 0.3
Epoch: 2162 	Training Loss: 1.156180 	Validation Loss: 1.365026 	 time: 0.3
Epoch: 2163 	Training Loss: 1.156137 	Validation Loss: 1.365822 	 time: 0.3
Epoch: 2164 	Training Loss: 1.156042 	Validation Loss: 1.366760 	 time: 0.3
Epoch: 2165 	Training Loss: 1.155995 	Validation Loss: 1.367129 	 time: 0.3
Epoch: 2166 	Training Loss: 1.155927 	Validation Loss: 1.367152 	 time: 0.3
Epoch: 2167 	Training Loss: 1.155900 	Validation Loss: 1.366789 	 time: 0.3
Epoch: 2168 	Training Loss: 1.155856 	Validation Loss: 1.366864 	 time: 0.3
Epoch: 2169 	Training Loss: 1.155839 	Validation Loss: 1.367536 	 time: 0.3
Epoch: 2170 	Training Loss: 1.155777 	Validation Loss: 1.368418 	 time: 0.3
Epoch: 2171 	Training Loss: 1.155740 	Validation Loss: 1.369181 	 time: 0.3
Epoch: 2172 	Training Loss: 1.155708 	Validation Loss: 1.369409 	 time: 0.3
Epoch: 2173 	Training Loss: 1.155686 	Validation Loss: 1.369043 	 time: 0.3
Epoch: 2174 	Training Loss: 1.155654 	Validation Loss: 1.368265 	 time: 0.3
Epoch: 2175 	Training Loss: 1.155632 	Validation Loss: 1.367587 	 time: 0.3
Epoch: 2176 	Training Loss: 1.155613 	Validation Loss: 1.366745 	 time: 0.3
Epoch: 2177 	Training Loss: 1.155600 	Validation Loss: 1.366009 	 time: 0.3
Epoch: 2178 	Training Loss: 1.155575 	Validation Loss: 1.365737 	 time: 0.3
Epoch: 2179 	Training Loss: 1.155555 	Validation Loss: 1.365575 	 time: 0.3
Epoch: 2180 	Training Loss: 1.155540 	Validation Loss: 1.365088 	 time: 0.3
Epoch: 2181 	Training Loss: 1.155526 	Validation Loss: 1.364796 	 time: 0.3
Epoch: 2182 	Training Loss: 1.155512 	Validation Loss: 1.365118 	 time: 0.3
Epoch: 2183 	Training Loss: 1.155490 	Validation Loss: 1.365481 	 time: 0.3
Epoch: 2184 	Training Loss: 1.155472 	Validation Loss: 1.365619 	 time: 0.3
Epoch: 2185 	Training Loss: 1.155455 	Validation Loss: 1.365913 	 time: 0.3
Epoch: 2186 	Training Loss: 1.155436 	Validation Loss: 1.366269 	 time: 0.3
Epoch: 2187 	Training Loss: 1.155409 	Validation Loss: 1.366143 	 time: 0.3
Epoch: 2188 	Training Loss: 1.155375 	Validation Loss: 1.365443 	 time: 0.2
Epoch: 2189 	Training Loss: 1.155336 	Validation Loss: 1.364607 	 time: 0.3
Epoch: 2190 	Training Loss: 1.155301 	Validation Loss: 1.363812 	 time: 0.3
Epoch: 2191 	Training Loss: 1.155266 	Validation Loss: 1.363266 	 time: 0.3
Epoch: 2192 	Training Loss: 1.155239 	Validation Loss: 1.362764 	 time: 0.3
Epoch: 2193 	Training Loss: 1.155221 	Validation Loss: 1.362502 	 time: 0.3
Epoch: 2194 	Training Loss: 1.155216 	Validation Loss: 1.362614 	 time: 0.3
Epoch: 2195 	Training Loss: 1.155211 	Validation Loss: 1.362729 	 time: 0.3
Epoch: 2196 	Training Loss: 1.155191 	Validation Loss: 1.362621 	 time: 0.3
Epoch: 2197 	Training Loss: 1.155169 	Validation Loss: 1.362771 	 time: 0.3
Epoch: 2198 	Training Loss: 1.155150 	Validation Loss: 1.363081 	 time: 0.3
Epoch: 2199 	Training Loss: 1.155136 	Validation Loss: 1.363247 	 time: 0.3
Epoch: 2200 	Training Loss: 1.155123 	Validation Loss: 1.363688 	 time: 0.3
Epoch: 2201 	Training Loss: 1.155112 	Validation Loss: 1.364402 	 time: 0.3
Epoch: 2202 	Training Loss: 1.155099 	Validation Loss: 1.364910 	 time: 0.3
Epoch: 2203 	Training Loss: 1.155083 	Validation Loss: 1.365393 	 time: 0.3
Epoch: 2204 	Training Loss: 1.155062 	Validation Loss: 1.365971 	 time: 0.3
Epoch: 2205 	Training Loss: 1.155040 	Validation Loss: 1.366220 	 time: 0.3
Epoch: 2206 	Training Loss: 1.155022 	Validation Loss: 1.366333 	 time: 0.3
Epoch: 2207 	Training Loss: 1.155007 	Validation Loss: 1.366499 	 time: 0.3
Epoch: 2208 	Training Loss: 1.154987 	Validation Loss: 1.366479 	 time: 0.3
Epoch: 2209 	Training Loss: 1.154954 	Validation Loss: 1.366394 	 time: 0.3
Epoch: 2210 	Training Loss: 1.154896 	Validation Loss: 1.366347 	 time: 0.3
Epoch: 2211 	Training Loss: 1.154780 	Validation Loss: 1.366094 	 time: 0.3
Epoch: 2212 	Training Loss: 1.154622 	Validation Loss: 1.365670 	 time: 0.3
Epoch: 2213 	Training Loss: 1.154548 	Validation Loss: 1.365228 	 time: 0.3
Epoch: 2214 	Training Loss: 1.154537 	Validation Loss: 1.364673 	 time: 0.3
Epoch: 2215 	Training Loss: 1.154535 	Validation Loss: 1.363878 	 time: 0.3
Epoch: 2216 	Training Loss: 1.154523 	Validation Loss: 1.362983 	 time: 0.3
Epoch: 2217 	Training Loss: 1.154507 	Validation Loss: 1.362270 	 time: 0.3
Epoch: 2218 	Training Loss: 1.154497 	Validation Loss: 1.361843 	 time: 0.3
Epoch: 2219 	Training Loss: 1.154496 	Validation Loss: 1.361677 	 time: 0.3
Epoch: 2220 	Training Loss: 1.154490 	Validation Loss: 1.361749 	 time: 0.3
Epoch: 2221 	Training Loss: 1.154476 	Validation Loss: 1.361966 	 time: 0.3
Epoch: 2222 	Training Loss: 1.154464 	Validation Loss: 1.362097 	 time: 0.3
Epoch: 2223 	Training Loss: 1.154459 	Validation Loss: 1.362025 	 time: 0.3
Epoch: 2224 	Training Loss: 1.154454 	Validation Loss: 1.361810 	 time: 0.3
Epoch: 2225 	Training Loss: 1.154445 	Validation Loss: 1.361612 	 time: 0.3
Epoch: 2226 	Training Loss: 1.154432 	Validation Loss: 1.361557 	 time: 0.3
Epoch: 2227 	Training Loss: 1.154418 	Validation Loss: 1.361535 	 time: 0.3
Epoch: 2228 	Training Loss: 1.154394 	Validation Loss: 1.361526 	 time: 0.3
Epoch: 2229 	Training Loss: 1.154380 	Validation Loss: 1.361701 	 time: 0.3
Epoch: 2230 	Training Loss: 1.154362 	Validation Loss: 1.361879 	 time: 0.3
Epoch: 2231 	Training Loss: 1.154341 	Validation Loss: 1.361829 	 time: 0.3
Epoch: 2232 	Training Loss: 1.154329 	Validation Loss: 1.361959 	 time: 0.3
Epoch: 2233 	Training Loss: 1.154325 	Validation Loss: 1.362200 	 time: 0.3
Epoch: 2234 	Training Loss: 1.154323 	Validation Loss: 1.362194 	 time: 0.3
Epoch: 2235 	Training Loss: 1.154317 	Validation Loss: 1.362366 	 time: 0.3
Epoch: 2236 	Training Loss: 1.154312 	Validation Loss: 1.362658 	 time: 0.3
Epoch: 2237 	Training Loss: 1.154307 	Validation Loss: 1.362728 	 time: 0.3
Epoch: 2238 	Training Loss: 1.154301 	Validation Loss: 1.362843 	 time: 0.3
Epoch: 2239 	Training Loss: 1.154295 	Validation Loss: 1.362935 	 time: 0.3
Epoch: 2240 	Training Loss: 1.154288 	Validation Loss: 1.362795 	 time: 0.3
Epoch: 2241 	Training Loss: 1.154279 	Validation Loss: 1.362664 	 time: 0.3
Epoch: 2242 	Training Loss: 1.154262 	Validation Loss: 1.362621 	 time: 0.3
Epoch: 2243 	Training Loss: 1.154244 	Validation Loss: 1.362535 	 time: 0.3
Epoch: 2244 	Training Loss: 1.154234 	Validation Loss: 1.362640 	 time: 0.3
Epoch: 2245 	Training Loss: 1.154229 	Validation Loss: 1.362781 	 time: 0.3
Epoch: 2246 	Training Loss: 1.154225 	Validation Loss: 1.362812 	 time: 0.3
Epoch: 2247 	Training Loss: 1.154220 	Validation Loss: 1.362885 	 time: 0.3
Epoch: 2248 	Training Loss: 1.154215 	Validation Loss: 1.362895 	 time: 0.3
Epoch: 2249 	Training Loss: 1.154211 	Validation Loss: 1.362838 	 time: 0.3
Epoch: 2250 	Training Loss: 1.154207 	Validation Loss: 1.362871 	 time: 0.3
Epoch: 2251 	Training Loss: 1.154203 	Validation Loss: 1.362874 	 time: 0.3
Epoch: 2252 	Training Loss: 1.154199 	Validation Loss: 1.362877 	 time: 0.3
Epoch: 2253 	Training Loss: 1.154195 	Validation Loss: 1.363001 	 time: 0.3
Epoch: 2254 	Training Loss: 1.154192 	Validation Loss: 1.363091 	 time: 0.3
Epoch: 2255 	Training Loss: 1.154188 	Validation Loss: 1.363167 	 time: 0.3
Epoch: 2256 	Training Loss: 1.154185 	Validation Loss: 1.363285 	 time: 0.3
Epoch: 2257 	Training Loss: 1.154182 	Validation Loss: 1.363310 	 time: 0.3
Epoch: 2258 	Training Loss: 1.154180 	Validation Loss: 1.363312 	 time: 0.3
Epoch: 2259 	Training Loss: 1.154177 	Validation Loss: 1.363364 	 time: 0.3
Epoch: 2260 	Training Loss: 1.154174 	Validation Loss: 1.363371 	 time: 0.3
Epoch: 2261 	Training Loss: 1.154171 	Validation Loss: 1.363408 	 time: 0.3
Epoch: 2262 	Training Loss: 1.154168 	Validation Loss: 1.363465 	 time: 0.3
Epoch: 2263 	Training Loss: 1.154166 	Validation Loss: 1.363464 	 time: 0.3
Epoch: 2264 	Training Loss: 1.154163 	Validation Loss: 1.363487 	 time: 0.3
Epoch: 2265 	Training Loss: 1.154160 	Validation Loss: 1.363499 	 time: 0.3
Epoch: 2266 	Training Loss: 1.154157 	Validation Loss: 1.363468 	 time: 0.3
Epoch: 2267 	Training Loss: 1.154153 	Validation Loss: 1.363464 	 time: 0.3
Epoch: 2268 	Training Loss: 1.154149 	Validation Loss: 1.363433 	 time: 0.3
Epoch: 2269 	Training Loss: 1.154145 	Validation Loss: 1.363400 	 time: 0.3
Epoch: 2270 	Training Loss: 1.154136 	Validation Loss: 1.363403 	 time: 0.3
Epoch: 2271 	Training Loss: 1.154128 	Validation Loss: 1.363359 	 time: 0.3
Epoch: 2272 	Training Loss: 1.154128 	Validation Loss: 1.363376 	 time: 0.3
Epoch: 2273 	Training Loss: 1.154120 	Validation Loss: 1.363360 	 time: 0.3
Epoch: 2274 	Training Loss: 1.154116 	Validation Loss: 1.363287 	 time: 0.3
Epoch: 2275 	Training Loss: 1.154113 	Validation Loss: 1.363313 	 time: 0.3
Epoch: 2276 	Training Loss: 1.154110 	Validation Loss: 1.363316 	 time: 0.3
Epoch: 2277 	Training Loss: 1.154106 	Validation Loss: 1.363290 	 time: 0.3
Epoch: 2278 	Training Loss: 1.154099 	Validation Loss: 1.363302 	 time: 0.3
Epoch: 2279 	Training Loss: 1.154091 	Validation Loss: 1.363250 	 time: 0.3
Epoch: 2280 	Training Loss: 1.154085 	Validation Loss: 1.363192 	 time: 0.3
Epoch: 2281 	Training Loss: 1.154080 	Validation Loss: 1.363051 	 time: 0.3
Epoch: 2282 	Training Loss: 1.154072 	Validation Loss: 1.362957 	 time: 0.3
Epoch: 2283 	Training Loss: 1.154063 	Validation Loss: 1.362790 	 time: 0.3
Epoch: 2284 	Training Loss: 1.154057 	Validation Loss: 1.362675 	 time: 0.3
Epoch: 2285 	Training Loss: 1.154055 	Validation Loss: 1.362553 	 time: 0.3
Epoch: 2286 	Training Loss: 1.154052 	Validation Loss: 1.362434 	 time: 0.3
Epoch: 2287 	Training Loss: 1.154047 	Validation Loss: 1.362342 	 time: 0.3
Epoch: 2288 	Training Loss: 1.154042 	Validation Loss: 1.362237 	 time: 0.3
Epoch: 2289 	Training Loss: 1.154037 	Validation Loss: 1.362212 	 time: 0.3
Epoch: 2290 	Training Loss: 1.154028 	Validation Loss: 1.362156 	 time: 0.3
Epoch: 2291 	Training Loss: 1.154019 	Validation Loss: 1.362152 	 time: 0.3
Epoch: 2292 	Training Loss: 1.154012 	Validation Loss: 1.362132 	 time: 0.3
Epoch: 2293 	Training Loss: 1.154007 	Validation Loss: 1.362109 	 time: 0.3
Epoch: 2294 	Training Loss: 1.154004 	Validation Loss: 1.362092 	 time: 0.3
Epoch: 2295 	Training Loss: 1.154001 	Validation Loss: 1.362138 	 time: 0.3
Epoch: 2296 	Training Loss: 1.153996 	Validation Loss: 1.362225 	 time: 0.3
Epoch: 2297 	Training Loss: 1.153992 	Validation Loss: 1.362303 	 time: 0.3
Epoch: 2298 	Training Loss: 1.153987 	Validation Loss: 1.362367 	 time: 0.3
Epoch: 2299 	Training Loss: 1.153981 	Validation Loss: 1.362368 	 time: 0.3
Epoch: 2300 	Training Loss: 1.153974 	Validation Loss: 1.362339 	 time: 0.3
Epoch: 2301 	Training Loss: 1.153967 	Validation Loss: 1.362279 	 time: 0.3
Epoch: 2302 	Training Loss: 1.153960 	Validation Loss: 1.362242 	 time: 0.3
Epoch: 2303 	Training Loss: 1.153952 	Validation Loss: 1.362205 	 time: 0.3
Epoch: 2304 	Training Loss: 1.153943 	Validation Loss: 1.362158 	 time: 0.3
Epoch: 2305 	Training Loss: 1.153938 	Validation Loss: 1.362109 	 time: 0.3
Epoch: 2306 	Training Loss: 1.153934 	Validation Loss: 1.362053 	 time: 0.3
Epoch: 2307 	Training Loss: 1.153931 	Validation Loss: 1.361999 	 time: 0.3
Epoch: 2308 	Training Loss: 1.153928 	Validation Loss: 1.361968 	 time: 0.3
Epoch: 2309 	Training Loss: 1.153926 	Validation Loss: 1.361971 	 time: 0.3
Epoch: 2310 	Training Loss: 1.153923 	Validation Loss: 1.361979 	 time: 0.3
Epoch: 2311 	Training Loss: 1.153921 	Validation Loss: 1.361985 	 time: 0.3
Epoch: 2312 	Training Loss: 1.153918 	Validation Loss: 1.361979 	 time: 0.3
Epoch: 2313 	Training Loss: 1.153915 	Validation Loss: 1.361954 	 time: 0.3
Epoch: 2314 	Training Loss: 1.153912 	Validation Loss: 1.361920 	 time: 0.3
Epoch: 2315 	Training Loss: 1.153910 	Validation Loss: 1.361907 	 time: 0.3
Epoch: 2316 	Training Loss: 1.153907 	Validation Loss: 1.361902 	 time: 0.3
Epoch: 2317 	Training Loss: 1.153904 	Validation Loss: 1.361899 	 time: 0.3
Epoch: 2318 	Training Loss: 1.153902 	Validation Loss: 1.361866 	 time: 0.3
Epoch: 2319 	Training Loss: 1.153899 	Validation Loss: 1.361826 	 time: 0.3
Epoch: 2320 	Training Loss: 1.153897 	Validation Loss: 1.361760 	 time: 0.3
Epoch: 2321 	Training Loss: 1.153894 	Validation Loss: 1.361734 	 time: 0.3
Epoch: 2322 	Training Loss: 1.153891 	Validation Loss: 1.361685 	 time: 0.3
Epoch: 2323 	Training Loss: 1.153889 	Validation Loss: 1.361704 	 time: 0.3
Epoch: 2324 	Training Loss: 1.153886 	Validation Loss: 1.361622 	 time: 0.3
Epoch: 2325 	Training Loss: 1.153883 	Validation Loss: 1.361668 	 time: 0.3
Epoch: 2326 	Training Loss: 1.153881 	Validation Loss: 1.361501 	 time: 0.3
Epoch: 2327 	Training Loss: 1.153880 	Validation Loss: 1.361646 	 time: 0.3
Epoch: 2328 	Training Loss: 1.153880 	Validation Loss: 1.361287 	 time: 0.3
Epoch: 2329 	Training Loss: 1.153883 	Validation Loss: 1.361664 	 time: 0.3
Epoch: 2330 	Training Loss: 1.153889 	Validation Loss: 1.360973 	 time: 0.3
Epoch: 2331 	Training Loss: 1.153909 	Validation Loss: 1.361647 	 time: 0.3
Epoch: 2332 	Training Loss: 1.153912 	Validation Loss: 1.360921 	 time: 0.3
Epoch: 2333 	Training Loss: 1.153916 	Validation Loss: 1.361476 	 time: 0.3
Epoch: 2334 	Training Loss: 1.153897 	Validation Loss: 1.360738 	 time: 0.3
Epoch: 2335 	Training Loss: 1.153887 	Validation Loss: 1.360811 	 time: 0.3
Epoch: 2336 	Training Loss: 1.153874 	Validation Loss: 1.361275 	 time: 0.3
Epoch: 2337 	Training Loss: 1.153860 	Validation Loss: 1.361197 	 time: 0.3
Epoch: 2338 	Training Loss: 1.153872 	Validation Loss: 1.361165 	 time: 0.3
Epoch: 2339 	Training Loss: 1.153866 	Validation Loss: 1.360699 	 time: 0.3
Epoch: 2340 	Training Loss: 1.153861 	Validation Loss: 1.361390 	 time: 0.3
Epoch: 2341 	Training Loss: 1.153866 	Validation Loss: 1.360536 	 time: 0.3
Epoch: 2342 	Training Loss: 1.153890 	Validation Loss: 1.361696 	 time: 0.3
Epoch: 2343 	Training Loss: 1.153942 	Validation Loss: 1.360843 	 time: 0.3
Epoch: 2344 	Training Loss: 1.153939 	Validation Loss: 1.362235 	 time: 0.3
Epoch: 2345 	Training Loss: 1.153974 	Validation Loss: 1.361007 	 time: 0.3
Epoch: 2346 	Training Loss: 1.153872 	Validation Loss: 1.359301 	 time: 0.3
Epoch: 2347 	Training Loss: 1.153960 	Validation Loss: 1.361022 	 time: 0.3
Epoch: 2348 	Training Loss: 1.153986 	Validation Loss: 1.360763 	 time: 0.3
Epoch: 2349 	Training Loss: 1.153965 	Validation Loss: 1.360500 	 time: 0.3
Epoch: 2350 	Training Loss: 1.153973 	Validation Loss: 1.360839 	 time: 0.3
Epoch: 2351 	Training Loss: 1.153992 	Validation Loss: 1.359855 	 time: 0.3
Epoch: 2352 	Training Loss: 1.153948 	Validation Loss: 1.359732 	 time: 0.3
Epoch: 2353 	Training Loss: 1.153923 	Validation Loss: 1.360135 	 time: 0.3
Epoch: 2354 	Training Loss: 1.153928 	Validation Loss: 1.358622 	 time: 0.3
Epoch: 2355 	Training Loss: 1.153928 	Validation Loss: 1.359976 	 time: 0.3
Epoch: 2356 	Training Loss: 1.153969 	Validation Loss: 1.360346 	 time: 0.3
Epoch: 2357 	Training Loss: 1.153989 	Validation Loss: 1.359859 	 time: 0.3
Epoch: 2358 	Training Loss: 1.153922 	Validation Loss: 1.360643 	 time: 0.3
Epoch: 2359 	Training Loss: 1.154012 	Validation Loss: 1.360022 	 time: 0.3
Epoch: 2360 	Training Loss: 1.153917 	Validation Loss: 1.358839 	 time: 0.3
Epoch: 2361 	Training Loss: 1.153940 	Validation Loss: 1.359840 	 time: 0.3
Epoch: 2362 	Training Loss: 1.153955 	Validation Loss: 1.359088 	 time: 0.3
Epoch: 2363 	Training Loss: 1.153957 	Validation Loss: 1.357993 	 time: 0.3
Epoch: 2364 	Training Loss: 1.153965 	Validation Loss: 1.359584 	 time: 0.3
Epoch: 2365 	Training Loss: 1.153903 	Validation Loss: 1.360790 	 time: 0.3
Epoch: 2366 	Training Loss: 1.153939 	Validation Loss: 1.362086 	 time: 0.3
Epoch: 2367 	Training Loss: 1.153930 	Validation Loss: 1.360872 	 time: 0.3
Epoch: 2368 	Training Loss: 1.153888 	Validation Loss: 1.360507 	 time: 0.3
Epoch: 2369 	Training Loss: 1.153913 	Validation Loss: 1.360990 	 time: 0.3
Epoch: 2370 	Training Loss: 1.153842 	Validation Loss: 1.360960 	 time: 0.3
Epoch: 2371 	Training Loss: 1.153906 	Validation Loss: 1.360187 	 time: 0.3
Epoch: 2372 	Training Loss: 1.153852 	Validation Loss: 1.360250 	 time: 0.3
Epoch: 2373 	Training Loss: 1.153875 	Validation Loss: 1.360426 	 time: 0.3
Epoch: 2374 	Training Loss: 1.153856 	Validation Loss: 1.361681 	 time: 0.3
Epoch: 2375 	Training Loss: 1.153870 	Validation Loss: 1.360578 	 time: 0.3
Epoch: 2376 	Training Loss: 1.153847 	Validation Loss: 1.360564 	 time: 0.3
Epoch: 2377 	Training Loss: 1.153843 	Validation Loss: 1.360935 	 time: 0.3
Epoch: 2378 	Training Loss: 1.153834 	Validation Loss: 1.360971 	 time: 0.3
Epoch: 2379 	Training Loss: 1.153827 	Validation Loss: 1.360619 	 time: 0.3
Epoch: 2380 	Training Loss: 1.153831 	Validation Loss: 1.360928 	 time: 0.3
Epoch: 2381 	Training Loss: 1.153808 	Validation Loss: 1.361272 	 time: 0.3
Epoch: 2382 	Training Loss: 1.153812 	Validation Loss: 1.361007 	 time: 0.3
Epoch: 2383 	Training Loss: 1.153837 	Validation Loss: 1.361925 	 time: 0.3
Epoch: 2384 	Training Loss: 1.153838 	Validation Loss: 1.361674 	 time: 0.3
Epoch: 2385 	Training Loss: 1.153880 	Validation Loss: 1.362158 	 time: 0.3
Epoch: 2386 	Training Loss: 1.153900 	Validation Loss: 1.361796 	 time: 0.3
Epoch: 2387 	Training Loss: 1.153892 	Validation Loss: 1.362240 	 time: 0.3
Epoch: 2388 	Training Loss: 1.153792 	Validation Loss: 1.362557 	 time: 0.3
Epoch: 2389 	Training Loss: 1.153908 	Validation Loss: 1.361847 	 time: 0.3
Epoch: 2390 	Training Loss: 1.153992 	Validation Loss: 1.361505 	 time: 0.3
Epoch: 2391 	Training Loss: 1.153937 	Validation Loss: 1.362143 	 time: 0.3
Epoch: 2392 	Training Loss: 1.153840 	Validation Loss: 1.362909 	 time: 0.3
Epoch: 2393 	Training Loss: 1.153841 	Validation Loss: 1.362888 	 time: 0.3
Epoch: 2394 	Training Loss: 1.153863 	Validation Loss: 1.362449 	 time: 0.3
Epoch: 2395 	Training Loss: 1.153835 	Validation Loss: 1.362378 	 time: 0.3
Epoch: 2396 	Training Loss: 1.153851 	Validation Loss: 1.362996 	 time: 0.3
Epoch: 2397 	Training Loss: 1.153873 	Validation Loss: 1.363526 	 time: 0.3
Epoch: 2398 	Training Loss: 1.153834 	Validation Loss: 1.363364 	 time: 0.3
Epoch: 2399 	Training Loss: 1.153837 	Validation Loss: 1.363267 	 time: 0.3
Epoch: 2400 	Training Loss: 1.153833 	Validation Loss: 1.363330 	 time: 0.3
Epoch: 2401 	Training Loss: 1.153806 	Validation Loss: 1.363114 	 time: 0.3
Epoch: 2402 	Training Loss: 1.153820 	Validation Loss: 1.362581 	 time: 0.3
Epoch: 2403 	Training Loss: 1.153802 	Validation Loss: 1.362715 	 time: 0.3
Epoch: 2404 	Training Loss: 1.153802 	Validation Loss: 1.363543 	 time: 0.3
Epoch: 2405 	Training Loss: 1.153797 	Validation Loss: 1.363803 	 time: 0.3
Epoch: 2406 	Training Loss: 1.153793 	Validation Loss: 1.363404 	 time: 0.3
Epoch: 2407 	Training Loss: 1.153783 	Validation Loss: 1.363373 	 time: 0.3
Epoch: 2408 	Training Loss: 1.153787 	Validation Loss: 1.363957 	 time: 0.3
Epoch: 2409 	Training Loss: 1.153781 	Validation Loss: 1.364112 	 time: 0.3
Epoch: 2410 	Training Loss: 1.153774 	Validation Loss: 1.363540 	 time: 0.3
Epoch: 2411 	Training Loss: 1.153766 	Validation Loss: 1.363468 	 time: 0.3
Epoch: 2412 	Training Loss: 1.153721 	Validation Loss: 1.363671 	 time: 0.3
Epoch: 2413 	Training Loss: 1.153722 	Validation Loss: 1.363387 	 time: 0.3
Epoch: 2414 	Training Loss: 1.153767 	Validation Loss: 1.363461 	 time: 0.3
Epoch: 2415 	Training Loss: 1.153753 	Validation Loss: 1.362754 	 time: 0.3
Epoch: 2416 	Training Loss: 1.153865 	Validation Loss: 1.363417 	 time: 0.3
Epoch: 2417 	Training Loss: 1.153847 	Validation Loss: 1.363440 	 time: 0.3
Epoch: 2418 	Training Loss: 1.153849 	Validation Loss: 1.363535 	 time: 0.3
Epoch: 2419 	Training Loss: 1.153811 	Validation Loss: 1.363463 	 time: 0.3
Epoch: 2420 	Training Loss: 1.153841 	Validation Loss: 1.363248 	 time: 0.3
Epoch: 2421 	Training Loss: 1.153894 	Validation Loss: 1.363477 	 time: 0.3
Epoch: 2422 	Training Loss: 1.153875 	Validation Loss: 1.362617 	 time: 0.3
Epoch: 2423 	Training Loss: 1.153758 	Validation Loss: 1.362874 	 time: 0.3
Epoch: 2424 	Training Loss: 1.153793 	Validation Loss: 1.363921 	 time: 0.3
Epoch: 2425 	Training Loss: 1.153816 	Validation Loss: 1.363241 	 time: 0.3
Epoch: 2426 	Training Loss: 1.153870 	Validation Loss: 1.363114 	 time: 0.3
Epoch: 2427 	Training Loss: 1.153961 	Validation Loss: 1.362807 	 time: 0.3
Epoch: 2428 	Training Loss: 1.153865 	Validation Loss: 1.362688 	 time: 0.3
Epoch: 2429 	Training Loss: 1.153875 	Validation Loss: 1.362909 	 time: 0.3
Epoch: 2430 	Training Loss: 1.153825 	Validation Loss: 1.363392 	 time: 0.3
Epoch: 2431 	Training Loss: 1.153858 	Validation Loss: 1.362906 	 time: 0.3
Epoch: 2432 	Training Loss: 1.153837 	Validation Loss: 1.363307 	 time: 0.3
Epoch: 2433 	Training Loss: 1.153823 	Validation Loss: 1.364209 	 time: 0.3
Epoch: 2434 	Training Loss: 1.153811 	Validation Loss: 1.363747 	 time: 0.3
Epoch: 2435 	Training Loss: 1.153820 	Validation Loss: 1.363748 	 time: 0.3
Epoch: 2436 	Training Loss: 1.153780 	Validation Loss: 1.364020 	 time: 0.3
Epoch: 2437 	Training Loss: 1.153801 	Validation Loss: 1.364106 	 time: 0.3
Epoch: 2438 	Training Loss: 1.153808 	Validation Loss: 1.364518 	 time: 0.3
Epoch: 2439 	Training Loss: 1.153822 	Validation Loss: 1.363697 	 time: 0.3
Epoch: 2440 	Training Loss: 1.153794 	Validation Loss: 1.363445 	 time: 0.3
Epoch: 2441 	Training Loss: 1.153793 	Validation Loss: 1.364505 	 time: 0.3
Epoch: 2442 	Training Loss: 1.153806 	Validation Loss: 1.364143 	 time: 0.3
Epoch: 2443 	Training Loss: 1.153785 	Validation Loss: 1.364147 	 time: 0.3
Epoch: 2444 	Training Loss: 1.153754 	Validation Loss: 1.364384 	 time: 0.3
Epoch: 2445 	Training Loss: 1.153786 	Validation Loss: 1.363821 	 time: 0.3
Epoch: 2446 	Training Loss: 1.153765 	Validation Loss: 1.364528 	 time: 0.3
Epoch: 2447 	Training Loss: 1.153755 	Validation Loss: 1.364210 	 time: 0.3
Epoch: 2448 	Training Loss: 1.153706 	Validation Loss: 1.364562 	 time: 0.3
Epoch: 2449 	Training Loss: 1.153726 	Validation Loss: 1.364237 	 time: 0.3
Epoch: 2450 	Training Loss: 1.153699 	Validation Loss: 1.363824 	 time: 0.3
Epoch: 2451 	Training Loss: 1.153706 	Validation Loss: 1.363909 	 time: 0.3
Epoch: 2452 	Training Loss: 1.153699 	Validation Loss: 1.364565 	 time: 0.3
Epoch: 2453 	Training Loss: 1.153690 	Validation Loss: 1.364553 	 time: 0.3
Epoch: 2454 	Training Loss: 1.153698 	Validation Loss: 1.364847 	 time: 0.3
Epoch: 2455 	Training Loss: 1.153686 	Validation Loss: 1.364593 	 time: 0.3
Epoch: 2456 	Training Loss: 1.153685 	Validation Loss: 1.364609 	 time: 0.3
Epoch: 2457 	Training Loss: 1.153670 	Validation Loss: 1.364940 	 time: 0.3
Epoch: 2458 	Training Loss: 1.153685 	Validation Loss: 1.364509 	 time: 0.3
Epoch: 2459 	Training Loss: 1.153675 	Validation Loss: 1.365019 	 time: 0.3
Epoch: 2460 	Training Loss: 1.153645 	Validation Loss: 1.364758 	 time: 0.3
Epoch: 2461 	Training Loss: 1.153613 	Validation Loss: 1.364627 	 time: 0.3
Epoch: 2462 	Training Loss: 1.153616 	Validation Loss: 1.364617 	 time: 0.3
Epoch: 2463 	Training Loss: 1.153614 	Validation Loss: 1.364458 	 time: 0.3
Epoch: 2464 	Training Loss: 1.153621 	Validation Loss: 1.364860 	 time: 0.3
Epoch: 2465 	Training Loss: 1.153605 	Validation Loss: 1.364729 	 time: 0.3
Epoch: 2466 	Training Loss: 1.153608 	Validation Loss: 1.364714 	 time: 0.3
Epoch: 2467 	Training Loss: 1.153599 	Validation Loss: 1.364910 	 time: 0.3
Epoch: 2468 	Training Loss: 1.153604 	Validation Loss: 1.364804 	 time: 0.3
Epoch: 2469 	Training Loss: 1.153600 	Validation Loss: 1.365189 	 time: 0.3
Epoch: 2470 	Training Loss: 1.153605 	Validation Loss: 1.364830 	 time: 0.3
Epoch: 2471 	Training Loss: 1.153602 	Validation Loss: 1.365239 	 time: 0.3
Epoch: 2472 	Training Loss: 1.153604 	Validation Loss: 1.365000 	 time: 0.3
Epoch: 2473 	Training Loss: 1.153596 	Validation Loss: 1.365247 	 time: 0.3
Epoch: 2474 	Training Loss: 1.153592 	Validation Loss: 1.365247 	 time: 0.3
Epoch: 2475 	Training Loss: 1.153588 	Validation Loss: 1.365126 	 time: 0.3
Epoch: 2476 	Training Loss: 1.153592 	Validation Loss: 1.365421 	 time: 0.3
Epoch: 2477 	Training Loss: 1.153588 	Validation Loss: 1.365275 	 time: 0.3
Epoch: 2478 	Training Loss: 1.153591 	Validation Loss: 1.365663 	 time: 0.3
Epoch: 2479 	Training Loss: 1.153591 	Validation Loss: 1.365207 	 time: 0.3
Epoch: 2480 	Training Loss: 1.153593 	Validation Loss: 1.365655 	 time: 0.3
Epoch: 2481 	Training Loss: 1.153588 	Validation Loss: 1.365690 	 time: 0.3
Epoch: 2482 	Training Loss: 1.153589 	Validation Loss: 1.365509 	 time: 0.3
Epoch: 2483 	Training Loss: 1.153586 	Validation Loss: 1.365970 	 time: 0.3
Epoch: 2484 	Training Loss: 1.153593 	Validation Loss: 1.365533 	 time: 0.3
Epoch: 2485 	Training Loss: 1.153599 	Validation Loss: 1.366130 	 time: 0.3
Epoch: 2486 	Training Loss: 1.153594 	Validation Loss: 1.365610 	 time: 0.3
Epoch: 2487 	Training Loss: 1.153594 	Validation Loss: 1.365825 	 time: 0.3
Epoch: 2488 	Training Loss: 1.153578 	Validation Loss: 1.365991 	 time: 0.3
Epoch: 2489 	Training Loss: 1.153574 	Validation Loss: 1.365755 	 time: 0.3
Epoch: 2490 	Training Loss: 1.153592 	Validation Loss: 1.366277 	 time: 0.3
Epoch: 2491 	Training Loss: 1.153596 	Validation Loss: 1.365529 	 time: 0.3
Epoch: 2492 	Training Loss: 1.153605 	Validation Loss: 1.366437 	 time: 0.3
Epoch: 2493 	Training Loss: 1.153589 	Validation Loss: 1.365824 	 time: 0.3
Epoch: 2494 	Training Loss: 1.153581 	Validation Loss: 1.366132 	 time: 0.3
Epoch: 2495 	Training Loss: 1.153570 	Validation Loss: 1.366149 	 time: 0.3
Epoch: 2496 	Training Loss: 1.153569 	Validation Loss: 1.366065 	 time: 0.3
Epoch: 2497 	Training Loss: 1.153567 	Validation Loss: 1.366422 	 time: 0.3
Epoch: 2498 	Training Loss: 1.153576 	Validation Loss: 1.366107 	 time: 0.3
Epoch: 2499 	Training Loss: 1.153583 	Validation Loss: 1.366534 	 time: 0.3
Epoch: 2500 	Training Loss: 1.153587 	Validation Loss: 1.366365 	 time: 0.3
Epoch: 2501 	Training Loss: 1.153589 	Validation Loss: 1.367031 	 time: 0.3
Epoch: 2502 	Training Loss: 1.153569 	Validation Loss: 1.366912 	 time: 0.3
Epoch: 2503 	Training Loss: 1.153591 	Validation Loss: 1.366513 	 time: 0.3
Epoch: 2504 	Training Loss: 1.153573 	Validation Loss: 1.367235 	 time: 0.3
Epoch: 2505 	Training Loss: 1.153602 	Validation Loss: 1.366569 	 time: 0.3
Epoch: 2506 	Training Loss: 1.153608 	Validation Loss: 1.367670 	 time: 0.3
Epoch: 2507 	Training Loss: 1.153647 	Validation Loss: 1.366558 	 time: 0.3
Epoch: 2508 	Training Loss: 1.153571 	Validation Loss: 1.366937 	 time: 0.3
Epoch: 2509 	Training Loss: 1.153587 	Validation Loss: 1.367174 	 time: 0.3
Epoch: 2510 	Training Loss: 1.153585 	Validation Loss: 1.366784 	 time: 0.3
Epoch: 2511 	Training Loss: 1.153581 	Validation Loss: 1.367588 	 time: 0.3
Epoch: 2512 	Training Loss: 1.153593 	Validation Loss: 1.367569 	 time: 0.3
Epoch: 2513 	Training Loss: 1.153617 	Validation Loss: 1.367149 	 time: 0.3
Epoch: 2514 	Training Loss: 1.153579 	Validation Loss: 1.368319 	 time: 0.3
Epoch: 2515 	Training Loss: 1.153635 	Validation Loss: 1.367089 	 time: 0.3
Epoch: 2516 	Training Loss: 1.153647 	Validation Loss: 1.367738 	 time: 0.3
Epoch: 2517 	Training Loss: 1.153574 	Validation Loss: 1.368711 	 time: 0.3
Epoch: 2518 	Training Loss: 1.153669 	Validation Loss: 1.367435 	 time: 0.3
Epoch: 2519 	Training Loss: 1.153669 	Validation Loss: 1.368189 	 time: 0.3
Epoch: 2520 	Training Loss: 1.153602 	Validation Loss: 1.368990 	 time: 0.3
Epoch: 2521 	Training Loss: 1.153694 	Validation Loss: 1.368149 	 time: 0.3
Epoch: 2522 	Training Loss: 1.153626 	Validation Loss: 1.368573 	 time: 0.3
Epoch: 2523 	Training Loss: 1.153630 	Validation Loss: 1.368433 	 time: 0.3
Epoch: 2524 	Training Loss: 1.153681 	Validation Loss: 1.368987 	 time: 0.3
Epoch: 2525 	Training Loss: 1.153571 	Validation Loss: 1.369044 	 time: 0.3
Epoch: 2526 	Training Loss: 1.153597 	Validation Loss: 1.369478 	 time: 0.3
Epoch: 2527 	Training Loss: 1.153610 	Validation Loss: 1.369627 	 time: 0.3
Epoch: 2528 	Training Loss: 1.153584 	Validation Loss: 1.369005 	 time: 0.3
Epoch: 2529 	Training Loss: 1.153571 	Validation Loss: 1.369832 	 time: 0.3
Epoch: 2530 	Training Loss: 1.153565 	Validation Loss: 1.369800 	 time: 0.3
Epoch: 2531 	Training Loss: 1.153560 	Validation Loss: 1.369264 	 time: 0.3
Epoch: 2532 	Training Loss: 1.153546 	Validation Loss: 1.370424 	 time: 0.3
Epoch: 2533 	Training Loss: 1.153596 	Validation Loss: 1.368833 	 time: 0.3
Epoch: 2534 	Training Loss: 1.153582 	Validation Loss: 1.369684 	 time: 0.3
Epoch: 2535 	Training Loss: 1.153561 	Validation Loss: 1.369673 	 time: 0.3
Epoch: 2536 	Training Loss: 1.153541 	Validation Loss: 1.369605 	 time: 0.3
Epoch: 2537 	Training Loss: 1.153536 	Validation Loss: 1.370136 	 time: 0.3
Epoch: 2538 	Training Loss: 1.153506 	Validation Loss: 1.369838 	 time: 0.3
Epoch: 2539 	Training Loss: 1.153572 	Validation Loss: 1.370277 	 time: 0.3
Epoch: 2540 	Training Loss: 1.153545 	Validation Loss: 1.370016 	 time: 0.3
Epoch: 2541 	Training Loss: 1.153511 	Validation Loss: 1.370331 	 time: 0.3
Epoch: 2542 	Training Loss: 1.153534 	Validation Loss: 1.370498 	 time: 0.3
Epoch: 2543 	Training Loss: 1.153588 	Validation Loss: 1.369458 	 time: 0.3
Epoch: 2544 	Training Loss: 1.153549 	Validation Loss: 1.370847 	 time: 0.3
Epoch: 2545 	Training Loss: 1.153598 	Validation Loss: 1.369694 	 time: 0.3
Epoch: 2546 	Training Loss: 1.153571 	Validation Loss: 1.369869 	 time: 0.3
Epoch: 2547 	Training Loss: 1.153528 	Validation Loss: 1.370872 	 time: 0.3
Epoch: 2548 	Training Loss: 1.153596 	Validation Loss: 1.369344 	 time: 0.3
Epoch: 2549 	Training Loss: 1.153605 	Validation Loss: 1.369864 	 time: 0.3
Epoch: 2550 	Training Loss: 1.153624 	Validation Loss: 1.370077 	 time: 0.3
Epoch: 2551 	Training Loss: 1.153558 	Validation Loss: 1.369545 	 time: 0.3
Epoch: 2552 	Training Loss: 1.153604 	Validation Loss: 1.369135 	 time: 0.3
Epoch: 2553 	Training Loss: 1.153568 	Validation Loss: 1.369089 	 time: 0.3
Epoch: 2554 	Training Loss: 1.153579 	Validation Loss: 1.369351 	 time: 0.3
Epoch: 2555 	Training Loss: 1.153555 	Validation Loss: 1.369481 	 time: 0.3
Epoch: 2556 	Training Loss: 1.153566 	Validation Loss: 1.369757 	 time: 0.3
Epoch: 2557 	Training Loss: 1.153543 	Validation Loss: 1.369682 	 time: 0.3
Epoch: 2558 	Training Loss: 1.153527 	Validation Loss: 1.368643 	 time: 0.3
Epoch: 2559 	Training Loss: 1.153583 	Validation Loss: 1.369955 	 time: 0.3
Epoch: 2560 	Training Loss: 1.153618 	Validation Loss: 1.368671 	 time: 0.3
Epoch: 2561 	Training Loss: 1.153580 	Validation Loss: 1.369323 	 time: 0.3
Epoch: 2562 	Training Loss: 1.153586 	Validation Loss: 1.369415 	 time: 0.3
Epoch: 2563 	Training Loss: 1.153528 	Validation Loss: 1.369053 	 time: 0.3
Epoch: 2564 	Training Loss: 1.153582 	Validation Loss: 1.368658 	 time: 0.3
Epoch: 2565 	Training Loss: 1.153593 	Validation Loss: 1.368151 	 time: 0.3
Epoch: 2566 	Training Loss: 1.153658 	Validation Loss: 1.368718 	 time: 0.3
Epoch: 2567 	Training Loss: 1.153604 	Validation Loss: 1.368942 	 time: 0.3
Epoch: 2568 	Training Loss: 1.153600 	Validation Loss: 1.369756 	 time: 0.3
Epoch: 2569 	Training Loss: 1.153562 	Validation Loss: 1.369568 	 time: 0.3
Epoch: 2570 	Training Loss: 1.153652 	Validation Loss: 1.369663 	 time: 0.3
Epoch: 2571 	Training Loss: 1.153547 	Validation Loss: 1.367871 	 time: 0.3
Epoch: 2572 	Training Loss: 1.153593 	Validation Loss: 1.368390 	 time: 0.3
Epoch: 2573 	Training Loss: 1.153579 	Validation Loss: 1.368478 	 time: 0.3
Epoch: 2574 	Training Loss: 1.153570 	Validation Loss: 1.368213 	 time: 0.3
Epoch: 2575 	Training Loss: 1.153597 	Validation Loss: 1.369012 	 time: 0.3
Epoch: 2576 	Training Loss: 1.153573 	Validation Loss: 1.368699 	 time: 0.3
Epoch: 2577 	Training Loss: 1.153660 	Validation Loss: 1.368041 	 time: 0.3
Epoch: 2578 	Training Loss: 1.153461 	Validation Loss: 1.368915 	 time: 0.3
Epoch: 2579 	Training Loss: 1.153528 	Validation Loss: 1.369300 	 time: 0.3
Epoch: 2580 	Training Loss: 1.153500 	Validation Loss: 1.370789 	 time: 0.3
Epoch: 2581 	Training Loss: 1.153439 	Validation Loss: 1.369777 	 time: 0.3
Epoch: 2582 	Training Loss: 1.153495 	Validation Loss: 1.370761 	 time: 0.3
Epoch: 2583 	Training Loss: 1.153446 	Validation Loss: 1.369100 	 time: 0.3
Epoch: 2584 	Training Loss: 1.153378 	Validation Loss: 1.368783 	 time: 0.3
Epoch: 2585 	Training Loss: 1.153434 	Validation Loss: 1.369435 	 time: 0.3
Epoch: 2586 	Training Loss: 1.153417 	Validation Loss: 1.369350 	 time: 0.3
Epoch: 2587 	Training Loss: 1.153356 	Validation Loss: 1.369895 	 time: 0.3
Epoch: 2588 	Training Loss: 1.153391 	Validation Loss: 1.371205 	 time: 0.3
Epoch: 2589 	Training Loss: 1.153381 	Validation Loss: 1.370242 	 time: 0.3
Epoch: 2590 	Training Loss: 1.153365 	Validation Loss: 1.370434 	 time: 0.3
Epoch: 2591 	Training Loss: 1.153331 	Validation Loss: 1.370250 	 time: 0.3
Epoch: 2592 	Training Loss: 1.153331 	Validation Loss: 1.370338 	 time: 0.3
Epoch: 2593 	Training Loss: 1.153357 	Validation Loss: 1.371404 	 time: 0.3
Epoch: 2594 	Training Loss: 1.153285 	Validation Loss: 1.371814 	 time: 0.3
Epoch: 2595 	Training Loss: 1.153383 	Validation Loss: 1.371622 	 time: 0.3
Epoch: 2596 	Training Loss: 1.153308 	Validation Loss: 1.371444 	 time: 0.3
Epoch: 2597 	Training Loss: 1.153381 	Validation Loss: 1.371998 	 time: 0.3
Epoch: 2598 	Training Loss: 1.153352 	Validation Loss: 1.371011 	 time: 0.3
Epoch: 2599 	Training Loss: 1.153298 	Validation Loss: 1.371021 	 time: 0.3
Epoch: 2600 	Training Loss: 1.153293 	Validation Loss: 1.371295 	 time: 0.3
Epoch: 2601 	Training Loss: 1.153291 	Validation Loss: 1.371254 	 time: 0.3
Epoch: 2602 	Training Loss: 1.153306 	Validation Loss: 1.370693 	 time: 0.3
Epoch: 2603 	Training Loss: 1.153278 	Validation Loss: 1.371329 	 time: 0.3
Epoch: 2604 	Training Loss: 1.153305 	Validation Loss: 1.372039 	 time: 0.3
Epoch: 2605 	Training Loss: 1.153249 	Validation Loss: 1.372341 	 time: 0.3
Epoch: 2606 	Training Loss: 1.153262 	Validation Loss: 1.372030 	 time: 0.3
Epoch: 2607 	Training Loss: 1.153218 	Validation Loss: 1.371796 	 time: 0.3
Epoch: 2608 	Training Loss: 1.153244 	Validation Loss: 1.371773 	 time: 0.3
Epoch: 2609 	Training Loss: 1.153234 	Validation Loss: 1.371667 	 time: 0.3
Epoch: 2610 	Training Loss: 1.153226 	Validation Loss: 1.371780 	 time: 0.3
Epoch: 2611 	Training Loss: 1.153207 	Validation Loss: 1.371943 	 time: 0.3
Epoch: 2612 	Training Loss: 1.153215 	Validation Loss: 1.372008 	 time: 0.3
Epoch: 2613 	Training Loss: 1.153223 	Validation Loss: 1.372505 	 time: 0.3
Epoch: 2614 	Training Loss: 1.153358 	Validation Loss: 1.370800 	 time: 0.3
Epoch: 2615 	Training Loss: 1.153576 	Validation Loss: 1.372481 	 time: 0.3
Epoch: 2616 	Training Loss: 1.153555 	Validation Loss: 1.371750 	 time: 0.3
Epoch: 2617 	Training Loss: 1.153402 	Validation Loss: 1.371098 	 time: 0.3
Epoch: 2618 	Training Loss: 1.153301 	Validation Loss: 1.371306 	 time: 0.3
Epoch: 2619 	Training Loss: 1.153434 	Validation Loss: 1.370080 	 time: 0.3
Epoch: 2620 	Training Loss: 1.153538 	Validation Loss: 1.370754 	 time: 0.3
Epoch: 2621 	Training Loss: 1.153478 	Validation Loss: 1.372259 	 time: 0.3
Epoch: 2622 	Training Loss: 1.153489 	Validation Loss: 1.371165 	 time: 0.3
Epoch: 2623 	Training Loss: 1.153543 	Validation Loss: 1.370742 	 time: 0.3
Epoch: 2624 	Training Loss: 1.153449 	Validation Loss: 1.370503 	 time: 0.3
Epoch: 2625 	Training Loss: 1.153502 	Validation Loss: 1.370798 	 time: 0.3
Epoch: 2626 	Training Loss: 1.153500 	Validation Loss: 1.371183 	 time: 0.3
Epoch: 2627 	Training Loss: 1.153353 	Validation Loss: 1.371611 	 time: 0.3
Epoch: 2628 	Training Loss: 1.153427 	Validation Loss: 1.373806 	 time: 0.3
Epoch: 2629 	Training Loss: 1.153506 	Validation Loss: 1.371229 	 time: 0.3
Epoch: 2630 	Training Loss: 1.153438 	Validation Loss: 1.371772 	 time: 0.3
Epoch: 2631 	Training Loss: 1.153397 	Validation Loss: 1.371944 	 time: 0.3
Epoch: 2632 	Training Loss: 1.153399 	Validation Loss: 1.370909 	 time: 0.3
Epoch: 2633 	Training Loss: 1.153383 	Validation Loss: 1.370322 	 time: 0.3
Epoch: 2634 	Training Loss: 1.153395 	Validation Loss: 1.370486 	 time: 0.3
Epoch: 2635 	Training Loss: 1.153377 	Validation Loss: 1.370216 	 time: 0.3
Epoch: 2636 	Training Loss: 1.153352 	Validation Loss: 1.370040 	 time: 0.3
Epoch: 2637 	Training Loss: 1.153334 	Validation Loss: 1.370492 	 time: 0.3
Epoch: 2638 	Training Loss: 1.153378 	Validation Loss: 1.369330 	 time: 0.3
Epoch: 2639 	Training Loss: 1.153368 	Validation Loss: 1.370643 	 time: 0.3
Epoch: 2640 	Training Loss: 1.153316 	Validation Loss: 1.370362 	 time: 0.3
Epoch: 2641 	Training Loss: 1.153365 	Validation Loss: 1.370131 	 time: 0.3
Epoch: 2642 	Training Loss: 1.153336 	Validation Loss: 1.370436 	 time: 0.3
Epoch: 2643 	Training Loss: 1.153337 	Validation Loss: 1.371000 	 time: 0.3
Epoch: 2644 	Training Loss: 1.153336 	Validation Loss: 1.371228 	 time: 0.3
Epoch: 2645 	Training Loss: 1.153324 	Validation Loss: 1.370382 	 time: 0.3
Epoch: 2646 	Training Loss: 1.153322 	Validation Loss: 1.371788 	 time: 0.3
Epoch: 2647 	Training Loss: 1.153349 	Validation Loss: 1.370203 	 time: 0.3
Epoch: 2648 	Training Loss: 1.153313 	Validation Loss: 1.370840 	 time: 0.3
Epoch: 2649 	Training Loss: 1.153296 	Validation Loss: 1.372468 	 time: 0.3
Epoch: 2650 	Training Loss: 1.153316 	Validation Loss: 1.371370 	 time: 0.3
Epoch: 2651 	Training Loss: 1.153292 	Validation Loss: 1.371215 	 time: 0.3
Epoch: 2652 	Training Loss: 1.153268 	Validation Loss: 1.372117 	 time: 0.3
Epoch: 2653 	Training Loss: 1.153326 	Validation Loss: 1.370891 	 time: 0.3
Epoch: 2654 	Training Loss: 1.153313 	Validation Loss: 1.371058 	 time: 0.3
Epoch: 2655 	Training Loss: 1.153283 	Validation Loss: 1.372209 	 time: 0.3
Epoch: 2656 	Training Loss: 1.153286 	Validation Loss: 1.372225 	 time: 0.3
Epoch: 2657 	Training Loss: 1.153268 	Validation Loss: 1.371835 	 time: 0.3
Epoch: 2658 	Training Loss: 1.153267 	Validation Loss: 1.371600 	 time: 0.3
Epoch: 2659 	Training Loss: 1.153270 	Validation Loss: 1.371881 	 time: 0.3
Epoch: 2660 	Training Loss: 1.153244 	Validation Loss: 1.372624 	 time: 0.3
Epoch: 2661 	Training Loss: 1.153252 	Validation Loss: 1.372323 	 time: 0.3
Epoch: 2662 	Training Loss: 1.153244 	Validation Loss: 1.372144 	 time: 0.3
Epoch: 2663 	Training Loss: 1.153268 	Validation Loss: 1.372316 	 time: 0.3
Epoch: 2664 	Training Loss: 1.153291 	Validation Loss: 1.372530 	 time: 0.3
Epoch: 2665 	Training Loss: 1.153277 	Validation Loss: 1.371835 	 time: 0.3
Epoch: 2666 	Training Loss: 1.153260 	Validation Loss: 1.372326 	 time: 0.3
Epoch: 2667 	Training Loss: 1.153262 	Validation Loss: 1.372132 	 time: 0.3
Epoch: 2668 	Training Loss: 1.153267 	Validation Loss: 1.371839 	 time: 0.3
Epoch: 2669 	Training Loss: 1.153248 	Validation Loss: 1.372464 	 time: 0.3
Epoch: 2670 	Training Loss: 1.153300 	Validation Loss: 1.371252 	 time: 0.3
Epoch: 2671 	Training Loss: 1.153306 	Validation Loss: 1.371960 	 time: 0.3
Epoch: 2672 	Training Loss: 1.153251 	Validation Loss: 1.373415 	 time: 0.3
Epoch: 2673 	Training Loss: 1.153285 	Validation Loss: 1.371579 	 time: 0.3
Epoch: 2674 	Training Loss: 1.153288 	Validation Loss: 1.371587 	 time: 0.3
Epoch: 2675 	Training Loss: 1.153239 	Validation Loss: 1.373509 	 time: 0.3
Epoch: 2676 	Training Loss: 1.153343 	Validation Loss: 1.371218 	 time: 0.3
Epoch: 2677 	Training Loss: 1.153250 	Validation Loss: 1.372178 	 time: 0.3
Epoch: 2678 	Training Loss: 1.153281 	Validation Loss: 1.373135 	 time: 0.3
Epoch: 2679 	Training Loss: 1.153368 	Validation Loss: 1.371920 	 time: 0.3
Epoch: 2680 	Training Loss: 1.153276 	Validation Loss: 1.371379 	 time: 0.3
Epoch: 2681 	Training Loss: 1.153312 	Validation Loss: 1.373514 	 time: 0.3
Epoch: 2682 	Training Loss: 1.153327 	Validation Loss: 1.372489 	 time: 0.3
Epoch: 2683 	Training Loss: 1.153234 	Validation Loss: 1.372475 	 time: 0.3
Epoch: 2684 	Training Loss: 1.153308 	Validation Loss: 1.371147 	 time: 0.3
Epoch: 2685 	Training Loss: 1.153392 	Validation Loss: 1.371974 	 time: 0.3
Epoch: 2686 	Training Loss: 1.153422 	Validation Loss: 1.371200 	 time: 0.3
Epoch: 2687 	Training Loss: 1.153324 	Validation Loss: 1.372672 	 time: 0.3
Epoch: 2688 	Training Loss: 1.153342 	Validation Loss: 1.371122 	 time: 0.3
Epoch: 2689 	Training Loss: 1.153288 	Validation Loss: 1.370387 	 time: 0.3
Epoch: 2690 	Training Loss: 1.153288 	Validation Loss: 1.370726 	 time: 0.3
Epoch: 2691 	Training Loss: 1.153257 	Validation Loss: 1.370860 	 time: 0.3
Epoch: 2692 	Training Loss: 1.153300 	Validation Loss: 1.370468 	 time: 0.3
Epoch: 2693 	Training Loss: 1.153267 	Validation Loss: 1.369795 	 time: 0.3
Epoch: 2694 	Training Loss: 1.153278 	Validation Loss: 1.369675 	 time: 0.3
Epoch: 2695 	Training Loss: 1.153249 	Validation Loss: 1.369629 	 time: 0.3
Epoch: 2696 	Training Loss: 1.153281 	Validation Loss: 1.370699 	 time: 0.3
Epoch: 2697 	Training Loss: 1.153267 	Validation Loss: 1.370575 	 time: 0.3
Epoch: 2698 	Training Loss: 1.153265 	Validation Loss: 1.370550 	 time: 0.3
Epoch: 2699 	Training Loss: 1.153262 	Validation Loss: 1.370825 	 time: 0.3
Epoch: 2700 	Training Loss: 1.153252 	Validation Loss: 1.371223 	 time: 0.3
Epoch: 2701 	Training Loss: 1.153227 	Validation Loss: 1.372203 	 time: 0.3
Epoch: 2702 	Training Loss: 1.153272 	Validation Loss: 1.371161 	 time: 0.3
Epoch: 2703 	Training Loss: 1.153203 	Validation Loss: 1.372027 	 time: 0.3
Epoch: 2704 	Training Loss: 1.153207 	Validation Loss: 1.370761 	 time: 0.3
Epoch: 2705 	Training Loss: 1.153216 	Validation Loss: 1.371408 	 time: 0.3
Epoch: 2706 	Training Loss: 1.153191 	Validation Loss: 1.371591 	 time: 0.3
Epoch: 2707 	Training Loss: 1.153212 	Validation Loss: 1.370586 	 time: 0.3
Epoch: 2708 	Training Loss: 1.153216 	Validation Loss: 1.371840 	 time: 0.3
Epoch: 2709 	Training Loss: 1.153236 	Validation Loss: 1.371298 	 time: 0.3
Epoch: 2710 	Training Loss: 1.153274 	Validation Loss: 1.371632 	 time: 0.3
Epoch: 2711 	Training Loss: 1.153266 	Validation Loss: 1.370972 	 time: 0.3
Epoch: 2712 	Training Loss: 1.153242 	Validation Loss: 1.372920 	 time: 0.3
Epoch: 2713 	Training Loss: 1.153257 	Validation Loss: 1.370712 	 time: 0.3
Epoch: 2714 	Training Loss: 1.153278 	Validation Loss: 1.370933 	 time: 0.3
Epoch: 2715 	Training Loss: 1.153274 	Validation Loss: 1.370793 	 time: 0.3
Epoch: 2716 	Training Loss: 1.153253 	Validation Loss: 1.372492 	 time: 0.3
Epoch: 2717 	Training Loss: 1.153354 	Validation Loss: 1.370033 	 time: 0.3
Epoch: 2718 	Training Loss: 1.153327 	Validation Loss: 1.371060 	 time: 0.3
Epoch: 2719 	Training Loss: 1.153327 	Validation Loss: 1.372568 	 time: 0.3
Epoch: 2720 	Training Loss: 1.153327 	Validation Loss: 1.369935 	 time: 0.3
Epoch: 2721 	Training Loss: 1.153492 	Validation Loss: 1.371037 	 time: 0.3
Epoch: 2722 	Training Loss: 1.153318 	Validation Loss: 1.370262 	 time: 0.3
Epoch: 2723 	Training Loss: 1.153297 	Validation Loss: 1.369488 	 time: 0.3
Epoch: 2724 	Training Loss: 1.153265 	Validation Loss: 1.370195 	 time: 0.3
Epoch: 2725 	Training Loss: 1.153229 	Validation Loss: 1.372555 	 time: 0.3
Epoch: 2726 	Training Loss: 1.153333 	Validation Loss: 1.368736 	 time: 0.3
Epoch: 2727 	Training Loss: 1.153458 	Validation Loss: 1.370241 	 time: 0.3
Epoch: 2728 	Training Loss: 1.153360 	Validation Loss: 1.371960 	 time: 0.3
Epoch: 2729 	Training Loss: 1.153225 	Validation Loss: 1.370327 	 time: 0.3
Epoch: 2730 	Training Loss: 1.153344 	Validation Loss: 1.370344 	 time: 0.3
Epoch: 2731 	Training Loss: 1.153199 	Validation Loss: 1.372411 	 time: 0.3
Epoch: 2732 	Training Loss: 1.153356 	Validation Loss: 1.371318 	 time: 0.3
Epoch: 2733 	Training Loss: 1.153285 	Validation Loss: 1.372630 	 time: 0.3
Epoch: 2734 	Training Loss: 1.153330 	Validation Loss: 1.372987 	 time: 0.3
Epoch: 2735 	Training Loss: 1.153272 	Validation Loss: 1.371256 	 time: 0.3
Epoch: 2736 	Training Loss: 1.153289 	Validation Loss: 1.371914 	 time: 0.3
Epoch: 2737 	Training Loss: 1.153281 	Validation Loss: 1.371598 	 time: 0.3
Epoch: 2738 	Training Loss: 1.153239 	Validation Loss: 1.372008 	 time: 0.3
Epoch: 2739 	Training Loss: 1.153219 	Validation Loss: 1.371003 	 time: 0.3
Epoch: 2740 	Training Loss: 1.153252 	Validation Loss: 1.371241 	 time: 0.3
Epoch: 2741 	Training Loss: 1.153261 	Validation Loss: 1.371968 	 time: 0.3
Epoch: 2742 	Training Loss: 1.153197 	Validation Loss: 1.373648 	 time: 0.3
Epoch: 2743 	Training Loss: 1.153263 	Validation Loss: 1.371121 	 time: 0.3
Epoch: 2744 	Training Loss: 1.153206 	Validation Loss: 1.371524 	 time: 0.3
Epoch: 2745 	Training Loss: 1.153251 	Validation Loss: 1.371588 	 time: 0.3
Epoch: 2746 	Training Loss: 1.153237 	Validation Loss: 1.370567 	 time: 0.3
Epoch: 2747 	Training Loss: 1.153274 	Validation Loss: 1.370983 	 time: 0.3
Epoch: 2748 	Training Loss: 1.153216 	Validation Loss: 1.370898 	 time: 0.3
Epoch: 2749 	Training Loss: 1.153231 	Validation Loss: 1.371015 	 time: 0.3
Epoch: 2750 	Training Loss: 1.153261 	Validation Loss: 1.372869 	 time: 0.3
Epoch: 2751 	Training Loss: 1.153244 	Validation Loss: 1.371696 	 time: 0.3
Epoch: 2752 	Training Loss: 1.153257 	Validation Loss: 1.372718 	 time: 0.3
Epoch: 2753 	Training Loss: 1.153294 	Validation Loss: 1.372048 	 time: 0.3
Epoch: 2754 	Training Loss: 1.153231 	Validation Loss: 1.372849 	 time: 0.3
Epoch: 2755 	Training Loss: 1.153244 	Validation Loss: 1.371332 	 time: 0.3
Epoch: 2756 	Training Loss: 1.153207 	Validation Loss: 1.371729 	 time: 0.3
Epoch: 2757 	Training Loss: 1.153265 	Validation Loss: 1.372534 	 time: 0.3
Epoch: 2758 	Training Loss: 1.153206 	Validation Loss: 1.373229 	 time: 0.3
Epoch: 2759 	Training Loss: 1.153211 	Validation Loss: 1.372283 	 time: 0.3
Epoch: 2760 	Training Loss: 1.153162 	Validation Loss: 1.373059 	 time: 0.3
Epoch: 2761 	Training Loss: 1.153228 	Validation Loss: 1.372057 	 time: 0.3
Epoch: 2762 	Training Loss: 1.153242 	Validation Loss: 1.372778 	 time: 0.3
Epoch: 2763 	Training Loss: 1.153240 	Validation Loss: 1.370524 	 time: 0.3
Epoch: 2764 	Training Loss: 1.153222 	Validation Loss: 1.370930 	 time: 0.3
Epoch: 2765 	Training Loss: 1.153244 	Validation Loss: 1.372396 	 time: 0.3
Epoch: 2766 	Training Loss: 1.153195 	Validation Loss: 1.372959 	 time: 0.3
Epoch: 2767 	Training Loss: 1.153232 	Validation Loss: 1.371208 	 time: 0.3
Epoch: 2768 	Training Loss: 1.153198 	Validation Loss: 1.373153 	 time: 0.3
Epoch: 2769 	Training Loss: 1.153269 	Validation Loss: 1.371392 	 time: 0.3
Epoch: 2770 	Training Loss: 1.153177 	Validation Loss: 1.371713 	 time: 0.3
Epoch: 2771 	Training Loss: 1.153190 	Validation Loss: 1.371906 	 time: 0.3
Epoch: 2772 	Training Loss: 1.153156 	Validation Loss: 1.370489 	 time: 0.3
Epoch: 2773 	Training Loss: 1.153238 	Validation Loss: 1.371969 	 time: 0.3
Epoch: 2774 	Training Loss: 1.153146 	Validation Loss: 1.371987 	 time: 0.3
Epoch: 2775 	Training Loss: 1.153145 	Validation Loss: 1.371563 	 time: 0.3
Epoch: 2776 	Training Loss: 1.153198 	Validation Loss: 1.373824 	 time: 0.3
Epoch: 2777 	Training Loss: 1.153293 	Validation Loss: 1.370121 	 time: 0.3
Epoch: 2778 	Training Loss: 1.153257 	Validation Loss: 1.370790 	 time: 0.3
Epoch: 2779 	Training Loss: 1.153114 	Validation Loss: 1.371858 	 time: 0.3
Epoch: 2780 	Training Loss: 1.153163 	Validation Loss: 1.369235 	 time: 0.3
Epoch: 2781 	Training Loss: 1.153243 	Validation Loss: 1.371148 	 time: 0.3
Epoch: 2782 	Training Loss: 1.153229 	Validation Loss: 1.371060 	 time: 0.3
Epoch: 2783 	Training Loss: 1.153121 	Validation Loss: 1.369570 	 time: 0.3
Epoch: 2784 	Training Loss: 1.153156 	Validation Loss: 1.368984 	 time: 0.3
Epoch: 2785 	Training Loss: 1.153203 	Validation Loss: 1.372218 	 time: 0.3
Epoch: 2786 	Training Loss: 1.153145 	Validation Loss: 1.370804 	 time: 0.3
Epoch: 2787 	Training Loss: 1.153209 	Validation Loss: 1.372016 	 time: 0.3
Epoch: 2788 	Training Loss: 1.153167 	Validation Loss: 1.374051 	 time: 0.3
Epoch: 2789 	Training Loss: 1.153145 	Validation Loss: 1.372041 	 time: 0.3
Epoch: 2790 	Training Loss: 1.153213 	Validation Loss: 1.374657 	 time: 0.3
Epoch: 2791 	Training Loss: 1.153226 	Validation Loss: 1.371854 	 time: 0.3
Epoch: 2792 	Training Loss: 1.153136 	Validation Loss: 1.373057 	 time: 0.3
Epoch: 2793 	Training Loss: 1.153187 	Validation Loss: 1.373798 	 time: 0.3
Epoch: 2794 	Training Loss: 1.153192 	Validation Loss: 1.371624 	 time: 0.3
Epoch: 2795 	Training Loss: 1.153124 	Validation Loss: 1.375453 	 time: 0.3
Epoch: 2796 	Training Loss: 1.153177 	Validation Loss: 1.373289 	 time: 0.3
Epoch: 2797 	Training Loss: 1.153108 	Validation Loss: 1.371801 	 time: 0.3
Epoch: 2798 	Training Loss: 1.153171 	Validation Loss: 1.373628 	 time: 0.3
Epoch: 2799 	Training Loss: 1.153199 	Validation Loss: 1.372209 	 time: 0.3
Epoch: 2800 	Training Loss: 1.153132 	Validation Loss: 1.371578 	 time: 0.3
Epoch: 2801 	Training Loss: 1.153128 	Validation Loss: 1.376155 	 time: 0.3
Epoch: 2802 	Training Loss: 1.153208 	Validation Loss: 1.372527 	 time: 0.3
Epoch: 2803 	Training Loss: 1.153204 	Validation Loss: 1.374209 	 time: 0.3
Epoch: 2804 	Training Loss: 1.153168 	Validation Loss: 1.376740 	 time: 0.3
Epoch: 2805 	Training Loss: 1.153171 	Validation Loss: 1.373263 	 time: 0.3
Epoch: 2806 	Training Loss: 1.153231 	Validation Loss: 1.374020 	 time: 0.3
Epoch: 2807 	Training Loss: 1.153145 	Validation Loss: 1.376114 	 time: 0.3
Epoch: 2808 	Training Loss: 1.153238 	Validation Loss: 1.370892 	 time: 0.3
Epoch: 2809 	Training Loss: 1.153143 	Validation Loss: 1.372661 	 time: 0.3
Epoch: 2810 	Training Loss: 1.153268 	Validation Loss: 1.373258 	 time: 0.3
Epoch: 2811 	Training Loss: 1.153170 	Validation Loss: 1.372545 	 time: 0.3
Epoch: 2812 	Training Loss: 1.153334 	Validation Loss: 1.374589 	 time: 0.3
Epoch: 2813 	Training Loss: 1.153419 	Validation Loss: 1.374240 	 time: 0.3
Epoch: 2814 	Training Loss: 1.153310 	Validation Loss: 1.377348 	 time: 0.3
Epoch: 2815 	Training Loss: 1.153316 	Validation Loss: 1.375545 	 time: 0.3
Epoch: 2816 	Training Loss: 1.153203 	Validation Loss: 1.376007 	 time: 0.3
Epoch: 2817 	Training Loss: 1.153272 	Validation Loss: 1.376312 	 time: 0.3
Epoch: 2818 	Training Loss: 1.153230 	Validation Loss: 1.379934 	 time: 0.3
Epoch: 2819 	Training Loss: 1.153303 	Validation Loss: 1.377635 	 time: 0.3
Epoch: 2820 	Training Loss: 1.153279 	Validation Loss: 1.379267 	 time: 0.3
Epoch: 2821 	Training Loss: 1.153360 	Validation Loss: 1.380042 	 time: 0.3
Epoch: 2822 	Training Loss: 1.153333 	Validation Loss: 1.379511 	 time: 0.3
Epoch: 2823 	Training Loss: 1.153219 	Validation Loss: 1.378831 	 time: 0.3
Epoch: 2824 	Training Loss: 1.153136 	Validation Loss: 1.378705 	 time: 0.3
Epoch: 2825 	Training Loss: 1.153257 	Validation Loss: 1.378972 	 time: 0.3
Epoch: 2826 	Training Loss: 1.153192 	Validation Loss: 1.378854 	 time: 0.3
Epoch: 2827 	Training Loss: 1.153236 	Validation Loss: 1.376869 	 time: 0.3
Epoch: 2828 	Training Loss: 1.153276 	Validation Loss: 1.377060 	 time: 0.3
Epoch: 2829 	Training Loss: 1.153266 	Validation Loss: 1.376417 	 time: 0.3
Epoch: 2830 	Training Loss: 1.153202 	Validation Loss: 1.377260 	 time: 0.3
Epoch: 2831 	Training Loss: 1.153199 	Validation Loss: 1.377795 	 time: 0.3
Epoch: 2832 	Training Loss: 1.153149 	Validation Loss: 1.379172 	 time: 0.3
Epoch: 2833 	Training Loss: 1.153159 	Validation Loss: 1.378972 	 time: 0.3
Epoch: 2834 	Training Loss: 1.153119 	Validation Loss: 1.379042 	 time: 0.3
Epoch: 2835 	Training Loss: 1.153093 	Validation Loss: 1.378778 	 time: 0.3
Epoch: 2836 	Training Loss: 1.153101 	Validation Loss: 1.377300 	 time: 0.3
Epoch: 2837 	Training Loss: 1.153051 	Validation Loss: 1.376743 	 time: 0.3
Epoch: 2838 	Training Loss: 1.153038 	Validation Loss: 1.375910 	 time: 0.3
Epoch: 2839 	Training Loss: 1.153068 	Validation Loss: 1.376497 	 time: 0.3
Epoch: 2840 	Training Loss: 1.153038 	Validation Loss: 1.376415 	 time: 0.3
Epoch: 2841 	Training Loss: 1.153047 	Validation Loss: 1.376672 	 time: 0.3
Epoch: 2842 	Training Loss: 1.153067 	Validation Loss: 1.377321 	 time: 0.3
Epoch: 2843 	Training Loss: 1.153026 	Validation Loss: 1.377814 	 time: 0.3
Epoch: 2844 	Training Loss: 1.152989 	Validation Loss: 1.377896 	 time: 0.3
Epoch: 2845 	Training Loss: 1.153043 	Validation Loss: 1.378111 	 time: 0.3
Epoch: 2846 	Training Loss: 1.153106 	Validation Loss: 1.376729 	 time: 0.3
Epoch: 2847 	Training Loss: 1.153048 	Validation Loss: 1.378112 	 time: 0.3
Epoch: 2848 	Training Loss: 1.153099 	Validation Loss: 1.376454 	 time: 0.3
Epoch: 2849 	Training Loss: 1.153069 	Validation Loss: 1.378081 	 time: 0.3
Epoch: 2850 	Training Loss: 1.153135 	Validation Loss: 1.375878 	 time: 0.3
Epoch: 2851 	Training Loss: 1.153102 	Validation Loss: 1.377974 	 time: 0.3
Epoch: 2852 	Training Loss: 1.153098 	Validation Loss: 1.376327 	 time: 0.3
Epoch: 2853 	Training Loss: 1.153073 	Validation Loss: 1.377224 	 time: 0.3
Epoch: 2854 	Training Loss: 1.153080 	Validation Loss: 1.377999 	 time: 0.3
Epoch: 2855 	Training Loss: 1.153109 	Validation Loss: 1.376042 	 time: 0.3
Epoch: 2856 	Training Loss: 1.153074 	Validation Loss: 1.377894 	 time: 0.3
Epoch: 2857 	Training Loss: 1.153038 	Validation Loss: 1.377381 	 time: 0.3
Epoch: 2858 	Training Loss: 1.152996 	Validation Loss: 1.377091 	 time: 0.3
Epoch: 2859 	Training Loss: 1.153039 	Validation Loss: 1.376546 	 time: 0.3
Epoch: 2860 	Training Loss: 1.152969 	Validation Loss: 1.378374 	 time: 0.3
Epoch: 2861 	Training Loss: 1.153059 	Validation Loss: 1.376254 	 time: 0.3
Epoch: 2862 	Training Loss: 1.153014 	Validation Loss: 1.377429 	 time: 0.3
Epoch: 2863 	Training Loss: 1.153075 	Validation Loss: 1.377282 	 time: 0.3
Epoch: 2864 	Training Loss: 1.153127 	Validation Loss: 1.376356 	 time: 0.3
Epoch: 2865 	Training Loss: 1.153053 	Validation Loss: 1.375855 	 time: 0.3
Epoch: 2866 	Training Loss: 1.153051 	Validation Loss: 1.376014 	 time: 0.3
Epoch: 2867 	Training Loss: 1.153143 	Validation Loss: 1.376640 	 time: 0.3
Epoch: 2868 	Training Loss: 1.153111 	Validation Loss: 1.376276 	 time: 0.3
Epoch: 2869 	Training Loss: 1.153022 	Validation Loss: 1.377398 	 time: 0.3
Epoch: 2870 	Training Loss: 1.153141 	Validation Loss: 1.377101 	 time: 0.3
Epoch: 2871 	Training Loss: 1.153169 	Validation Loss: 1.375414 	 time: 0.3
Epoch: 2872 	Training Loss: 1.153146 	Validation Loss: 1.376394 	 time: 0.3
Epoch: 2873 	Training Loss: 1.153105 	Validation Loss: 1.377253 	 time: 0.3
Epoch: 2874 	Training Loss: 1.153139 	Validation Loss: 1.375793 	 time: 0.3
Epoch: 2875 	Training Loss: 1.153271 	Validation Loss: 1.376368 	 time: 0.3
Epoch: 2876 	Training Loss: 1.153319 	Validation Loss: 1.376284 	 time: 0.3
Epoch: 2877 	Training Loss: 1.153390 	Validation Loss: 1.373233 	 time: 0.3
Epoch: 2878 	Training Loss: 1.153454 	Validation Loss: 1.377209 	 time: 0.3
Epoch: 2879 	Training Loss: 1.153442 	Validation Loss: 1.377100 	 time: 0.3
Epoch: 2880 	Training Loss: 1.153319 	Validation Loss: 1.376618 	 time: 0.3
Epoch: 2881 	Training Loss: 1.153443 	Validation Loss: 1.375222 	 time: 0.3
Epoch: 2882 	Training Loss: 1.153397 	Validation Loss: 1.374185 	 time: 0.3
Epoch: 2883 	Training Loss: 1.153383 	Validation Loss: 1.375416 	 time: 0.3
Epoch: 2884 	Training Loss: 1.153362 	Validation Loss: 1.374299 	 time: 0.3
Epoch: 2885 	Training Loss: 1.153278 	Validation Loss: 1.373852 	 time: 0.3
Epoch: 2886 	Training Loss: 1.153357 	Validation Loss: 1.373408 	 time: 0.3
Epoch: 2887 	Training Loss: 1.153323 	Validation Loss: 1.375652 	 time: 0.3
Epoch: 2888 	Training Loss: 1.153400 	Validation Loss: 1.375777 	 time: 0.3
Epoch: 2889 	Training Loss: 1.153323 	Validation Loss: 1.377966 	 time: 0.3
Epoch: 2890 	Training Loss: 1.153339 	Validation Loss: 1.378957 	 time: 0.3
Epoch: 2891 	Training Loss: 1.153410 	Validation Loss: 1.377539 	 time: 0.3
Epoch: 2892 	Training Loss: 1.153389 	Validation Loss: 1.376380 	 time: 0.3
Epoch: 2893 	Training Loss: 1.153325 	Validation Loss: 1.376568 	 time: 0.3
Epoch: 2894 	Training Loss: 1.153334 	Validation Loss: 1.378562 	 time: 0.3
Epoch: 2895 	Training Loss: 1.153314 	Validation Loss: 1.378359 	 time: 0.3
Epoch: 2896 	Training Loss: 1.153206 	Validation Loss: 1.379321 	 time: 0.3
Epoch: 2897 	Training Loss: 1.153208 	Validation Loss: 1.378166 	 time: 0.3
Epoch: 2898 	Training Loss: 1.153176 	Validation Loss: 1.376871 	 time: 0.3
Epoch: 2899 	Training Loss: 1.153150 	Validation Loss: 1.377082 	 time: 0.3
Epoch: 2900 	Training Loss: 1.153162 	Validation Loss: 1.377856 	 time: 0.3
Epoch: 2901 	Training Loss: 1.153147 	Validation Loss: 1.379816 	 time: 0.3
Epoch: 2902 	Training Loss: 1.153137 	Validation Loss: 1.379902 	 time: 0.3
Epoch: 2903 	Training Loss: 1.153086 	Validation Loss: 1.379085 	 time: 0.3
Epoch: 2904 	Training Loss: 1.153067 	Validation Loss: 1.378690 	 time: 0.3
Epoch: 2905 	Training Loss: 1.153066 	Validation Loss: 1.379848 	 time: 0.3
Epoch: 2906 	Training Loss: 1.153042 	Validation Loss: 1.379352 	 time: 0.3
Epoch: 2907 	Training Loss: 1.153043 	Validation Loss: 1.378706 	 time: 0.3
Epoch: 2908 	Training Loss: 1.153050 	Validation Loss: 1.377943 	 time: 0.3
Epoch: 2909 	Training Loss: 1.153039 	Validation Loss: 1.377166 	 time: 0.3
Epoch: 2910 	Training Loss: 1.153030 	Validation Loss: 1.377465 	 time: 0.3
Epoch: 2911 	Training Loss: 1.153025 	Validation Loss: 1.377566 	 time: 0.3
Epoch: 2912 	Training Loss: 1.153029 	Validation Loss: 1.378793 	 time: 0.3
Epoch: 2913 	Training Loss: 1.153031 	Validation Loss: 1.377556 	 time: 0.3
Epoch: 2914 	Training Loss: 1.153036 	Validation Loss: 1.378993 	 time: 0.3
Epoch: 2915 	Training Loss: 1.153023 	Validation Loss: 1.378864 	 time: 0.3
Epoch: 2916 	Training Loss: 1.153018 	Validation Loss: 1.379291 	 time: 0.3
Epoch: 2917 	Training Loss: 1.153009 	Validation Loss: 1.378020 	 time: 0.3
Epoch: 2918 	Training Loss: 1.153003 	Validation Loss: 1.377544 	 time: 0.3
Epoch: 2919 	Training Loss: 1.153001 	Validation Loss: 1.377266 	 time: 0.3
Epoch: 2920 	Training Loss: 1.153001 	Validation Loss: 1.377097 	 time: 0.3
Epoch: 2921 	Training Loss: 1.152999 	Validation Loss: 1.377605 	 time: 0.3
Epoch: 2922 	Training Loss: 1.153002 	Validation Loss: 1.376985 	 time: 0.3
Epoch: 2923 	Training Loss: 1.153013 	Validation Loss: 1.377887 	 time: 0.3
Epoch: 2924 	Training Loss: 1.153004 	Validation Loss: 1.376834 	 time: 0.3
Epoch: 2925 	Training Loss: 1.153007 	Validation Loss: 1.377858 	 time: 0.3
Epoch: 2926 	Training Loss: 1.152992 	Validation Loss: 1.377920 	 time: 0.3
Epoch: 2927 	Training Loss: 1.152990 	Validation Loss: 1.376990 	 time: 0.3
Epoch: 2928 	Training Loss: 1.152998 	Validation Loss: 1.378044 	 time: 0.3
Epoch: 2929 	Training Loss: 1.153017 	Validation Loss: 1.376842 	 time: 0.3
Epoch: 2930 	Training Loss: 1.153027 	Validation Loss: 1.377676 	 time: 0.3
Epoch: 2931 	Training Loss: 1.152992 	Validation Loss: 1.377003 	 time: 0.3
Epoch: 2932 	Training Loss: 1.152986 	Validation Loss: 1.377233 	 time: 0.3
Epoch: 2933 	Training Loss: 1.152978 	Validation Loss: 1.378193 	 time: 0.3
Epoch: 2934 	Training Loss: 1.153002 	Validation Loss: 1.376822 	 time: 0.3
Epoch: 2935 	Training Loss: 1.153030 	Validation Loss: 1.377591 	 time: 0.3
Epoch: 2936 	Training Loss: 1.153005 	Validation Loss: 1.378939 	 time: 0.3
Epoch: 2937 	Training Loss: 1.153002 	Validation Loss: 1.377162 	 time: 0.3
Epoch: 2938 	Training Loss: 1.153052 	Validation Loss: 1.378233 	 time: 0.3
Epoch: 2939 	Training Loss: 1.153017 	Validation Loss: 1.378432 	 time: 0.3
Epoch: 2940 	Training Loss: 1.152977 	Validation Loss: 1.377197 	 time: 0.3
Epoch: 2941 	Training Loss: 1.153046 	Validation Loss: 1.378857 	 time: 0.3
Epoch: 2942 	Training Loss: 1.153001 	Validation Loss: 1.377695 	 time: 0.3
Epoch: 2943 	Training Loss: 1.152996 	Validation Loss: 1.377688 	 time: 0.3
Epoch: 2944 	Training Loss: 1.153034 	Validation Loss: 1.378300 	 time: 0.3
Epoch: 2945 	Training Loss: 1.153057 	Validation Loss: 1.378339 	 time: 0.3
Epoch: 2946 	Training Loss: 1.153117 	Validation Loss: 1.378888 	 time: 0.3
Epoch: 2947 	Training Loss: 1.153071 	Validation Loss: 1.376612 	 time: 0.3
Epoch: 2948 	Training Loss: 1.153084 	Validation Loss: 1.379050 	 time: 0.3
Epoch: 2949 	Training Loss: 1.153028 	Validation Loss: 1.377435 	 time: 0.3
Epoch: 2950 	Training Loss: 1.153054 	Validation Loss: 1.377551 	 time: 0.3
Epoch: 2951 	Training Loss: 1.153176 	Validation Loss: 1.378595 	 time: 0.3
Epoch: 2952 	Training Loss: 1.153013 	Validation Loss: 1.380074 	 time: 0.3
Epoch: 2953 	Training Loss: 1.153270 	Validation Loss: 1.377605 	 time: 0.3
Epoch: 2954 	Training Loss: 1.153564 	Validation Loss: 1.376685 	 time: 0.3
Epoch: 2955 	Training Loss: 1.153531 	Validation Loss: 1.381037 	 time: 0.3
Epoch: 2956 	Training Loss: 1.153545 	Validation Loss: 1.376464 	 time: 0.3
Epoch: 2957 	Training Loss: 1.153344 	Validation Loss: 1.375298 	 time: 0.3
Epoch: 2958 	Training Loss: 1.153737 	Validation Loss: 1.377261 	 time: 0.3
Epoch: 2959 	Training Loss: 1.153408 	Validation Loss: 1.378271 	 time: 0.3
Epoch: 2960 	Training Loss: 1.153455 	Validation Loss: 1.375789 	 time: 0.3
Epoch: 2961 	Training Loss: 1.153600 	Validation Loss: 1.377675 	 time: 0.3
Epoch: 2962 	Training Loss: 1.153638 	Validation Loss: 1.377738 	 time: 0.3
Epoch: 2963 	Training Loss: 1.153484 	Validation Loss: 1.378199 	 time: 0.3
Epoch: 2964 	Training Loss: 1.153593 	Validation Loss: 1.375276 	 time: 0.3
Epoch: 2965 	Training Loss: 1.153565 	Validation Loss: 1.373028 	 time: 0.3
Epoch: 2966 	Training Loss: 1.153518 	Validation Loss: 1.377057 	 time: 0.3
Epoch: 2967 	Training Loss: 1.153519 	Validation Loss: 1.382931 	 time: 0.3
Epoch: 2968 	Training Loss: 1.153288 	Validation Loss: 1.375876 	 time: 0.3
Epoch: 2969 	Training Loss: 1.153741 	Validation Loss: 1.377772 	 time: 0.3
Epoch: 2970 	Training Loss: 1.153625 	Validation Loss: 1.383624 	 time: 0.3
Epoch: 2971 	Training Loss: 1.154241 	Validation Loss: 1.375414 	 time: 0.3
Epoch: 2972 	Training Loss: 1.155378 	Validation Loss: 1.379322 	 time: 0.3
Epoch: 2973 	Training Loss: 1.155388 	Validation Loss: 1.379246 	 time: 0.3
Epoch: 2974 	Training Loss: 1.155704 	Validation Loss: 1.374861 	 time: 0.3
Epoch: 2975 	Training Loss: 1.155353 	Validation Loss: 1.377236 	 time: 0.3
Epoch: 2976 	Training Loss: 1.154965 	Validation Loss: 1.379090 	 time: 0.3
Epoch: 2977 	Training Loss: 1.155988 	Validation Loss: 1.372090 	 time: 0.3
Epoch: 2978 	Training Loss: 1.158488 	Validation Loss: 1.379008 	 time: 0.3
Epoch: 2979 	Training Loss: 1.157478 	Validation Loss: 1.375849 	 time: 0.3
Epoch: 2980 	Training Loss: 1.158819 	Validation Loss: 1.375987 	 time: 0.3
Epoch: 2981 	Training Loss: 1.157333 	Validation Loss: 1.369965 	 time: 0.3
Epoch: 2982 	Training Loss: 1.155821 	Validation Loss: 1.377586 	 time: 0.3
Epoch: 2983 	Training Loss: 1.157120 	Validation Loss: 1.373067 	 time: 0.3
Epoch: 2984 	Training Loss: 1.156958 	Validation Loss: 1.376773 	 time: 0.3
Epoch: 2985 	Training Loss: 1.155571 	Validation Loss: 1.377225 	 time: 0.3
Epoch: 2986 	Training Loss: 1.156790 	Validation Loss: 1.384176 	 time: 0.3
Epoch: 2987 	Training Loss: 1.155259 	Validation Loss: 1.379877 	 time: 0.3
Epoch: 2988 	Training Loss: 1.155832 	Validation Loss: 1.370995 	 time: 0.3
Epoch: 2989 	Training Loss: 1.155606 	Validation Loss: 1.374615 	 time: 0.3
Epoch: 2990 	Training Loss: 1.155984 	Validation Loss: 1.356405 	 time: 0.3
Validation loss decreased from 1.356641 to 1.356405. Model was saved
Epoch: 2991 	Training Loss: 1.156848 	Validation Loss: 1.366414 	 time: 0.3
Epoch: 2992 	Training Loss: 1.159422 	Validation Loss: 1.361339 	 time: 0.3
Epoch: 2993 	Training Loss: 1.159325 	Validation Loss: 1.366536 	 time: 0.3
Epoch: 2994 	Training Loss: 1.156991 	Validation Loss: 1.364618 	 time: 0.3
Epoch: 2995 	Training Loss: 1.157155 	Validation Loss: 1.356374 	 time: 0.3
Validation loss decreased from 1.356405 to 1.356374. Model was saved
Epoch: 2996 	Training Loss: 1.157727 	Validation Loss: 1.351652 	 time: 0.3
Validation loss decreased from 1.356374 to 1.351652. Model was saved
Epoch: 2997 	Training Loss: 1.156042 	Validation Loss: 1.364713 	 time: 0.3
Epoch: 2998 	Training Loss: 1.157864 	Validation Loss: 1.351331 	 time: 0.3
Validation loss decreased from 1.351652 to 1.351331. Model was saved
Epoch: 2999 	Training Loss: 1.156738 	Validation Loss: 1.348604 	 time: 0.3
Validation loss decreased from 1.351331 to 1.348604. Model was saved
Epoch: 3000 	Training Loss: 1.155448 	Validation Loss: 1.364727 	 time: 0.3
