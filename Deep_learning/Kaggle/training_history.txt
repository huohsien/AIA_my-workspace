Epoch: 1 	Training Loss: 1.794052 	Validation Loss: 1.804901 	 time: 0.3
Validation loss decreased from inf to 1.804901. Model was saved
Epoch: 2 	Training Loss: 1.805312 	Validation Loss: 1.799830 	 time: 0.3
Validation loss decreased from 1.804901 to 1.799830. Model was saved
Epoch: 3 	Training Loss: 1.799796 	Validation Loss: 1.793855 	 time: 0.3
Validation loss decreased from 1.799830 to 1.793855. Model was saved
Epoch: 4 	Training Loss: 1.793686 	Validation Loss: 1.790432 	 time: 0.3
Validation loss decreased from 1.793855 to 1.790432. Model was saved
Epoch: 5 	Training Loss: 1.790193 	Validation Loss: 1.789033 	 time: 0.3
Validation loss decreased from 1.790432 to 1.789033. Model was saved
Epoch: 6 	Training Loss: 1.788333 	Validation Loss: 1.787705 	 time: 0.3
Validation loss decreased from 1.789033 to 1.787705. Model was saved
Epoch: 7 	Training Loss: 1.786293 	Validation Loss: 1.785952 	 time: 0.3
Validation loss decreased from 1.787705 to 1.785952. Model was saved
Epoch: 8 	Training Loss: 1.783557 	Validation Loss: 1.783759 	 time: 0.3
Validation loss decreased from 1.785952 to 1.783759. Model was saved
Epoch: 9 	Training Loss: 1.779848 	Validation Loss: 1.780993 	 time: 0.3
Validation loss decreased from 1.783759 to 1.780993. Model was saved
Epoch: 10 	Training Loss: 1.774887 	Validation Loss: 1.777849 	 time: 0.3
Validation loss decreased from 1.780993 to 1.777849. Model was saved
Epoch: 11 	Training Loss: 1.769167 	Validation Loss: 1.774492 	 time: 0.3
Validation loss decreased from 1.777849 to 1.774492. Model was saved
Epoch: 12 	Training Loss: 1.763056 	Validation Loss: 1.771126 	 time: 0.3
Validation loss decreased from 1.774492 to 1.771126. Model was saved
Epoch: 13 	Training Loss: 1.756363 	Validation Loss: 1.767830 	 time: 0.3
Validation loss decreased from 1.771126 to 1.767830. Model was saved
Epoch: 14 	Training Loss: 1.748937 	Validation Loss: 1.763635 	 time: 0.3
Validation loss decreased from 1.767830 to 1.763635. Model was saved
Epoch: 15 	Training Loss: 1.740486 	Validation Loss: 1.757906 	 time: 0.3
Validation loss decreased from 1.763635 to 1.757906. Model was saved
Epoch: 16 	Training Loss: 1.730971 	Validation Loss: 1.751388 	 time: 0.3
Validation loss decreased from 1.757906 to 1.751388. Model was saved
Epoch: 17 	Training Loss: 1.721245 	Validation Loss: 1.744684 	 time: 0.3
Validation loss decreased from 1.751388 to 1.744684. Model was saved
Epoch: 18 	Training Loss: 1.711025 	Validation Loss: 1.737568 	 time: 0.3
Validation loss decreased from 1.744684 to 1.737568. Model was saved
Epoch: 19 	Training Loss: 1.699893 	Validation Loss: 1.729693 	 time: 0.3
Validation loss decreased from 1.737568 to 1.729693. Model was saved
Epoch: 20 	Training Loss: 1.688740 	Validation Loss: 1.720238 	 time: 0.3
Validation loss decreased from 1.729693 to 1.720238. Model was saved
Epoch: 21 	Training Loss: 1.677215 	Validation Loss: 1.710770 	 time: 0.3
Validation loss decreased from 1.720238 to 1.710770. Model was saved
Epoch: 22 	Training Loss: 1.665505 	Validation Loss: 1.702914 	 time: 0.3
Validation loss decreased from 1.710770 to 1.702914. Model was saved
Epoch: 23 	Training Loss: 1.654038 	Validation Loss: 1.694806 	 time: 0.3
Validation loss decreased from 1.702914 to 1.694806. Model was saved
Epoch: 24 	Training Loss: 1.642506 	Validation Loss: 1.685857 	 time: 0.3
Validation loss decreased from 1.694806 to 1.685857. Model was saved
Epoch: 25 	Training Loss: 1.631069 	Validation Loss: 1.677114 	 time: 0.3
Validation loss decreased from 1.685857 to 1.677114. Model was saved
Epoch: 26 	Training Loss: 1.619958 	Validation Loss: 1.668803 	 time: 0.3
Validation loss decreased from 1.677114 to 1.668803. Model was saved
Epoch: 27 	Training Loss: 1.609119 	Validation Loss: 1.661123 	 time: 0.3
Validation loss decreased from 1.668803 to 1.661123. Model was saved
Epoch: 28 	Training Loss: 1.598874 	Validation Loss: 1.654081 	 time: 0.3
Validation loss decreased from 1.661123 to 1.654081. Model was saved
Epoch: 29 	Training Loss: 1.589037 	Validation Loss: 1.649250 	 time: 0.3
Validation loss decreased from 1.654081 to 1.649250. Model was saved
Epoch: 30 	Training Loss: 1.579731 	Validation Loss: 1.645303 	 time: 0.3
Validation loss decreased from 1.649250 to 1.645303. Model was saved
Epoch: 31 	Training Loss: 1.570964 	Validation Loss: 1.641588 	 time: 0.3
Validation loss decreased from 1.645303 to 1.641588. Model was saved
Epoch: 32 	Training Loss: 1.562635 	Validation Loss: 1.638232 	 time: 0.3
Validation loss decreased from 1.641588 to 1.638232. Model was saved
Epoch: 33 	Training Loss: 1.554907 	Validation Loss: 1.634980 	 time: 0.3
Validation loss decreased from 1.638232 to 1.634980. Model was saved
Epoch: 34 	Training Loss: 1.547894 	Validation Loss: 1.631827 	 time: 0.3
Validation loss decreased from 1.634980 to 1.631827. Model was saved
Epoch: 35 	Training Loss: 1.541145 	Validation Loss: 1.628565 	 time: 0.3
Validation loss decreased from 1.631827 to 1.628565. Model was saved
Epoch: 36 	Training Loss: 1.534584 	Validation Loss: 1.625472 	 time: 0.3
Validation loss decreased from 1.628565 to 1.625472. Model was saved
Epoch: 37 	Training Loss: 1.528282 	Validation Loss: 1.622273 	 time: 0.3
Validation loss decreased from 1.625472 to 1.622273. Model was saved
Epoch: 38 	Training Loss: 1.522436 	Validation Loss: 1.619503 	 time: 0.3
Validation loss decreased from 1.622273 to 1.619503. Model was saved
Epoch: 39 	Training Loss: 1.517175 	Validation Loss: 1.617573 	 time: 0.3
Validation loss decreased from 1.619503 to 1.617573. Model was saved
Epoch: 40 	Training Loss: 1.512275 	Validation Loss: 1.616042 	 time: 0.3
Validation loss decreased from 1.617573 to 1.616042. Model was saved
Epoch: 41 	Training Loss: 1.507708 	Validation Loss: 1.614360 	 time: 0.3
Validation loss decreased from 1.616042 to 1.614360. Model was saved
Epoch: 42 	Training Loss: 1.503369 	Validation Loss: 1.612631 	 time: 0.3
Validation loss decreased from 1.614360 to 1.612631. Model was saved
Epoch: 43 	Training Loss: 1.499297 	Validation Loss: 1.611438 	 time: 0.3
Validation loss decreased from 1.612631 to 1.611438. Model was saved
Epoch: 44 	Training Loss: 1.495412 	Validation Loss: 1.610640 	 time: 0.3
Validation loss decreased from 1.611438 to 1.610640. Model was saved
Epoch: 45 	Training Loss: 1.491780 	Validation Loss: 1.608469 	 time: 0.3
Validation loss decreased from 1.610640 to 1.608469. Model was saved
Epoch: 46 	Training Loss: 1.488351 	Validation Loss: 1.606960 	 time: 0.3
Validation loss decreased from 1.608469 to 1.606960. Model was saved
Epoch: 47 	Training Loss: 1.485145 	Validation Loss: 1.603747 	 time: 0.3
Validation loss decreased from 1.606960 to 1.603747. Model was saved
Epoch: 48 	Training Loss: 1.482048 	Validation Loss: 1.603410 	 time: 0.3
Validation loss decreased from 1.603747 to 1.603410. Model was saved
Epoch: 49 	Training Loss: 1.478995 	Validation Loss: 1.600810 	 time: 0.3
Validation loss decreased from 1.603410 to 1.600810. Model was saved
Epoch: 50 	Training Loss: 1.476052 	Validation Loss: 1.598705 	 time: 0.3
Validation loss decreased from 1.600810 to 1.598705. Model was saved
Epoch: 51 	Training Loss: 1.473227 	Validation Loss: 1.598052 	 time: 0.3
Validation loss decreased from 1.598705 to 1.598052. Model was saved
Epoch: 52 	Training Loss: 1.470509 	Validation Loss: 1.596139 	 time: 0.3
Validation loss decreased from 1.598052 to 1.596139. Model was saved
Epoch: 53 	Training Loss: 1.467952 	Validation Loss: 1.597503 	 time: 0.3
Epoch: 54 	Training Loss: 1.465621 	Validation Loss: 1.593217 	 time: 0.3
Validation loss decreased from 1.596139 to 1.593217. Model was saved
Epoch: 55 	Training Loss: 1.463302 	Validation Loss: 1.593854 	 time: 0.3
Epoch: 56 	Training Loss: 1.460933 	Validation Loss: 1.593144 	 time: 0.3
Validation loss decreased from 1.593217 to 1.593144. Model was saved
Epoch: 57 	Training Loss: 1.458828 	Validation Loss: 1.589834 	 time: 0.3
Validation loss decreased from 1.593144 to 1.589834. Model was saved
Epoch: 58 	Training Loss: 1.456870 	Validation Loss: 1.590289 	 time: 0.3
Epoch: 59 	Training Loss: 1.454810 	Validation Loss: 1.585542 	 time: 0.3
Validation loss decreased from 1.589834 to 1.585542. Model was saved
Epoch: 60 	Training Loss: 1.452715 	Validation Loss: 1.585316 	 time: 0.3
Validation loss decreased from 1.585542 to 1.585316. Model was saved
Epoch: 61 	Training Loss: 1.450714 	Validation Loss: 1.583479 	 time: 0.3
Validation loss decreased from 1.585316 to 1.583479. Model was saved
Epoch: 62 	Training Loss: 1.448883 	Validation Loss: 1.579130 	 time: 0.3
Validation loss decreased from 1.583479 to 1.579130. Model was saved
Epoch: 63 	Training Loss: 1.447193 	Validation Loss: 1.580778 	 time: 0.3
Epoch: 64 	Training Loss: 1.445476 	Validation Loss: 1.576505 	 time: 0.3
Validation loss decreased from 1.579130 to 1.576505. Model was saved
Epoch: 65 	Training Loss: 1.443719 	Validation Loss: 1.576926 	 time: 0.3
Epoch: 66 	Training Loss: 1.441966 	Validation Loss: 1.574870 	 time: 0.3
Validation loss decreased from 1.576505 to 1.574870. Model was saved
Epoch: 67 	Training Loss: 1.440339 	Validation Loss: 1.573429 	 time: 0.3
Validation loss decreased from 1.574870 to 1.573429. Model was saved
Epoch: 68 	Training Loss: 1.438788 	Validation Loss: 1.573665 	 time: 0.3
Epoch: 69 	Training Loss: 1.437270 	Validation Loss: 1.570565 	 time: 0.3
Validation loss decreased from 1.573429 to 1.570565. Model was saved
Epoch: 70 	Training Loss: 1.435786 	Validation Loss: 1.571248 	 time: 0.3
Epoch: 71 	Training Loss: 1.434382 	Validation Loss: 1.568638 	 time: 0.3
Validation loss decreased from 1.570565 to 1.568638. Model was saved
Epoch: 72 	Training Loss: 1.433141 	Validation Loss: 1.569628 	 time: 0.3
Epoch: 73 	Training Loss: 1.431657 	Validation Loss: 1.565796 	 time: 0.3
Validation loss decreased from 1.568638 to 1.565796. Model was saved
Epoch: 74 	Training Loss: 1.430086 	Validation Loss: 1.567006 	 time: 0.3
Epoch: 75 	Training Loss: 1.428665 	Validation Loss: 1.565366 	 time: 0.3
Validation loss decreased from 1.565796 to 1.565366. Model was saved
Epoch: 76 	Training Loss: 1.427366 	Validation Loss: 1.562876 	 time: 0.3
Validation loss decreased from 1.565366 to 1.562876. Model was saved
Epoch: 77 	Training Loss: 1.426067 	Validation Loss: 1.564212 	 time: 0.3
Epoch: 78 	Training Loss: 1.424792 	Validation Loss: 1.560149 	 time: 0.3
Validation loss decreased from 1.562876 to 1.560149. Model was saved
Epoch: 79 	Training Loss: 1.423463 	Validation Loss: 1.560588 	 time: 0.3
Epoch: 80 	Training Loss: 1.422052 	Validation Loss: 1.558893 	 time: 0.3
Validation loss decreased from 1.560149 to 1.558893. Model was saved
Epoch: 81 	Training Loss: 1.420800 	Validation Loss: 1.555757 	 time: 0.3
Validation loss decreased from 1.558893 to 1.555757. Model was saved
Epoch: 82 	Training Loss: 1.419645 	Validation Loss: 1.557058 	 time: 0.3
Epoch: 83 	Training Loss: 1.418411 	Validation Loss: 1.553231 	 time: 0.3
Validation loss decreased from 1.555757 to 1.553231. Model was saved
Epoch: 84 	Training Loss: 1.417118 	Validation Loss: 1.552270 	 time: 0.3
Validation loss decreased from 1.553231 to 1.552270. Model was saved
Epoch: 85 	Training Loss: 1.415873 	Validation Loss: 1.552210 	 time: 0.3
Validation loss decreased from 1.552270 to 1.552210. Model was saved
Epoch: 86 	Training Loss: 1.414659 	Validation Loss: 1.549064 	 time: 0.3
Validation loss decreased from 1.552210 to 1.549064. Model was saved
Epoch: 87 	Training Loss: 1.413406 	Validation Loss: 1.548608 	 time: 0.3
Validation loss decreased from 1.549064 to 1.548608. Model was saved
Epoch: 88 	Training Loss: 1.412198 	Validation Loss: 1.547940 	 time: 0.3
Validation loss decreased from 1.548608 to 1.547940. Model was saved
Epoch: 89 	Training Loss: 1.411056 	Validation Loss: 1.545863 	 time: 0.3
Validation loss decreased from 1.547940 to 1.545863. Model was saved
Epoch: 90 	Training Loss: 1.409907 	Validation Loss: 1.545378 	 time: 0.3
Validation loss decreased from 1.545863 to 1.545378. Model was saved
Epoch: 91 	Training Loss: 1.408741 	Validation Loss: 1.544527 	 time: 0.3
Validation loss decreased from 1.545378 to 1.544527. Model was saved
Epoch: 92 	Training Loss: 1.407547 	Validation Loss: 1.542899 	 time: 0.3
Validation loss decreased from 1.544527 to 1.542899. Model was saved
Epoch: 93 	Training Loss: 1.406377 	Validation Loss: 1.542799 	 time: 0.3
Validation loss decreased from 1.542899 to 1.542799. Model was saved
Epoch: 94 	Training Loss: 1.405226 	Validation Loss: 1.540907 	 time: 0.3
Validation loss decreased from 1.542799 to 1.540907. Model was saved
Epoch: 95 	Training Loss: 1.404064 	Validation Loss: 1.541072 	 time: 0.3
Epoch: 96 	Training Loss: 1.402897 	Validation Loss: 1.539426 	 time: 0.3
Validation loss decreased from 1.540907 to 1.539426. Model was saved
Epoch: 97 	Training Loss: 1.401746 	Validation Loss: 1.538811 	 time: 0.3
Validation loss decreased from 1.539426 to 1.538811. Model was saved
Epoch: 98 	Training Loss: 1.400613 	Validation Loss: 1.538644 	 time: 0.3
Validation loss decreased from 1.538811 to 1.538644. Model was saved
Epoch: 99 	Training Loss: 1.399491 	Validation Loss: 1.536591 	 time: 0.3
Validation loss decreased from 1.538644 to 1.536591. Model was saved
Epoch: 100 	Training Loss: 1.398368 	Validation Loss: 1.537105 	 time: 0.3
Epoch: 101 	Training Loss: 1.397235 	Validation Loss: 1.535415 	 time: 0.3
Validation loss decreased from 1.536591 to 1.535415. Model was saved
Epoch: 102 	Training Loss: 1.396096 	Validation Loss: 1.534687 	 time: 0.3
Validation loss decreased from 1.535415 to 1.534687. Model was saved
Epoch: 103 	Training Loss: 1.394964 	Validation Loss: 1.534503 	 time: 0.3
Validation loss decreased from 1.534687 to 1.534503. Model was saved
Epoch: 104 	Training Loss: 1.393850 	Validation Loss: 1.532434 	 time: 0.3
Validation loss decreased from 1.534503 to 1.532434. Model was saved
Epoch: 105 	Training Loss: 1.392762 	Validation Loss: 1.533251 	 time: 0.3
Epoch: 106 	Training Loss: 1.391735 	Validation Loss: 1.530704 	 time: 0.3
Validation loss decreased from 1.532434 to 1.530704. Model was saved
Epoch: 107 	Training Loss: 1.390727 	Validation Loss: 1.531470 	 time: 0.3
Epoch: 108 	Training Loss: 1.389749 	Validation Loss: 1.529730 	 time: 0.3
Validation loss decreased from 1.530704 to 1.529730. Model was saved
Epoch: 109 	Training Loss: 1.388632 	Validation Loss: 1.528599 	 time: 0.3
Validation loss decreased from 1.529730 to 1.528599. Model was saved
Epoch: 110 	Training Loss: 1.387457 	Validation Loss: 1.528714 	 time: 0.3
Epoch: 111 	Training Loss: 1.386289 	Validation Loss: 1.525970 	 time: 0.3
Validation loss decreased from 1.528599 to 1.525970. Model was saved
Epoch: 112 	Training Loss: 1.385235 	Validation Loss: 1.527240 	 time: 0.3
Epoch: 113 	Training Loss: 1.384273 	Validation Loss: 1.524917 	 time: 0.3
Validation loss decreased from 1.525970 to 1.524917. Model was saved
Epoch: 114 	Training Loss: 1.383302 	Validation Loss: 1.524480 	 time: 0.3
Validation loss decreased from 1.524917 to 1.524480. Model was saved
Epoch: 115 	Training Loss: 1.382309 	Validation Loss: 1.524233 	 time: 0.3
Validation loss decreased from 1.524480 to 1.524233. Model was saved
Epoch: 116 	Training Loss: 1.381241 	Validation Loss: 1.522147 	 time: 0.3
Validation loss decreased from 1.524233 to 1.522147. Model was saved
Epoch: 117 	Training Loss: 1.380128 	Validation Loss: 1.522336 	 time: 0.3
Epoch: 118 	Training Loss: 1.379026 	Validation Loss: 1.520885 	 time: 0.3
Validation loss decreased from 1.522147 to 1.520885. Model was saved
Epoch: 119 	Training Loss: 1.377988 	Validation Loss: 1.520609 	 time: 0.3
Validation loss decreased from 1.520885 to 1.520609. Model was saved
Epoch: 120 	Training Loss: 1.376999 	Validation Loss: 1.519321 	 time: 0.3
Validation loss decreased from 1.520609 to 1.519321. Model was saved
Epoch: 121 	Training Loss: 1.376026 	Validation Loss: 1.519388 	 time: 0.3
Epoch: 122 	Training Loss: 1.375061 	Validation Loss: 1.517424 	 time: 0.3
Validation loss decreased from 1.519321 to 1.517424. Model was saved
Epoch: 123 	Training Loss: 1.374071 	Validation Loss: 1.517902 	 time: 0.3
Epoch: 124 	Training Loss: 1.373089 	Validation Loss: 1.515904 	 time: 0.3
Validation loss decreased from 1.517424 to 1.515904. Model was saved
Epoch: 125 	Training Loss: 1.372098 	Validation Loss: 1.516889 	 time: 0.3
Epoch: 126 	Training Loss: 1.371151 	Validation Loss: 1.514504 	 time: 0.3
Validation loss decreased from 1.515904 to 1.514504. Model was saved
Epoch: 127 	Training Loss: 1.370120 	Validation Loss: 1.516578 	 time: 0.3
Epoch: 128 	Training Loss: 1.369086 	Validation Loss: 1.513153 	 time: 0.3
Validation loss decreased from 1.514504 to 1.513153. Model was saved
Epoch: 129 	Training Loss: 1.367996 	Validation Loss: 1.515427 	 time: 0.3
Epoch: 130 	Training Loss: 1.366942 	Validation Loss: 1.512969 	 time: 0.3
Validation loss decreased from 1.513153 to 1.512969. Model was saved
Epoch: 131 	Training Loss: 1.365943 	Validation Loss: 1.514286 	 time: 0.3
Epoch: 132 	Training Loss: 1.365047 	Validation Loss: 1.512987 	 time: 0.3
Epoch: 133 	Training Loss: 1.364245 	Validation Loss: 1.514029 	 time: 0.3
Epoch: 134 	Training Loss: 1.363394 	Validation Loss: 1.512729 	 time: 0.3
Validation loss decreased from 1.512969 to 1.512729. Model was saved
Epoch: 135 	Training Loss: 1.362376 	Validation Loss: 1.512151 	 time: 0.3
Validation loss decreased from 1.512729 to 1.512151. Model was saved
Epoch: 136 	Training Loss: 1.361265 	Validation Loss: 1.513734 	 time: 0.3
Epoch: 137 	Training Loss: 1.360341 	Validation Loss: 1.510118 	 time: 0.3
Validation loss decreased from 1.512151 to 1.510118. Model was saved
Epoch: 138 	Training Loss: 1.359467 	Validation Loss: 1.512964 	 time: 0.3
Epoch: 139 	Training Loss: 1.358469 	Validation Loss: 1.510898 	 time: 0.3
Epoch: 140 	Training Loss: 1.357443 	Validation Loss: 1.509467 	 time: 0.3
Validation loss decreased from 1.510118 to 1.509467. Model was saved
Epoch: 141 	Training Loss: 1.356572 	Validation Loss: 1.512338 	 time: 0.3
Epoch: 142 	Training Loss: 1.355786 	Validation Loss: 1.507987 	 time: 0.3
Validation loss decreased from 1.509467 to 1.507987. Model was saved
Epoch: 143 	Training Loss: 1.354753 	Validation Loss: 1.509558 	 time: 0.3
Epoch: 144 	Training Loss: 1.353722 	Validation Loss: 1.509019 	 time: 0.3
Epoch: 145 	Training Loss: 1.352819 	Validation Loss: 1.506527 	 time: 0.3
Validation loss decreased from 1.507987 to 1.506527. Model was saved
Epoch: 146 	Training Loss: 1.351969 	Validation Loss: 1.508642 	 time: 0.3
Epoch: 147 	Training Loss: 1.351067 	Validation Loss: 1.505733 	 time: 0.3
Validation loss decreased from 1.506527 to 1.505733. Model was saved
Epoch: 148 	Training Loss: 1.350066 	Validation Loss: 1.505833 	 time: 0.3
Epoch: 149 	Training Loss: 1.349084 	Validation Loss: 1.506131 	 time: 0.3
Epoch: 150 	Training Loss: 1.348184 	Validation Loss: 1.503144 	 time: 0.3
Validation loss decreased from 1.505733 to 1.503144. Model was saved
Epoch: 151 	Training Loss: 1.347330 	Validation Loss: 1.505727 	 time: 0.3
Epoch: 152 	Training Loss: 1.346506 	Validation Loss: 1.501743 	 time: 0.3
Validation loss decreased from 1.503144 to 1.501743. Model was saved
Epoch: 153 	Training Loss: 1.345650 	Validation Loss: 1.504106 	 time: 0.3
Epoch: 154 	Training Loss: 1.345036 	Validation Loss: 1.501668 	 time: 0.3
Validation loss decreased from 1.501743 to 1.501668. Model was saved
Epoch: 155 	Training Loss: 1.343943 	Validation Loss: 1.500677 	 time: 0.3
Validation loss decreased from 1.501668 to 1.500677. Model was saved
Epoch: 156 	Training Loss: 1.342691 	Validation Loss: 1.502090 	 time: 0.3
Epoch: 157 	Training Loss: 1.341867 	Validation Loss: 1.498756 	 time: 0.3
Validation loss decreased from 1.500677 to 1.498756. Model was saved
Epoch: 158 	Training Loss: 1.341123 	Validation Loss: 1.500818 	 time: 0.3
Epoch: 159 	Training Loss: 1.340176 	Validation Loss: 1.498391 	 time: 0.3
Validation loss decreased from 1.498756 to 1.498391. Model was saved
Epoch: 160 	Training Loss: 1.339082 	Validation Loss: 1.497461 	 time: 0.3
Validation loss decreased from 1.498391 to 1.497461. Model was saved
Epoch: 161 	Training Loss: 1.338127 	Validation Loss: 1.498420 	 time: 0.3
Epoch: 162 	Training Loss: 1.337247 	Validation Loss: 1.495082 	 time: 0.3
Validation loss decreased from 1.497461 to 1.495082. Model was saved
Epoch: 163 	Training Loss: 1.336425 	Validation Loss: 1.497184 	 time: 0.3
Epoch: 164 	Training Loss: 1.335572 	Validation Loss: 1.494089 	 time: 0.3
Validation loss decreased from 1.495082 to 1.494089. Model was saved
Epoch: 165 	Training Loss: 1.334375 	Validation Loss: 1.494511 	 time: 0.3
Epoch: 166 	Training Loss: 1.333420 	Validation Loss: 1.494049 	 time: 0.3
Validation loss decreased from 1.494089 to 1.494049. Model was saved
Epoch: 167 	Training Loss: 1.332616 	Validation Loss: 1.492871 	 time: 0.3
Validation loss decreased from 1.494049 to 1.492871. Model was saved
Epoch: 168 	Training Loss: 1.331598 	Validation Loss: 1.493548 	 time: 0.3
Epoch: 169 	Training Loss: 1.330636 	Validation Loss: 1.491902 	 time: 0.3
Validation loss decreased from 1.492871 to 1.491902. Model was saved
Epoch: 170 	Training Loss: 1.329761 	Validation Loss: 1.493160 	 time: 0.3
Epoch: 171 	Training Loss: 1.328810 	Validation Loss: 1.490717 	 time: 0.3
Validation loss decreased from 1.491902 to 1.490717. Model was saved
Epoch: 172 	Training Loss: 1.327890 	Validation Loss: 1.493204 	 time: 0.3
Epoch: 173 	Training Loss: 1.327195 	Validation Loss: 1.489725 	 time: 0.3
Validation loss decreased from 1.490717 to 1.489725. Model was saved
Epoch: 174 	Training Loss: 1.326329 	Validation Loss: 1.491576 	 time: 0.3
Epoch: 175 	Training Loss: 1.325230 	Validation Loss: 1.489964 	 time: 0.3
Epoch: 176 	Training Loss: 1.324265 	Validation Loss: 1.488928 	 time: 0.3
Validation loss decreased from 1.489725 to 1.488928. Model was saved
Epoch: 177 	Training Loss: 1.323523 	Validation Loss: 1.490204 	 time: 0.3
Epoch: 178 	Training Loss: 1.322747 	Validation Loss: 1.487604 	 time: 0.3
Validation loss decreased from 1.488928 to 1.487604. Model was saved
Epoch: 179 	Training Loss: 1.321848 	Validation Loss: 1.488518 	 time: 0.3
Epoch: 180 	Training Loss: 1.320958 	Validation Loss: 1.486945 	 time: 0.3
Validation loss decreased from 1.487604 to 1.486945. Model was saved
Epoch: 181 	Training Loss: 1.320059 	Validation Loss: 1.486333 	 time: 0.3
Validation loss decreased from 1.486945 to 1.486333. Model was saved
Epoch: 182 	Training Loss: 1.319239 	Validation Loss: 1.486428 	 time: 0.3
Epoch: 183 	Training Loss: 1.318511 	Validation Loss: 1.484945 	 time: 0.3
Validation loss decreased from 1.486333 to 1.484945. Model was saved
Epoch: 184 	Training Loss: 1.317704 	Validation Loss: 1.485016 	 time: 0.3
Epoch: 185 	Training Loss: 1.316790 	Validation Loss: 1.484143 	 time: 0.3
Validation loss decreased from 1.484945 to 1.484143. Model was saved
Epoch: 186 	Training Loss: 1.315912 	Validation Loss: 1.483309 	 time: 0.3
Validation loss decreased from 1.484143 to 1.483309. Model was saved
Epoch: 187 	Training Loss: 1.315137 	Validation Loss: 1.483688 	 time: 0.3
Epoch: 188 	Training Loss: 1.314385 	Validation Loss: 1.482055 	 time: 0.3
Validation loss decreased from 1.483309 to 1.482055. Model was saved
Epoch: 189 	Training Loss: 1.313604 	Validation Loss: 1.482738 	 time: 0.3
Epoch: 190 	Training Loss: 1.312830 	Validation Loss: 1.480942 	 time: 0.3
Validation loss decreased from 1.482055 to 1.480942. Model was saved
Epoch: 191 	Training Loss: 1.312041 	Validation Loss: 1.481670 	 time: 0.3
Epoch: 192 	Training Loss: 1.311256 	Validation Loss: 1.479472 	 time: 0.3
Validation loss decreased from 1.480942 to 1.479472. Model was saved
Epoch: 193 	Training Loss: 1.310507 	Validation Loss: 1.481206 	 time: 0.3
Epoch: 194 	Training Loss: 1.309886 	Validation Loss: 1.477316 	 time: 0.3
Validation loss decreased from 1.479472 to 1.477316. Model was saved
Epoch: 195 	Training Loss: 1.309386 	Validation Loss: 1.481466 	 time: 0.3
Epoch: 196 	Training Loss: 1.308988 	Validation Loss: 1.476481 	 time: 0.3
Validation loss decreased from 1.477316 to 1.476481. Model was saved
Epoch: 197 	Training Loss: 1.307798 	Validation Loss: 1.476684 	 time: 0.3
Epoch: 198 	Training Loss: 1.306832 	Validation Loss: 1.477520 	 time: 0.3
Epoch: 199 	Training Loss: 1.306189 	Validation Loss: 1.474316 	 time: 0.3
Validation loss decreased from 1.476481 to 1.474316. Model was saved
Epoch: 200 	Training Loss: 1.305654 	Validation Loss: 1.475685 	 time: 0.3
Epoch: 201 	Training Loss: 1.305078 	Validation Loss: 1.473503 	 time: 0.3
Validation loss decreased from 1.474316 to 1.473503. Model was saved
Epoch: 202 	Training Loss: 1.304077 	Validation Loss: 1.472355 	 time: 0.3
Validation loss decreased from 1.473503 to 1.472355. Model was saved
Epoch: 203 	Training Loss: 1.303277 	Validation Loss: 1.472176 	 time: 0.3
Validation loss decreased from 1.472355 to 1.472176. Model was saved
Epoch: 204 	Training Loss: 1.302813 	Validation Loss: 1.470419 	 time: 0.3
Validation loss decreased from 1.472176 to 1.470419. Model was saved
Epoch: 205 	Training Loss: 1.302153 	Validation Loss: 1.470508 	 time: 0.3
Epoch: 206 	Training Loss: 1.301322 	Validation Loss: 1.468547 	 time: 0.3
Validation loss decreased from 1.470419 to 1.468547. Model was saved
Epoch: 207 	Training Loss: 1.300560 	Validation Loss: 1.468794 	 time: 0.3
Epoch: 208 	Training Loss: 1.299959 	Validation Loss: 1.468309 	 time: 0.3
Validation loss decreased from 1.468547 to 1.468309. Model was saved
Epoch: 209 	Training Loss: 1.299330 	Validation Loss: 1.467176 	 time: 0.3
Validation loss decreased from 1.468309 to 1.467176. Model was saved
Epoch: 210 	Training Loss: 1.298570 	Validation Loss: 1.467669 	 time: 0.3
Epoch: 211 	Training Loss: 1.297832 	Validation Loss: 1.466717 	 time: 0.3
Validation loss decreased from 1.467176 to 1.466717. Model was saved
Epoch: 212 	Training Loss: 1.297104 	Validation Loss: 1.466083 	 time: 0.3
Validation loss decreased from 1.466717 to 1.466083. Model was saved
Epoch: 213 	Training Loss: 1.296436 	Validation Loss: 1.467189 	 time: 0.3
Epoch: 214 	Training Loss: 1.295841 	Validation Loss: 1.465071 	 time: 0.3
Validation loss decreased from 1.466083 to 1.465071. Model was saved
Epoch: 215 	Training Loss: 1.295200 	Validation Loss: 1.466912 	 time: 0.3
Epoch: 216 	Training Loss: 1.294519 	Validation Loss: 1.465062 	 time: 0.3
Validation loss decreased from 1.465071 to 1.465062. Model was saved
Epoch: 217 	Training Loss: 1.293812 	Validation Loss: 1.466201 	 time: 0.3
Epoch: 218 	Training Loss: 1.293207 	Validation Loss: 1.465402 	 time: 0.3
Epoch: 219 	Training Loss: 1.292674 	Validation Loss: 1.466317 	 time: 0.3
Epoch: 220 	Training Loss: 1.292168 	Validation Loss: 1.465044 	 time: 0.3
Validation loss decreased from 1.465062 to 1.465044. Model was saved
Epoch: 221 	Training Loss: 1.291385 	Validation Loss: 1.465240 	 time: 0.3
Epoch: 222 	Training Loss: 1.290514 	Validation Loss: 1.464549 	 time: 0.3
Validation loss decreased from 1.465044 to 1.464549. Model was saved
Epoch: 223 	Training Loss: 1.289797 	Validation Loss: 1.463815 	 time: 0.3
Validation loss decreased from 1.464549 to 1.463815. Model was saved
Epoch: 224 	Training Loss: 1.289284 	Validation Loss: 1.464603 	 time: 0.3
Epoch: 225 	Training Loss: 1.288831 	Validation Loss: 1.462816 	 time: 0.3
Validation loss decreased from 1.463815 to 1.462816. Model was saved
Epoch: 226 	Training Loss: 1.288204 	Validation Loss: 1.463534 	 time: 0.3
Epoch: 227 	Training Loss: 1.287497 	Validation Loss: 1.462199 	 time: 0.3
Validation loss decreased from 1.462816 to 1.462199. Model was saved
Epoch: 228 	Training Loss: 1.286763 	Validation Loss: 1.462042 	 time: 0.3
Validation loss decreased from 1.462199 to 1.462042. Model was saved
Epoch: 229 	Training Loss: 1.286131 	Validation Loss: 1.462133 	 time: 0.3
Epoch: 230 	Training Loss: 1.285593 	Validation Loss: 1.461150 	 time: 0.3
Validation loss decreased from 1.462042 to 1.461150. Model was saved
Epoch: 231 	Training Loss: 1.285071 	Validation Loss: 1.461863 	 time: 0.3
Epoch: 232 	Training Loss: 1.284525 	Validation Loss: 1.460415 	 time: 0.3
Validation loss decreased from 1.461150 to 1.460415. Model was saved
Epoch: 233 	Training Loss: 1.283898 	Validation Loss: 1.461074 	 time: 0.3
Epoch: 234 	Training Loss: 1.283260 	Validation Loss: 1.459749 	 time: 0.3
Validation loss decreased from 1.460415 to 1.459749. Model was saved
Epoch: 235 	Training Loss: 1.282608 	Validation Loss: 1.460137 	 time: 0.3
Epoch: 236 	Training Loss: 1.282000 	Validation Loss: 1.459285 	 time: 0.3
Validation loss decreased from 1.459749 to 1.459285. Model was saved
Epoch: 237 	Training Loss: 1.281442 	Validation Loss: 1.459816 	 time: 0.3
Epoch: 238 	Training Loss: 1.280945 	Validation Loss: 1.459073 	 time: 0.3
Validation loss decreased from 1.459285 to 1.459073. Model was saved
Epoch: 239 	Training Loss: 1.280494 	Validation Loss: 1.460384 	 time: 0.3
Epoch: 240 	Training Loss: 1.280187 	Validation Loss: 1.458743 	 time: 0.3
Validation loss decreased from 1.459073 to 1.458743. Model was saved
Epoch: 241 	Training Loss: 1.279754 	Validation Loss: 1.460942 	 time: 0.3
Epoch: 242 	Training Loss: 1.279369 	Validation Loss: 1.457890 	 time: 0.3
Validation loss decreased from 1.458743 to 1.457890. Model was saved
Epoch: 243 	Training Loss: 1.278658 	Validation Loss: 1.460567 	 time: 0.3
Epoch: 244 	Training Loss: 1.278306 	Validation Loss: 1.458182 	 time: 0.3
Epoch: 245 	Training Loss: 1.277565 	Validation Loss: 1.458725 	 time: 0.3
Epoch: 246 	Training Loss: 1.276842 	Validation Loss: 1.459544 	 time: 0.3
Epoch: 247 	Training Loss: 1.276547 	Validation Loss: 1.457675 	 time: 0.3
Validation loss decreased from 1.457890 to 1.457675. Model was saved
Epoch: 248 	Training Loss: 1.276222 	Validation Loss: 1.460007 	 time: 0.3
Epoch: 249 	Training Loss: 1.275637 	Validation Loss: 1.457907 	 time: 0.3
Epoch: 250 	Training Loss: 1.274901 	Validation Loss: 1.458631 	 time: 0.3
Epoch: 251 	Training Loss: 1.274495 	Validation Loss: 1.459183 	 time: 0.3
Epoch: 252 	Training Loss: 1.274140 	Validation Loss: 1.457544 	 time: 0.3
Validation loss decreased from 1.457675 to 1.457544. Model was saved
Epoch: 253 	Training Loss: 1.273592 	Validation Loss: 1.459460 	 time: 0.3
Epoch: 254 	Training Loss: 1.273169 	Validation Loss: 1.457639 	 time: 0.3
Epoch: 255 	Training Loss: 1.272776 	Validation Loss: 1.458497 	 time: 0.3
Epoch: 256 	Training Loss: 1.272231 	Validation Loss: 1.457554 	 time: 0.3
Epoch: 257 	Training Loss: 1.271626 	Validation Loss: 1.457377 	 time: 0.3
Validation loss decreased from 1.457544 to 1.457377. Model was saved
Epoch: 258 	Training Loss: 1.271217 	Validation Loss: 1.457752 	 time: 0.3
Epoch: 259 	Training Loss: 1.270887 	Validation Loss: 1.456662 	 time: 0.3
Validation loss decreased from 1.457377 to 1.456662. Model was saved
Epoch: 260 	Training Loss: 1.270439 	Validation Loss: 1.457668 	 time: 0.3
Epoch: 261 	Training Loss: 1.269962 	Validation Loss: 1.455823 	 time: 0.3
Validation loss decreased from 1.456662 to 1.455823. Model was saved
Epoch: 262 	Training Loss: 1.269543 	Validation Loss: 1.457492 	 time: 0.3
Epoch: 263 	Training Loss: 1.269137 	Validation Loss: 1.455848 	 time: 0.3
Epoch: 264 	Training Loss: 1.268623 	Validation Loss: 1.456775 	 time: 0.3
Epoch: 265 	Training Loss: 1.268129 	Validation Loss: 1.456580 	 time: 0.3
Epoch: 266 	Training Loss: 1.267716 	Validation Loss: 1.456199 	 time: 0.3
Epoch: 267 	Training Loss: 1.267321 	Validation Loss: 1.456854 	 time: 0.3
Epoch: 268 	Training Loss: 1.266884 	Validation Loss: 1.455777 	 time: 0.3
Validation loss decreased from 1.455823 to 1.455777. Model was saved
Epoch: 269 	Training Loss: 1.266434 	Validation Loss: 1.456300 	 time: 0.3
Epoch: 270 	Training Loss: 1.266024 	Validation Loss: 1.455329 	 time: 0.3
Validation loss decreased from 1.455777 to 1.455329. Model was saved
Epoch: 271 	Training Loss: 1.265631 	Validation Loss: 1.455706 	 time: 0.3
Epoch: 272 	Training Loss: 1.265222 	Validation Loss: 1.454459 	 time: 0.3
Validation loss decreased from 1.455329 to 1.454459. Model was saved
Epoch: 273 	Training Loss: 1.264798 	Validation Loss: 1.455492 	 time: 0.3
Epoch: 274 	Training Loss: 1.264451 	Validation Loss: 1.453599 	 time: 0.3
Validation loss decreased from 1.454459 to 1.453599. Model was saved
Epoch: 275 	Training Loss: 1.264168 	Validation Loss: 1.455962 	 time: 0.3
Epoch: 276 	Training Loss: 1.264186 	Validation Loss: 1.453430 	 time: 0.3
Validation loss decreased from 1.453599 to 1.453430. Model was saved
Epoch: 277 	Training Loss: 1.263820 	Validation Loss: 1.455124 	 time: 0.3
Epoch: 278 	Training Loss: 1.263260 	Validation Loss: 1.453277 	 time: 0.3
Validation loss decreased from 1.453430 to 1.453277. Model was saved
Epoch: 279 	Training Loss: 1.262275 	Validation Loss: 1.453207 	 time: 0.3
Validation loss decreased from 1.453277 to 1.453207. Model was saved
Epoch: 280 	Training Loss: 1.262161 	Validation Loss: 1.455015 	 time: 0.3
Epoch: 281 	Training Loss: 1.262309 	Validation Loss: 1.452936 	 time: 0.3
Validation loss decreased from 1.453207 to 1.452936. Model was saved
Epoch: 282 	Training Loss: 1.261347 	Validation Loss: 1.453335 	 time: 0.3
Epoch: 283 	Training Loss: 1.260721 	Validation Loss: 1.453598 	 time: 0.3
Epoch: 284 	Training Loss: 1.260659 	Validation Loss: 1.453156 	 time: 0.3
Epoch: 285 	Training Loss: 1.260281 	Validation Loss: 1.452989 	 time: 0.3
Epoch: 286 	Training Loss: 1.259668 	Validation Loss: 1.452918 	 time: 0.3
Validation loss decreased from 1.452936 to 1.452918. Model was saved
Epoch: 287 	Training Loss: 1.259158 	Validation Loss: 1.452291 	 time: 0.3
Validation loss decreased from 1.452918 to 1.452291. Model was saved
Epoch: 288 	Training Loss: 1.258950 	Validation Loss: 1.453049 	 time: 0.3
Epoch: 289 	Training Loss: 1.258681 	Validation Loss: 1.451919 	 time: 0.3
Validation loss decreased from 1.452291 to 1.451919. Model was saved
Epoch: 290 	Training Loss: 1.258034 	Validation Loss: 1.451701 	 time: 0.3
Validation loss decreased from 1.451919 to 1.451701. Model was saved
Epoch: 291 	Training Loss: 1.257522 	Validation Loss: 1.451896 	 time: 0.3
Epoch: 292 	Training Loss: 1.257231 	Validation Loss: 1.451114 	 time: 0.3
Validation loss decreased from 1.451701 to 1.451114. Model was saved
Epoch: 293 	Training Loss: 1.256863 	Validation Loss: 1.451411 	 time: 0.3
Epoch: 294 	Training Loss: 1.256389 	Validation Loss: 1.450655 	 time: 0.3
Validation loss decreased from 1.451114 to 1.450655. Model was saved
Epoch: 295 	Training Loss: 1.255860 	Validation Loss: 1.450574 	 time: 0.3
Validation loss decreased from 1.450655 to 1.450574. Model was saved
Epoch: 296 	Training Loss: 1.255464 	Validation Loss: 1.450439 	 time: 0.3
Validation loss decreased from 1.450574 to 1.450439. Model was saved
Epoch: 297 	Training Loss: 1.255151 	Validation Loss: 1.450301 	 time: 0.3
Validation loss decreased from 1.450439 to 1.450301. Model was saved
Epoch: 298 	Training Loss: 1.254759 	Validation Loss: 1.449779 	 time: 0.3
Validation loss decreased from 1.450301 to 1.449779. Model was saved
Epoch: 299 	Training Loss: 1.254324 	Validation Loss: 1.450112 	 time: 0.3
Epoch: 300 	Training Loss: 1.253867 	Validation Loss: 1.448857 	 time: 0.3
Validation loss decreased from 1.449779 to 1.448857. Model was saved
Epoch: 301 	Training Loss: 1.253485 	Validation Loss: 1.450071 	 time: 0.3
Epoch: 302 	Training Loss: 1.253188 	Validation Loss: 1.448189 	 time: 0.3
Validation loss decreased from 1.448857 to 1.448189. Model was saved
Epoch: 303 	Training Loss: 1.252883 	Validation Loss: 1.450122 	 time: 0.3
Epoch: 304 	Training Loss: 1.252661 	Validation Loss: 1.447624 	 time: 0.3
Validation loss decreased from 1.448189 to 1.447624. Model was saved
Epoch: 305 	Training Loss: 1.252315 	Validation Loss: 1.449784 	 time: 0.3
Epoch: 306 	Training Loss: 1.251951 	Validation Loss: 1.447738 	 time: 0.3
Epoch: 307 	Training Loss: 1.251263 	Validation Loss: 1.447981 	 time: 0.3
Epoch: 308 	Training Loss: 1.250793 	Validation Loss: 1.448450 	 time: 0.3
Epoch: 309 	Training Loss: 1.250609 	Validation Loss: 1.446877 	 time: 0.3
Validation loss decreased from 1.447624 to 1.446877. Model was saved
Epoch: 310 	Training Loss: 1.250342 	Validation Loss: 1.448067 	 time: 0.3
Epoch: 311 	Training Loss: 1.249964 	Validation Loss: 1.446338 	 time: 0.3
Validation loss decreased from 1.446877 to 1.446338. Model was saved
Epoch: 312 	Training Loss: 1.249350 	Validation Loss: 1.446019 	 time: 0.3
Validation loss decreased from 1.446338 to 1.446019. Model was saved
Epoch: 313 	Training Loss: 1.248910 	Validation Loss: 1.446401 	 time: 0.3
Epoch: 314 	Training Loss: 1.248668 	Validation Loss: 1.444676 	 time: 0.3
Validation loss decreased from 1.446019 to 1.444676. Model was saved
Epoch: 315 	Training Loss: 1.248397 	Validation Loss: 1.445882 	 time: 0.3
Epoch: 316 	Training Loss: 1.248027 	Validation Loss: 1.444260 	 time: 0.3
Validation loss decreased from 1.444676 to 1.444260. Model was saved
Epoch: 317 	Training Loss: 1.247515 	Validation Loss: 1.444487 	 time: 0.3
Epoch: 318 	Training Loss: 1.247093 	Validation Loss: 1.444492 	 time: 0.3
Epoch: 319 	Training Loss: 1.246803 	Validation Loss: 1.443779 	 time: 0.3
Validation loss decreased from 1.444260 to 1.443779. Model was saved
Epoch: 320 	Training Loss: 1.246545 	Validation Loss: 1.444452 	 time: 0.3
Epoch: 321 	Training Loss: 1.246280 	Validation Loss: 1.443700 	 time: 0.3
Validation loss decreased from 1.443779 to 1.443700. Model was saved
Epoch: 322 	Training Loss: 1.245947 	Validation Loss: 1.443867 	 time: 0.3
Epoch: 323 	Training Loss: 1.245789 	Validation Loss: 1.444300 	 time: 0.3
Epoch: 324 	Training Loss: 1.245558 	Validation Loss: 1.442843 	 time: 0.3
Validation loss decreased from 1.443700 to 1.442843. Model was saved
Epoch: 325 	Training Loss: 1.245182 	Validation Loss: 1.443431 	 time: 0.3
Epoch: 326 	Training Loss: 1.244549 	Validation Loss: 1.442207 	 time: 0.3
Validation loss decreased from 1.442843 to 1.442207. Model was saved
Epoch: 327 	Training Loss: 1.244095 	Validation Loss: 1.442596 	 time: 0.3
Epoch: 328 	Training Loss: 1.243899 	Validation Loss: 1.442773 	 time: 0.3
Epoch: 329 	Training Loss: 1.243672 	Validation Loss: 1.442109 	 time: 0.3
Validation loss decreased from 1.442207 to 1.442109. Model was saved
Epoch: 330 	Training Loss: 1.243380 	Validation Loss: 1.442073 	 time: 0.3
Validation loss decreased from 1.442109 to 1.442073. Model was saved
Epoch: 331 	Training Loss: 1.242844 	Validation Loss: 1.440818 	 time: 0.3
Validation loss decreased from 1.442073 to 1.440818. Model was saved
Epoch: 332 	Training Loss: 1.242334 	Validation Loss: 1.440768 	 time: 0.3
Validation loss decreased from 1.440818 to 1.440768. Model was saved
Epoch: 333 	Training Loss: 1.241937 	Validation Loss: 1.439910 	 time: 0.3
Validation loss decreased from 1.440768 to 1.439910. Model was saved
Epoch: 334 	Training Loss: 1.241621 	Validation Loss: 1.439662 	 time: 0.3
Validation loss decreased from 1.439910 to 1.439662. Model was saved
Epoch: 335 	Training Loss: 1.241304 	Validation Loss: 1.438981 	 time: 0.3
Validation loss decreased from 1.439662 to 1.438981. Model was saved
Epoch: 336 	Training Loss: 1.240880 	Validation Loss: 1.438090 	 time: 0.3
Validation loss decreased from 1.438981 to 1.438090. Model was saved
Epoch: 337 	Training Loss: 1.240411 	Validation Loss: 1.437825 	 time: 0.3
Validation loss decreased from 1.438090 to 1.437825. Model was saved
Epoch: 338 	Training Loss: 1.239877 	Validation Loss: 1.437101 	 time: 0.3
Validation loss decreased from 1.437825 to 1.437101. Model was saved
Epoch: 339 	Training Loss: 1.239377 	Validation Loss: 1.436513 	 time: 0.3
Validation loss decreased from 1.437101 to 1.436513. Model was saved
Epoch: 340 	Training Loss: 1.238893 	Validation Loss: 1.436281 	 time: 0.3
Validation loss decreased from 1.436513 to 1.436281. Model was saved
Epoch: 341 	Training Loss: 1.238477 	Validation Loss: 1.435328 	 time: 0.3
Validation loss decreased from 1.436281 to 1.435328. Model was saved
Epoch: 342 	Training Loss: 1.237999 	Validation Loss: 1.435614 	 time: 0.3
Epoch: 343 	Training Loss: 1.237574 	Validation Loss: 1.434933 	 time: 0.3
Validation loss decreased from 1.435328 to 1.434933. Model was saved
Epoch: 344 	Training Loss: 1.237040 	Validation Loss: 1.435214 	 time: 0.3
Epoch: 345 	Training Loss: 1.236673 	Validation Loss: 1.435348 	 time: 0.3
Epoch: 346 	Training Loss: 1.236246 	Validation Loss: 1.435286 	 time: 0.3
Epoch: 347 	Training Loss: 1.236165 	Validation Loss: 1.434824 	 time: 0.3
Validation loss decreased from 1.434933 to 1.434824. Model was saved
Epoch: 348 	Training Loss: 1.235208 	Validation Loss: 1.432809 	 time: 0.3
Validation loss decreased from 1.434824 to 1.432809. Model was saved
Epoch: 349 	Training Loss: 1.234422 	Validation Loss: 1.433308 	 time: 0.3
Epoch: 350 	Training Loss: 1.234164 	Validation Loss: 1.432545 	 time: 0.3
Validation loss decreased from 1.432809 to 1.432545. Model was saved
Epoch: 351 	Training Loss: 1.233866 	Validation Loss: 1.432654 	 time: 0.3
Epoch: 352 	Training Loss: 1.233498 	Validation Loss: 1.431664 	 time: 0.3
Validation loss decreased from 1.432545 to 1.431664. Model was saved
Epoch: 353 	Training Loss: 1.232538 	Validation Loss: 1.431127 	 time: 0.3
Validation loss decreased from 1.431664 to 1.431127. Model was saved
Epoch: 354 	Training Loss: 1.232040 	Validation Loss: 1.431359 	 time: 0.3
Epoch: 355 	Training Loss: 1.231861 	Validation Loss: 1.431038 	 time: 0.3
Validation loss decreased from 1.431127 to 1.431038. Model was saved
Epoch: 356 	Training Loss: 1.231362 	Validation Loss: 1.429991 	 time: 0.3
Validation loss decreased from 1.431038 to 1.429991. Model was saved
Epoch: 357 	Training Loss: 1.230732 	Validation Loss: 1.429660 	 time: 0.3
Validation loss decreased from 1.429991 to 1.429660. Model was saved
Epoch: 358 	Training Loss: 1.230206 	Validation Loss: 1.428989 	 time: 0.3
Validation loss decreased from 1.429660 to 1.428989. Model was saved
Epoch: 359 	Training Loss: 1.229967 	Validation Loss: 1.429171 	 time: 0.3
Epoch: 360 	Training Loss: 1.229671 	Validation Loss: 1.428913 	 time: 0.3
Validation loss decreased from 1.428989 to 1.428913. Model was saved
Epoch: 361 	Training Loss: 1.229103 	Validation Loss: 1.427616 	 time: 0.3
Validation loss decreased from 1.428913 to 1.427616. Model was saved
Epoch: 362 	Training Loss: 1.228572 	Validation Loss: 1.427964 	 time: 0.3
Epoch: 363 	Training Loss: 1.228242 	Validation Loss: 1.426791 	 time: 0.3
Validation loss decreased from 1.427616 to 1.426791. Model was saved
Epoch: 364 	Training Loss: 1.227914 	Validation Loss: 1.427233 	 time: 0.3
Epoch: 365 	Training Loss: 1.227516 	Validation Loss: 1.426099 	 time: 0.3
Validation loss decreased from 1.426791 to 1.426099. Model was saved
Epoch: 366 	Training Loss: 1.227027 	Validation Loss: 1.425809 	 time: 0.3
Validation loss decreased from 1.426099 to 1.425809. Model was saved
Epoch: 367 	Training Loss: 1.226716 	Validation Loss: 1.425485 	 time: 0.3
Validation loss decreased from 1.425809 to 1.425485. Model was saved
Epoch: 368 	Training Loss: 1.226479 	Validation Loss: 1.425248 	 time: 0.3
Validation loss decreased from 1.425485 to 1.425248. Model was saved
Epoch: 369 	Training Loss: 1.226165 	Validation Loss: 1.424629 	 time: 0.3
Validation loss decreased from 1.425248 to 1.424629. Model was saved
Epoch: 370 	Training Loss: 1.225736 	Validation Loss: 1.424644 	 time: 0.3
Epoch: 371 	Training Loss: 1.225376 	Validation Loss: 1.423841 	 time: 0.3
Validation loss decreased from 1.424629 to 1.423841. Model was saved
Epoch: 372 	Training Loss: 1.225128 	Validation Loss: 1.424659 	 time: 0.3
Epoch: 373 	Training Loss: 1.224895 	Validation Loss: 1.423421 	 time: 0.3
Validation loss decreased from 1.423841 to 1.423421. Model was saved
Epoch: 374 	Training Loss: 1.224568 	Validation Loss: 1.424675 	 time: 0.3
Epoch: 375 	Training Loss: 1.224276 	Validation Loss: 1.423198 	 time: 0.3
Validation loss decreased from 1.423421 to 1.423198. Model was saved
Epoch: 376 	Training Loss: 1.224054 	Validation Loss: 1.424746 	 time: 0.3
Epoch: 377 	Training Loss: 1.223892 	Validation Loss: 1.422974 	 time: 0.3
Validation loss decreased from 1.423198 to 1.422974. Model was saved
Epoch: 378 	Training Loss: 1.223699 	Validation Loss: 1.424359 	 time: 0.3
Epoch: 379 	Training Loss: 1.223479 	Validation Loss: 1.422051 	 time: 0.3
Validation loss decreased from 1.422974 to 1.422051. Model was saved
Epoch: 380 	Training Loss: 1.223116 	Validation Loss: 1.423528 	 time: 0.3
Epoch: 381 	Training Loss: 1.222808 	Validation Loss: 1.422174 	 time: 0.3
Epoch: 382 	Training Loss: 1.222336 	Validation Loss: 1.422470 	 time: 0.3
Epoch: 383 	Training Loss: 1.221916 	Validation Loss: 1.422541 	 time: 0.3
Epoch: 384 	Training Loss: 1.221622 	Validation Loss: 1.421505 	 time: 0.3
Validation loss decreased from 1.422051 to 1.421505. Model was saved
Epoch: 385 	Training Loss: 1.221436 	Validation Loss: 1.422895 	 time: 0.3
Epoch: 386 	Training Loss: 1.221285 	Validation Loss: 1.420897 	 time: 0.3
Validation loss decreased from 1.421505 to 1.420897. Model was saved
Epoch: 387 	Training Loss: 1.220995 	Validation Loss: 1.422639 	 time: 0.3
Epoch: 388 	Training Loss: 1.220700 	Validation Loss: 1.420835 	 time: 0.3
Validation loss decreased from 1.420897 to 1.420835. Model was saved
Epoch: 389 	Training Loss: 1.220346 	Validation Loss: 1.421860 	 time: 0.3
Epoch: 390 	Training Loss: 1.220012 	Validation Loss: 1.420985 	 time: 0.3
Epoch: 391 	Training Loss: 1.219692 	Validation Loss: 1.420810 	 time: 0.3
Validation loss decreased from 1.420835 to 1.420810. Model was saved
Epoch: 392 	Training Loss: 1.219416 	Validation Loss: 1.420999 	 time: 0.3
Epoch: 393 	Training Loss: 1.219188 	Validation Loss: 1.419972 	 time: 0.3
Validation loss decreased from 1.420810 to 1.419972. Model was saved
Epoch: 394 	Training Loss: 1.218963 	Validation Loss: 1.420548 	 time: 0.3
Epoch: 395 	Training Loss: 1.218711 	Validation Loss: 1.418818 	 time: 0.3
Validation loss decreased from 1.419972 to 1.418818. Model was saved
Epoch: 396 	Training Loss: 1.218421 	Validation Loss: 1.419331 	 time: 0.3
Epoch: 397 	Training Loss: 1.218139 	Validation Loss: 1.417561 	 time: 0.3
Validation loss decreased from 1.418818 to 1.417561. Model was saved
Epoch: 398 	Training Loss: 1.217871 	Validation Loss: 1.417796 	 time: 0.3
Epoch: 399 	Training Loss: 1.217617 	Validation Loss: 1.416425 	 time: 0.3
Validation loss decreased from 1.417561 to 1.416425. Model was saved
Epoch: 400 	Training Loss: 1.217366 	Validation Loss: 1.416341 	 time: 0.3
Validation loss decreased from 1.416425 to 1.416341. Model was saved
Epoch: 401 	Training Loss: 1.217118 	Validation Loss: 1.415655 	 time: 0.3
Validation loss decreased from 1.416341 to 1.415655. Model was saved
Epoch: 402 	Training Loss: 1.216913 	Validation Loss: 1.415612 	 time: 0.3
Validation loss decreased from 1.415655 to 1.415612. Model was saved
Epoch: 403 	Training Loss: 1.216692 	Validation Loss: 1.415244 	 time: 0.3
Validation loss decreased from 1.415612 to 1.415244. Model was saved
Epoch: 404 	Training Loss: 1.216519 	Validation Loss: 1.415476 	 time: 0.3
Epoch: 405 	Training Loss: 1.216297 	Validation Loss: 1.414770 	 time: 0.3
Validation loss decreased from 1.415244 to 1.414770. Model was saved
Epoch: 406 	Training Loss: 1.216135 	Validation Loss: 1.415623 	 time: 0.3
Epoch: 407 	Training Loss: 1.215951 	Validation Loss: 1.413972 	 time: 0.3
Validation loss decreased from 1.414770 to 1.413972. Model was saved
Epoch: 408 	Training Loss: 1.215805 	Validation Loss: 1.415738 	 time: 0.3
Epoch: 409 	Training Loss: 1.215643 	Validation Loss: 1.413249 	 time: 0.3
Validation loss decreased from 1.413972 to 1.413249. Model was saved
Epoch: 410 	Training Loss: 1.215342 	Validation Loss: 1.414956 	 time: 0.3
Epoch: 411 	Training Loss: 1.215032 	Validation Loss: 1.412820 	 time: 0.3
Validation loss decreased from 1.413249 to 1.412820. Model was saved
Epoch: 412 	Training Loss: 1.214682 	Validation Loss: 1.413584 	 time: 0.3
Epoch: 413 	Training Loss: 1.214392 	Validation Loss: 1.412522 	 time: 0.3
Validation loss decreased from 1.412820 to 1.412522. Model was saved
Epoch: 414 	Training Loss: 1.214118 	Validation Loss: 1.412464 	 time: 0.3
Validation loss decreased from 1.412522 to 1.412464. Model was saved
Epoch: 415 	Training Loss: 1.213856 	Validation Loss: 1.412399 	 time: 0.3
Validation loss decreased from 1.412464 to 1.412399. Model was saved
Epoch: 416 	Training Loss: 1.213613 	Validation Loss: 1.411798 	 time: 0.3
Validation loss decreased from 1.412399 to 1.411798. Model was saved
Epoch: 417 	Training Loss: 1.213406 	Validation Loss: 1.412495 	 time: 0.3
Epoch: 418 	Training Loss: 1.213226 	Validation Loss: 1.411287 	 time: 0.3
Validation loss decreased from 1.411798 to 1.411287. Model was saved
Epoch: 419 	Training Loss: 1.213061 	Validation Loss: 1.412529 	 time: 0.3
Epoch: 420 	Training Loss: 1.212879 	Validation Loss: 1.410561 	 time: 0.3
Validation loss decreased from 1.411287 to 1.410561. Model was saved
Epoch: 421 	Training Loss: 1.212679 	Validation Loss: 1.412710 	 time: 0.3
Epoch: 422 	Training Loss: 1.212497 	Validation Loss: 1.410337 	 time: 0.3
Validation loss decreased from 1.410561 to 1.410337. Model was saved
Epoch: 423 	Training Loss: 1.212273 	Validation Loss: 1.412885 	 time: 0.3
Epoch: 424 	Training Loss: 1.212068 	Validation Loss: 1.410776 	 time: 0.3
Epoch: 425 	Training Loss: 1.211777 	Validation Loss: 1.412410 	 time: 0.3
Epoch: 426 	Training Loss: 1.211489 	Validation Loss: 1.411366 	 time: 0.3
Epoch: 427 	Training Loss: 1.211203 	Validation Loss: 1.411318 	 time: 0.3
Epoch: 428 	Training Loss: 1.210952 	Validation Loss: 1.411422 	 time: 0.3
Epoch: 429 	Training Loss: 1.210743 	Validation Loss: 1.410360 	 time: 0.3
Epoch: 430 	Training Loss: 1.210557 	Validation Loss: 1.411072 	 time: 0.3
Epoch: 431 	Training Loss: 1.210389 	Validation Loss: 1.409966 	 time: 0.3
Validation loss decreased from 1.410337 to 1.409966. Model was saved
Epoch: 432 	Training Loss: 1.210233 	Validation Loss: 1.410676 	 time: 0.3
Epoch: 433 	Training Loss: 1.210088 	Validation Loss: 1.409873 	 time: 0.3
Validation loss decreased from 1.409966 to 1.409873. Model was saved
Epoch: 434 	Training Loss: 1.209939 	Validation Loss: 1.410267 	 time: 0.3
Epoch: 435 	Training Loss: 1.209756 	Validation Loss: 1.409660 	 time: 0.3
Validation loss decreased from 1.409873 to 1.409660. Model was saved
Epoch: 436 	Training Loss: 1.209560 	Validation Loss: 1.409986 	 time: 0.3
Epoch: 437 	Training Loss: 1.209327 	Validation Loss: 1.409275 	 time: 0.3
Validation loss decreased from 1.409660 to 1.409275. Model was saved
Epoch: 438 	Training Loss: 1.209093 	Validation Loss: 1.409878 	 time: 0.3
Epoch: 439 	Training Loss: 1.208852 	Validation Loss: 1.408758 	 time: 0.3
Validation loss decreased from 1.409275 to 1.408758. Model was saved
Epoch: 440 	Training Loss: 1.208617 	Validation Loss: 1.409499 	 time: 0.3
Epoch: 441 	Training Loss: 1.208395 	Validation Loss: 1.408256 	 time: 0.3
Validation loss decreased from 1.408758 to 1.408256. Model was saved
Epoch: 442 	Training Loss: 1.208180 	Validation Loss: 1.408718 	 time: 0.3
Epoch: 443 	Training Loss: 1.207976 	Validation Loss: 1.407799 	 time: 0.3
Validation loss decreased from 1.408256 to 1.407799. Model was saved
Epoch: 444 	Training Loss: 1.207777 	Validation Loss: 1.407853 	 time: 0.3
Epoch: 445 	Training Loss: 1.207582 	Validation Loss: 1.407504 	 time: 0.3
Validation loss decreased from 1.407799 to 1.407504. Model was saved
Epoch: 446 	Training Loss: 1.207390 	Validation Loss: 1.407176 	 time: 0.3
Validation loss decreased from 1.407504 to 1.407176. Model was saved
Epoch: 447 	Training Loss: 1.207205 	Validation Loss: 1.407317 	 time: 0.3
Epoch: 448 	Training Loss: 1.207029 	Validation Loss: 1.406568 	 time: 0.3
Validation loss decreased from 1.407176 to 1.406568. Model was saved
Epoch: 449 	Training Loss: 1.206862 	Validation Loss: 1.407147 	 time: 0.3
Epoch: 450 	Training Loss: 1.206705 	Validation Loss: 1.405891 	 time: 0.3
Validation loss decreased from 1.406568 to 1.405891. Model was saved
Epoch: 451 	Training Loss: 1.206556 	Validation Loss: 1.407040 	 time: 0.3
Epoch: 452 	Training Loss: 1.206428 	Validation Loss: 1.405190 	 time: 0.3
Validation loss decreased from 1.405891 to 1.405190. Model was saved
Epoch: 453 	Training Loss: 1.206317 	Validation Loss: 1.407367 	 time: 0.3
Epoch: 454 	Training Loss: 1.206284 	Validation Loss: 1.404769 	 time: 0.3
Validation loss decreased from 1.405190 to 1.404769. Model was saved
Epoch: 455 	Training Loss: 1.206272 	Validation Loss: 1.408141 	 time: 0.3
Epoch: 456 	Training Loss: 1.206240 	Validation Loss: 1.405368 	 time: 0.3
Epoch: 457 	Training Loss: 1.205685 	Validation Loss: 1.405232 	 time: 0.3
Epoch: 458 	Training Loss: 1.205217 	Validation Loss: 1.407282 	 time: 0.3
Epoch: 459 	Training Loss: 1.205195 	Validation Loss: 1.403900 	 time: 0.3
Validation loss decreased from 1.404769 to 1.403900. Model was saved
Epoch: 460 	Training Loss: 1.205166 	Validation Loss: 1.407528 	 time: 0.3
Epoch: 461 	Training Loss: 1.204929 	Validation Loss: 1.405418 	 time: 0.3
Epoch: 462 	Training Loss: 1.204504 	Validation Loss: 1.404860 	 time: 0.3
Epoch: 463 	Training Loss: 1.204373 	Validation Loss: 1.407339 	 time: 0.3
Epoch: 464 	Training Loss: 1.204397 	Validation Loss: 1.404057 	 time: 0.3
Epoch: 465 	Training Loss: 1.204167 	Validation Loss: 1.406004 	 time: 0.3
Epoch: 466 	Training Loss: 1.203876 	Validation Loss: 1.405311 	 time: 0.3
Epoch: 467 	Training Loss: 1.203719 	Validation Loss: 1.404258 	 time: 0.3
Epoch: 468 	Training Loss: 1.203654 	Validation Loss: 1.405985 	 time: 0.3
Epoch: 469 	Training Loss: 1.203479 	Validation Loss: 1.404589 	 time: 0.3
Epoch: 470 	Training Loss: 1.203208 	Validation Loss: 1.404468 	 time: 0.3
Epoch: 471 	Training Loss: 1.203050 	Validation Loss: 1.405848 	 time: 0.3
Epoch: 472 	Training Loss: 1.202987 	Validation Loss: 1.404070 	 time: 0.3
Epoch: 473 	Training Loss: 1.202844 	Validation Loss: 1.405148 	 time: 0.3
Epoch: 474 	Training Loss: 1.202634 	Validation Loss: 1.404887 	 time: 0.3
Epoch: 475 	Training Loss: 1.202474 	Validation Loss: 1.403298 	 time: 0.3
Validation loss decreased from 1.403900 to 1.403298. Model was saved
Epoch: 476 	Training Loss: 1.202369 	Validation Loss: 1.404633 	 time: 0.3
Epoch: 477 	Training Loss: 1.202221 	Validation Loss: 1.402006 	 time: 0.3
Validation loss decreased from 1.403298 to 1.402006. Model was saved
Epoch: 478 	Training Loss: 1.202004 	Validation Loss: 1.401307 	 time: 0.3
Validation loss decreased from 1.402006 to 1.401307. Model was saved
Epoch: 479 	Training Loss: 1.201826 	Validation Loss: 1.400447 	 time: 0.3
Validation loss decreased from 1.401307 to 1.400447. Model was saved
Epoch: 480 	Training Loss: 1.201712 	Validation Loss: 1.398729 	 time: 0.3
Validation loss decreased from 1.400447 to 1.398729. Model was saved
Epoch: 481 	Training Loss: 1.201594 	Validation Loss: 1.398554 	 time: 0.3
Validation loss decreased from 1.398729 to 1.398554. Model was saved
Epoch: 482 	Training Loss: 1.201445 	Validation Loss: 1.398630 	 time: 0.3
Epoch: 483 	Training Loss: 1.201321 	Validation Loss: 1.397298 	 time: 0.3
Validation loss decreased from 1.398554 to 1.397298. Model was saved
Epoch: 484 	Training Loss: 1.201224 	Validation Loss: 1.399041 	 time: 0.3
Epoch: 485 	Training Loss: 1.201127 	Validation Loss: 1.397420 	 time: 0.3
Epoch: 486 	Training Loss: 1.200988 	Validation Loss: 1.398691 	 time: 0.3
Epoch: 487 	Training Loss: 1.200884 	Validation Loss: 1.397923 	 time: 0.3
Epoch: 488 	Training Loss: 1.200780 	Validation Loss: 1.398495 	 time: 0.3
Epoch: 489 	Training Loss: 1.200713 	Validation Loss: 1.397273 	 time: 0.3
Validation loss decreased from 1.397298 to 1.397273. Model was saved
Epoch: 490 	Training Loss: 1.200578 	Validation Loss: 1.398387 	 time: 0.3
Epoch: 491 	Training Loss: 1.200503 	Validation Loss: 1.396088 	 time: 0.3
Validation loss decreased from 1.397273 to 1.396088. Model was saved
Epoch: 492 	Training Loss: 1.200353 	Validation Loss: 1.397697 	 time: 0.3
Epoch: 493 	Training Loss: 1.200219 	Validation Loss: 1.395721 	 time: 0.3
Validation loss decreased from 1.396088 to 1.395721. Model was saved
Epoch: 494 	Training Loss: 1.199992 	Validation Loss: 1.396288 	 time: 0.3
Epoch: 495 	Training Loss: 1.199793 	Validation Loss: 1.395849 	 time: 0.3
Epoch: 496 	Training Loss: 1.199601 	Validation Loss: 1.395342 	 time: 0.3
Validation loss decreased from 1.395721 to 1.395342. Model was saved
Epoch: 497 	Training Loss: 1.199435 	Validation Loss: 1.395547 	 time: 0.3
Epoch: 498 	Training Loss: 1.199296 	Validation Loss: 1.394642 	 time: 0.3
Validation loss decreased from 1.395342 to 1.394642. Model was saved
Epoch: 499 	Training Loss: 1.199181 	Validation Loss: 1.394800 	 time: 0.3
Epoch: 500 	Training Loss: 1.199088 	Validation Loss: 1.393496 	 time: 0.3
Validation loss decreased from 1.394642 to 1.393496. Model was saved
Epoch: 501 	Training Loss: 1.198985 	Validation Loss: 1.394340 	 time: 0.3
Epoch: 502 	Training Loss: 1.198950 	Validation Loss: 1.392207 	 time: 0.3
Validation loss decreased from 1.393496 to 1.392207. Model was saved
Epoch: 503 	Training Loss: 1.198881 	Validation Loss: 1.394218 	 time: 0.3
Epoch: 504 	Training Loss: 1.198930 	Validation Loss: 1.391513 	 time: 0.3
Validation loss decreased from 1.392207 to 1.391513. Model was saved
Epoch: 505 	Training Loss: 1.198793 	Validation Loss: 1.393234 	 time: 0.3
Epoch: 506 	Training Loss: 1.198739 	Validation Loss: 1.390783 	 time: 0.3
Validation loss decreased from 1.391513 to 1.390783. Model was saved
Epoch: 507 	Training Loss: 1.198400 	Validation Loss: 1.391083 	 time: 0.3
Epoch: 508 	Training Loss: 1.198118 	Validation Loss: 1.389829 	 time: 0.3
Validation loss decreased from 1.390783 to 1.389829. Model was saved
Epoch: 509 	Training Loss: 1.197827 	Validation Loss: 1.389435 	 time: 0.3
Validation loss decreased from 1.389829 to 1.389435. Model was saved
Epoch: 510 	Training Loss: 1.197674 	Validation Loss: 1.389779 	 time: 0.3
Epoch: 511 	Training Loss: 1.197639 	Validation Loss: 1.388395 	 time: 0.3
Validation loss decreased from 1.389435 to 1.388395. Model was saved
Epoch: 512 	Training Loss: 1.197630 	Validation Loss: 1.390128 	 time: 0.3
Epoch: 513 	Training Loss: 1.197666 	Validation Loss: 1.387115 	 time: 0.3
Validation loss decreased from 1.388395 to 1.387115. Model was saved
Epoch: 514 	Training Loss: 1.197560 	Validation Loss: 1.389572 	 time: 0.3
Epoch: 515 	Training Loss: 1.197500 	Validation Loss: 1.386283 	 time: 0.3
Validation loss decreased from 1.387115 to 1.386283. Model was saved
Epoch: 516 	Training Loss: 1.197234 	Validation Loss: 1.388218 	 time: 0.3
Epoch: 517 	Training Loss: 1.197034 	Validation Loss: 1.385974 	 time: 0.3
Validation loss decreased from 1.386283 to 1.385974. Model was saved
Epoch: 518 	Training Loss: 1.196776 	Validation Loss: 1.386864 	 time: 0.3
Epoch: 519 	Training Loss: 1.196581 	Validation Loss: 1.386562 	 time: 0.3
Epoch: 520 	Training Loss: 1.196438 	Validation Loss: 1.385947 	 time: 0.3
Validation loss decreased from 1.385974 to 1.385947. Model was saved
Epoch: 521 	Training Loss: 1.196337 	Validation Loss: 1.387136 	 time: 0.3
Epoch: 522 	Training Loss: 1.196259 	Validation Loss: 1.384938 	 time: 0.3
Validation loss decreased from 1.385947 to 1.384938. Model was saved
Epoch: 523 	Training Loss: 1.196200 	Validation Loss: 1.387246 	 time: 0.3
Epoch: 524 	Training Loss: 1.196241 	Validation Loss: 1.383856 	 time: 0.3
Validation loss decreased from 1.384938 to 1.383856. Model was saved
Epoch: 525 	Training Loss: 1.196258 	Validation Loss: 1.387772 	 time: 0.3
Epoch: 526 	Training Loss: 1.196492 	Validation Loss: 1.383322 	 time: 0.3
Validation loss decreased from 1.383856 to 1.383322. Model was saved
Epoch: 527 	Training Loss: 1.196347 	Validation Loss: 1.386267 	 time: 0.3
Epoch: 528 	Training Loss: 1.196127 	Validation Loss: 1.383356 	 time: 0.3
Epoch: 529 	Training Loss: 1.195575 	Validation Loss: 1.383036 	 time: 0.3
Validation loss decreased from 1.383322 to 1.383036. Model was saved
Epoch: 530 	Training Loss: 1.195453 	Validation Loss: 1.386749 	 time: 0.3
Epoch: 531 	Training Loss: 1.195668 	Validation Loss: 1.382778 	 time: 0.3
Validation loss decreased from 1.383036 to 1.382778. Model was saved
Epoch: 532 	Training Loss: 1.195650 	Validation Loss: 1.386482 	 time: 0.3
Epoch: 533 	Training Loss: 1.195535 	Validation Loss: 1.382124 	 time: 0.3
Validation loss decreased from 1.382778 to 1.382124. Model was saved
Epoch: 534 	Training Loss: 1.195111 	Validation Loss: 1.382248 	 time: 0.3
Epoch: 535 	Training Loss: 1.194934 	Validation Loss: 1.384088 	 time: 0.3
Epoch: 536 	Training Loss: 1.195016 	Validation Loss: 1.381154 	 time: 0.3
Validation loss decreased from 1.382124 to 1.381154. Model was saved
Epoch: 537 	Training Loss: 1.194995 	Validation Loss: 1.384349 	 time: 0.3
Epoch: 538 	Training Loss: 1.194855 	Validation Loss: 1.381041 	 time: 0.3
Validation loss decreased from 1.381154 to 1.381041. Model was saved
Epoch: 539 	Training Loss: 1.194571 	Validation Loss: 1.381396 	 time: 0.3
Epoch: 540 	Training Loss: 1.194434 	Validation Loss: 1.383243 	 time: 0.3
Epoch: 541 	Training Loss: 1.194425 	Validation Loss: 1.380332 	 time: 0.3
Validation loss decreased from 1.381041 to 1.380332. Model was saved
Epoch: 542 	Training Loss: 1.194364 	Validation Loss: 1.383920 	 time: 0.3
Epoch: 543 	Training Loss: 1.194257 	Validation Loss: 1.380362 	 time: 0.3
Epoch: 544 	Training Loss: 1.194050 	Validation Loss: 1.382378 	 time: 0.3
Epoch: 545 	Training Loss: 1.193873 	Validation Loss: 1.381740 	 time: 0.3
Epoch: 546 	Training Loss: 1.193743 	Validation Loss: 1.380852 	 time: 0.3
Epoch: 547 	Training Loss: 1.193668 	Validation Loss: 1.383075 	 time: 0.3
Epoch: 548 	Training Loss: 1.193611 	Validation Loss: 1.380725 	 time: 0.3
Epoch: 549 	Training Loss: 1.193516 	Validation Loss: 1.383271 	 time: 0.3
Epoch: 550 	Training Loss: 1.193403 	Validation Loss: 1.380881 	 time: 0.3
Epoch: 551 	Training Loss: 1.193275 	Validation Loss: 1.382455 	 time: 0.3
Epoch: 552 	Training Loss: 1.193152 	Validation Loss: 1.380912 	 time: 0.3
Epoch: 553 	Training Loss: 1.193022 	Validation Loss: 1.381480 	 time: 0.3
Epoch: 554 	Training Loss: 1.192902 	Validation Loss: 1.381466 	 time: 0.3
Epoch: 555 	Training Loss: 1.192794 	Validation Loss: 1.380843 	 time: 0.3
Epoch: 556 	Training Loss: 1.192697 	Validation Loss: 1.382206 	 time: 0.3
Epoch: 557 	Training Loss: 1.192607 	Validation Loss: 1.381078 	 time: 0.3
Epoch: 558 	Training Loss: 1.192511 	Validation Loss: 1.382223 	 time: 0.3
Epoch: 559 	Training Loss: 1.192410 	Validation Loss: 1.381214 	 time: 0.3
Epoch: 560 	Training Loss: 1.192299 	Validation Loss: 1.381451 	 time: 0.3
Epoch: 561 	Training Loss: 1.192186 	Validation Loss: 1.380797 	 time: 0.3
Epoch: 562 	Training Loss: 1.192075 	Validation Loss: 1.380347 	 time: 0.3
Epoch: 563 	Training Loss: 1.191968 	Validation Loss: 1.380316 	 time: 0.3
Validation loss decreased from 1.380332 to 1.380316. Model was saved
Epoch: 564 	Training Loss: 1.191865 	Validation Loss: 1.379724 	 time: 0.3
Validation loss decreased from 1.380316 to 1.379724. Model was saved
Epoch: 565 	Training Loss: 1.191760 	Validation Loss: 1.380389 	 time: 0.3
Epoch: 566 	Training Loss: 1.191649 	Validation Loss: 1.379594 	 time: 0.3
Validation loss decreased from 1.379724 to 1.379594. Model was saved
Epoch: 567 	Training Loss: 1.191540 	Validation Loss: 1.380305 	 time: 0.3
Epoch: 568 	Training Loss: 1.191440 	Validation Loss: 1.379707 	 time: 0.3
Epoch: 569 	Training Loss: 1.191340 	Validation Loss: 1.379882 	 time: 0.3
Epoch: 570 	Training Loss: 1.191233 	Validation Loss: 1.379563 	 time: 0.3
Validation loss decreased from 1.379594 to 1.379563. Model was saved
Epoch: 571 	Training Loss: 1.191121 	Validation Loss: 1.379376 	 time: 0.3
Validation loss decreased from 1.379563 to 1.379376. Model was saved
Epoch: 572 	Training Loss: 1.191003 	Validation Loss: 1.379400 	 time: 0.3
Epoch: 573 	Training Loss: 1.190876 	Validation Loss: 1.379349 	 time: 0.3
Validation loss decreased from 1.379376 to 1.379349. Model was saved
Epoch: 574 	Training Loss: 1.190744 	Validation Loss: 1.379560 	 time: 0.3
Epoch: 575 	Training Loss: 1.190622 	Validation Loss: 1.379355 	 time: 0.3
Epoch: 576 	Training Loss: 1.190509 	Validation Loss: 1.379721 	 time: 0.3
Epoch: 577 	Training Loss: 1.190400 	Validation Loss: 1.379085 	 time: 0.3
Validation loss decreased from 1.379349 to 1.379085. Model was saved
Epoch: 578 	Training Loss: 1.190294 	Validation Loss: 1.379406 	 time: 0.3
Epoch: 579 	Training Loss: 1.190192 	Validation Loss: 1.378454 	 time: 0.3
Validation loss decreased from 1.379085 to 1.378454. Model was saved
Epoch: 580 	Training Loss: 1.190091 	Validation Loss: 1.378707 	 time: 0.3
Epoch: 581 	Training Loss: 1.189990 	Validation Loss: 1.377800 	 time: 0.3
Validation loss decreased from 1.378454 to 1.377800. Model was saved
Epoch: 582 	Training Loss: 1.189892 	Validation Loss: 1.378112 	 time: 0.3
Epoch: 583 	Training Loss: 1.189797 	Validation Loss: 1.377351 	 time: 0.3
Validation loss decreased from 1.377800 to 1.377351. Model was saved
Epoch: 584 	Training Loss: 1.189703 	Validation Loss: 1.377905 	 time: 0.3
Epoch: 585 	Training Loss: 1.189608 	Validation Loss: 1.377370 	 time: 0.3
Epoch: 586 	Training Loss: 1.189512 	Validation Loss: 1.377930 	 time: 0.3
Epoch: 587 	Training Loss: 1.189416 	Validation Loss: 1.377616 	 time: 0.3
Epoch: 588 	Training Loss: 1.189317 	Validation Loss: 1.377905 	 time: 0.3
Epoch: 589 	Training Loss: 1.189210 	Validation Loss: 1.377605 	 time: 0.3
Epoch: 590 	Training Loss: 1.189092 	Validation Loss: 1.377413 	 time: 0.3
Epoch: 591 	Training Loss: 1.188975 	Validation Loss: 1.377009 	 time: 0.3
Validation loss decreased from 1.377351 to 1.377009. Model was saved
Epoch: 592 	Training Loss: 1.188873 	Validation Loss: 1.376601 	 time: 0.3
Validation loss decreased from 1.377009 to 1.376601. Model was saved
Epoch: 593 	Training Loss: 1.188780 	Validation Loss: 1.376230 	 time: 0.3
Validation loss decreased from 1.376601 to 1.376230. Model was saved
Epoch: 594 	Training Loss: 1.188688 	Validation Loss: 1.375702 	 time: 0.3
Validation loss decreased from 1.376230 to 1.375702. Model was saved
Epoch: 595 	Training Loss: 1.188594 	Validation Loss: 1.375502 	 time: 0.3
Validation loss decreased from 1.375702 to 1.375502. Model was saved
Epoch: 596 	Training Loss: 1.188498 	Validation Loss: 1.375145 	 time: 0.3
Validation loss decreased from 1.375502 to 1.375145. Model was saved
Epoch: 597 	Training Loss: 1.188400 	Validation Loss: 1.375215 	 time: 0.3
Epoch: 598 	Training Loss: 1.188301 	Validation Loss: 1.375195 	 time: 0.3
Epoch: 599 	Training Loss: 1.188200 	Validation Loss: 1.375558 	 time: 0.3
Epoch: 600 	Training Loss: 1.188097 	Validation Loss: 1.375750 	 time: 0.3
Epoch: 601 	Training Loss: 1.187993 	Validation Loss: 1.376232 	 time: 0.3
Epoch: 602 	Training Loss: 1.187890 	Validation Loss: 1.376229 	 time: 0.3
Epoch: 603 	Training Loss: 1.187789 	Validation Loss: 1.376613 	 time: 0.3
Epoch: 604 	Training Loss: 1.187690 	Validation Loss: 1.376213 	 time: 0.3
Epoch: 605 	Training Loss: 1.187594 	Validation Loss: 1.376433 	 time: 0.3
Epoch: 606 	Training Loss: 1.187501 	Validation Loss: 1.375717 	 time: 0.3
Epoch: 607 	Training Loss: 1.187412 	Validation Loss: 1.376029 	 time: 0.3
Epoch: 608 	Training Loss: 1.187328 	Validation Loss: 1.375110 	 time: 0.3
Validation loss decreased from 1.375145 to 1.375110. Model was saved
Epoch: 609 	Training Loss: 1.187250 	Validation Loss: 1.375802 	 time: 0.3
Epoch: 610 	Training Loss: 1.187179 	Validation Loss: 1.374503 	 time: 0.3
Validation loss decreased from 1.375110 to 1.374503. Model was saved
Epoch: 611 	Training Loss: 1.187119 	Validation Loss: 1.375759 	 time: 0.3
Epoch: 612 	Training Loss: 1.187077 	Validation Loss: 1.373893 	 time: 0.3
Validation loss decreased from 1.374503 to 1.373893. Model was saved
Epoch: 613 	Training Loss: 1.187058 	Validation Loss: 1.375925 	 time: 0.3
Epoch: 614 	Training Loss: 1.187085 	Validation Loss: 1.373520 	 time: 0.3
Validation loss decreased from 1.373893 to 1.373520. Model was saved
Epoch: 615 	Training Loss: 1.187161 	Validation Loss: 1.376385 	 time: 0.3
Epoch: 616 	Training Loss: 1.187229 	Validation Loss: 1.373150 	 time: 0.3
Validation loss decreased from 1.373520 to 1.373150. Model was saved
Epoch: 617 	Training Loss: 1.187068 	Validation Loss: 1.375281 	 time: 0.3
Epoch: 618 	Training Loss: 1.186797 	Validation Loss: 1.372346 	 time: 0.3
Validation loss decreased from 1.373150 to 1.372346. Model was saved
Epoch: 619 	Training Loss: 1.186500 	Validation Loss: 1.374750 	 time: 0.3
Epoch: 620 	Training Loss: 1.186429 	Validation Loss: 1.374200 	 time: 0.3
Epoch: 621 	Training Loss: 1.186362 	Validation Loss: 1.375000 	 time: 0.3
Epoch: 622 	Training Loss: 1.186238 	Validation Loss: 1.375783 	 time: 0.3
Epoch: 623 	Training Loss: 1.186010 	Validation Loss: 1.373823 	 time: 0.3
Epoch: 624 	Training Loss: 1.185866 	Validation Loss: 1.376411 	 time: 0.3
Epoch: 625 	Training Loss: 1.185884 	Validation Loss: 1.373154 	 time: 0.3
Epoch: 626 	Training Loss: 1.185834 	Validation Loss: 1.376566 	 time: 0.3
Epoch: 627 	Training Loss: 1.185762 	Validation Loss: 1.373504 	 time: 0.3
Epoch: 628 	Training Loss: 1.185491 	Validation Loss: 1.376523 	 time: 0.3
Epoch: 629 	Training Loss: 1.185328 	Validation Loss: 1.374359 	 time: 0.3
Epoch: 630 	Training Loss: 1.185232 	Validation Loss: 1.376328 	 time: 0.3
Epoch: 631 	Training Loss: 1.185189 	Validation Loss: 1.374244 	 time: 0.3
Epoch: 632 	Training Loss: 1.185071 	Validation Loss: 1.375874 	 time: 0.3
Epoch: 633 	Training Loss: 1.184932 	Validation Loss: 1.373107 	 time: 0.3
Epoch: 634 	Training Loss: 1.184808 	Validation Loss: 1.376275 	 time: 0.3
Epoch: 635 	Training Loss: 1.184808 	Validation Loss: 1.372748 	 time: 0.3
Epoch: 636 	Training Loss: 1.184729 	Validation Loss: 1.377501 	 time: 0.3
Epoch: 637 	Training Loss: 1.184723 	Validation Loss: 1.373304 	 time: 0.3
Epoch: 638 	Training Loss: 1.184573 	Validation Loss: 1.378567 	 time: 0.3
Epoch: 639 	Training Loss: 1.184602 	Validation Loss: 1.374099 	 time: 0.3
Epoch: 640 	Training Loss: 1.184543 	Validation Loss: 1.378710 	 time: 0.3
Epoch: 641 	Training Loss: 1.184581 	Validation Loss: 1.374122 	 time: 0.3
Epoch: 642 	Training Loss: 1.184278 	Validation Loss: 1.376430 	 time: 0.3
Epoch: 643 	Training Loss: 1.183932 	Validation Loss: 1.374550 	 time: 0.3
Epoch: 644 	Training Loss: 1.183669 	Validation Loss: 1.375407 	 time: 0.3
Epoch: 645 	Training Loss: 1.183655 	Validation Loss: 1.377842 	 time: 0.3
Epoch: 646 	Training Loss: 1.183723 	Validation Loss: 1.375084 	 time: 0.3
Epoch: 647 	Training Loss: 1.183682 	Validation Loss: 1.378080 	 time: 0.3
Epoch: 648 	Training Loss: 1.183586 	Validation Loss: 1.374731 	 time: 0.3
Epoch: 649 	Training Loss: 1.183392 	Validation Loss: 1.376034 	 time: 0.3
Epoch: 650 	Training Loss: 1.183243 	Validation Loss: 1.375383 	 time: 0.3
Epoch: 651 	Training Loss: 1.183159 	Validation Loss: 1.374015 	 time: 0.3
Epoch: 652 	Training Loss: 1.183144 	Validation Loss: 1.376236 	 time: 0.3
Epoch: 653 	Training Loss: 1.183154 	Validation Loss: 1.372979 	 time: 0.3
Epoch: 654 	Training Loss: 1.183067 	Validation Loss: 1.375404 	 time: 0.3
Epoch: 655 	Training Loss: 1.182928 	Validation Loss: 1.373482 	 time: 0.3
Epoch: 656 	Training Loss: 1.182774 	Validation Loss: 1.373506 	 time: 0.3
Epoch: 657 	Training Loss: 1.182695 	Validation Loss: 1.374741 	 time: 0.3
Epoch: 658 	Training Loss: 1.182662 	Validation Loss: 1.372515 	 time: 0.3
Epoch: 659 	Training Loss: 1.182587 	Validation Loss: 1.374792 	 time: 0.3
Epoch: 660 	Training Loss: 1.182485 	Validation Loss: 1.373215 	 time: 0.3
Epoch: 661 	Training Loss: 1.182355 	Validation Loss: 1.374085 	 time: 0.3
Epoch: 662 	Training Loss: 1.182237 	Validation Loss: 1.374573 	 time: 0.3
Epoch: 663 	Training Loss: 1.182155 	Validation Loss: 1.373672 	 time: 0.3
Epoch: 664 	Training Loss: 1.182098 	Validation Loss: 1.375232 	 time: 0.3
Epoch: 665 	Training Loss: 1.182049 	Validation Loss: 1.373830 	 time: 0.3
Epoch: 666 	Training Loss: 1.181983 	Validation Loss: 1.374884 	 time: 0.3
Epoch: 667 	Training Loss: 1.181893 	Validation Loss: 1.374124 	 time: 0.3
Epoch: 668 	Training Loss: 1.181793 	Validation Loss: 1.374309 	 time: 0.3
Epoch: 669 	Training Loss: 1.181706 	Validation Loss: 1.374737 	 time: 0.3
Epoch: 670 	Training Loss: 1.181642 	Validation Loss: 1.373937 	 time: 0.3
Epoch: 671 	Training Loss: 1.181587 	Validation Loss: 1.374831 	 time: 0.3
Epoch: 672 	Training Loss: 1.181522 	Validation Loss: 1.373739 	 time: 0.3
Epoch: 673 	Training Loss: 1.181443 	Validation Loss: 1.374242 	 time: 0.3
Epoch: 674 	Training Loss: 1.181361 	Validation Loss: 1.374066 	 time: 0.3
Epoch: 675 	Training Loss: 1.181285 	Validation Loss: 1.373742 	 time: 0.3
Epoch: 676 	Training Loss: 1.181212 	Validation Loss: 1.374444 	 time: 0.3
Epoch: 677 	Training Loss: 1.181137 	Validation Loss: 1.373560 	 time: 0.3
Epoch: 678 	Training Loss: 1.181061 	Validation Loss: 1.374189 	 time: 0.3
Epoch: 679 	Training Loss: 1.180987 	Validation Loss: 1.373596 	 time: 0.3
Epoch: 680 	Training Loss: 1.180912 	Validation Loss: 1.373650 	 time: 0.3
Epoch: 681 	Training Loss: 1.180837 	Validation Loss: 1.373747 	 time: 0.3
Epoch: 682 	Training Loss: 1.180763 	Validation Loss: 1.373255 	 time: 0.3
Epoch: 683 	Training Loss: 1.180687 	Validation Loss: 1.373869 	 time: 0.3
Epoch: 684 	Training Loss: 1.180604 	Validation Loss: 1.373181 	 time: 0.3
Epoch: 685 	Training Loss: 1.180514 	Validation Loss: 1.373840 	 time: 0.3
Epoch: 686 	Training Loss: 1.180421 	Validation Loss: 1.373517 	 time: 0.3
Epoch: 687 	Training Loss: 1.180333 	Validation Loss: 1.373746 	 time: 0.3
Epoch: 688 	Training Loss: 1.180254 	Validation Loss: 1.373990 	 time: 0.3
Epoch: 689 	Training Loss: 1.180182 	Validation Loss: 1.373712 	 time: 0.3
Epoch: 690 	Training Loss: 1.180113 	Validation Loss: 1.374271 	 time: 0.3
Epoch: 691 	Training Loss: 1.180045 	Validation Loss: 1.373977 	 time: 0.3
Epoch: 692 	Training Loss: 1.179977 	Validation Loss: 1.374534 	 time: 0.3
Epoch: 693 	Training Loss: 1.179909 	Validation Loss: 1.374486 	 time: 0.3
Epoch: 694 	Training Loss: 1.179844 	Validation Loss: 1.374771 	 time: 0.3
Epoch: 695 	Training Loss: 1.179783 	Validation Loss: 1.374886 	 time: 0.3
Epoch: 696 	Training Loss: 1.179726 	Validation Loss: 1.374810 	 time: 0.3
Epoch: 697 	Training Loss: 1.179673 	Validation Loss: 1.375039 	 time: 0.3
Epoch: 698 	Training Loss: 1.179621 	Validation Loss: 1.374723 	 time: 0.3
Epoch: 699 	Training Loss: 1.179568 	Validation Loss: 1.375019 	 time: 0.3
Epoch: 700 	Training Loss: 1.179513 	Validation Loss: 1.374774 	 time: 0.3
Epoch: 701 	Training Loss: 1.179454 	Validation Loss: 1.374919 	 time: 0.3
Epoch: 702 	Training Loss: 1.179393 	Validation Loss: 1.374933 	 time: 0.3
Epoch: 703 	Training Loss: 1.179327 	Validation Loss: 1.374875 	 time: 0.3
Epoch: 704 	Training Loss: 1.179259 	Validation Loss: 1.375155 	 time: 0.3
Epoch: 705 	Training Loss: 1.179196 	Validation Loss: 1.375068 	 time: 0.3
Epoch: 706 	Training Loss: 1.179139 	Validation Loss: 1.375415 	 time: 0.3
Epoch: 707 	Training Loss: 1.179087 	Validation Loss: 1.375440 	 time: 0.3
Epoch: 708 	Training Loss: 1.179037 	Validation Loss: 1.375748 	 time: 0.3
Epoch: 709 	Training Loss: 1.178990 	Validation Loss: 1.375825 	 time: 0.3
Epoch: 710 	Training Loss: 1.178944 	Validation Loss: 1.375921 	 time: 0.3
Epoch: 711 	Training Loss: 1.178899 	Validation Loss: 1.375974 	 time: 0.3
Epoch: 712 	Training Loss: 1.178854 	Validation Loss: 1.375797 	 time: 0.3
Epoch: 713 	Training Loss: 1.178810 	Validation Loss: 1.375812 	 time: 0.3
Epoch: 714 	Training Loss: 1.178765 	Validation Loss: 1.375546 	 time: 0.3
Epoch: 715 	Training Loss: 1.178720 	Validation Loss: 1.375533 	 time: 0.3
Epoch: 716 	Training Loss: 1.178672 	Validation Loss: 1.375358 	 time: 0.3
Epoch: 717 	Training Loss: 1.178623 	Validation Loss: 1.375334 	 time: 0.3
Epoch: 718 	Training Loss: 1.178572 	Validation Loss: 1.375278 	 time: 0.3
Epoch: 719 	Training Loss: 1.178518 	Validation Loss: 1.375278 	 time: 0.3
Epoch: 720 	Training Loss: 1.178463 	Validation Loss: 1.375387 	 time: 0.3
Epoch: 721 	Training Loss: 1.178410 	Validation Loss: 1.375402 	 time: 0.3
Epoch: 722 	Training Loss: 1.178359 	Validation Loss: 1.375616 	 time: 0.3
Epoch: 723 	Training Loss: 1.178308 	Validation Loss: 1.375630 	 time: 0.3
Epoch: 724 	Training Loss: 1.178258 	Validation Loss: 1.375827 	 time: 0.3
Epoch: 725 	Training Loss: 1.178209 	Validation Loss: 1.375830 	 time: 0.3
Epoch: 726 	Training Loss: 1.178161 	Validation Loss: 1.375923 	 time: 0.3
Epoch: 727 	Training Loss: 1.178113 	Validation Loss: 1.375910 	 time: 0.3
Epoch: 728 	Training Loss: 1.178061 	Validation Loss: 1.375973 	 time: 0.3
Epoch: 729 	Training Loss: 1.178008 	Validation Loss: 1.376046 	 time: 0.3
Epoch: 730 	Training Loss: 1.177959 	Validation Loss: 1.376081 	 time: 0.3
Epoch: 731 	Training Loss: 1.177916 	Validation Loss: 1.376180 	 time: 0.3
Epoch: 732 	Training Loss: 1.177872 	Validation Loss: 1.376172 	 time: 0.3
Epoch: 733 	Training Loss: 1.177827 	Validation Loss: 1.376264 	 time: 0.3
Epoch: 734 	Training Loss: 1.177780 	Validation Loss: 1.376229 	 time: 0.3
Epoch: 735 	Training Loss: 1.177731 	Validation Loss: 1.376279 	 time: 0.3
Epoch: 736 	Training Loss: 1.177680 	Validation Loss: 1.376244 	 time: 0.3
Epoch: 737 	Training Loss: 1.177626 	Validation Loss: 1.376238 	 time: 0.3
Epoch: 738 	Training Loss: 1.177569 	Validation Loss: 1.376155 	 time: 0.3
Epoch: 739 	Training Loss: 1.177506 	Validation Loss: 1.376023 	 time: 0.3
Epoch: 740 	Training Loss: 1.177437 	Validation Loss: 1.375849 	 time: 0.3
Epoch: 741 	Training Loss: 1.177364 	Validation Loss: 1.375664 	 time: 0.3
Epoch: 742 	Training Loss: 1.177290 	Validation Loss: 1.375651 	 time: 0.3
Epoch: 743 	Training Loss: 1.177219 	Validation Loss: 1.375748 	 time: 0.3
Epoch: 744 	Training Loss: 1.177151 	Validation Loss: 1.376044 	 time: 0.3
Epoch: 745 	Training Loss: 1.177087 	Validation Loss: 1.376405 	 time: 0.3
Epoch: 746 	Training Loss: 1.177028 	Validation Loss: 1.376774 	 time: 0.3
Epoch: 747 	Training Loss: 1.176967 	Validation Loss: 1.377026 	 time: 0.3
Epoch: 748 	Training Loss: 1.176904 	Validation Loss: 1.377217 	 time: 0.3
Epoch: 749 	Training Loss: 1.176844 	Validation Loss: 1.377305 	 time: 0.3
Epoch: 750 	Training Loss: 1.176787 	Validation Loss: 1.377342 	 time: 0.3
Epoch: 751 	Training Loss: 1.176730 	Validation Loss: 1.377206 	 time: 0.3
Epoch: 752 	Training Loss: 1.176676 	Validation Loss: 1.377124 	 time: 0.3
Epoch: 753 	Training Loss: 1.176624 	Validation Loss: 1.376958 	 time: 0.3
Epoch: 754 	Training Loss: 1.176574 	Validation Loss: 1.376778 	 time: 0.3
Epoch: 755 	Training Loss: 1.176526 	Validation Loss: 1.376516 	 time: 0.3
Epoch: 756 	Training Loss: 1.176478 	Validation Loss: 1.376164 	 time: 0.3
Epoch: 757 	Training Loss: 1.176429 	Validation Loss: 1.375654 	 time: 0.3
Epoch: 758 	Training Loss: 1.176376 	Validation Loss: 1.375122 	 time: 0.3
Epoch: 759 	Training Loss: 1.176321 	Validation Loss: 1.374583 	 time: 0.3
Epoch: 760 	Training Loss: 1.176265 	Validation Loss: 1.374228 	 time: 0.3
Epoch: 761 	Training Loss: 1.176203 	Validation Loss: 1.374034 	 time: 0.3
Epoch: 762 	Training Loss: 1.176133 	Validation Loss: 1.373917 	 time: 0.3
Epoch: 763 	Training Loss: 1.176069 	Validation Loss: 1.373841 	 time: 0.3
Epoch: 764 	Training Loss: 1.176018 	Validation Loss: 1.373860 	 time: 0.3
Epoch: 765 	Training Loss: 1.175971 	Validation Loss: 1.373751 	 time: 0.3
Epoch: 766 	Training Loss: 1.175925 	Validation Loss: 1.373765 	 time: 0.3
Epoch: 767 	Training Loss: 1.175880 	Validation Loss: 1.373713 	 time: 0.3
Epoch: 768 	Training Loss: 1.175835 	Validation Loss: 1.373819 	 time: 0.3
Epoch: 769 	Training Loss: 1.175786 	Validation Loss: 1.373974 	 time: 0.3
Epoch: 770 	Training Loss: 1.175736 	Validation Loss: 1.374215 	 time: 0.3
Epoch: 771 	Training Loss: 1.175686 	Validation Loss: 1.374413 	 time: 0.3
Epoch: 772 	Training Loss: 1.175635 	Validation Loss: 1.374670 	 time: 0.3
Epoch: 773 	Training Loss: 1.175584 	Validation Loss: 1.374795 	 time: 0.3
Epoch: 774 	Training Loss: 1.175530 	Validation Loss: 1.375004 	 time: 0.3
Epoch: 775 	Training Loss: 1.175477 	Validation Loss: 1.375008 	 time: 0.3
Epoch: 776 	Training Loss: 1.175430 	Validation Loss: 1.375080 	 time: 0.3
Epoch: 777 	Training Loss: 1.175388 	Validation Loss: 1.374969 	 time: 0.3
Epoch: 778 	Training Loss: 1.175348 	Validation Loss: 1.375014 	 time: 0.3
Epoch: 779 	Training Loss: 1.175310 	Validation Loss: 1.374930 	 time: 0.3
Epoch: 780 	Training Loss: 1.175272 	Validation Loss: 1.374986 	 time: 0.3
Epoch: 781 	Training Loss: 1.175236 	Validation Loss: 1.374886 	 time: 0.3
Epoch: 782 	Training Loss: 1.175200 	Validation Loss: 1.374826 	 time: 0.3
Epoch: 783 	Training Loss: 1.175165 	Validation Loss: 1.374568 	 time: 0.3
Epoch: 784 	Training Loss: 1.175130 	Validation Loss: 1.374384 	 time: 0.3
Epoch: 785 	Training Loss: 1.175095 	Validation Loss: 1.374050 	 time: 0.3
Epoch: 786 	Training Loss: 1.175060 	Validation Loss: 1.373884 	 time: 0.3
Epoch: 787 	Training Loss: 1.175024 	Validation Loss: 1.373620 	 time: 0.3
Epoch: 788 	Training Loss: 1.174987 	Validation Loss: 1.373565 	 time: 0.3
Epoch: 789 	Training Loss: 1.174948 	Validation Loss: 1.373410 	 time: 0.3
Epoch: 790 	Training Loss: 1.174910 	Validation Loss: 1.373479 	 time: 0.3
Epoch: 791 	Training Loss: 1.174875 	Validation Loss: 1.373400 	 time: 0.3
Epoch: 792 	Training Loss: 1.174843 	Validation Loss: 1.373543 	 time: 0.3
Epoch: 793 	Training Loss: 1.174812 	Validation Loss: 1.373477 	 time: 0.3
Epoch: 794 	Training Loss: 1.174780 	Validation Loss: 1.373603 	 time: 0.3
Epoch: 795 	Training Loss: 1.174747 	Validation Loss: 1.373485 	 time: 0.3
Epoch: 796 	Training Loss: 1.174713 	Validation Loss: 1.373558 	 time: 0.3
Epoch: 797 	Training Loss: 1.174677 	Validation Loss: 1.373377 	 time: 0.3
Epoch: 798 	Training Loss: 1.174639 	Validation Loss: 1.373395 	 time: 0.3
Epoch: 799 	Training Loss: 1.174598 	Validation Loss: 1.373179 	 time: 0.3
Epoch: 800 	Training Loss: 1.174555 	Validation Loss: 1.373142 	 time: 0.3
Epoch: 801 	Training Loss: 1.174513 	Validation Loss: 1.372915 	 time: 0.3
Epoch: 802 	Training Loss: 1.174476 	Validation Loss: 1.372834 	 time: 0.3
Epoch: 803 	Training Loss: 1.174443 	Validation Loss: 1.372630 	 time: 0.3
Epoch: 804 	Training Loss: 1.174411 	Validation Loss: 1.372504 	 time: 0.3
Epoch: 805 	Training Loss: 1.174379 	Validation Loss: 1.372336 	 time: 0.3
Validation loss decreased from 1.372346 to 1.372336. Model was saved
Epoch: 806 	Training Loss: 1.174347 	Validation Loss: 1.372213 	 time: 0.3
Validation loss decreased from 1.372336 to 1.372213. Model was saved
Epoch: 807 	Training Loss: 1.174313 	Validation Loss: 1.372101 	 time: 0.3
Validation loss decreased from 1.372213 to 1.372101. Model was saved
Epoch: 808 	Training Loss: 1.174277 	Validation Loss: 1.371978 	 time: 0.3
Validation loss decreased from 1.372101 to 1.371978. Model was saved
Epoch: 809 	Training Loss: 1.174239 	Validation Loss: 1.371885 	 time: 0.3
Validation loss decreased from 1.371978 to 1.371885. Model was saved
Epoch: 810 	Training Loss: 1.174193 	Validation Loss: 1.371659 	 time: 0.3
Validation loss decreased from 1.371885 to 1.371659. Model was saved
Epoch: 811 	Training Loss: 1.174132 	Validation Loss: 1.371418 	 time: 0.3
Validation loss decreased from 1.371659 to 1.371418. Model was saved
Epoch: 812 	Training Loss: 1.174060 	Validation Loss: 1.371315 	 time: 0.3
Validation loss decreased from 1.371418 to 1.371315. Model was saved
Epoch: 813 	Training Loss: 1.174012 	Validation Loss: 1.371080 	 time: 0.3
Validation loss decreased from 1.371315 to 1.371080. Model was saved
Epoch: 814 	Training Loss: 1.173981 	Validation Loss: 1.371090 	 time: 0.3
Epoch: 815 	Training Loss: 1.173954 	Validation Loss: 1.371079 	 time: 0.3
Validation loss decreased from 1.371080 to 1.371079. Model was saved
Epoch: 816 	Training Loss: 1.173926 	Validation Loss: 1.371106 	 time: 0.3
Epoch: 817 	Training Loss: 1.173899 	Validation Loss: 1.371164 	 time: 0.3
Epoch: 818 	Training Loss: 1.173870 	Validation Loss: 1.371129 	 time: 0.3
Epoch: 819 	Training Loss: 1.173840 	Validation Loss: 1.371069 	 time: 0.3
Validation loss decreased from 1.371079 to 1.371069. Model was saved
Epoch: 820 	Training Loss: 1.173810 	Validation Loss: 1.371066 	 time: 0.3
Validation loss decreased from 1.371069 to 1.371066. Model was saved
Epoch: 821 	Training Loss: 1.173779 	Validation Loss: 1.370968 	 time: 0.3
Validation loss decreased from 1.371066 to 1.370968. Model was saved
Epoch: 822 	Training Loss: 1.173746 	Validation Loss: 1.370930 	 time: 0.3
Validation loss decreased from 1.370968 to 1.370930. Model was saved
Epoch: 823 	Training Loss: 1.173710 	Validation Loss: 1.370802 	 time: 0.3
Validation loss decreased from 1.370930 to 1.370802. Model was saved
Epoch: 824 	Training Loss: 1.173670 	Validation Loss: 1.370595 	 time: 0.3
Validation loss decreased from 1.370802 to 1.370595. Model was saved
Epoch: 825 	Training Loss: 1.173625 	Validation Loss: 1.370395 	 time: 0.3
Validation loss decreased from 1.370595 to 1.370395. Model was saved
Epoch: 826 	Training Loss: 1.173575 	Validation Loss: 1.370132 	 time: 0.3
Validation loss decreased from 1.370395 to 1.370132. Model was saved
Epoch: 827 	Training Loss: 1.173526 	Validation Loss: 1.369978 	 time: 0.3
Validation loss decreased from 1.370132 to 1.369978. Model was saved
Epoch: 828 	Training Loss: 1.173488 	Validation Loss: 1.369649 	 time: 0.3
Validation loss decreased from 1.369978 to 1.369649. Model was saved
Epoch: 829 	Training Loss: 1.173452 	Validation Loss: 1.369719 	 time: 0.3
Epoch: 830 	Training Loss: 1.173419 	Validation Loss: 1.369623 	 time: 0.3
Validation loss decreased from 1.369649 to 1.369623. Model was saved
Epoch: 831 	Training Loss: 1.173389 	Validation Loss: 1.369948 	 time: 0.3
Epoch: 832 	Training Loss: 1.173360 	Validation Loss: 1.370065 	 time: 0.3
Epoch: 833 	Training Loss: 1.173331 	Validation Loss: 1.370339 	 time: 0.3
Epoch: 834 	Training Loss: 1.173302 	Validation Loss: 1.370282 	 time: 0.3
Epoch: 835 	Training Loss: 1.173272 	Validation Loss: 1.370311 	 time: 0.3
Epoch: 836 	Training Loss: 1.173237 	Validation Loss: 1.369919 	 time: 0.3
Epoch: 837 	Training Loss: 1.173197 	Validation Loss: 1.369725 	 time: 0.3
Epoch: 838 	Training Loss: 1.173147 	Validation Loss: 1.369272 	 time: 0.3
Validation loss decreased from 1.369623 to 1.369272. Model was saved
Epoch: 839 	Training Loss: 1.173086 	Validation Loss: 1.368981 	 time: 0.3
Validation loss decreased from 1.369272 to 1.368981. Model was saved
Epoch: 840 	Training Loss: 1.173019 	Validation Loss: 1.368631 	 time: 0.3
Validation loss decreased from 1.368981 to 1.368631. Model was saved
Epoch: 841 	Training Loss: 1.172955 	Validation Loss: 1.368400 	 time: 0.3
Validation loss decreased from 1.368631 to 1.368400. Model was saved
Epoch: 842 	Training Loss: 1.172894 	Validation Loss: 1.368174 	 time: 0.3
Validation loss decreased from 1.368400 to 1.368174. Model was saved
Epoch: 843 	Training Loss: 1.172841 	Validation Loss: 1.368007 	 time: 0.3
Validation loss decreased from 1.368174 to 1.368007. Model was saved
Epoch: 844 	Training Loss: 1.172797 	Validation Loss: 1.367875 	 time: 0.3
Validation loss decreased from 1.368007 to 1.367875. Model was saved
Epoch: 845 	Training Loss: 1.172759 	Validation Loss: 1.367728 	 time: 0.3
Validation loss decreased from 1.367875 to 1.367728. Model was saved
Epoch: 846 	Training Loss: 1.172723 	Validation Loss: 1.367686 	 time: 0.3
Validation loss decreased from 1.367728 to 1.367686. Model was saved
Epoch: 847 	Training Loss: 1.172685 	Validation Loss: 1.367725 	 time: 0.3
Epoch: 848 	Training Loss: 1.172641 	Validation Loss: 1.367845 	 time: 0.3
Epoch: 849 	Training Loss: 1.172591 	Validation Loss: 1.368193 	 time: 0.3
Epoch: 850 	Training Loss: 1.172539 	Validation Loss: 1.368414 	 time: 0.3
Epoch: 851 	Training Loss: 1.172481 	Validation Loss: 1.368643 	 time: 0.3
Epoch: 852 	Training Loss: 1.172419 	Validation Loss: 1.368443 	 time: 0.3
Epoch: 853 	Training Loss: 1.172369 	Validation Loss: 1.368492 	 time: 0.3
Epoch: 854 	Training Loss: 1.172328 	Validation Loss: 1.367929 	 time: 0.3
Epoch: 855 	Training Loss: 1.172291 	Validation Loss: 1.368050 	 time: 0.3
Epoch: 856 	Training Loss: 1.172257 	Validation Loss: 1.367443 	 time: 0.3
Validation loss decreased from 1.367686 to 1.367443. Model was saved
Epoch: 857 	Training Loss: 1.172224 	Validation Loss: 1.367859 	 time: 0.3
Epoch: 858 	Training Loss: 1.172190 	Validation Loss: 1.367239 	 time: 0.3
Validation loss decreased from 1.367443 to 1.367239. Model was saved
Epoch: 859 	Training Loss: 1.172157 	Validation Loss: 1.367828 	 time: 0.3
Epoch: 860 	Training Loss: 1.172123 	Validation Loss: 1.366954 	 time: 0.3
Validation loss decreased from 1.367239 to 1.366954. Model was saved
Epoch: 861 	Training Loss: 1.172086 	Validation Loss: 1.367644 	 time: 0.3
Epoch: 862 	Training Loss: 1.172044 	Validation Loss: 1.366740 	 time: 0.3
Validation loss decreased from 1.366954 to 1.366740. Model was saved
Epoch: 863 	Training Loss: 1.171990 	Validation Loss: 1.367740 	 time: 0.3
Epoch: 864 	Training Loss: 1.171928 	Validation Loss: 1.367149 	 time: 0.3
Epoch: 865 	Training Loss: 1.171889 	Validation Loss: 1.368257 	 time: 0.3
Epoch: 866 	Training Loss: 1.171860 	Validation Loss: 1.367402 	 time: 0.3
Epoch: 867 	Training Loss: 1.171834 	Validation Loss: 1.368500 	 time: 0.3
Epoch: 868 	Training Loss: 1.171809 	Validation Loss: 1.367193 	 time: 0.3
Epoch: 869 	Training Loss: 1.171785 	Validation Loss: 1.368340 	 time: 0.3
Epoch: 870 	Training Loss: 1.171759 	Validation Loss: 1.366760 	 time: 0.3
Epoch: 871 	Training Loss: 1.171733 	Validation Loss: 1.368074 	 time: 0.3
Epoch: 872 	Training Loss: 1.171706 	Validation Loss: 1.366251 	 time: 0.3
Validation loss decreased from 1.366740 to 1.366251. Model was saved
Epoch: 873 	Training Loss: 1.171677 	Validation Loss: 1.367921 	 time: 0.3
Epoch: 874 	Training Loss: 1.171649 	Validation Loss: 1.365911 	 time: 0.3
Validation loss decreased from 1.366251 to 1.365911. Model was saved
Epoch: 875 	Training Loss: 1.171622 	Validation Loss: 1.367713 	 time: 0.3
Epoch: 876 	Training Loss: 1.171592 	Validation Loss: 1.365834 	 time: 0.3
Validation loss decreased from 1.365911 to 1.365834. Model was saved
Epoch: 877 	Training Loss: 1.171554 	Validation Loss: 1.367463 	 time: 0.3
Epoch: 878 	Training Loss: 1.171503 	Validation Loss: 1.365977 	 time: 0.3
Epoch: 879 	Training Loss: 1.171423 	Validation Loss: 1.367469 	 time: 0.3
Epoch: 880 	Training Loss: 1.171345 	Validation Loss: 1.366356 	 time: 0.3
Epoch: 881 	Training Loss: 1.171301 	Validation Loss: 1.367255 	 time: 0.3
Epoch: 882 	Training Loss: 1.171265 	Validation Loss: 1.366438 	 time: 0.3
Epoch: 883 	Training Loss: 1.171228 	Validation Loss: 1.366776 	 time: 0.3
Epoch: 884 	Training Loss: 1.171190 	Validation Loss: 1.365916 	 time: 0.3
Epoch: 885 	Training Loss: 1.171151 	Validation Loss: 1.365858 	 time: 0.3
Epoch: 886 	Training Loss: 1.171110 	Validation Loss: 1.365185 	 time: 0.3
Validation loss decreased from 1.365834 to 1.365185. Model was saved
Epoch: 887 	Training Loss: 1.171059 	Validation Loss: 1.364944 	 time: 0.3
Validation loss decreased from 1.365185 to 1.364944. Model was saved
Epoch: 888 	Training Loss: 1.171003 	Validation Loss: 1.364831 	 time: 0.3
Validation loss decreased from 1.364944 to 1.364831. Model was saved
Epoch: 889 	Training Loss: 1.170963 	Validation Loss: 1.364665 	 time: 0.3
Validation loss decreased from 1.364831 to 1.364665. Model was saved
Epoch: 890 	Training Loss: 1.170931 	Validation Loss: 1.364923 	 time: 0.3
Epoch: 891 	Training Loss: 1.170897 	Validation Loss: 1.364867 	 time: 0.3
Epoch: 892 	Training Loss: 1.170865 	Validation Loss: 1.365404 	 time: 0.3
Epoch: 893 	Training Loss: 1.170835 	Validation Loss: 1.365363 	 time: 0.3
Epoch: 894 	Training Loss: 1.170808 	Validation Loss: 1.366152 	 time: 0.3
Epoch: 895 	Training Loss: 1.170784 	Validation Loss: 1.365845 	 time: 0.3
Epoch: 896 	Training Loss: 1.170760 	Validation Loss: 1.366659 	 time: 0.3
Epoch: 897 	Training Loss: 1.170735 	Validation Loss: 1.366076 	 time: 0.3
Epoch: 898 	Training Loss: 1.170707 	Validation Loss: 1.366886 	 time: 0.3
Epoch: 899 	Training Loss: 1.170673 	Validation Loss: 1.366028 	 time: 0.3
Epoch: 900 	Training Loss: 1.170629 	Validation Loss: 1.367031 	 time: 0.3
Epoch: 901 	Training Loss: 1.170590 	Validation Loss: 1.365746 	 time: 0.3
Epoch: 902 	Training Loss: 1.170573 	Validation Loss: 1.367065 	 time: 0.3
Epoch: 903 	Training Loss: 1.170564 	Validation Loss: 1.365213 	 time: 0.3
Epoch: 904 	Training Loss: 1.170560 	Validation Loss: 1.367034 	 time: 0.3
Epoch: 905 	Training Loss: 1.170560 	Validation Loss: 1.364697 	 time: 0.3
Epoch: 906 	Training Loss: 1.170561 	Validation Loss: 1.367221 	 time: 0.3
Epoch: 907 	Training Loss: 1.170562 	Validation Loss: 1.364531 	 time: 0.3
Validation loss decreased from 1.364665 to 1.364531. Model was saved
Epoch: 908 	Training Loss: 1.170558 	Validation Loss: 1.367360 	 time: 0.3
Epoch: 909 	Training Loss: 1.170542 	Validation Loss: 1.364765 	 time: 0.3
Epoch: 910 	Training Loss: 1.170517 	Validation Loss: 1.367372 	 time: 0.3
Epoch: 911 	Training Loss: 1.170462 	Validation Loss: 1.365112 	 time: 0.3
Epoch: 912 	Training Loss: 1.170404 	Validation Loss: 1.367096 	 time: 0.3
Epoch: 913 	Training Loss: 1.170321 	Validation Loss: 1.365628 	 time: 0.3
Epoch: 914 	Training Loss: 1.170244 	Validation Loss: 1.366431 	 time: 0.3
Epoch: 915 	Training Loss: 1.170184 	Validation Loss: 1.366321 	 time: 0.3
Epoch: 916 	Training Loss: 1.170151 	Validation Loss: 1.365906 	 time: 0.3
Epoch: 917 	Training Loss: 1.170134 	Validation Loss: 1.367031 	 time: 0.3
Epoch: 918 	Training Loss: 1.170121 	Validation Loss: 1.365657 	 time: 0.3
Epoch: 919 	Training Loss: 1.170106 	Validation Loss: 1.367616 	 time: 0.3
Epoch: 920 	Training Loss: 1.170084 	Validation Loss: 1.365476 	 time: 0.3
Epoch: 921 	Training Loss: 1.170073 	Validation Loss: 1.368441 	 time: 0.3
Epoch: 922 	Training Loss: 1.170102 	Validation Loss: 1.365182 	 time: 0.3
Epoch: 923 	Training Loss: 1.170178 	Validation Loss: 1.369949 	 time: 0.3
Epoch: 924 	Training Loss: 1.170406 	Validation Loss: 1.364401 	 time: 0.3
Validation loss decreased from 1.364531 to 1.364401. Model was saved
Epoch: 925 	Training Loss: 1.170651 	Validation Loss: 1.371081 	 time: 0.3
Epoch: 926 	Training Loss: 1.170561 	Validation Loss: 1.365690 	 time: 0.3
Epoch: 927 	Training Loss: 1.169924 	Validation Loss: 1.364009 	 time: 0.3
Validation loss decreased from 1.364401 to 1.364009. Model was saved
Epoch: 928 	Training Loss: 1.170187 	Validation Loss: 1.369982 	 time: 0.3
Epoch: 929 	Training Loss: 1.170449 	Validation Loss: 1.365106 	 time: 0.3
Epoch: 930 	Training Loss: 1.169875 	Validation Loss: 1.364592 	 time: 0.3
Epoch: 931 	Training Loss: 1.170028 	Validation Loss: 1.369357 	 time: 0.3
Epoch: 932 	Training Loss: 1.170281 	Validation Loss: 1.365327 	 time: 0.3
Epoch: 933 	Training Loss: 1.169694 	Validation Loss: 1.364315 	 time: 0.3
Epoch: 934 	Training Loss: 1.169972 	Validation Loss: 1.369102 	 time: 0.3
Epoch: 935 	Training Loss: 1.170077 	Validation Loss: 1.365483 	 time: 0.3
Epoch: 936 	Training Loss: 1.169489 	Validation Loss: 1.363153 	 time: 0.3
Validation loss decreased from 1.364009 to 1.363153. Model was saved
Epoch: 937 	Training Loss: 1.169825 	Validation Loss: 1.368253 	 time: 0.3
Epoch: 938 	Training Loss: 1.169886 	Validation Loss: 1.364847 	 time: 0.3
Epoch: 939 	Training Loss: 1.169413 	Validation Loss: 1.362464 	 time: 0.3
Validation loss decreased from 1.363153 to 1.362464. Model was saved
Epoch: 940 	Training Loss: 1.169717 	Validation Loss: 1.368179 	 time: 0.3
Epoch: 941 	Training Loss: 1.169767 	Validation Loss: 1.364581 	 time: 0.3
Epoch: 942 	Training Loss: 1.169297 	Validation Loss: 1.363157 	 time: 0.3
Epoch: 943 	Training Loss: 1.169586 	Validation Loss: 1.368685 	 time: 0.3
Epoch: 944 	Training Loss: 1.169540 	Validation Loss: 1.365195 	 time: 0.3
Epoch: 945 	Training Loss: 1.169226 	Validation Loss: 1.363040 	 time: 0.3
Epoch: 946 	Training Loss: 1.169487 	Validation Loss: 1.368497 	 time: 0.3
Epoch: 947 	Training Loss: 1.169364 	Validation Loss: 1.365979 	 time: 0.3
Epoch: 948 	Training Loss: 1.169124 	Validation Loss: 1.362468 	 time: 0.3
Epoch: 949 	Training Loss: 1.169350 	Validation Loss: 1.367090 	 time: 0.3
Epoch: 950 	Training Loss: 1.169180 	Validation Loss: 1.366129 	 time: 0.3
Epoch: 951 	Training Loss: 1.169032 	Validation Loss: 1.362530 	 time: 0.3
Epoch: 952 	Training Loss: 1.169144 	Validation Loss: 1.365797 	 time: 0.3
Epoch: 953 	Training Loss: 1.168995 	Validation Loss: 1.365312 	 time: 0.3
Epoch: 954 	Training Loss: 1.168962 	Validation Loss: 1.361592 	 time: 0.3
Validation loss decreased from 1.362464 to 1.361592. Model was saved
Epoch: 955 	Training Loss: 1.169020 	Validation Loss: 1.364205 	 time: 0.3
Epoch: 956 	Training Loss: 1.168874 	Validation Loss: 1.364119 	 time: 0.3
Epoch: 957 	Training Loss: 1.168893 	Validation Loss: 1.360650 	 time: 0.3
Validation loss decreased from 1.361592 to 1.360650. Model was saved
Epoch: 958 	Training Loss: 1.168885 	Validation Loss: 1.362780 	 time: 0.3
Epoch: 959 	Training Loss: 1.168751 	Validation Loss: 1.363940 	 time: 0.3
Epoch: 960 	Training Loss: 1.168776 	Validation Loss: 1.361210 	 time: 0.3
Epoch: 961 	Training Loss: 1.168733 	Validation Loss: 1.362283 	 time: 0.3
Epoch: 962 	Training Loss: 1.168663 	Validation Loss: 1.363801 	 time: 0.3
Epoch: 963 	Training Loss: 1.168678 	Validation Loss: 1.361954 	 time: 0.3
Epoch: 964 	Training Loss: 1.168640 	Validation Loss: 1.362351 	 time: 0.3
Epoch: 965 	Training Loss: 1.168582 	Validation Loss: 1.363627 	 time: 0.3
Epoch: 966 	Training Loss: 1.168592 	Validation Loss: 1.361746 	 time: 0.3
Epoch: 967 	Training Loss: 1.168533 	Validation Loss: 1.362105 	 time: 0.3
Epoch: 968 	Training Loss: 1.168490 	Validation Loss: 1.363300 	 time: 0.3
Epoch: 969 	Training Loss: 1.168485 	Validation Loss: 1.361398 	 time: 0.3
Epoch: 970 	Training Loss: 1.168441 	Validation Loss: 1.361419 	 time: 0.3
Epoch: 971 	Training Loss: 1.168407 	Validation Loss: 1.362672 	 time: 0.3
Epoch: 972 	Training Loss: 1.168400 	Validation Loss: 1.361446 	 time: 0.3
Epoch: 973 	Training Loss: 1.168365 	Validation Loss: 1.361286 	 time: 0.3
Epoch: 974 	Training Loss: 1.168337 	Validation Loss: 1.362241 	 time: 0.3
Epoch: 975 	Training Loss: 1.168328 	Validation Loss: 1.361269 	 time: 0.3
Epoch: 976 	Training Loss: 1.168291 	Validation Loss: 1.361277 	 time: 0.3
Epoch: 977 	Training Loss: 1.168256 	Validation Loss: 1.361914 	 time: 0.3
Epoch: 978 	Training Loss: 1.168227 	Validation Loss: 1.361170 	 time: 0.3
Epoch: 979 	Training Loss: 1.168166 	Validation Loss: 1.361456 	 time: 0.3
Epoch: 980 	Training Loss: 1.168097 	Validation Loss: 1.362080 	 time: 0.3
Epoch: 981 	Training Loss: 1.168067 	Validation Loss: 1.361382 	 time: 0.3
Epoch: 982 	Training Loss: 1.168040 	Validation Loss: 1.361545 	 time: 0.3
Epoch: 983 	Training Loss: 1.168016 	Validation Loss: 1.362121 	 time: 0.3
Epoch: 984 	Training Loss: 1.167987 	Validation Loss: 1.361557 	 time: 0.3
Epoch: 985 	Training Loss: 1.167951 	Validation Loss: 1.361695 	 time: 0.3
Epoch: 986 	Training Loss: 1.167924 	Validation Loss: 1.361887 	 time: 0.3
Epoch: 987 	Training Loss: 1.167907 	Validation Loss: 1.361182 	 time: 0.3
Epoch: 988 	Training Loss: 1.167885 	Validation Loss: 1.361305 	 time: 0.3
Epoch: 989 	Training Loss: 1.167867 	Validation Loss: 1.361447 	 time: 0.3
Epoch: 990 	Training Loss: 1.167852 	Validation Loss: 1.360683 	 time: 0.3
Epoch: 991 	Training Loss: 1.167833 	Validation Loss: 1.360701 	 time: 0.3
Epoch: 992 	Training Loss: 1.167813 	Validation Loss: 1.360922 	 time: 0.3
Epoch: 993 	Training Loss: 1.167797 	Validation Loss: 1.360363 	 time: 0.3
Validation loss decreased from 1.360650 to 1.360363. Model was saved
Epoch: 994 	Training Loss: 1.167778 	Validation Loss: 1.360373 	 time: 0.3
Epoch: 995 	Training Loss: 1.167761 	Validation Loss: 1.360646 	 time: 0.3
Epoch: 996 	Training Loss: 1.167746 	Validation Loss: 1.360391 	 time: 0.3
Epoch: 997 	Training Loss: 1.167730 	Validation Loss: 1.360583 	 time: 0.3
Epoch: 998 	Training Loss: 1.167714 	Validation Loss: 1.360909 	 time: 0.3
Epoch: 999 	Training Loss: 1.167700 	Validation Loss: 1.360792 	 time: 0.3
Epoch: 1000 	Training Loss: 1.167685 	Validation Loss: 1.361104 	 time: 0.3
Epoch: 1001 	Training Loss: 1.167669 	Validation Loss: 1.361493 	 time: 0.3
Epoch: 1002 	Training Loss: 1.167655 	Validation Loss: 1.361389 	 time: 0.3
Epoch: 1003 	Training Loss: 1.167642 	Validation Loss: 1.361547 	 time: 0.3
Epoch: 1004 	Training Loss: 1.167627 	Validation Loss: 1.361752 	 time: 0.3
Epoch: 1005 	Training Loss: 1.167614 	Validation Loss: 1.361596 	 time: 0.3
Epoch: 1006 	Training Loss: 1.167601 	Validation Loss: 1.361586 	 time: 0.3
Epoch: 1007 	Training Loss: 1.167587 	Validation Loss: 1.361590 	 time: 0.3
Epoch: 1008 	Training Loss: 1.167573 	Validation Loss: 1.361458 	 time: 0.3
Epoch: 1009 	Training Loss: 1.167560 	Validation Loss: 1.361469 	 time: 0.3
Epoch: 1010 	Training Loss: 1.167546 	Validation Loss: 1.361386 	 time: 0.3
Epoch: 1011 	Training Loss: 1.167532 	Validation Loss: 1.361215 	 time: 0.3
Epoch: 1012 	Training Loss: 1.167518 	Validation Loss: 1.361205 	 time: 0.3
Epoch: 1013 	Training Loss: 1.167503 	Validation Loss: 1.361100 	 time: 0.3
Epoch: 1014 	Training Loss: 1.167488 	Validation Loss: 1.360934 	 time: 0.3
Epoch: 1015 	Training Loss: 1.167473 	Validation Loss: 1.360902 	 time: 0.3
Epoch: 1016 	Training Loss: 1.167456 	Validation Loss: 1.360812 	 time: 0.3
Epoch: 1017 	Training Loss: 1.167438 	Validation Loss: 1.360729 	 time: 0.3
Epoch: 1018 	Training Loss: 1.167418 	Validation Loss: 1.360734 	 time: 0.3
Epoch: 1019 	Training Loss: 1.167397 	Validation Loss: 1.360648 	 time: 0.3
Epoch: 1020 	Training Loss: 1.167375 	Validation Loss: 1.360634 	 time: 0.3
Epoch: 1021 	Training Loss: 1.167356 	Validation Loss: 1.360666 	 time: 0.3
Epoch: 1022 	Training Loss: 1.167336 	Validation Loss: 1.360524 	 time: 0.3
Epoch: 1023 	Training Loss: 1.167315 	Validation Loss: 1.360460 	 time: 0.3
Epoch: 1024 	Training Loss: 1.167295 	Validation Loss: 1.360506 	 time: 0.3
Epoch: 1025 	Training Loss: 1.167274 	Validation Loss: 1.360422 	 time: 0.3
Epoch: 1026 	Training Loss: 1.167255 	Validation Loss: 1.360436 	 time: 0.3
Epoch: 1027 	Training Loss: 1.167235 	Validation Loss: 1.360581 	 time: 0.3
Epoch: 1028 	Training Loss: 1.167215 	Validation Loss: 1.360675 	 time: 0.3
Epoch: 1029 	Training Loss: 1.167194 	Validation Loss: 1.360908 	 time: 0.3
Epoch: 1030 	Training Loss: 1.167173 	Validation Loss: 1.361212 	 time: 0.3
Epoch: 1031 	Training Loss: 1.167152 	Validation Loss: 1.361426 	 time: 0.3
Epoch: 1032 	Training Loss: 1.167132 	Validation Loss: 1.361676 	 time: 0.3
Epoch: 1033 	Training Loss: 1.167111 	Validation Loss: 1.361833 	 time: 0.3
Epoch: 1034 	Training Loss: 1.167089 	Validation Loss: 1.361836 	 time: 0.3
Epoch: 1035 	Training Loss: 1.167066 	Validation Loss: 1.361832 	 time: 0.3
Epoch: 1036 	Training Loss: 1.167046 	Validation Loss: 1.361748 	 time: 0.3
Epoch: 1037 	Training Loss: 1.167024 	Validation Loss: 1.361590 	 time: 0.3
Epoch: 1038 	Training Loss: 1.166997 	Validation Loss: 1.361434 	 time: 0.3
Epoch: 1039 	Training Loss: 1.166961 	Validation Loss: 1.361239 	 time: 0.3
Epoch: 1040 	Training Loss: 1.166924 	Validation Loss: 1.361072 	 time: 0.3
Epoch: 1041 	Training Loss: 1.166897 	Validation Loss: 1.361031 	 time: 0.3
Epoch: 1042 	Training Loss: 1.166875 	Validation Loss: 1.361056 	 time: 0.3
Epoch: 1043 	Training Loss: 1.166856 	Validation Loss: 1.361132 	 time: 0.3
Epoch: 1044 	Training Loss: 1.166837 	Validation Loss: 1.361193 	 time: 0.3
Epoch: 1045 	Training Loss: 1.166820 	Validation Loss: 1.361155 	 time: 0.3
Epoch: 1046 	Training Loss: 1.166800 	Validation Loss: 1.361029 	 time: 0.3
Epoch: 1047 	Training Loss: 1.166776 	Validation Loss: 1.360811 	 time: 0.3
Epoch: 1048 	Training Loss: 1.166742 	Validation Loss: 1.360568 	 time: 0.3
Epoch: 1049 	Training Loss: 1.166695 	Validation Loss: 1.360461 	 time: 0.3
Epoch: 1050 	Training Loss: 1.166653 	Validation Loss: 1.360470 	 time: 0.3
Epoch: 1051 	Training Loss: 1.166629 	Validation Loss: 1.360488 	 time: 0.3
Epoch: 1052 	Training Loss: 1.166605 	Validation Loss: 1.360795 	 time: 0.3
Epoch: 1053 	Training Loss: 1.166575 	Validation Loss: 1.361107 	 time: 0.3
Epoch: 1054 	Training Loss: 1.166541 	Validation Loss: 1.361288 	 time: 0.3
Epoch: 1055 	Training Loss: 1.166505 	Validation Loss: 1.361522 	 time: 0.3
Epoch: 1056 	Training Loss: 1.166476 	Validation Loss: 1.361516 	 time: 0.3
Epoch: 1057 	Training Loss: 1.166451 	Validation Loss: 1.361506 	 time: 0.3
Epoch: 1058 	Training Loss: 1.166426 	Validation Loss: 1.361593 	 time: 0.3
Epoch: 1059 	Training Loss: 1.166403 	Validation Loss: 1.361524 	 time: 0.3
Epoch: 1060 	Training Loss: 1.166380 	Validation Loss: 1.361584 	 time: 0.3
Epoch: 1061 	Training Loss: 1.166358 	Validation Loss: 1.361683 	 time: 0.3
Epoch: 1062 	Training Loss: 1.166335 	Validation Loss: 1.361621 	 time: 0.3
Epoch: 1063 	Training Loss: 1.166314 	Validation Loss: 1.361576 	 time: 0.3
Epoch: 1064 	Training Loss: 1.166296 	Validation Loss: 1.361485 	 time: 0.3
Epoch: 1065 	Training Loss: 1.166280 	Validation Loss: 1.361418 	 time: 0.3
Epoch: 1066 	Training Loss: 1.166263 	Validation Loss: 1.361366 	 time: 0.3
Epoch: 1067 	Training Loss: 1.166246 	Validation Loss: 1.361265 	 time: 0.3
Epoch: 1068 	Training Loss: 1.166230 	Validation Loss: 1.361209 	 time: 0.3
Epoch: 1069 	Training Loss: 1.166214 	Validation Loss: 1.361079 	 time: 0.3
Epoch: 1070 	Training Loss: 1.166198 	Validation Loss: 1.360892 	 time: 0.3
Epoch: 1071 	Training Loss: 1.166182 	Validation Loss: 1.360786 	 time: 0.3
Epoch: 1072 	Training Loss: 1.166165 	Validation Loss: 1.360692 	 time: 0.3
Epoch: 1073 	Training Loss: 1.166148 	Validation Loss: 1.360650 	 time: 0.3
Epoch: 1074 	Training Loss: 1.166129 	Validation Loss: 1.360698 	 time: 0.3
Epoch: 1075 	Training Loss: 1.166110 	Validation Loss: 1.360782 	 time: 0.3
Epoch: 1076 	Training Loss: 1.166093 	Validation Loss: 1.360970 	 time: 0.3
Epoch: 1077 	Training Loss: 1.166076 	Validation Loss: 1.361201 	 time: 0.3
Epoch: 1078 	Training Loss: 1.166061 	Validation Loss: 1.361384 	 time: 0.3
Epoch: 1079 	Training Loss: 1.166045 	Validation Loss: 1.361563 	 time: 0.3
Epoch: 1080 	Training Loss: 1.166031 	Validation Loss: 1.361670 	 time: 0.3
Epoch: 1081 	Training Loss: 1.166018 	Validation Loss: 1.361680 	 time: 0.3
Epoch: 1082 	Training Loss: 1.166005 	Validation Loss: 1.361683 	 time: 0.3
Epoch: 1083 	Training Loss: 1.165991 	Validation Loss: 1.361668 	 time: 0.3
Epoch: 1084 	Training Loss: 1.165978 	Validation Loss: 1.361639 	 time: 0.3
Epoch: 1085 	Training Loss: 1.165965 	Validation Loss: 1.361607 	 time: 0.3
Epoch: 1086 	Training Loss: 1.165952 	Validation Loss: 1.361549 	 time: 0.3
Epoch: 1087 	Training Loss: 1.165939 	Validation Loss: 1.361496 	 time: 0.3
Epoch: 1088 	Training Loss: 1.165926 	Validation Loss: 1.361433 	 time: 0.3
Epoch: 1089 	Training Loss: 1.165913 	Validation Loss: 1.361327 	 time: 0.3
Epoch: 1090 	Training Loss: 1.165899 	Validation Loss: 1.361211 	 time: 0.3
Epoch: 1091 	Training Loss: 1.165884 	Validation Loss: 1.361091 	 time: 0.3
Epoch: 1092 	Training Loss: 1.165866 	Validation Loss: 1.360970 	 time: 0.3
Epoch: 1093 	Training Loss: 1.165848 	Validation Loss: 1.360886 	 time: 0.3
Epoch: 1094 	Training Loss: 1.165829 	Validation Loss: 1.360820 	 time: 0.3
Epoch: 1095 	Training Loss: 1.165810 	Validation Loss: 1.360740 	 time: 0.3
Epoch: 1096 	Training Loss: 1.165791 	Validation Loss: 1.360634 	 time: 0.3
Epoch: 1097 	Training Loss: 1.165770 	Validation Loss: 1.360507 	 time: 0.3
Epoch: 1098 	Training Loss: 1.165746 	Validation Loss: 1.360416 	 time: 0.3
Epoch: 1099 	Training Loss: 1.165724 	Validation Loss: 1.360352 	 time: 0.3
Validation loss decreased from 1.360363 to 1.360352. Model was saved
Epoch: 1100 	Training Loss: 1.165701 	Validation Loss: 1.360277 	 time: 0.3
Validation loss decreased from 1.360352 to 1.360277. Model was saved
Epoch: 1101 	Training Loss: 1.165677 	Validation Loss: 1.360276 	 time: 0.3
Validation loss decreased from 1.360277 to 1.360276. Model was saved
Epoch: 1102 	Training Loss: 1.165655 	Validation Loss: 1.360300 	 time: 0.3
Epoch: 1103 	Training Loss: 1.165638 	Validation Loss: 1.360252 	 time: 0.3
Validation loss decreased from 1.360276 to 1.360252. Model was saved
Epoch: 1104 	Training Loss: 1.165623 	Validation Loss: 1.360212 	 time: 0.3
Validation loss decreased from 1.360252 to 1.360212. Model was saved
Epoch: 1105 	Training Loss: 1.165608 	Validation Loss: 1.360176 	 time: 0.3
Validation loss decreased from 1.360212 to 1.360176. Model was saved
Epoch: 1106 	Training Loss: 1.165593 	Validation Loss: 1.360168 	 time: 0.3
Validation loss decreased from 1.360176 to 1.360168. Model was saved
Epoch: 1107 	Training Loss: 1.165576 	Validation Loss: 1.360185 	 time: 0.3
Epoch: 1108 	Training Loss: 1.165557 	Validation Loss: 1.360172 	 time: 0.3
Epoch: 1109 	Training Loss: 1.165534 	Validation Loss: 1.360198 	 time: 0.3
Epoch: 1110 	Training Loss: 1.165507 	Validation Loss: 1.360248 	 time: 0.3
Epoch: 1111 	Training Loss: 1.165482 	Validation Loss: 1.360299 	 time: 0.3
Epoch: 1112 	Training Loss: 1.165460 	Validation Loss: 1.360347 	 time: 0.3
Epoch: 1113 	Training Loss: 1.165442 	Validation Loss: 1.360390 	 time: 0.3
Epoch: 1114 	Training Loss: 1.165427 	Validation Loss: 1.360459 	 time: 0.3
Epoch: 1115 	Training Loss: 1.165411 	Validation Loss: 1.360502 	 time: 0.3
Epoch: 1116 	Training Loss: 1.165394 	Validation Loss: 1.360525 	 time: 0.3
Epoch: 1117 	Training Loss: 1.165376 	Validation Loss: 1.360560 	 time: 0.3
Epoch: 1118 	Training Loss: 1.165355 	Validation Loss: 1.360590 	 time: 0.3
Epoch: 1119 	Training Loss: 1.165332 	Validation Loss: 1.360656 	 time: 0.3
Epoch: 1120 	Training Loss: 1.165307 	Validation Loss: 1.360757 	 time: 0.3
Epoch: 1121 	Training Loss: 1.165286 	Validation Loss: 1.360826 	 time: 0.3
Epoch: 1122 	Training Loss: 1.165268 	Validation Loss: 1.360815 	 time: 0.3
Epoch: 1123 	Training Loss: 1.165251 	Validation Loss: 1.360801 	 time: 0.3
Epoch: 1124 	Training Loss: 1.165234 	Validation Loss: 1.360819 	 time: 0.3
Epoch: 1125 	Training Loss: 1.165220 	Validation Loss: 1.360872 	 time: 0.3
Epoch: 1126 	Training Loss: 1.165208 	Validation Loss: 1.360890 	 time: 0.3
Epoch: 1127 	Training Loss: 1.165195 	Validation Loss: 1.360850 	 time: 0.3
Epoch: 1128 	Training Loss: 1.165182 	Validation Loss: 1.360804 	 time: 0.3
Epoch: 1129 	Training Loss: 1.165168 	Validation Loss: 1.360728 	 time: 0.3
Epoch: 1130 	Training Loss: 1.165155 	Validation Loss: 1.360651 	 time: 0.3
Epoch: 1131 	Training Loss: 1.165142 	Validation Loss: 1.360599 	 time: 0.3
Epoch: 1132 	Training Loss: 1.165128 	Validation Loss: 1.360565 	 time: 0.3
Epoch: 1133 	Training Loss: 1.165115 	Validation Loss: 1.360520 	 time: 0.3
Epoch: 1134 	Training Loss: 1.165100 	Validation Loss: 1.360459 	 time: 0.3
Epoch: 1135 	Training Loss: 1.165087 	Validation Loss: 1.360418 	 time: 0.3
Epoch: 1136 	Training Loss: 1.165073 	Validation Loss: 1.360391 	 time: 0.3
Epoch: 1137 	Training Loss: 1.165060 	Validation Loss: 1.360354 	 time: 0.3
Epoch: 1138 	Training Loss: 1.165046 	Validation Loss: 1.360316 	 time: 0.3
Epoch: 1139 	Training Loss: 1.165031 	Validation Loss: 1.360283 	 time: 0.3
Epoch: 1140 	Training Loss: 1.165015 	Validation Loss: 1.360223 	 time: 0.3
Epoch: 1141 	Training Loss: 1.164994 	Validation Loss: 1.360152 	 time: 0.3
Validation loss decreased from 1.360168 to 1.360152. Model was saved
Epoch: 1142 	Training Loss: 1.164968 	Validation Loss: 1.360240 	 time: 0.3
Epoch: 1143 	Training Loss: 1.164949 	Validation Loss: 1.360322 	 time: 0.3
Epoch: 1144 	Training Loss: 1.164935 	Validation Loss: 1.360417 	 time: 0.3
Epoch: 1145 	Training Loss: 1.164917 	Validation Loss: 1.360633 	 time: 0.3
Epoch: 1146 	Training Loss: 1.164896 	Validation Loss: 1.360751 	 time: 0.3
Epoch: 1147 	Training Loss: 1.164877 	Validation Loss: 1.360849 	 time: 0.3
Epoch: 1148 	Training Loss: 1.164863 	Validation Loss: 1.360839 	 time: 0.3
Epoch: 1149 	Training Loss: 1.164849 	Validation Loss: 1.360736 	 time: 0.3
Epoch: 1150 	Training Loss: 1.164835 	Validation Loss: 1.360642 	 time: 0.3
Epoch: 1151 	Training Loss: 1.164820 	Validation Loss: 1.360562 	 time: 0.3
Epoch: 1152 	Training Loss: 1.164805 	Validation Loss: 1.360485 	 time: 0.3
Epoch: 1153 	Training Loss: 1.164789 	Validation Loss: 1.360480 	 time: 0.3
Epoch: 1154 	Training Loss: 1.164768 	Validation Loss: 1.360521 	 time: 0.3
Epoch: 1155 	Training Loss: 1.164746 	Validation Loss: 1.360617 	 time: 0.3
Epoch: 1156 	Training Loss: 1.164728 	Validation Loss: 1.360646 	 time: 0.3
Epoch: 1157 	Training Loss: 1.164711 	Validation Loss: 1.360709 	 time: 0.3
Epoch: 1158 	Training Loss: 1.164695 	Validation Loss: 1.360672 	 time: 0.3
Epoch: 1159 	Training Loss: 1.164681 	Validation Loss: 1.360695 	 time: 0.3
Epoch: 1160 	Training Loss: 1.164668 	Validation Loss: 1.360523 	 time: 0.3
Epoch: 1161 	Training Loss: 1.164652 	Validation Loss: 1.360457 	 time: 0.3
Epoch: 1162 	Training Loss: 1.164634 	Validation Loss: 1.360170 	 time: 0.3
Epoch: 1163 	Training Loss: 1.164614 	Validation Loss: 1.360192 	 time: 0.3
Epoch: 1164 	Training Loss: 1.164591 	Validation Loss: 1.359975 	 time: 0.3
Validation loss decreased from 1.360152 to 1.359975. Model was saved
Epoch: 1165 	Training Loss: 1.164568 	Validation Loss: 1.360193 	 time: 0.3
Epoch: 1166 	Training Loss: 1.164548 	Validation Loss: 1.359877 	 time: 0.3
Validation loss decreased from 1.359975 to 1.359877. Model was saved
Epoch: 1167 	Training Loss: 1.164527 	Validation Loss: 1.360292 	 time: 0.3
Epoch: 1168 	Training Loss: 1.164509 	Validation Loss: 1.359757 	 time: 0.3
Validation loss decreased from 1.359877 to 1.359757. Model was saved
Epoch: 1169 	Training Loss: 1.164482 	Validation Loss: 1.360132 	 time: 0.3
Epoch: 1170 	Training Loss: 1.164447 	Validation Loss: 1.359555 	 time: 0.3
Validation loss decreased from 1.359757 to 1.359555. Model was saved
Epoch: 1171 	Training Loss: 1.164398 	Validation Loss: 1.359842 	 time: 0.3
Epoch: 1172 	Training Loss: 1.164371 	Validation Loss: 1.359961 	 time: 0.3
Epoch: 1173 	Training Loss: 1.164382 	Validation Loss: 1.359959 	 time: 0.3
Epoch: 1174 	Training Loss: 1.164487 	Validation Loss: 1.360162 	 time: 0.3
Epoch: 1175 	Training Loss: 1.164621 	Validation Loss: 1.361027 	 time: 0.3
Epoch: 1176 	Training Loss: 1.164728 	Validation Loss: 1.359761 	 time: 0.3
Epoch: 1177 	Training Loss: 1.164335 	Validation Loss: 1.360148 	 time: 0.3
Epoch: 1178 	Training Loss: 1.164369 	Validation Loss: 1.360968 	 time: 0.3
Epoch: 1179 	Training Loss: 1.164625 	Validation Loss: 1.359717 	 time: 0.3
Epoch: 1180 	Training Loss: 1.164340 	Validation Loss: 1.360489 	 time: 0.3
Epoch: 1181 	Training Loss: 1.164254 	Validation Loss: 1.359991 	 time: 0.3
Epoch: 1182 	Training Loss: 1.164395 	Validation Loss: 1.359073 	 time: 0.3
Validation loss decreased from 1.359555 to 1.359073. Model was saved
Epoch: 1183 	Training Loss: 1.164245 	Validation Loss: 1.360393 	 time: 0.3
Epoch: 1184 	Training Loss: 1.164221 	Validation Loss: 1.359921 	 time: 0.3
Epoch: 1185 	Training Loss: 1.164321 	Validation Loss: 1.359056 	 time: 0.3
Validation loss decreased from 1.359073 to 1.359056. Model was saved
Epoch: 1186 	Training Loss: 1.164219 	Validation Loss: 1.361151 	 time: 0.3
Epoch: 1187 	Training Loss: 1.164159 	Validation Loss: 1.360262 	 time: 0.3
Epoch: 1188 	Training Loss: 1.164198 	Validation Loss: 1.359395 	 time: 0.3
Epoch: 1189 	Training Loss: 1.164158 	Validation Loss: 1.360749 	 time: 0.3
Epoch: 1190 	Training Loss: 1.164104 	Validation Loss: 1.360260 	 time: 0.3
Epoch: 1191 	Training Loss: 1.164130 	Validation Loss: 1.359522 	 time: 0.3
Epoch: 1192 	Training Loss: 1.164108 	Validation Loss: 1.359907 	 time: 0.3
Epoch: 1193 	Training Loss: 1.164062 	Validation Loss: 1.359800 	 time: 0.3
Epoch: 1194 	Training Loss: 1.164064 	Validation Loss: 1.359717 	 time: 0.3
Epoch: 1195 	Training Loss: 1.164066 	Validation Loss: 1.359762 	 time: 0.3
Epoch: 1196 	Training Loss: 1.164023 	Validation Loss: 1.360053 	 time: 0.3
Epoch: 1197 	Training Loss: 1.164015 	Validation Loss: 1.360033 	 time: 0.3
Epoch: 1198 	Training Loss: 1.164010 	Validation Loss: 1.359958 	 time: 0.3
Epoch: 1199 	Training Loss: 1.163984 	Validation Loss: 1.360407 	 time: 0.3
Epoch: 1200 	Training Loss: 1.163965 	Validation Loss: 1.359820 	 time: 0.3
Epoch: 1201 	Training Loss: 1.163962 	Validation Loss: 1.359846 	 time: 0.3
Epoch: 1202 	Training Loss: 1.163945 	Validation Loss: 1.359852 	 time: 0.3
Epoch: 1203 	Training Loss: 1.163913 	Validation Loss: 1.359349 	 time: 0.3
Epoch: 1204 	Training Loss: 1.163897 	Validation Loss: 1.359227 	 time: 0.3
Epoch: 1205 	Training Loss: 1.163886 	Validation Loss: 1.359288 	 time: 0.3
Epoch: 1206 	Training Loss: 1.163859 	Validation Loss: 1.359425 	 time: 0.3
Epoch: 1207 	Training Loss: 1.163830 	Validation Loss: 1.358938 	 time: 0.3
Validation loss decreased from 1.359056 to 1.358938. Model was saved
Epoch: 1208 	Training Loss: 1.163813 	Validation Loss: 1.359685 	 time: 0.3
Epoch: 1209 	Training Loss: 1.163793 	Validation Loss: 1.359556 	 time: 0.3
Epoch: 1210 	Training Loss: 1.163776 	Validation Loss: 1.359316 	 time: 0.3
Epoch: 1211 	Training Loss: 1.163765 	Validation Loss: 1.359179 	 time: 0.3
Epoch: 1212 	Training Loss: 1.163752 	Validation Loss: 1.359231 	 time: 0.3
Epoch: 1213 	Training Loss: 1.163736 	Validation Loss: 1.358750 	 time: 0.3
Validation loss decreased from 1.358938 to 1.358750. Model was saved
Epoch: 1214 	Training Loss: 1.163721 	Validation Loss: 1.358704 	 time: 0.3
Validation loss decreased from 1.358750 to 1.358704. Model was saved
Epoch: 1215 	Training Loss: 1.163711 	Validation Loss: 1.359066 	 time: 0.3
Epoch: 1216 	Training Loss: 1.163701 	Validation Loss: 1.358898 	 time: 0.3
Epoch: 1217 	Training Loss: 1.163688 	Validation Loss: 1.359246 	 time: 0.3
Epoch: 1218 	Training Loss: 1.163675 	Validation Loss: 1.359359 	 time: 0.3
Epoch: 1219 	Training Loss: 1.163666 	Validation Loss: 1.359352 	 time: 0.3
Epoch: 1220 	Training Loss: 1.163653 	Validation Loss: 1.359446 	 time: 0.3
Epoch: 1221 	Training Loss: 1.163637 	Validation Loss: 1.359486 	 time: 0.3
Epoch: 1222 	Training Loss: 1.163623 	Validation Loss: 1.359204 	 time: 0.3
Epoch: 1223 	Training Loss: 1.163609 	Validation Loss: 1.359223 	 time: 0.3
Epoch: 1224 	Training Loss: 1.163594 	Validation Loss: 1.359163 	 time: 0.3
Epoch: 1225 	Training Loss: 1.163580 	Validation Loss: 1.358764 	 time: 0.3
Epoch: 1226 	Training Loss: 1.163568 	Validation Loss: 1.358699 	 time: 0.3
Validation loss decreased from 1.358704 to 1.358699. Model was saved
Epoch: 1227 	Training Loss: 1.163556 	Validation Loss: 1.358622 	 time: 0.3
Validation loss decreased from 1.358699 to 1.358622. Model was saved
Epoch: 1228 	Training Loss: 1.163542 	Validation Loss: 1.358408 	 time: 0.3
Validation loss decreased from 1.358622 to 1.358408. Model was saved
Epoch: 1229 	Training Loss: 1.163527 	Validation Loss: 1.358209 	 time: 0.3
Validation loss decreased from 1.358408 to 1.358209. Model was saved
Epoch: 1230 	Training Loss: 1.163512 	Validation Loss: 1.358253 	 time: 0.3
Epoch: 1231 	Training Loss: 1.163496 	Validation Loss: 1.358035 	 time: 0.3
Validation loss decreased from 1.358209 to 1.358035. Model was saved
Epoch: 1232 	Training Loss: 1.163479 	Validation Loss: 1.357854 	 time: 0.3
Validation loss decreased from 1.358035 to 1.357854. Model was saved
Epoch: 1233 	Training Loss: 1.163463 	Validation Loss: 1.357806 	 time: 0.3
Validation loss decreased from 1.357854 to 1.357806. Model was saved
Epoch: 1234 	Training Loss: 1.163447 	Validation Loss: 1.357769 	 time: 0.3
Validation loss decreased from 1.357806 to 1.357769. Model was saved
Epoch: 1235 	Training Loss: 1.163432 	Validation Loss: 1.357670 	 time: 0.3
Validation loss decreased from 1.357769 to 1.357670. Model was saved
Epoch: 1236 	Training Loss: 1.163416 	Validation Loss: 1.357709 	 time: 0.3
Epoch: 1237 	Training Loss: 1.163401 	Validation Loss: 1.357711 	 time: 0.3
Epoch: 1238 	Training Loss: 1.163383 	Validation Loss: 1.357659 	 time: 0.3
Validation loss decreased from 1.357670 to 1.357659. Model was saved
Epoch: 1239 	Training Loss: 1.163363 	Validation Loss: 1.357689 	 time: 0.3
Epoch: 1240 	Training Loss: 1.163339 	Validation Loss: 1.357656 	 time: 0.3
Validation loss decreased from 1.357659 to 1.357656. Model was saved
Epoch: 1241 	Training Loss: 1.163310 	Validation Loss: 1.357677 	 time: 0.3
Epoch: 1242 	Training Loss: 1.163278 	Validation Loss: 1.357765 	 time: 0.3
Epoch: 1243 	Training Loss: 1.163248 	Validation Loss: 1.357797 	 time: 0.3
Epoch: 1244 	Training Loss: 1.163216 	Validation Loss: 1.357777 	 time: 0.3
Epoch: 1245 	Training Loss: 1.163190 	Validation Loss: 1.357821 	 time: 0.3
Epoch: 1246 	Training Loss: 1.163175 	Validation Loss: 1.357722 	 time: 0.3
Epoch: 1247 	Training Loss: 1.163162 	Validation Loss: 1.357556 	 time: 0.3
Validation loss decreased from 1.357656 to 1.357556. Model was saved
Epoch: 1248 	Training Loss: 1.163151 	Validation Loss: 1.357438 	 time: 0.3
Validation loss decreased from 1.357556 to 1.357438. Model was saved
Epoch: 1249 	Training Loss: 1.163139 	Validation Loss: 1.357332 	 time: 0.3
Validation loss decreased from 1.357438 to 1.357332. Model was saved
Epoch: 1250 	Training Loss: 1.163129 	Validation Loss: 1.357125 	 time: 0.3
Validation loss decreased from 1.357332 to 1.357125. Model was saved
Epoch: 1251 	Training Loss: 1.163118 	Validation Loss: 1.357026 	 time: 0.3
Validation loss decreased from 1.357125 to 1.357026. Model was saved
Epoch: 1252 	Training Loss: 1.163108 	Validation Loss: 1.356954 	 time: 0.3
Validation loss decreased from 1.357026 to 1.356954. Model was saved
Epoch: 1253 	Training Loss: 1.163099 	Validation Loss: 1.356853 	 time: 0.3
Validation loss decreased from 1.356954 to 1.356853. Model was saved
Epoch: 1254 	Training Loss: 1.163089 	Validation Loss: 1.356790 	 time: 0.3
Validation loss decreased from 1.356853 to 1.356790. Model was saved
Epoch: 1255 	Training Loss: 1.163079 	Validation Loss: 1.356827 	 time: 0.3
Epoch: 1256 	Training Loss: 1.163068 	Validation Loss: 1.356801 	 time: 0.3
Epoch: 1257 	Training Loss: 1.163058 	Validation Loss: 1.356786 	 time: 0.3
Validation loss decreased from 1.356790 to 1.356786. Model was saved
Epoch: 1258 	Training Loss: 1.163046 	Validation Loss: 1.356753 	 time: 0.3
Validation loss decreased from 1.356786 to 1.356753. Model was saved
Epoch: 1259 	Training Loss: 1.163033 	Validation Loss: 1.356695 	 time: 0.3
Validation loss decreased from 1.356753 to 1.356695. Model was saved
Epoch: 1260 	Training Loss: 1.163019 	Validation Loss: 1.356617 	 time: 0.3
Validation loss decreased from 1.356695 to 1.356617. Model was saved
Epoch: 1261 	Training Loss: 1.163007 	Validation Loss: 1.356586 	 time: 0.3
Validation loss decreased from 1.356617 to 1.356586. Model was saved
Epoch: 1262 	Training Loss: 1.162996 	Validation Loss: 1.356555 	 time: 0.3
Validation loss decreased from 1.356586 to 1.356555. Model was saved
Epoch: 1263 	Training Loss: 1.162984 	Validation Loss: 1.356559 	 time: 0.3
Epoch: 1264 	Training Loss: 1.162972 	Validation Loss: 1.356550 	 time: 0.3
Validation loss decreased from 1.356555 to 1.356550. Model was saved
Epoch: 1265 	Training Loss: 1.162961 	Validation Loss: 1.356544 	 time: 0.3
Validation loss decreased from 1.356550 to 1.356544. Model was saved
Epoch: 1266 	Training Loss: 1.162952 	Validation Loss: 1.356590 	 time: 0.3
Epoch: 1267 	Training Loss: 1.162942 	Validation Loss: 1.356682 	 time: 0.3
Epoch: 1268 	Training Loss: 1.162933 	Validation Loss: 1.356753 	 time: 0.3
Epoch: 1269 	Training Loss: 1.162924 	Validation Loss: 1.356841 	 time: 0.3
Epoch: 1270 	Training Loss: 1.162914 	Validation Loss: 1.356893 	 time: 0.3
Epoch: 1271 	Training Loss: 1.162904 	Validation Loss: 1.356876 	 time: 0.3
Epoch: 1272 	Training Loss: 1.162894 	Validation Loss: 1.356854 	 time: 0.3
Epoch: 1273 	Training Loss: 1.162884 	Validation Loss: 1.356844 	 time: 0.3
Epoch: 1274 	Training Loss: 1.162873 	Validation Loss: 1.356792 	 time: 0.3
Epoch: 1275 	Training Loss: 1.162859 	Validation Loss: 1.356747 	 time: 0.3
Epoch: 1276 	Training Loss: 1.162841 	Validation Loss: 1.356685 	 time: 0.3
Epoch: 1277 	Training Loss: 1.162819 	Validation Loss: 1.356579 	 time: 0.3
Epoch: 1278 	Training Loss: 1.162798 	Validation Loss: 1.356462 	 time: 0.3
Validation loss decreased from 1.356544 to 1.356462. Model was saved
Epoch: 1279 	Training Loss: 1.162782 	Validation Loss: 1.356393 	 time: 0.3
Validation loss decreased from 1.356462 to 1.356393. Model was saved
Epoch: 1280 	Training Loss: 1.162771 	Validation Loss: 1.356313 	 time: 0.3
Validation loss decreased from 1.356393 to 1.356313. Model was saved
Epoch: 1281 	Training Loss: 1.162762 	Validation Loss: 1.356260 	 time: 0.3
Validation loss decreased from 1.356313 to 1.356260. Model was saved
Epoch: 1282 	Training Loss: 1.162753 	Validation Loss: 1.356225 	 time: 0.3
Validation loss decreased from 1.356260 to 1.356225. Model was saved
Epoch: 1283 	Training Loss: 1.162745 	Validation Loss: 1.356219 	 time: 0.3
Validation loss decreased from 1.356225 to 1.356219. Model was saved
Epoch: 1284 	Training Loss: 1.162736 	Validation Loss: 1.356174 	 time: 0.3
Validation loss decreased from 1.356219 to 1.356174. Model was saved
Epoch: 1285 	Training Loss: 1.162728 	Validation Loss: 1.356183 	 time: 0.3
Epoch: 1286 	Training Loss: 1.162720 	Validation Loss: 1.356148 	 time: 0.3
Validation loss decreased from 1.356174 to 1.356148. Model was saved
Epoch: 1287 	Training Loss: 1.162712 	Validation Loss: 1.356172 	 time: 0.3
Epoch: 1288 	Training Loss: 1.162704 	Validation Loss: 1.356130 	 time: 0.3
Validation loss decreased from 1.356148 to 1.356130. Model was saved
Epoch: 1289 	Training Loss: 1.162697 	Validation Loss: 1.356182 	 time: 0.3
Epoch: 1290 	Training Loss: 1.162688 	Validation Loss: 1.356142 	 time: 0.3
Epoch: 1291 	Training Loss: 1.162680 	Validation Loss: 1.356213 	 time: 0.3
Epoch: 1292 	Training Loss: 1.162670 	Validation Loss: 1.356156 	 time: 0.3
Epoch: 1293 	Training Loss: 1.162658 	Validation Loss: 1.356257 	 time: 0.3
Epoch: 1294 	Training Loss: 1.162641 	Validation Loss: 1.356148 	 time: 0.3
Epoch: 1295 	Training Loss: 1.162611 	Validation Loss: 1.356254 	 time: 0.3
Epoch: 1296 	Training Loss: 1.162581 	Validation Loss: 1.356210 	 time: 0.3
Epoch: 1297 	Training Loss: 1.162559 	Validation Loss: 1.356485 	 time: 0.3
Epoch: 1298 	Training Loss: 1.162537 	Validation Loss: 1.356358 	 time: 0.3
Epoch: 1299 	Training Loss: 1.162529 	Validation Loss: 1.357010 	 time: 0.3
Epoch: 1300 	Training Loss: 1.162543 	Validation Loss: 1.356302 	 time: 0.3
Epoch: 1301 	Training Loss: 1.162594 	Validation Loss: 1.357394 	 time: 0.3
Epoch: 1302 	Training Loss: 1.162666 	Validation Loss: 1.355654 	 time: 0.3
Validation loss decreased from 1.356130 to 1.355654. Model was saved
Epoch: 1303 	Training Loss: 1.162762 	Validation Loss: 1.357704 	 time: 0.3
Epoch: 1304 	Training Loss: 1.162913 	Validation Loss: 1.355305 	 time: 0.3
Validation loss decreased from 1.355654 to 1.355305. Model was saved
Epoch: 1305 	Training Loss: 1.162935 	Validation Loss: 1.357754 	 time: 0.3
Epoch: 1306 	Training Loss: 1.162866 	Validation Loss: 1.355613 	 time: 0.3
Epoch: 1307 	Training Loss: 1.162527 	Validation Loss: 1.355551 	 time: 0.3
Epoch: 1308 	Training Loss: 1.162507 	Validation Loss: 1.357903 	 time: 0.3
Epoch: 1309 	Training Loss: 1.162817 	Validation Loss: 1.356212 	 time: 0.3
Epoch: 1310 	Training Loss: 1.162800 	Validation Loss: 1.357525 	 time: 0.3
Epoch: 1311 	Training Loss: 1.162569 	Validation Loss: 1.355973 	 time: 0.3
Epoch: 1312 	Training Loss: 1.162435 	Validation Loss: 1.355325 	 time: 0.3
Epoch: 1313 	Training Loss: 1.162548 	Validation Loss: 1.357705 	 time: 0.3
Epoch: 1314 	Training Loss: 1.162590 	Validation Loss: 1.355634 	 time: 0.3
Epoch: 1315 	Training Loss: 1.162410 	Validation Loss: 1.354781 	 time: 0.3
Validation loss decreased from 1.355305 to 1.354781. Model was saved
Epoch: 1316 	Training Loss: 1.162416 	Validation Loss: 1.356742 	 time: 0.3
Epoch: 1317 	Training Loss: 1.162493 	Validation Loss: 1.356124 	 time: 0.3
Epoch: 1318 	Training Loss: 1.162495 	Validation Loss: 1.356104 	 time: 0.3
Epoch: 1319 	Training Loss: 1.162356 	Validation Loss: 1.356235 	 time: 0.3
Epoch: 1320 	Training Loss: 1.162465 	Validation Loss: 1.355470 	 time: 0.3
Epoch: 1321 	Training Loss: 1.162487 	Validation Loss: 1.357308 	 time: 0.3
Epoch: 1322 	Training Loss: 1.162429 	Validation Loss: 1.355867 	 time: 0.3
Epoch: 1323 	Training Loss: 1.162318 	Validation Loss: 1.354997 	 time: 0.3
Epoch: 1324 	Training Loss: 1.162466 	Validation Loss: 1.357315 	 time: 0.3
Epoch: 1325 	Training Loss: 1.162563 	Validation Loss: 1.356224 	 time: 0.3
Epoch: 1326 	Training Loss: 1.162463 	Validation Loss: 1.355823 	 time: 0.3
Epoch: 1327 	Training Loss: 1.162276 	Validation Loss: 1.356343 	 time: 0.3
Epoch: 1328 	Training Loss: 1.162479 	Validation Loss: 1.355635 	 time: 0.3
Epoch: 1329 	Training Loss: 1.162507 	Validation Loss: 1.357753 	 time: 0.3
Epoch: 1330 	Training Loss: 1.162452 	Validation Loss: 1.356111 	 time: 0.3
Epoch: 1331 	Training Loss: 1.162225 	Validation Loss: 1.354469 	 time: 0.3
Validation loss decreased from 1.354781 to 1.354469. Model was saved
Epoch: 1332 	Training Loss: 1.162406 	Validation Loss: 1.356925 	 time: 0.3
Epoch: 1333 	Training Loss: 1.162438 	Validation Loss: 1.356661 	 time: 0.3
Epoch: 1334 	Training Loss: 1.162488 	Validation Loss: 1.356122 	 time: 0.3
Epoch: 1335 	Training Loss: 1.162157 	Validation Loss: 1.355915 	 time: 0.3
Epoch: 1336 	Training Loss: 1.162473 	Validation Loss: 1.355133 	 time: 0.3
Epoch: 1337 	Training Loss: 1.162462 	Validation Loss: 1.358710 	 time: 0.3
Epoch: 1338 	Training Loss: 1.162578 	Validation Loss: 1.355798 	 time: 0.3
Epoch: 1339 	Training Loss: 1.162171 	Validation Loss: 1.354130 	 time: 0.3
Validation loss decreased from 1.354469 to 1.354130. Model was saved
Epoch: 1340 	Training Loss: 1.162417 	Validation Loss: 1.357131 	 time: 0.3
Epoch: 1341 	Training Loss: 1.162501 	Validation Loss: 1.356034 	 time: 0.3
Epoch: 1342 	Training Loss: 1.162383 	Validation Loss: 1.356426 	 time: 0.3
Epoch: 1343 	Training Loss: 1.162228 	Validation Loss: 1.356297 	 time: 0.3
Epoch: 1344 	Training Loss: 1.162282 	Validation Loss: 1.354949 	 time: 0.3
Epoch: 1345 	Training Loss: 1.162192 	Validation Loss: 1.356971 	 time: 0.3
Epoch: 1346 	Training Loss: 1.162026 	Validation Loss: 1.357550 	 time: 0.3
Epoch: 1347 	Training Loss: 1.162082 	Validation Loss: 1.355401 	 time: 0.3
Epoch: 1348 	Training Loss: 1.161978 	Validation Loss: 1.355692 	 time: 0.3
Epoch: 1349 	Training Loss: 1.162030 	Validation Loss: 1.357168 	 time: 0.3
Epoch: 1350 	Training Loss: 1.161941 	Validation Loss: 1.357324 	 time: 0.3
Epoch: 1351 	Training Loss: 1.161936 	Validation Loss: 1.356701 	 time: 0.3
Epoch: 1352 	Training Loss: 1.161836 	Validation Loss: 1.356095 	 time: 0.3
Epoch: 1353 	Training Loss: 1.161900 	Validation Loss: 1.356719 	 time: 0.3
Epoch: 1354 	Training Loss: 1.161833 	Validation Loss: 1.357552 	 time: 0.3
Epoch: 1355 	Training Loss: 1.161811 	Validation Loss: 1.357273 	 time: 0.3
Epoch: 1356 	Training Loss: 1.161711 	Validation Loss: 1.356256 	 time: 0.3
Epoch: 1357 	Training Loss: 1.161780 	Validation Loss: 1.356768 	 time: 0.3
Epoch: 1358 	Training Loss: 1.161725 	Validation Loss: 1.357682 	 time: 0.3
Epoch: 1359 	Training Loss: 1.161739 	Validation Loss: 1.357251 	 time: 0.3
Epoch: 1360 	Training Loss: 1.161632 	Validation Loss: 1.356336 	 time: 0.3
Epoch: 1361 	Training Loss: 1.161687 	Validation Loss: 1.356546 	 time: 0.3
Epoch: 1362 	Training Loss: 1.161655 	Validation Loss: 1.358166 	 time: 0.3
Epoch: 1363 	Training Loss: 1.161642 	Validation Loss: 1.356584 	 time: 0.3
Epoch: 1364 	Training Loss: 1.161542 	Validation Loss: 1.355284 	 time: 0.3
Epoch: 1365 	Training Loss: 1.161571 	Validation Loss: 1.356006 	 time: 0.3
Epoch: 1366 	Training Loss: 1.161502 	Validation Loss: 1.355926 	 time: 0.3
Epoch: 1367 	Training Loss: 1.161527 	Validation Loss: 1.355845 	 time: 0.3
Epoch: 1368 	Training Loss: 1.161482 	Validation Loss: 1.355498 	 time: 0.3
Epoch: 1369 	Training Loss: 1.161482 	Validation Loss: 1.354840 	 time: 0.3
Epoch: 1370 	Training Loss: 1.161461 	Validation Loss: 1.355717 	 time: 0.3
Epoch: 1371 	Training Loss: 1.161412 	Validation Loss: 1.355984 	 time: 0.3
Epoch: 1372 	Training Loss: 1.161419 	Validation Loss: 1.355025 	 time: 0.3
Epoch: 1373 	Training Loss: 1.161408 	Validation Loss: 1.355262 	 time: 0.3
Epoch: 1374 	Training Loss: 1.161373 	Validation Loss: 1.355273 	 time: 0.3
Epoch: 1375 	Training Loss: 1.161337 	Validation Loss: 1.355186 	 time: 0.3
Epoch: 1376 	Training Loss: 1.161311 	Validation Loss: 1.355363 	 time: 0.3
Epoch: 1377 	Training Loss: 1.161270 	Validation Loss: 1.355272 	 time: 0.3
Epoch: 1378 	Training Loss: 1.161233 	Validation Loss: 1.355322 	 time: 0.3
Epoch: 1379 	Training Loss: 1.161211 	Validation Loss: 1.355506 	 time: 0.3
Epoch: 1380 	Training Loss: 1.161211 	Validation Loss: 1.355332 	 time: 0.3
Epoch: 1381 	Training Loss: 1.161180 	Validation Loss: 1.355356 	 time: 0.3
Epoch: 1382 	Training Loss: 1.161145 	Validation Loss: 1.355601 	 time: 0.3
Epoch: 1383 	Training Loss: 1.161129 	Validation Loss: 1.355806 	 time: 0.3
Epoch: 1384 	Training Loss: 1.161115 	Validation Loss: 1.355399 	 time: 0.3
Epoch: 1385 	Training Loss: 1.161089 	Validation Loss: 1.355223 	 time: 0.3
Epoch: 1386 	Training Loss: 1.161078 	Validation Loss: 1.355332 	 time: 0.3
Epoch: 1387 	Training Loss: 1.161069 	Validation Loss: 1.355199 	 time: 0.3
Epoch: 1388 	Training Loss: 1.161058 	Validation Loss: 1.355078 	 time: 0.3
Epoch: 1389 	Training Loss: 1.161040 	Validation Loss: 1.354941 	 time: 0.3
Epoch: 1390 	Training Loss: 1.161031 	Validation Loss: 1.354906 	 time: 0.3
Epoch: 1391 	Training Loss: 1.161022 	Validation Loss: 1.355077 	 time: 0.3
Epoch: 1392 	Training Loss: 1.161007 	Validation Loss: 1.355188 	 time: 0.3
Epoch: 1393 	Training Loss: 1.160991 	Validation Loss: 1.355255 	 time: 0.3
Epoch: 1394 	Training Loss: 1.160977 	Validation Loss: 1.355573 	 time: 0.3
Epoch: 1395 	Training Loss: 1.160954 	Validation Loss: 1.355771 	 time: 0.3
Epoch: 1396 	Training Loss: 1.160933 	Validation Loss: 1.355553 	 time: 0.3
Epoch: 1397 	Training Loss: 1.160924 	Validation Loss: 1.355808 	 time: 0.3
Epoch: 1398 	Training Loss: 1.160916 	Validation Loss: 1.355883 	 time: 0.3
Epoch: 1399 	Training Loss: 1.160908 	Validation Loss: 1.355727 	 time: 0.3
Epoch: 1400 	Training Loss: 1.160898 	Validation Loss: 1.355588 	 time: 0.3
Epoch: 1401 	Training Loss: 1.160890 	Validation Loss: 1.355531 	 time: 0.3
Epoch: 1402 	Training Loss: 1.160882 	Validation Loss: 1.355525 	 time: 0.3
Epoch: 1403 	Training Loss: 1.160873 	Validation Loss: 1.355442 	 time: 0.3
Epoch: 1404 	Training Loss: 1.160865 	Validation Loss: 1.355523 	 time: 0.3
Epoch: 1405 	Training Loss: 1.160857 	Validation Loss: 1.355570 	 time: 0.3
Epoch: 1406 	Training Loss: 1.160849 	Validation Loss: 1.355629 	 time: 0.3
Epoch: 1407 	Training Loss: 1.160842 	Validation Loss: 1.355539 	 time: 0.3
Epoch: 1408 	Training Loss: 1.160836 	Validation Loss: 1.355447 	 time: 0.3
Epoch: 1409 	Training Loss: 1.160829 	Validation Loss: 1.355626 	 time: 0.3
Epoch: 1410 	Training Loss: 1.160823 	Validation Loss: 1.355547 	 time: 0.3
Epoch: 1411 	Training Loss: 1.160816 	Validation Loss: 1.355504 	 time: 0.3
Epoch: 1412 	Training Loss: 1.160809 	Validation Loss: 1.355616 	 time: 0.3
Epoch: 1413 	Training Loss: 1.160802 	Validation Loss: 1.355688 	 time: 0.3
Epoch: 1414 	Training Loss: 1.160796 	Validation Loss: 1.355683 	 time: 0.3
Epoch: 1415 	Training Loss: 1.160790 	Validation Loss: 1.355696 	 time: 0.3
Epoch: 1416 	Training Loss: 1.160784 	Validation Loss: 1.355794 	 time: 0.3
Epoch: 1417 	Training Loss: 1.160778 	Validation Loss: 1.355759 	 time: 0.3
Epoch: 1418 	Training Loss: 1.160771 	Validation Loss: 1.355776 	 time: 0.3
Epoch: 1419 	Training Loss: 1.160765 	Validation Loss: 1.355783 	 time: 0.3
Epoch: 1420 	Training Loss: 1.160758 	Validation Loss: 1.355761 	 time: 0.3
Epoch: 1421 	Training Loss: 1.160753 	Validation Loss: 1.355787 	 time: 0.3
Epoch: 1422 	Training Loss: 1.160746 	Validation Loss: 1.355701 	 time: 0.3
Epoch: 1423 	Training Loss: 1.160739 	Validation Loss: 1.355716 	 time: 0.3
Epoch: 1424 	Training Loss: 1.160733 	Validation Loss: 1.355765 	 time: 0.3
Epoch: 1425 	Training Loss: 1.160726 	Validation Loss: 1.355783 	 time: 0.3
Epoch: 1426 	Training Loss: 1.160719 	Validation Loss: 1.355793 	 time: 0.3
Epoch: 1427 	Training Loss: 1.160711 	Validation Loss: 1.355841 	 time: 0.3
Epoch: 1428 	Training Loss: 1.160703 	Validation Loss: 1.355893 	 time: 0.3
Epoch: 1429 	Training Loss: 1.160693 	Validation Loss: 1.355851 	 time: 0.3
Epoch: 1430 	Training Loss: 1.160679 	Validation Loss: 1.355942 	 time: 0.3
Epoch: 1431 	Training Loss: 1.160665 	Validation Loss: 1.355975 	 time: 0.3
Epoch: 1432 	Training Loss: 1.160653 	Validation Loss: 1.356009 	 time: 0.3
Epoch: 1433 	Training Loss: 1.160645 	Validation Loss: 1.355936 	 time: 0.3
Epoch: 1434 	Training Loss: 1.160640 	Validation Loss: 1.355927 	 time: 0.3
Epoch: 1435 	Training Loss: 1.160634 	Validation Loss: 1.355898 	 time: 0.3
Epoch: 1436 	Training Loss: 1.160628 	Validation Loss: 1.355888 	 time: 0.3
Epoch: 1437 	Training Loss: 1.160623 	Validation Loss: 1.355874 	 time: 0.3
Epoch: 1438 	Training Loss: 1.160617 	Validation Loss: 1.355856 	 time: 0.3
Epoch: 1439 	Training Loss: 1.160612 	Validation Loss: 1.355839 	 time: 0.3
Epoch: 1440 	Training Loss: 1.160606 	Validation Loss: 1.355765 	 time: 0.3
Epoch: 1441 	Training Loss: 1.160600 	Validation Loss: 1.355739 	 time: 0.3
Epoch: 1442 	Training Loss: 1.160595 	Validation Loss: 1.355710 	 time: 0.3
Epoch: 1443 	Training Loss: 1.160590 	Validation Loss: 1.355719 	 time: 0.3
Epoch: 1444 	Training Loss: 1.160584 	Validation Loss: 1.355684 	 time: 0.3
Epoch: 1445 	Training Loss: 1.160578 	Validation Loss: 1.355712 	 time: 0.3
Epoch: 1446 	Training Loss: 1.160573 	Validation Loss: 1.355697 	 time: 0.3
Epoch: 1447 	Training Loss: 1.160568 	Validation Loss: 1.355715 	 time: 0.3
Epoch: 1448 	Training Loss: 1.160562 	Validation Loss: 1.355689 	 time: 0.3
Epoch: 1449 	Training Loss: 1.160557 	Validation Loss: 1.355738 	 time: 0.3
Epoch: 1450 	Training Loss: 1.160552 	Validation Loss: 1.355729 	 time: 0.3
Epoch: 1451 	Training Loss: 1.160547 	Validation Loss: 1.355784 	 time: 0.3
Epoch: 1452 	Training Loss: 1.160541 	Validation Loss: 1.355768 	 time: 0.3
Epoch: 1453 	Training Loss: 1.160535 	Validation Loss: 1.355827 	 time: 0.3
Epoch: 1454 	Training Loss: 1.160530 	Validation Loss: 1.355776 	 time: 0.3
Epoch: 1455 	Training Loss: 1.160524 	Validation Loss: 1.355840 	 time: 0.3
Epoch: 1456 	Training Loss: 1.160519 	Validation Loss: 1.355784 	 time: 0.3
Epoch: 1457 	Training Loss: 1.160512 	Validation Loss: 1.355911 	 time: 0.3
Epoch: 1458 	Training Loss: 1.160506 	Validation Loss: 1.355824 	 time: 0.3
Epoch: 1459 	Training Loss: 1.160499 	Validation Loss: 1.355998 	 time: 0.3
Epoch: 1460 	Training Loss: 1.160492 	Validation Loss: 1.355805 	 time: 0.3
Epoch: 1461 	Training Loss: 1.160488 	Validation Loss: 1.356014 	 time: 0.3
Epoch: 1462 	Training Loss: 1.160487 	Validation Loss: 1.355575 	 time: 0.3
Epoch: 1463 	Training Loss: 1.160489 	Validation Loss: 1.355933 	 time: 0.3
Epoch: 1464 	Training Loss: 1.160492 	Validation Loss: 1.355296 	 time: 0.3
Epoch: 1465 	Training Loss: 1.160488 	Validation Loss: 1.355797 	 time: 0.3
Epoch: 1466 	Training Loss: 1.160478 	Validation Loss: 1.355129 	 time: 0.3
Epoch: 1467 	Training Loss: 1.160460 	Validation Loss: 1.355659 	 time: 0.3
Epoch: 1468 	Training Loss: 1.160443 	Validation Loss: 1.355283 	 time: 0.3
Epoch: 1469 	Training Loss: 1.160424 	Validation Loss: 1.355798 	 time: 0.3
Epoch: 1470 	Training Loss: 1.160407 	Validation Loss: 1.355386 	 time: 0.3
Epoch: 1471 	Training Loss: 1.160393 	Validation Loss: 1.355888 	 time: 0.3
Epoch: 1472 	Training Loss: 1.160389 	Validation Loss: 1.355433 	 time: 0.3
Epoch: 1473 	Training Loss: 1.160381 	Validation Loss: 1.355993 	 time: 0.3
Epoch: 1474 	Training Loss: 1.160357 	Validation Loss: 1.355336 	 time: 0.3
Epoch: 1475 	Training Loss: 1.160333 	Validation Loss: 1.355576 	 time: 0.3
Epoch: 1476 	Training Loss: 1.160340 	Validation Loss: 1.355437 	 time: 0.3
Epoch: 1477 	Training Loss: 1.160348 	Validation Loss: 1.355569 	 time: 0.3
Epoch: 1478 	Training Loss: 1.160298 	Validation Loss: 1.354593 	 time: 0.3
Epoch: 1479 	Training Loss: 1.160267 	Validation Loss: 1.355701 	 time: 0.3
Epoch: 1480 	Training Loss: 1.160282 	Validation Loss: 1.354693 	 time: 0.3
Epoch: 1481 	Training Loss: 1.160324 	Validation Loss: 1.355884 	 time: 0.3
Epoch: 1482 	Training Loss: 1.160321 	Validation Loss: 1.354673 	 time: 0.3
Epoch: 1483 	Training Loss: 1.160213 	Validation Loss: 1.355233 	 time: 0.3
Epoch: 1484 	Training Loss: 1.160115 	Validation Loss: 1.355739 	 time: 0.3
Epoch: 1485 	Training Loss: 1.160138 	Validation Loss: 1.355310 	 time: 0.3
Epoch: 1486 	Training Loss: 1.160193 	Validation Loss: 1.356298 	 time: 0.3
Epoch: 1487 	Training Loss: 1.160306 	Validation Loss: 1.354751 	 time: 0.3
Epoch: 1488 	Training Loss: 1.160418 	Validation Loss: 1.357709 	 time: 0.3
Epoch: 1489 	Training Loss: 1.160729 	Validation Loss: 1.355940 	 time: 0.3
Epoch: 1490 	Training Loss: 1.160590 	Validation Loss: 1.354767 	 time: 0.3
Epoch: 1491 	Training Loss: 1.160186 	Validation Loss: 1.356635 	 time: 0.3
Epoch: 1492 	Training Loss: 1.160330 	Validation Loss: 1.355334 	 time: 0.3
Epoch: 1493 	Training Loss: 1.160318 	Validation Loss: 1.355219 	 time: 0.3
Epoch: 1494 	Training Loss: 1.160024 	Validation Loss: 1.355082 	 time: 0.3
Epoch: 1495 	Training Loss: 1.160279 	Validation Loss: 1.354738 	 time: 0.3
Epoch: 1496 	Training Loss: 1.160029 	Validation Loss: 1.355362 	 time: 0.3
Epoch: 1497 	Training Loss: 1.160017 	Validation Loss: 1.355866 	 time: 0.3
Epoch: 1498 	Training Loss: 1.160208 	Validation Loss: 1.354199 	 time: 0.3
Epoch: 1499 	Training Loss: 1.159971 	Validation Loss: 1.354629 	 time: 0.3
Epoch: 1500 	Training Loss: 1.159835 	Validation Loss: 1.355924 	 time: 0.3
Epoch: 1501 	Training Loss: 1.159969 	Validation Loss: 1.354256 	 time: 0.3
Epoch: 1502 	Training Loss: 1.159878 	Validation Loss: 1.355022 	 time: 0.3
Epoch: 1503 	Training Loss: 1.159750 	Validation Loss: 1.354888 	 time: 0.3
Epoch: 1504 	Training Loss: 1.159737 	Validation Loss: 1.354710 	 time: 0.3
Epoch: 1505 	Training Loss: 1.159665 	Validation Loss: 1.355358 	 time: 0.3
Epoch: 1506 	Training Loss: 1.159649 	Validation Loss: 1.355445 	 time: 0.3
Epoch: 1507 	Training Loss: 1.159695 	Validation Loss: 1.355300 	 time: 0.3
Epoch: 1508 	Training Loss: 1.159648 	Validation Loss: 1.355178 	 time: 0.3
Epoch: 1509 	Training Loss: 1.159593 	Validation Loss: 1.355990 	 time: 0.3
Epoch: 1510 	Training Loss: 1.159622 	Validation Loss: 1.355164 	 time: 0.3
Epoch: 1511 	Training Loss: 1.159577 	Validation Loss: 1.355181 	 time: 0.3
Epoch: 1512 	Training Loss: 1.159531 	Validation Loss: 1.355955 	 time: 0.3
Epoch: 1513 	Training Loss: 1.159557 	Validation Loss: 1.355406 	 time: 0.3
Epoch: 1514 	Training Loss: 1.159540 	Validation Loss: 1.355440 	 time: 0.3
Epoch: 1515 	Training Loss: 1.159467 	Validation Loss: 1.355788 	 time: 0.3
Epoch: 1516 	Training Loss: 1.159376 	Validation Loss: 1.355505 	 time: 0.3
Epoch: 1517 	Training Loss: 1.159331 	Validation Loss: 1.355567 	 time: 0.3
Epoch: 1518 	Training Loss: 1.159316 	Validation Loss: 1.355732 	 time: 0.3
Epoch: 1519 	Training Loss: 1.159322 	Validation Loss: 1.355895 	 time: 0.3
Epoch: 1520 	Training Loss: 1.159322 	Validation Loss: 1.355177 	 time: 0.3
Epoch: 1521 	Training Loss: 1.159306 	Validation Loss: 1.355953 	 time: 0.3
Epoch: 1522 	Training Loss: 1.159296 	Validation Loss: 1.355397 	 time: 0.3
Epoch: 1523 	Training Loss: 1.159260 	Validation Loss: 1.354933 	 time: 0.3
Epoch: 1524 	Training Loss: 1.159237 	Validation Loss: 1.355488 	 time: 0.3
Epoch: 1525 	Training Loss: 1.159232 	Validation Loss: 1.355365 	 time: 0.3
Epoch: 1526 	Training Loss: 1.159246 	Validation Loss: 1.355850 	 time: 0.3
Epoch: 1527 	Training Loss: 1.159208 	Validation Loss: 1.355320 	 time: 0.3
Epoch: 1528 	Training Loss: 1.159205 	Validation Loss: 1.355550 	 time: 0.3
Epoch: 1529 	Training Loss: 1.159186 	Validation Loss: 1.355540 	 time: 0.3
Epoch: 1530 	Training Loss: 1.159160 	Validation Loss: 1.354946 	 time: 0.3
Epoch: 1531 	Training Loss: 1.159153 	Validation Loss: 1.354888 	 time: 0.3
Epoch: 1532 	Training Loss: 1.159157 	Validation Loss: 1.354779 	 time: 0.3
Epoch: 1533 	Training Loss: 1.159133 	Validation Loss: 1.354984 	 time: 0.3
Epoch: 1534 	Training Loss: 1.159140 	Validation Loss: 1.354621 	 time: 0.3
Epoch: 1535 	Training Loss: 1.159121 	Validation Loss: 1.354625 	 time: 0.3
Epoch: 1536 	Training Loss: 1.159099 	Validation Loss: 1.354605 	 time: 0.3
Epoch: 1537 	Training Loss: 1.159089 	Validation Loss: 1.354894 	 time: 0.3
Epoch: 1538 	Training Loss: 1.159088 	Validation Loss: 1.354742 	 time: 0.3
Epoch: 1539 	Training Loss: 1.159072 	Validation Loss: 1.354694 	 time: 0.3
Epoch: 1540 	Training Loss: 1.159064 	Validation Loss: 1.355183 	 time: 0.3
Epoch: 1541 	Training Loss: 1.159063 	Validation Loss: 1.355131 	 time: 0.3
Epoch: 1542 	Training Loss: 1.159050 	Validation Loss: 1.354998 	 time: 0.3
Epoch: 1543 	Training Loss: 1.159039 	Validation Loss: 1.354813 	 time: 0.3
Epoch: 1544 	Training Loss: 1.159029 	Validation Loss: 1.355279 	 time: 0.3
Epoch: 1545 	Training Loss: 1.159010 	Validation Loss: 1.354705 	 time: 0.3
Epoch: 1546 	Training Loss: 1.158987 	Validation Loss: 1.355045 	 time: 0.3
Epoch: 1547 	Training Loss: 1.158985 	Validation Loss: 1.354884 	 time: 0.3
Epoch: 1548 	Training Loss: 1.158981 	Validation Loss: 1.355276 	 time: 0.3
Epoch: 1549 	Training Loss: 1.158966 	Validation Loss: 1.354777 	 time: 0.3
Epoch: 1550 	Training Loss: 1.158947 	Validation Loss: 1.354991 	 time: 0.3
Epoch: 1551 	Training Loss: 1.158938 	Validation Loss: 1.354952 	 time: 0.3
Epoch: 1552 	Training Loss: 1.158926 	Validation Loss: 1.354842 	 time: 0.3
Epoch: 1553 	Training Loss: 1.158919 	Validation Loss: 1.354834 	 time: 0.3
Epoch: 1554 	Training Loss: 1.158915 	Validation Loss: 1.354686 	 time: 0.3
Epoch: 1555 	Training Loss: 1.158909 	Validation Loss: 1.355000 	 time: 0.3
Epoch: 1556 	Training Loss: 1.158908 	Validation Loss: 1.354737 	 time: 0.3
Epoch: 1557 	Training Loss: 1.158906 	Validation Loss: 1.354966 	 time: 0.3
Epoch: 1558 	Training Loss: 1.158906 	Validation Loss: 1.354526 	 time: 0.3
Epoch: 1559 	Training Loss: 1.158908 	Validation Loss: 1.355149 	 time: 0.3
Epoch: 1560 	Training Loss: 1.158904 	Validation Loss: 1.354579 	 time: 0.3
Epoch: 1561 	Training Loss: 1.158885 	Validation Loss: 1.354831 	 time: 0.3
Epoch: 1562 	Training Loss: 1.158869 	Validation Loss: 1.354848 	 time: 0.3
Epoch: 1563 	Training Loss: 1.158854 	Validation Loss: 1.354997 	 time: 0.3
Epoch: 1564 	Training Loss: 1.158844 	Validation Loss: 1.354817 	 time: 0.3
Epoch: 1565 	Training Loss: 1.158835 	Validation Loss: 1.354817 	 time: 0.3
Epoch: 1566 	Training Loss: 1.158827 	Validation Loss: 1.354990 	 time: 0.3
Epoch: 1567 	Training Loss: 1.158820 	Validation Loss: 1.354790 	 time: 0.3
Epoch: 1568 	Training Loss: 1.158814 	Validation Loss: 1.355030 	 time: 0.3
Epoch: 1569 	Training Loss: 1.158808 	Validation Loss: 1.354779 	 time: 0.3
Epoch: 1570 	Training Loss: 1.158803 	Validation Loss: 1.355219 	 time: 0.3
Epoch: 1571 	Training Loss: 1.158798 	Validation Loss: 1.354805 	 time: 0.3
Epoch: 1572 	Training Loss: 1.158782 	Validation Loss: 1.355263 	 time: 0.3
Epoch: 1573 	Training Loss: 1.158739 	Validation Loss: 1.354860 	 time: 0.3
Epoch: 1574 	Training Loss: 1.158654 	Validation Loss: 1.355291 	 time: 0.3
Epoch: 1575 	Training Loss: 1.158559 	Validation Loss: 1.355098 	 time: 0.3
Epoch: 1576 	Training Loss: 1.158517 	Validation Loss: 1.355047 	 time: 0.3
Epoch: 1577 	Training Loss: 1.158507 	Validation Loss: 1.355696 	 time: 0.3
Epoch: 1578 	Training Loss: 1.158511 	Validation Loss: 1.355138 	 time: 0.3
Epoch: 1579 	Training Loss: 1.158540 	Validation Loss: 1.356131 	 time: 0.3
Epoch: 1580 	Training Loss: 1.158552 	Validation Loss: 1.354906 	 time: 0.3
Epoch: 1581 	Training Loss: 1.158520 	Validation Loss: 1.355723 	 time: 0.3
Epoch: 1582 	Training Loss: 1.158448 	Validation Loss: 1.355309 	 time: 0.3
Epoch: 1583 	Training Loss: 1.158393 	Validation Loss: 1.354966 	 time: 0.3
Epoch: 1584 	Training Loss: 1.158413 	Validation Loss: 1.355765 	 time: 0.3
Epoch: 1585 	Training Loss: 1.158498 	Validation Loss: 1.354867 	 time: 0.3
Epoch: 1586 	Training Loss: 1.158596 	Validation Loss: 1.355852 	 time: 0.3
Epoch: 1587 	Training Loss: 1.158575 	Validation Loss: 1.354679 	 time: 0.3
Epoch: 1588 	Training Loss: 1.158394 	Validation Loss: 1.354151 	 time: 0.3
Epoch: 1589 	Training Loss: 1.158427 	Validation Loss: 1.356307 	 time: 0.3
Epoch: 1590 	Training Loss: 1.158740 	Validation Loss: 1.354515 	 time: 0.3
Epoch: 1591 	Training Loss: 1.158736 	Validation Loss: 1.355429 	 time: 0.3
Epoch: 1592 	Training Loss: 1.158461 	Validation Loss: 1.354993 	 time: 0.3
Epoch: 1593 	Training Loss: 1.158418 	Validation Loss: 1.353805 	 time: 0.3
Validation loss decreased from 1.354130 to 1.353805. Model was saved
Epoch: 1594 	Training Loss: 1.158520 	Validation Loss: 1.355190 	 time: 0.3
Epoch: 1595 	Training Loss: 1.158366 	Validation Loss: 1.355341 	 time: 0.3
Epoch: 1596 	Training Loss: 1.158525 	Validation Loss: 1.353277 	 time: 0.3
Validation loss decreased from 1.353805 to 1.353277. Model was saved
Epoch: 1597 	Training Loss: 1.158652 	Validation Loss: 1.355811 	 time: 0.3
Epoch: 1598 	Training Loss: 1.158458 	Validation Loss: 1.355363 	 time: 0.3
Epoch: 1599 	Training Loss: 1.158533 	Validation Loss: 1.354316 	 time: 0.3
Epoch: 1600 	Training Loss: 1.158420 	Validation Loss: 1.354750 	 time: 0.3
Epoch: 1601 	Training Loss: 1.158375 	Validation Loss: 1.355266 	 time: 0.3
Epoch: 1602 	Training Loss: 1.158463 	Validation Loss: 1.354804 	 time: 0.3
Epoch: 1603 	Training Loss: 1.158354 	Validation Loss: 1.355102 	 time: 0.3
Epoch: 1604 	Training Loss: 1.158283 	Validation Loss: 1.354847 	 time: 0.3
Epoch: 1605 	Training Loss: 1.158399 	Validation Loss: 1.354837 	 time: 0.3
Epoch: 1606 	Training Loss: 1.158233 	Validation Loss: 1.355695 	 time: 0.3
Epoch: 1607 	Training Loss: 1.158299 	Validation Loss: 1.354975 	 time: 0.3
Epoch: 1608 	Training Loss: 1.158388 	Validation Loss: 1.355558 	 time: 0.3
Epoch: 1609 	Training Loss: 1.158278 	Validation Loss: 1.354604 	 time: 0.3
Epoch: 1610 	Training Loss: 1.158205 	Validation Loss: 1.354407 	 time: 0.3
Epoch: 1611 	Training Loss: 1.158222 	Validation Loss: 1.355760 	 time: 0.3
Epoch: 1612 	Training Loss: 1.158277 	Validation Loss: 1.355141 	 time: 0.3
Epoch: 1613 	Training Loss: 1.158106 	Validation Loss: 1.354430 	 time: 0.3
Epoch: 1614 	Training Loss: 1.158143 	Validation Loss: 1.355759 	 time: 0.3
Epoch: 1615 	Training Loss: 1.158266 	Validation Loss: 1.354603 	 time: 0.3
Epoch: 1616 	Training Loss: 1.158119 	Validation Loss: 1.354704 	 time: 0.3
Epoch: 1617 	Training Loss: 1.158165 	Validation Loss: 1.355512 	 time: 0.3
Epoch: 1618 	Training Loss: 1.158095 	Validation Loss: 1.355325 	 time: 0.3
Epoch: 1619 	Training Loss: 1.158028 	Validation Loss: 1.354769 	 time: 0.3
Epoch: 1620 	Training Loss: 1.158044 	Validation Loss: 1.356347 	 time: 0.3
Epoch: 1621 	Training Loss: 1.158006 	Validation Loss: 1.355239 	 time: 0.3
Epoch: 1622 	Training Loss: 1.157952 	Validation Loss: 1.353857 	 time: 0.3
Epoch: 1623 	Training Loss: 1.158009 	Validation Loss: 1.354696 	 time: 0.3
Epoch: 1624 	Training Loss: 1.157991 	Validation Loss: 1.355210 	 time: 0.3
Epoch: 1625 	Training Loss: 1.157975 	Validation Loss: 1.355159 	 time: 0.3
Epoch: 1626 	Training Loss: 1.157936 	Validation Loss: 1.355517 	 time: 0.3
Epoch: 1627 	Training Loss: 1.157901 	Validation Loss: 1.354589 	 time: 0.3
Epoch: 1628 	Training Loss: 1.157921 	Validation Loss: 1.354911 	 time: 0.3
Epoch: 1629 	Training Loss: 1.157875 	Validation Loss: 1.355475 	 time: 0.3
Epoch: 1630 	Training Loss: 1.157868 	Validation Loss: 1.354764 	 time: 0.3
Epoch: 1631 	Training Loss: 1.157867 	Validation Loss: 1.354357 	 time: 0.3
Epoch: 1632 	Training Loss: 1.157854 	Validation Loss: 1.355327 	 time: 0.3
Epoch: 1633 	Training Loss: 1.157837 	Validation Loss: 1.355033 	 time: 0.3
Epoch: 1634 	Training Loss: 1.157837 	Validation Loss: 1.354543 	 time: 0.3
Epoch: 1635 	Training Loss: 1.157827 	Validation Loss: 1.354714 	 time: 0.3
Epoch: 1636 	Training Loss: 1.157798 	Validation Loss: 1.354868 	 time: 0.3
Epoch: 1637 	Training Loss: 1.157807 	Validation Loss: 1.354946 	 time: 0.3
Epoch: 1638 	Training Loss: 1.157794 	Validation Loss: 1.354811 	 time: 0.3
Epoch: 1639 	Training Loss: 1.157778 	Validation Loss: 1.354736 	 time: 0.3
Epoch: 1640 	Training Loss: 1.157787 	Validation Loss: 1.355163 	 time: 0.3
Epoch: 1641 	Training Loss: 1.157771 	Validation Loss: 1.355333 	 time: 0.3
Epoch: 1642 	Training Loss: 1.157760 	Validation Loss: 1.355072 	 time: 0.3
Epoch: 1643 	Training Loss: 1.157762 	Validation Loss: 1.355141 	 time: 0.3
Epoch: 1644 	Training Loss: 1.157756 	Validation Loss: 1.354970 	 time: 0.3
Epoch: 1645 	Training Loss: 1.157752 	Validation Loss: 1.355155 	 time: 0.3
Epoch: 1646 	Training Loss: 1.157742 	Validation Loss: 1.355043 	 time: 0.3
Epoch: 1647 	Training Loss: 1.157736 	Validation Loss: 1.354879 	 time: 0.3
Epoch: 1648 	Training Loss: 1.157736 	Validation Loss: 1.355420 	 time: 0.3
Epoch: 1649 	Training Loss: 1.157729 	Validation Loss: 1.355256 	 time: 0.3
Epoch: 1650 	Training Loss: 1.157722 	Validation Loss: 1.355041 	 time: 0.3
Epoch: 1651 	Training Loss: 1.157719 	Validation Loss: 1.355097 	 time: 0.3
Epoch: 1652 	Training Loss: 1.157711 	Validation Loss: 1.355134 	 time: 0.3
Epoch: 1653 	Training Loss: 1.157707 	Validation Loss: 1.355195 	 time: 0.3
Epoch: 1654 	Training Loss: 1.157706 	Validation Loss: 1.354999 	 time: 0.3
Epoch: 1655 	Training Loss: 1.157699 	Validation Loss: 1.355039 	 time: 0.3
Epoch: 1656 	Training Loss: 1.157691 	Validation Loss: 1.355192 	 time: 0.3
Epoch: 1657 	Training Loss: 1.157690 	Validation Loss: 1.355094 	 time: 0.3
Epoch: 1658 	Training Loss: 1.157686 	Validation Loss: 1.354962 	 time: 0.3
Epoch: 1659 	Training Loss: 1.157679 	Validation Loss: 1.354869 	 time: 0.3
Epoch: 1660 	Training Loss: 1.157674 	Validation Loss: 1.354987 	 time: 0.3
Epoch: 1661 	Training Loss: 1.157672 	Validation Loss: 1.354863 	 time: 0.3
Epoch: 1662 	Training Loss: 1.157663 	Validation Loss: 1.354675 	 time: 0.3
Epoch: 1663 	Training Loss: 1.157657 	Validation Loss: 1.354852 	 time: 0.3
Epoch: 1664 	Training Loss: 1.157647 	Validation Loss: 1.354728 	 time: 0.3
Epoch: 1665 	Training Loss: 1.157631 	Validation Loss: 1.354590 	 time: 0.3
Epoch: 1666 	Training Loss: 1.157616 	Validation Loss: 1.354468 	 time: 0.3
Epoch: 1667 	Training Loss: 1.157608 	Validation Loss: 1.354376 	 time: 0.3
Epoch: 1668 	Training Loss: 1.157601 	Validation Loss: 1.354345 	 time: 0.3
Epoch: 1669 	Training Loss: 1.157593 	Validation Loss: 1.353938 	 time: 0.3
Epoch: 1670 	Training Loss: 1.157584 	Validation Loss: 1.353612 	 time: 0.3
Epoch: 1671 	Training Loss: 1.157577 	Validation Loss: 1.353512 	 time: 0.3
Epoch: 1672 	Training Loss: 1.157567 	Validation Loss: 1.353299 	 time: 0.3
Epoch: 1673 	Training Loss: 1.157561 	Validation Loss: 1.353061 	 time: 0.3
Validation loss decreased from 1.353277 to 1.353061. Model was saved
Epoch: 1674 	Training Loss: 1.157557 	Validation Loss: 1.352935 	 time: 0.3
Validation loss decreased from 1.353061 to 1.352935. Model was saved
Epoch: 1675 	Training Loss: 1.157550 	Validation Loss: 1.353010 	 time: 0.3
Epoch: 1676 	Training Loss: 1.157541 	Validation Loss: 1.352820 	 time: 0.3
Validation loss decreased from 1.352935 to 1.352820. Model was saved
Epoch: 1677 	Training Loss: 1.157529 	Validation Loss: 1.352838 	 time: 0.3
Epoch: 1678 	Training Loss: 1.157511 	Validation Loss: 1.352657 	 time: 0.3
Validation loss decreased from 1.352820 to 1.352657. Model was saved
Epoch: 1679 	Training Loss: 1.157486 	Validation Loss: 1.352714 	 time: 0.3
Epoch: 1680 	Training Loss: 1.157464 	Validation Loss: 1.352431 	 time: 0.3
Validation loss decreased from 1.352657 to 1.352431. Model was saved
Epoch: 1681 	Training Loss: 1.157456 	Validation Loss: 1.352303 	 time: 0.3
Validation loss decreased from 1.352431 to 1.352303. Model was saved
Epoch: 1682 	Training Loss: 1.157452 	Validation Loss: 1.352236 	 time: 0.3
Validation loss decreased from 1.352303 to 1.352236. Model was saved
Epoch: 1683 	Training Loss: 1.157446 	Validation Loss: 1.352366 	 time: 0.3
Epoch: 1684 	Training Loss: 1.157437 	Validation Loss: 1.352257 	 time: 0.3
Epoch: 1685 	Training Loss: 1.157426 	Validation Loss: 1.352361 	 time: 0.3
Epoch: 1686 	Training Loss: 1.157411 	Validation Loss: 1.352109 	 time: 0.3
Validation loss decreased from 1.352236 to 1.352109. Model was saved
Epoch: 1687 	Training Loss: 1.157393 	Validation Loss: 1.352204 	 time: 0.3
Epoch: 1688 	Training Loss: 1.157384 	Validation Loss: 1.351990 	 time: 0.3
Validation loss decreased from 1.352109 to 1.351990. Model was saved
Epoch: 1689 	Training Loss: 1.157379 	Validation Loss: 1.352387 	 time: 0.3
Epoch: 1690 	Training Loss: 1.157372 	Validation Loss: 1.352173 	 time: 0.3
Epoch: 1691 	Training Loss: 1.157364 	Validation Loss: 1.352810 	 time: 0.3
Epoch: 1692 	Training Loss: 1.157353 	Validation Loss: 1.352467 	 time: 0.3
Epoch: 1693 	Training Loss: 1.157333 	Validation Loss: 1.352922 	 time: 0.3
Epoch: 1694 	Training Loss: 1.157279 	Validation Loss: 1.352944 	 time: 0.3
Epoch: 1695 	Training Loss: 1.157215 	Validation Loss: 1.351935 	 time: 0.3
Validation loss decreased from 1.351990 to 1.351935. Model was saved
Epoch: 1696 	Training Loss: 1.157215 	Validation Loss: 1.353117 	 time: 0.3
Epoch: 1697 	Training Loss: 1.157214 	Validation Loss: 1.352152 	 time: 0.3
Epoch: 1698 	Training Loss: 1.157222 	Validation Loss: 1.353739 	 time: 0.3
Epoch: 1699 	Training Loss: 1.157233 	Validation Loss: 1.352436 	 time: 0.3
Epoch: 1700 	Training Loss: 1.157215 	Validation Loss: 1.353255 	 time: 0.3
Epoch: 1701 	Training Loss: 1.157181 	Validation Loss: 1.351996 	 time: 0.3
Epoch: 1702 	Training Loss: 1.157158 	Validation Loss: 1.352776 	 time: 0.3
Epoch: 1703 	Training Loss: 1.157135 	Validation Loss: 1.351789 	 time: 0.3
Validation loss decreased from 1.351935 to 1.351789. Model was saved
Epoch: 1704 	Training Loss: 1.157097 	Validation Loss: 1.352357 	 time: 0.3
Epoch: 1705 	Training Loss: 1.157058 	Validation Loss: 1.352730 	 time: 0.3
Epoch: 1706 	Training Loss: 1.157041 	Validation Loss: 1.352726 	 time: 0.3
Epoch: 1707 	Training Loss: 1.157024 	Validation Loss: 1.353153 	 time: 0.3
Epoch: 1708 	Training Loss: 1.157005 	Validation Loss: 1.352849 	 time: 0.3
Epoch: 1709 	Training Loss: 1.156994 	Validation Loss: 1.353470 	 time: 0.3
Epoch: 1710 	Training Loss: 1.156994 	Validation Loss: 1.352458 	 time: 0.3
Epoch: 1711 	Training Loss: 1.157010 	Validation Loss: 1.354358 	 time: 0.3
Epoch: 1712 	Training Loss: 1.157067 	Validation Loss: 1.352275 	 time: 0.3
Epoch: 1713 	Training Loss: 1.157156 	Validation Loss: 1.356694 	 time: 0.3
Epoch: 1714 	Training Loss: 1.157312 	Validation Loss: 1.352478 	 time: 0.3
Epoch: 1715 	Training Loss: 1.157289 	Validation Loss: 1.355002 	 time: 0.3
Epoch: 1716 	Training Loss: 1.157163 	Validation Loss: 1.352815 	 time: 0.3
Epoch: 1717 	Training Loss: 1.156976 	Validation Loss: 1.350988 	 time: 0.3
Validation loss decreased from 1.351789 to 1.350988. Model was saved
Epoch: 1718 	Training Loss: 1.157005 	Validation Loss: 1.356414 	 time: 0.3
Epoch: 1719 	Training Loss: 1.157567 	Validation Loss: 1.351887 	 time: 0.3
Epoch: 1720 	Training Loss: 1.157607 	Validation Loss: 1.355261 	 time: 0.3
Epoch: 1721 	Training Loss: 1.157091 	Validation Loss: 1.355454 	 time: 0.3
Epoch: 1722 	Training Loss: 1.157329 	Validation Loss: 1.351139 	 time: 0.3
Epoch: 1723 	Training Loss: 1.157525 	Validation Loss: 1.355165 	 time: 0.3
Epoch: 1724 	Training Loss: 1.157147 	Validation Loss: 1.355963 	 time: 0.3
Epoch: 1725 	Training Loss: 1.157413 	Validation Loss: 1.351636 	 time: 0.3
Epoch: 1726 	Training Loss: 1.157333 	Validation Loss: 1.351800 	 time: 0.3
Epoch: 1727 	Training Loss: 1.157035 	Validation Loss: 1.352769 	 time: 0.3
Epoch: 1728 	Training Loss: 1.157208 	Validation Loss: 1.352564 	 time: 0.3
Epoch: 1729 	Training Loss: 1.156971 	Validation Loss: 1.352143 	 time: 0.3
Epoch: 1730 	Training Loss: 1.157042 	Validation Loss: 1.351320 	 time: 0.3
Epoch: 1731 	Training Loss: 1.156907 	Validation Loss: 1.351176 	 time: 0.3
Epoch: 1732 	Training Loss: 1.156990 	Validation Loss: 1.353339 	 time: 0.3
Epoch: 1733 	Training Loss: 1.156827 	Validation Loss: 1.353987 	 time: 0.3
Epoch: 1734 	Training Loss: 1.156912 	Validation Loss: 1.352056 	 time: 0.3
Epoch: 1735 	Training Loss: 1.156779 	Validation Loss: 1.351863 	 time: 0.3
Epoch: 1736 	Training Loss: 1.156801 	Validation Loss: 1.352291 	 time: 0.3
Epoch: 1737 	Training Loss: 1.156702 	Validation Loss: 1.352985 	 time: 0.3
Epoch: 1738 	Training Loss: 1.156726 	Validation Loss: 1.353306 	 time: 0.3
Epoch: 1739 	Training Loss: 1.156666 	Validation Loss: 1.352583 	 time: 0.3
Epoch: 1740 	Training Loss: 1.156594 	Validation Loss: 1.352821 	 time: 0.3
Epoch: 1741 	Training Loss: 1.156671 	Validation Loss: 1.353411 	 time: 0.3
Epoch: 1742 	Training Loss: 1.156526 	Validation Loss: 1.354404 	 time: 0.3
Epoch: 1743 	Training Loss: 1.156586 	Validation Loss: 1.353470 	 time: 0.3
Epoch: 1744 	Training Loss: 1.156558 	Validation Loss: 1.352202 	 time: 0.3
Epoch: 1745 	Training Loss: 1.156528 	Validation Loss: 1.353490 	 time: 0.3
Epoch: 1746 	Training Loss: 1.156483 	Validation Loss: 1.353848 	 time: 0.3
Epoch: 1747 	Training Loss: 1.156466 	Validation Loss: 1.352762 	 time: 0.3
Epoch: 1748 	Training Loss: 1.156485 	Validation Loss: 1.353316 	 time: 0.3
Epoch: 1749 	Training Loss: 1.156422 	Validation Loss: 1.353888 	 time: 0.3
Epoch: 1750 	Training Loss: 1.156411 	Validation Loss: 1.353274 	 time: 0.3
Epoch: 1751 	Training Loss: 1.156379 	Validation Loss: 1.353868 	 time: 0.3
Epoch: 1752 	Training Loss: 1.156353 	Validation Loss: 1.354553 	 time: 0.3
Epoch: 1753 	Training Loss: 1.156329 	Validation Loss: 1.353942 	 time: 0.3
Epoch: 1754 	Training Loss: 1.156286 	Validation Loss: 1.353861 	 time: 0.3
Epoch: 1755 	Training Loss: 1.156263 	Validation Loss: 1.354162 	 time: 0.3
Epoch: 1756 	Training Loss: 1.156243 	Validation Loss: 1.354292 	 time: 0.3
Epoch: 1757 	Training Loss: 1.156238 	Validation Loss: 1.354376 	 time: 0.3
Epoch: 1758 	Training Loss: 1.156217 	Validation Loss: 1.354215 	 time: 0.3
Epoch: 1759 	Training Loss: 1.156200 	Validation Loss: 1.354061 	 time: 0.3
Epoch: 1760 	Training Loss: 1.156194 	Validation Loss: 1.354796 	 time: 0.3
Epoch: 1761 	Training Loss: 1.156168 	Validation Loss: 1.354383 	 time: 0.3
Epoch: 1762 	Training Loss: 1.156144 	Validation Loss: 1.354227 	 time: 0.3
Epoch: 1763 	Training Loss: 1.156143 	Validation Loss: 1.354740 	 time: 0.3
Epoch: 1764 	Training Loss: 1.156126 	Validation Loss: 1.354820 	 time: 0.3
Epoch: 1765 	Training Loss: 1.156114 	Validation Loss: 1.355006 	 time: 0.3
Epoch: 1766 	Training Loss: 1.156107 	Validation Loss: 1.354964 	 time: 0.3
Epoch: 1767 	Training Loss: 1.156094 	Validation Loss: 1.354587 	 time: 0.3
Epoch: 1768 	Training Loss: 1.156086 	Validation Loss: 1.354913 	 time: 0.3
Epoch: 1769 	Training Loss: 1.156075 	Validation Loss: 1.354735 	 time: 0.3
Epoch: 1770 	Training Loss: 1.156064 	Validation Loss: 1.354507 	 time: 0.3
Epoch: 1771 	Training Loss: 1.156057 	Validation Loss: 1.354648 	 time: 0.3
Epoch: 1772 	Training Loss: 1.156042 	Validation Loss: 1.354568 	 time: 0.3
Epoch: 1773 	Training Loss: 1.156028 	Validation Loss: 1.354695 	 time: 0.3
Epoch: 1774 	Training Loss: 1.156018 	Validation Loss: 1.354552 	 time: 0.3
Epoch: 1775 	Training Loss: 1.156004 	Validation Loss: 1.354226 	 time: 0.3
Epoch: 1776 	Training Loss: 1.155994 	Validation Loss: 1.354413 	 time: 0.3
Epoch: 1777 	Training Loss: 1.155981 	Validation Loss: 1.354190 	 time: 0.3
Epoch: 1778 	Training Loss: 1.155965 	Validation Loss: 1.354015 	 time: 0.3
Epoch: 1779 	Training Loss: 1.155947 	Validation Loss: 1.354131 	 time: 0.3
Epoch: 1780 	Training Loss: 1.155928 	Validation Loss: 1.354220 	 time: 0.3
Epoch: 1781 	Training Loss: 1.155910 	Validation Loss: 1.354530 	 time: 0.3
Epoch: 1782 	Training Loss: 1.155894 	Validation Loss: 1.354381 	 time: 0.3
Epoch: 1783 	Training Loss: 1.155881 	Validation Loss: 1.354503 	 time: 0.3
Epoch: 1784 	Training Loss: 1.155868 	Validation Loss: 1.354462 	 time: 0.3
Epoch: 1785 	Training Loss: 1.155852 	Validation Loss: 1.354137 	 time: 0.3
Epoch: 1786 	Training Loss: 1.155827 	Validation Loss: 1.353983 	 time: 0.3
Epoch: 1787 	Training Loss: 1.155803 	Validation Loss: 1.353713 	 time: 0.3
Epoch: 1788 	Training Loss: 1.155785 	Validation Loss: 1.353773 	 time: 0.3
Epoch: 1789 	Training Loss: 1.155753 	Validation Loss: 1.353854 	 time: 0.3
Epoch: 1790 	Training Loss: 1.155695 	Validation Loss: 1.353853 	 time: 0.3
Epoch: 1791 	Training Loss: 1.155639 	Validation Loss: 1.354113 	 time: 0.3
Epoch: 1792 	Training Loss: 1.155617 	Validation Loss: 1.354161 	 time: 0.3
Epoch: 1793 	Training Loss: 1.155606 	Validation Loss: 1.354173 	 time: 0.3
Epoch: 1794 	Training Loss: 1.155596 	Validation Loss: 1.354075 	 time: 0.3
Epoch: 1795 	Training Loss: 1.155586 	Validation Loss: 1.354044 	 time: 0.3
Epoch: 1796 	Training Loss: 1.155574 	Validation Loss: 1.354151 	 time: 0.3
Epoch: 1797 	Training Loss: 1.155555 	Validation Loss: 1.354100 	 time: 0.3
Epoch: 1798 	Training Loss: 1.155517 	Validation Loss: 1.354276 	 time: 0.3
Epoch: 1799 	Training Loss: 1.155461 	Validation Loss: 1.354620 	 time: 0.3
Epoch: 1800 	Training Loss: 1.155428 	Validation Loss: 1.354893 	 time: 0.3
Epoch: 1801 	Training Loss: 1.155416 	Validation Loss: 1.355190 	 time: 0.3
Epoch: 1802 	Training Loss: 1.155407 	Validation Loss: 1.355398 	 time: 0.3
Epoch: 1803 	Training Loss: 1.155396 	Validation Loss: 1.355481 	 time: 0.3
Epoch: 1804 	Training Loss: 1.155383 	Validation Loss: 1.355321 	 time: 0.3
Epoch: 1805 	Training Loss: 1.155370 	Validation Loss: 1.355087 	 time: 0.3
Epoch: 1806 	Training Loss: 1.155358 	Validation Loss: 1.354893 	 time: 0.3
Epoch: 1807 	Training Loss: 1.155347 	Validation Loss: 1.354663 	 time: 0.3
Epoch: 1808 	Training Loss: 1.155335 	Validation Loss: 1.354490 	 time: 0.3
Epoch: 1809 	Training Loss: 1.155318 	Validation Loss: 1.354343 	 time: 0.3
Epoch: 1810 	Training Loss: 1.155299 	Validation Loss: 1.354278 	 time: 0.3
Epoch: 1811 	Training Loss: 1.155276 	Validation Loss: 1.354257 	 time: 0.3
Epoch: 1812 	Training Loss: 1.155254 	Validation Loss: 1.354222 	 time: 0.3
Epoch: 1813 	Training Loss: 1.155235 	Validation Loss: 1.354225 	 time: 0.3
Epoch: 1814 	Training Loss: 1.155214 	Validation Loss: 1.354240 	 time: 0.3
Epoch: 1815 	Training Loss: 1.155191 	Validation Loss: 1.354299 	 time: 0.3
Epoch: 1816 	Training Loss: 1.155165 	Validation Loss: 1.354328 	 time: 0.3
Epoch: 1817 	Training Loss: 1.155136 	Validation Loss: 1.354412 	 time: 0.3
Epoch: 1818 	Training Loss: 1.155097 	Validation Loss: 1.354640 	 time: 0.3
Epoch: 1819 	Training Loss: 1.155033 	Validation Loss: 1.354674 	 time: 0.3
Epoch: 1820 	Training Loss: 1.154992 	Validation Loss: 1.354572 	 time: 0.3
Epoch: 1821 	Training Loss: 1.154981 	Validation Loss: 1.355040 	 time: 0.3
Epoch: 1822 	Training Loss: 1.154972 	Validation Loss: 1.355225 	 time: 0.3
Epoch: 1823 	Training Loss: 1.154962 	Validation Loss: 1.355694 	 time: 0.3
Epoch: 1824 	Training Loss: 1.154944 	Validation Loss: 1.355428 	 time: 0.3
Epoch: 1825 	Training Loss: 1.154915 	Validation Loss: 1.355583 	 time: 0.3
Epoch: 1826 	Training Loss: 1.154893 	Validation Loss: 1.355693 	 time: 0.3
Epoch: 1827 	Training Loss: 1.154884 	Validation Loss: 1.355482 	 time: 0.3
Epoch: 1828 	Training Loss: 1.154870 	Validation Loss: 1.355355 	 time: 0.3
Epoch: 1829 	Training Loss: 1.154843 	Validation Loss: 1.355226 	 time: 0.3
Epoch: 1830 	Training Loss: 1.154809 	Validation Loss: 1.355744 	 time: 0.3
Epoch: 1831 	Training Loss: 1.154784 	Validation Loss: 1.355522 	 time: 0.3
Epoch: 1832 	Training Loss: 1.154776 	Validation Loss: 1.355805 	 time: 0.3
Epoch: 1833 	Training Loss: 1.154777 	Validation Loss: 1.355453 	 time: 0.3
Epoch: 1834 	Training Loss: 1.154778 	Validation Loss: 1.355794 	 time: 0.3
Epoch: 1835 	Training Loss: 1.154771 	Validation Loss: 1.355101 	 time: 0.3
Epoch: 1836 	Training Loss: 1.154755 	Validation Loss: 1.355201 	 time: 0.3
Epoch: 1837 	Training Loss: 1.154738 	Validation Loss: 1.354877 	 time: 0.3
Epoch: 1838 	Training Loss: 1.154721 	Validation Loss: 1.354837 	 time: 0.3
Epoch: 1839 	Training Loss: 1.154709 	Validation Loss: 1.354658 	 time: 0.3
Epoch: 1840 	Training Loss: 1.154700 	Validation Loss: 1.354190 	 time: 0.3
Epoch: 1841 	Training Loss: 1.154691 	Validation Loss: 1.354703 	 time: 0.3
Epoch: 1842 	Training Loss: 1.154682 	Validation Loss: 1.353963 	 time: 0.3
Epoch: 1843 	Training Loss: 1.154667 	Validation Loss: 1.354552 	 time: 0.3
Epoch: 1844 	Training Loss: 1.154648 	Validation Loss: 1.353944 	 time: 0.3
Epoch: 1845 	Training Loss: 1.154620 	Validation Loss: 1.354355 	 time: 0.3
Epoch: 1846 	Training Loss: 1.154579 	Validation Loss: 1.354034 	 time: 0.3
Epoch: 1847 	Training Loss: 1.154535 	Validation Loss: 1.354155 	 time: 0.3
Epoch: 1848 	Training Loss: 1.154512 	Validation Loss: 1.354138 	 time: 0.3
Epoch: 1849 	Training Loss: 1.154501 	Validation Loss: 1.354397 	 time: 0.3
Epoch: 1850 	Training Loss: 1.154493 	Validation Loss: 1.354415 	 time: 0.3
Epoch: 1851 	Training Loss: 1.154482 	Validation Loss: 1.354508 	 time: 0.3
Epoch: 1852 	Training Loss: 1.154472 	Validation Loss: 1.354641 	 time: 0.3
Epoch: 1853 	Training Loss: 1.154464 	Validation Loss: 1.354605 	 time: 0.3
Epoch: 1854 	Training Loss: 1.154454 	Validation Loss: 1.354533 	 time: 0.3
Epoch: 1855 	Training Loss: 1.154444 	Validation Loss: 1.354360 	 time: 0.3
Epoch: 1856 	Training Loss: 1.154434 	Validation Loss: 1.354352 	 time: 0.3
Epoch: 1857 	Training Loss: 1.154420 	Validation Loss: 1.354235 	 time: 0.3
Epoch: 1858 	Training Loss: 1.154394 	Validation Loss: 1.354115 	 time: 0.3
Epoch: 1859 	Training Loss: 1.154323 	Validation Loss: 1.353941 	 time: 0.3
Epoch: 1860 	Training Loss: 1.154268 	Validation Loss: 1.354314 	 time: 0.3
Epoch: 1861 	Training Loss: 1.154261 	Validation Loss: 1.354403 	 time: 0.3
Epoch: 1862 	Training Loss: 1.154252 	Validation Loss: 1.354621 	 time: 0.3
Epoch: 1863 	Training Loss: 1.154235 	Validation Loss: 1.354764 	 time: 0.3
Epoch: 1864 	Training Loss: 1.154218 	Validation Loss: 1.354717 	 time: 0.3
Epoch: 1865 	Training Loss: 1.154203 	Validation Loss: 1.354975 	 time: 0.3
Epoch: 1866 	Training Loss: 1.154184 	Validation Loss: 1.354442 	 time: 0.3
Epoch: 1867 	Training Loss: 1.154174 	Validation Loss: 1.355208 	 time: 0.3
Epoch: 1868 	Training Loss: 1.154185 	Validation Loss: 1.354608 	 time: 0.3
Epoch: 1869 	Training Loss: 1.154221 	Validation Loss: 1.356149 	 time: 0.3
Epoch: 1870 	Training Loss: 1.154303 	Validation Loss: 1.354977 	 time: 0.3
Epoch: 1871 	Training Loss: 1.154221 	Validation Loss: 1.355462 	 time: 0.3
Epoch: 1872 	Training Loss: 1.154081 	Validation Loss: 1.355710 	 time: 0.3
Epoch: 1873 	Training Loss: 1.154014 	Validation Loss: 1.355256 	 time: 0.3
Epoch: 1874 	Training Loss: 1.154175 	Validation Loss: 1.358313 	 time: 0.3
Epoch: 1875 	Training Loss: 1.154747 	Validation Loss: 1.354736 	 time: 0.3
Epoch: 1876 	Training Loss: 1.154377 	Validation Loss: 1.355870 	 time: 0.3
Epoch: 1877 	Training Loss: 1.154113 	Validation Loss: 1.356154 	 time: 0.3
Epoch: 1878 	Training Loss: 1.154282 	Validation Loss: 1.354322 	 time: 0.3
Epoch: 1879 	Training Loss: 1.153990 	Validation Loss: 1.354153 	 time: 0.3
Epoch: 1880 	Training Loss: 1.154214 	Validation Loss: 1.356037 	 time: 0.3
Epoch: 1881 	Training Loss: 1.154342 	Validation Loss: 1.355723 	 time: 0.3
Epoch: 1882 	Training Loss: 1.154010 	Validation Loss: 1.354420 	 time: 0.3
Epoch: 1883 	Training Loss: 1.154299 	Validation Loss: 1.355115 	 time: 0.3
Epoch: 1884 	Training Loss: 1.153979 	Validation Loss: 1.355227 	 time: 0.3
Epoch: 1885 	Training Loss: 1.154103 	Validation Loss: 1.355070 	 time: 0.3
Epoch: 1886 	Training Loss: 1.154098 	Validation Loss: 1.355949 	 time: 0.3
Epoch: 1887 	Training Loss: 1.153819 	Validation Loss: 1.355829 	 time: 0.3
Epoch: 1888 	Training Loss: 1.153931 	Validation Loss: 1.355210 	 time: 0.3
Epoch: 1889 	Training Loss: 1.153937 	Validation Loss: 1.355423 	 time: 0.3
Epoch: 1890 	Training Loss: 1.153808 	Validation Loss: 1.356096 	 time: 0.3
Epoch: 1891 	Training Loss: 1.153851 	Validation Loss: 1.356352 	 time: 0.3
Epoch: 1892 	Training Loss: 1.153784 	Validation Loss: 1.356217 	 time: 0.3
Epoch: 1893 	Training Loss: 1.153690 	Validation Loss: 1.356177 	 time: 0.3
Epoch: 1894 	Training Loss: 1.153768 	Validation Loss: 1.356214 	 time: 0.3
Epoch: 1895 	Training Loss: 1.153667 	Validation Loss: 1.356185 	 time: 0.3
Epoch: 1896 	Training Loss: 1.153626 	Validation Loss: 1.356123 	 time: 0.3
Epoch: 1897 	Training Loss: 1.153621 	Validation Loss: 1.356242 	 time: 0.3
Epoch: 1898 	Training Loss: 1.153553 	Validation Loss: 1.356508 	 time: 0.3
Epoch: 1899 	Training Loss: 1.153525 	Validation Loss: 1.356454 	 time: 0.3
Epoch: 1900 	Training Loss: 1.153543 	Validation Loss: 1.356199 	 time: 0.3
Epoch: 1901 	Training Loss: 1.153492 	Validation Loss: 1.355772 	 time: 0.3
Epoch: 1902 	Training Loss: 1.153468 	Validation Loss: 1.355404 	 time: 0.3
Epoch: 1903 	Training Loss: 1.153474 	Validation Loss: 1.355313 	 time: 0.3
Epoch: 1904 	Training Loss: 1.153443 	Validation Loss: 1.355187 	 time: 0.3
Epoch: 1905 	Training Loss: 1.153407 	Validation Loss: 1.355039 	 time: 0.3
Epoch: 1906 	Training Loss: 1.153385 	Validation Loss: 1.354894 	 time: 0.3
Epoch: 1907 	Training Loss: 1.153358 	Validation Loss: 1.354676 	 time: 0.3
Epoch: 1908 	Training Loss: 1.153331 	Validation Loss: 1.354334 	 time: 0.3
Epoch: 1909 	Training Loss: 1.153314 	Validation Loss: 1.354383 	 time: 0.3
Epoch: 1910 	Training Loss: 1.153289 	Validation Loss: 1.354714 	 time: 0.3
Epoch: 1911 	Training Loss: 1.153264 	Validation Loss: 1.354749 	 time: 0.3
Epoch: 1912 	Training Loss: 1.153260 	Validation Loss: 1.354530 	 time: 0.3
Epoch: 1913 	Training Loss: 1.153248 	Validation Loss: 1.354193 	 time: 0.3
Epoch: 1914 	Training Loss: 1.153235 	Validation Loss: 1.354416 	 time: 0.3
Epoch: 1915 	Training Loss: 1.153231 	Validation Loss: 1.354392 	 time: 0.3
Epoch: 1916 	Training Loss: 1.153215 	Validation Loss: 1.354772 	 time: 0.3
Epoch: 1917 	Training Loss: 1.153207 	Validation Loss: 1.354886 	 time: 0.3
Epoch: 1918 	Training Loss: 1.153195 	Validation Loss: 1.355096 	 time: 0.3
Epoch: 1919 	Training Loss: 1.153188 	Validation Loss: 1.354890 	 time: 0.3
Epoch: 1920 	Training Loss: 1.153179 	Validation Loss: 1.355107 	 time: 0.3
Epoch: 1921 	Training Loss: 1.153169 	Validation Loss: 1.355167 	 time: 0.3
Epoch: 1922 	Training Loss: 1.153160 	Validation Loss: 1.355379 	 time: 0.3
Epoch: 1923 	Training Loss: 1.153151 	Validation Loss: 1.355170 	 time: 0.3
Epoch: 1924 	Training Loss: 1.153145 	Validation Loss: 1.355218 	 time: 0.3
Epoch: 1925 	Training Loss: 1.153137 	Validation Loss: 1.355093 	 time: 0.3
Epoch: 1926 	Training Loss: 1.153130 	Validation Loss: 1.355198 	 time: 0.3
Epoch: 1927 	Training Loss: 1.153123 	Validation Loss: 1.355110 	 time: 0.3
Epoch: 1928 	Training Loss: 1.153114 	Validation Loss: 1.355214 	 time: 0.3
Epoch: 1929 	Training Loss: 1.153110 	Validation Loss: 1.355112 	 time: 0.3
Epoch: 1930 	Training Loss: 1.153102 	Validation Loss: 1.355132 	 time: 0.3
Epoch: 1931 	Training Loss: 1.153096 	Validation Loss: 1.355004 	 time: 0.3
Epoch: 1932 	Training Loss: 1.153091 	Validation Loss: 1.355144 	 time: 0.3
Epoch: 1933 	Training Loss: 1.153082 	Validation Loss: 1.355118 	 time: 0.3
Epoch: 1934 	Training Loss: 1.153075 	Validation Loss: 1.355193 	 time: 0.3
Epoch: 1935 	Training Loss: 1.153066 	Validation Loss: 1.355075 	 time: 0.3
Epoch: 1936 	Training Loss: 1.153052 	Validation Loss: 1.355190 	 time: 0.3
Epoch: 1937 	Training Loss: 1.153027 	Validation Loss: 1.355060 	 time: 0.3
Epoch: 1938 	Training Loss: 1.153000 	Validation Loss: 1.355272 	 time: 0.3
Epoch: 1939 	Training Loss: 1.152999 	Validation Loss: 1.355075 	 time: 0.3
Epoch: 1940 	Training Loss: 1.153004 	Validation Loss: 1.355507 	 time: 0.3
Epoch: 1941 	Training Loss: 1.153029 	Validation Loss: 1.355243 	 time: 0.3
Epoch: 1942 	Training Loss: 1.153015 	Validation Loss: 1.355404 	 time: 0.3
Epoch: 1943 	Training Loss: 1.152988 	Validation Loss: 1.354903 	 time: 0.3
Epoch: 1944 	Training Loss: 1.152944 	Validation Loss: 1.354805 	 time: 0.3
Epoch: 1945 	Training Loss: 1.152902 	Validation Loss: 1.353868 	 time: 0.3
Epoch: 1946 	Training Loss: 1.152877 	Validation Loss: 1.352318 	 time: 0.3
Epoch: 1947 	Training Loss: 1.152857 	Validation Loss: 1.349250 	 time: 0.3
Validation loss decreased from 1.350988 to 1.349250. Model was saved
Epoch: 1948 	Training Loss: 1.152840 	Validation Loss: 1.347607 	 time: 0.3
Validation loss decreased from 1.349250 to 1.347607. Model was saved
Epoch: 1949 	Training Loss: 1.152817 	Validation Loss: 1.345977 	 time: 0.3
Validation loss decreased from 1.347607 to 1.345977. Model was saved
Epoch: 1950 	Training Loss: 1.152805 	Validation Loss: 1.345242 	 time: 0.3
Validation loss decreased from 1.345977 to 1.345242. Model was saved
Epoch: 1951 	Training Loss: 1.152770 	Validation Loss: 1.344784 	 time: 0.3
Validation loss decreased from 1.345242 to 1.344784. Model was saved
Epoch: 1952 	Training Loss: 1.152773 	Validation Loss: 1.344471 	 time: 0.3
Validation loss decreased from 1.344784 to 1.344471. Model was saved
Epoch: 1953 	Training Loss: 1.152766 	Validation Loss: 1.344563 	 time: 0.3
Epoch: 1954 	Training Loss: 1.152771 	Validation Loss: 1.344193 	 time: 0.3
Validation loss decreased from 1.344471 to 1.344193. Model was saved
Epoch: 1955 	Training Loss: 1.152760 	Validation Loss: 1.344588 	 time: 0.3
Epoch: 1956 	Training Loss: 1.152758 	Validation Loss: 1.344405 	 time: 0.3
Epoch: 1957 	Training Loss: 1.152779 	Validation Loss: 1.344847 	 time: 0.3
Epoch: 1958 	Training Loss: 1.152743 	Validation Loss: 1.344542 	 time: 0.3
Epoch: 1959 	Training Loss: 1.152718 	Validation Loss: 1.345659 	 time: 0.3
Epoch: 1960 	Training Loss: 1.152836 	Validation Loss: 1.345185 	 time: 0.3
Epoch: 1961 	Training Loss: 1.152704 	Validation Loss: 1.344869 	 time: 0.3
Epoch: 1962 	Training Loss: 1.152639 	Validation Loss: 1.344924 	 time: 0.3
Epoch: 1963 	Training Loss: 1.152767 	Validation Loss: 1.345147 	 time: 0.3
Epoch: 1964 	Training Loss: 1.152687 	Validation Loss: 1.345215 	 time: 0.3
Epoch: 1965 	Training Loss: 1.152701 	Validation Loss: 1.344991 	 time: 0.3
Epoch: 1966 	Training Loss: 1.152842 	Validation Loss: 1.344298 	 time: 0.3
Epoch: 1967 	Training Loss: 1.152743 	Validation Loss: 1.344794 	 time: 0.3
Epoch: 1968 	Training Loss: 1.152594 	Validation Loss: 1.345251 	 time: 0.3
Epoch: 1969 	Training Loss: 1.152664 	Validation Loss: 1.344331 	 time: 0.3
Epoch: 1970 	Training Loss: 1.152755 	Validation Loss: 1.344382 	 time: 0.3
Epoch: 1971 	Training Loss: 1.152611 	Validation Loss: 1.344067 	 time: 0.3
Validation loss decreased from 1.344193 to 1.344067. Model was saved
Epoch: 1972 	Training Loss: 1.152649 	Validation Loss: 1.343560 	 time: 0.3
Validation loss decreased from 1.344067 to 1.343560. Model was saved
Epoch: 1973 	Training Loss: 1.152594 	Validation Loss: 1.343752 	 time: 0.3
Epoch: 1974 	Training Loss: 1.152532 	Validation Loss: 1.343321 	 time: 0.3
Validation loss decreased from 1.343560 to 1.343321. Model was saved
Epoch: 1975 	Training Loss: 1.152507 	Validation Loss: 1.343329 	 time: 0.3
Epoch: 1976 	Training Loss: 1.152583 	Validation Loss: 1.343405 	 time: 0.3
Epoch: 1977 	Training Loss: 1.152516 	Validation Loss: 1.343243 	 time: 0.3
Validation loss decreased from 1.343321 to 1.343243. Model was saved
Epoch: 1978 	Training Loss: 1.152432 	Validation Loss: 1.343709 	 time: 0.3
Epoch: 1979 	Training Loss: 1.152514 	Validation Loss: 1.343789 	 time: 0.3
Epoch: 1980 	Training Loss: 1.152621 	Validation Loss: 1.343393 	 time: 0.3
Epoch: 1981 	Training Loss: 1.152448 	Validation Loss: 1.342904 	 time: 0.3
Validation loss decreased from 1.343243 to 1.342904. Model was saved
Epoch: 1982 	Training Loss: 1.152440 	Validation Loss: 1.343144 	 time: 0.3
Epoch: 1983 	Training Loss: 1.152483 	Validation Loss: 1.343863 	 time: 0.3
Epoch: 1984 	Training Loss: 1.152402 	Validation Loss: 1.343693 	 time: 0.3
Epoch: 1985 	Training Loss: 1.152402 	Validation Loss: 1.343001 	 time: 0.3
Epoch: 1986 	Training Loss: 1.152505 	Validation Loss: 1.342809 	 time: 0.3
Validation loss decreased from 1.342904 to 1.342809. Model was saved
Epoch: 1987 	Training Loss: 1.152398 	Validation Loss: 1.343883 	 time: 0.3
Epoch: 1988 	Training Loss: 1.152387 	Validation Loss: 1.344096 	 time: 0.3
Epoch: 1989 	Training Loss: 1.152542 	Validation Loss: 1.343629 	 time: 0.3
Epoch: 1990 	Training Loss: 1.152506 	Validation Loss: 1.343462 	 time: 0.3
Epoch: 1991 	Training Loss: 1.152474 	Validation Loss: 1.344925 	 time: 0.3
Epoch: 1992 	Training Loss: 1.152597 	Validation Loss: 1.343517 	 time: 0.3
Epoch: 1993 	Training Loss: 1.152289 	Validation Loss: 1.343923 	 time: 0.3
Epoch: 1994 	Training Loss: 1.152483 	Validation Loss: 1.345457 	 time: 0.3
Epoch: 1995 	Training Loss: 1.153119 	Validation Loss: 1.344038 	 time: 0.3
Epoch: 1996 	Training Loss: 1.152583 	Validation Loss: 1.343850 	 time: 0.3
Epoch: 1997 	Training Loss: 1.152515 	Validation Loss: 1.343642 	 time: 0.3
Epoch: 1998 	Training Loss: 1.152537 	Validation Loss: 1.343582 	 time: 0.3
Epoch: 1999 	Training Loss: 1.152553 	Validation Loss: 1.345515 	 time: 0.3
Epoch: 2000 	Training Loss: 1.152679 	Validation Loss: 1.344308 	 time: 0.3
Epoch: 2001 	Training Loss: 1.152316 	Validation Loss: 1.342550 	 time: 0.3
Validation loss decreased from 1.342809 to 1.342550. Model was saved
Epoch: 2002 	Training Loss: 1.152397 	Validation Loss: 1.341981 	 time: 0.3
Validation loss decreased from 1.342550 to 1.341981. Model was saved
Epoch: 2003 	Training Loss: 1.152313 	Validation Loss: 1.345404 	 time: 0.3
Epoch: 2004 	Training Loss: 1.152467 	Validation Loss: 1.344774 	 time: 0.3
Epoch: 2005 	Training Loss: 1.152648 	Validation Loss: 1.345217 	 time: 0.3
Epoch: 2006 	Training Loss: 1.152369 	Validation Loss: 1.346267 	 time: 0.3
Epoch: 2007 	Training Loss: 1.152406 	Validation Loss: 1.344919 	 time: 0.3
Epoch: 2008 	Training Loss: 1.152388 	Validation Loss: 1.344224 	 time: 0.3
Epoch: 2009 	Training Loss: 1.152211 	Validation Loss: 1.343600 	 time: 0.3
Epoch: 2010 	Training Loss: 1.152248 	Validation Loss: 1.344973 	 time: 0.3
Epoch: 2011 	Training Loss: 1.152224 	Validation Loss: 1.346507 	 time: 0.3
Epoch: 2012 	Training Loss: 1.152085 	Validation Loss: 1.347743 	 time: 0.3
Epoch: 2013 	Training Loss: 1.152013 	Validation Loss: 1.347233 	 time: 0.3
Epoch: 2014 	Training Loss: 1.152155 	Validation Loss: 1.346908 	 time: 0.3
Epoch: 2015 	Training Loss: 1.151929 	Validation Loss: 1.346183 	 time: 0.3
Epoch: 2016 	Training Loss: 1.151923 	Validation Loss: 1.344955 	 time: 0.3
Epoch: 2017 	Training Loss: 1.152017 	Validation Loss: 1.344720 	 time: 0.3
Epoch: 2018 	Training Loss: 1.151882 	Validation Loss: 1.345137 	 time: 0.3
Epoch: 2019 	Training Loss: 1.151860 	Validation Loss: 1.345135 	 time: 0.3
Epoch: 2020 	Training Loss: 1.151891 	Validation Loss: 1.345069 	 time: 0.3
Epoch: 2021 	Training Loss: 1.151866 	Validation Loss: 1.345041 	 time: 0.3
Epoch: 2022 	Training Loss: 1.151822 	Validation Loss: 1.344586 	 time: 0.3
Epoch: 2023 	Training Loss: 1.151822 	Validation Loss: 1.343745 	 time: 0.3
Epoch: 2024 	Training Loss: 1.151774 	Validation Loss: 1.343391 	 time: 0.3
Epoch: 2025 	Training Loss: 1.151811 	Validation Loss: 1.343655 	 time: 0.3
Epoch: 2026 	Training Loss: 1.151766 	Validation Loss: 1.344116 	 time: 0.3
Epoch: 2027 	Training Loss: 1.151768 	Validation Loss: 1.344222 	 time: 0.3
Epoch: 2028 	Training Loss: 1.151726 	Validation Loss: 1.344154 	 time: 0.3
Epoch: 2029 	Training Loss: 1.151729 	Validation Loss: 1.344377 	 time: 0.3
Epoch: 2030 	Training Loss: 1.151701 	Validation Loss: 1.344434 	 time: 0.3
Epoch: 2031 	Training Loss: 1.151714 	Validation Loss: 1.344131 	 time: 0.3
Epoch: 2032 	Training Loss: 1.151691 	Validation Loss: 1.344180 	 time: 0.3
Epoch: 2033 	Training Loss: 1.151690 	Validation Loss: 1.344599 	 time: 0.3
Epoch: 2034 	Training Loss: 1.151672 	Validation Loss: 1.344671 	 time: 0.3
Epoch: 2035 	Training Loss: 1.151668 	Validation Loss: 1.344604 	 time: 0.3
Epoch: 2036 	Training Loss: 1.151667 	Validation Loss: 1.344647 	 time: 0.3
Epoch: 2037 	Training Loss: 1.151649 	Validation Loss: 1.344694 	 time: 0.3
Epoch: 2038 	Training Loss: 1.151643 	Validation Loss: 1.344725 	 time: 0.3
Epoch: 2039 	Training Loss: 1.151641 	Validation Loss: 1.344869 	 time: 0.3
Epoch: 2040 	Training Loss: 1.151631 	Validation Loss: 1.344931 	 time: 0.3
Epoch: 2041 	Training Loss: 1.151612 	Validation Loss: 1.344959 	 time: 0.3
Epoch: 2042 	Training Loss: 1.151613 	Validation Loss: 1.344979 	 time: 0.3
Epoch: 2043 	Training Loss: 1.151602 	Validation Loss: 1.344950 	 time: 0.3
Epoch: 2044 	Training Loss: 1.151593 	Validation Loss: 1.345041 	 time: 0.3
Epoch: 2045 	Training Loss: 1.151590 	Validation Loss: 1.345221 	 time: 0.3
Epoch: 2046 	Training Loss: 1.151584 	Validation Loss: 1.345326 	 time: 0.3
Epoch: 2047 	Training Loss: 1.151578 	Validation Loss: 1.345455 	 time: 0.3
Epoch: 2048 	Training Loss: 1.151576 	Validation Loss: 1.345487 	 time: 0.3
Epoch: 2049 	Training Loss: 1.151569 	Validation Loss: 1.345290 	 time: 0.3
Epoch: 2050 	Training Loss: 1.151561 	Validation Loss: 1.345162 	 time: 0.3
Epoch: 2051 	Training Loss: 1.151558 	Validation Loss: 1.345220 	 time: 0.3
Epoch: 2052 	Training Loss: 1.151553 	Validation Loss: 1.345133 	 time: 0.3
Epoch: 2053 	Training Loss: 1.151546 	Validation Loss: 1.345143 	 time: 0.3
Epoch: 2054 	Training Loss: 1.151543 	Validation Loss: 1.345258 	 time: 0.3
Epoch: 2055 	Training Loss: 1.151535 	Validation Loss: 1.345209 	 time: 0.3
Epoch: 2056 	Training Loss: 1.151530 	Validation Loss: 1.345031 	 time: 0.3
Epoch: 2057 	Training Loss: 1.151526 	Validation Loss: 1.345002 	 time: 0.3
Epoch: 2058 	Training Loss: 1.151521 	Validation Loss: 1.344998 	 time: 0.3
Epoch: 2059 	Training Loss: 1.151515 	Validation Loss: 1.345031 	 time: 0.3
Epoch: 2060 	Training Loss: 1.151510 	Validation Loss: 1.345142 	 time: 0.3
Epoch: 2061 	Training Loss: 1.151506 	Validation Loss: 1.345138 	 time: 0.3
Epoch: 2062 	Training Loss: 1.151502 	Validation Loss: 1.345131 	 time: 0.3
Epoch: 2063 	Training Loss: 1.151497 	Validation Loss: 1.345081 	 time: 0.3
Epoch: 2064 	Training Loss: 1.151494 	Validation Loss: 1.345081 	 time: 0.3
Epoch: 2065 	Training Loss: 1.151489 	Validation Loss: 1.345127 	 time: 0.3
Epoch: 2066 	Training Loss: 1.151485 	Validation Loss: 1.345189 	 time: 0.3
Epoch: 2067 	Training Loss: 1.151482 	Validation Loss: 1.345150 	 time: 0.3
Epoch: 2068 	Training Loss: 1.151478 	Validation Loss: 1.345163 	 time: 0.3
Epoch: 2069 	Training Loss: 1.151474 	Validation Loss: 1.345142 	 time: 0.3
Epoch: 2070 	Training Loss: 1.151470 	Validation Loss: 1.345171 	 time: 0.3
Epoch: 2071 	Training Loss: 1.151467 	Validation Loss: 1.345251 	 time: 0.3
Epoch: 2072 	Training Loss: 1.151464 	Validation Loss: 1.345341 	 time: 0.3
Epoch: 2073 	Training Loss: 1.151461 	Validation Loss: 1.345327 	 time: 0.3
Epoch: 2074 	Training Loss: 1.151458 	Validation Loss: 1.345388 	 time: 0.3
Epoch: 2075 	Training Loss: 1.151455 	Validation Loss: 1.345383 	 time: 0.3
Epoch: 2076 	Training Loss: 1.151452 	Validation Loss: 1.345410 	 time: 0.3
Epoch: 2077 	Training Loss: 1.151449 	Validation Loss: 1.345425 	 time: 0.3
Epoch: 2078 	Training Loss: 1.151446 	Validation Loss: 1.345527 	 time: 0.3
Epoch: 2079 	Training Loss: 1.151443 	Validation Loss: 1.345481 	 time: 0.3
Epoch: 2080 	Training Loss: 1.151441 	Validation Loss: 1.345565 	 time: 0.3
Epoch: 2081 	Training Loss: 1.151439 	Validation Loss: 1.345491 	 time: 0.3
Epoch: 2082 	Training Loss: 1.151437 	Validation Loss: 1.345608 	 time: 0.3
Epoch: 2083 	Training Loss: 1.151435 	Validation Loss: 1.345537 	 time: 0.3
Epoch: 2084 	Training Loss: 1.151437 	Validation Loss: 1.345752 	 time: 0.3
Epoch: 2085 	Training Loss: 1.151436 	Validation Loss: 1.345602 	 time: 0.3
Epoch: 2086 	Training Loss: 1.151441 	Validation Loss: 1.345868 	 time: 0.3
Epoch: 2087 	Training Loss: 1.151437 	Validation Loss: 1.345628 	 time: 0.3
Epoch: 2088 	Training Loss: 1.151439 	Validation Loss: 1.345890 	 time: 0.3
Epoch: 2089 	Training Loss: 1.151428 	Validation Loss: 1.345698 	 time: 0.3
Epoch: 2090 	Training Loss: 1.151423 	Validation Loss: 1.345933 	 time: 0.3
Epoch: 2091 	Training Loss: 1.151410 	Validation Loss: 1.345791 	 time: 0.3
Epoch: 2092 	Training Loss: 1.151400 	Validation Loss: 1.345943 	 time: 0.3
Epoch: 2093 	Training Loss: 1.151386 	Validation Loss: 1.345835 	 time: 0.3
Epoch: 2094 	Training Loss: 1.151371 	Validation Loss: 1.345951 	 time: 0.3
Epoch: 2095 	Training Loss: 1.151353 	Validation Loss: 1.345916 	 time: 0.3
Epoch: 2096 	Training Loss: 1.151339 	Validation Loss: 1.346045 	 time: 0.3
Epoch: 2097 	Training Loss: 1.151330 	Validation Loss: 1.346031 	 time: 0.3
Epoch: 2098 	Training Loss: 1.151332 	Validation Loss: 1.346213 	 time: 0.3
Epoch: 2099 	Training Loss: 1.151339 	Validation Loss: 1.346187 	 time: 0.3
Epoch: 2100 	Training Loss: 1.151371 	Validation Loss: 1.346356 	 time: 0.3
Epoch: 2101 	Training Loss: 1.151364 	Validation Loss: 1.346227 	 time: 0.3
Epoch: 2102 	Training Loss: 1.151386 	Validation Loss: 1.346276 	 time: 0.3
Epoch: 2103 	Training Loss: 1.151319 	Validation Loss: 1.345972 	 time: 0.3
Epoch: 2104 	Training Loss: 1.151274 	Validation Loss: 1.345841 	 time: 0.3
Epoch: 2105 	Training Loss: 1.151254 	Validation Loss: 1.345900 	 time: 0.3
Epoch: 2106 	Training Loss: 1.151252 	Validation Loss: 1.345958 	 time: 0.3
Epoch: 2107 	Training Loss: 1.151319 	Validation Loss: 1.346017 	 time: 0.3
Epoch: 2108 	Training Loss: 1.151280 	Validation Loss: 1.345770 	 time: 0.3
Epoch: 2109 	Training Loss: 1.151229 	Validation Loss: 1.345941 	 time: 0.3
Epoch: 2110 	Training Loss: 1.151175 	Validation Loss: 1.346234 	 time: 0.3
Epoch: 2111 	Training Loss: 1.151208 	Validation Loss: 1.346369 	 time: 0.3
Epoch: 2112 	Training Loss: 1.151332 	Validation Loss: 1.347203 	 time: 0.3
Epoch: 2113 	Training Loss: 1.151339 	Validation Loss: 1.346672 	 time: 0.3
Epoch: 2114 	Training Loss: 1.151389 	Validation Loss: 1.346901 	 time: 0.3
Epoch: 2115 	Training Loss: 1.151238 	Validation Loss: 1.347842 	 time: 0.3
Epoch: 2116 	Training Loss: 1.151370 	Validation Loss: 1.346805 	 time: 0.3
Epoch: 2117 	Training Loss: 1.151674 	Validation Loss: 1.347051 	 time: 0.3
Epoch: 2118 	Training Loss: 1.151399 	Validation Loss: 1.346722 	 time: 0.3
Epoch: 2119 	Training Loss: 1.151512 	Validation Loss: 1.347640 	 time: 0.3
Epoch: 2120 	Training Loss: 1.152109 	Validation Loss: 1.349516 	 time: 0.3
Epoch: 2121 	Training Loss: 1.151833 	Validation Loss: 1.349223 	 time: 0.3
Epoch: 2122 	Training Loss: 1.151904 	Validation Loss: 1.349626 	 time: 0.3
Epoch: 2123 	Training Loss: 1.152345 	Validation Loss: 1.346428 	 time: 0.3
Epoch: 2124 	Training Loss: 1.151759 	Validation Loss: 1.347546 	 time: 0.3
Epoch: 2125 	Training Loss: 1.152024 	Validation Loss: 1.347288 	 time: 0.3
Epoch: 2126 	Training Loss: 1.151613 	Validation Loss: 1.347054 	 time: 0.3
Epoch: 2127 	Training Loss: 1.152010 	Validation Loss: 1.349290 	 time: 0.3
Epoch: 2128 	Training Loss: 1.151526 	Validation Loss: 1.349271 	 time: 0.3
Epoch: 2129 	Training Loss: 1.151887 	Validation Loss: 1.346655 	 time: 0.3
Epoch: 2130 	Training Loss: 1.151577 	Validation Loss: 1.346310 	 time: 0.3
Epoch: 2131 	Training Loss: 1.151845 	Validation Loss: 1.348412 	 time: 0.3
Epoch: 2132 	Training Loss: 1.151447 	Validation Loss: 1.348970 	 time: 0.3
Epoch: 2133 	Training Loss: 1.151881 	Validation Loss: 1.347437 	 time: 0.3
Epoch: 2134 	Training Loss: 1.151676 	Validation Loss: 1.348013 	 time: 0.3
Epoch: 2135 	Training Loss: 1.152134 	Validation Loss: 1.348821 	 time: 0.3
Epoch: 2136 	Training Loss: 1.151642 	Validation Loss: 1.348860 	 time: 0.3
Epoch: 2137 	Training Loss: 1.152592 	Validation Loss: 1.349821 	 time: 0.3
Epoch: 2138 	Training Loss: 1.152044 	Validation Loss: 1.350677 	 time: 0.3
Epoch: 2139 	Training Loss: 1.153916 	Validation Loss: 1.347246 	 time: 0.3
Epoch: 2140 	Training Loss: 1.151425 	Validation Loss: 1.346330 	 time: 0.3
Epoch: 2141 	Training Loss: 1.157138 	Validation Loss: 1.346034 	 time: 0.3
Epoch: 2142 	Training Loss: 1.151755 	Validation Loss: 1.348505 	 time: 0.3
Epoch: 2143 	Training Loss: 1.155111 	Validation Loss: 1.349887 	 time: 0.3
Epoch: 2144 	Training Loss: 1.156335 	Validation Loss: 1.349671 	 time: 0.3
Epoch: 2145 	Training Loss: 1.152728 	Validation Loss: 1.349337 	 time: 0.3
Epoch: 2146 	Training Loss: 1.152675 	Validation Loss: 1.348104 	 time: 0.3
Epoch: 2147 	Training Loss: 1.154230 	Validation Loss: 1.346145 	 time: 0.3
Epoch: 2148 	Training Loss: 1.152725 	Validation Loss: 1.347526 	 time: 0.3
Epoch: 2149 	Training Loss: 1.152221 	Validation Loss: 1.346350 	 time: 0.3
Epoch: 2150 	Training Loss: 1.152747 	Validation Loss: 1.348266 	 time: 0.3
Epoch: 2151 	Training Loss: 1.152666 	Validation Loss: 1.349575 	 time: 0.3
Epoch: 2152 	Training Loss: 1.152782 	Validation Loss: 1.347812 	 time: 0.3
Epoch: 2153 	Training Loss: 1.151900 	Validation Loss: 1.346961 	 time: 0.3
Epoch: 2154 	Training Loss: 1.151617 	Validation Loss: 1.345768 	 time: 0.3
Epoch: 2155 	Training Loss: 1.152021 	Validation Loss: 1.344720 	 time: 0.3
Epoch: 2156 	Training Loss: 1.152364 	Validation Loss: 1.345055 	 time: 0.3
Epoch: 2157 	Training Loss: 1.152078 	Validation Loss: 1.346653 	 time: 0.3
Epoch: 2158 	Training Loss: 1.151637 	Validation Loss: 1.347963 	 time: 0.3
Epoch: 2159 	Training Loss: 1.151431 	Validation Loss: 1.348727 	 time: 0.3
Epoch: 2160 	Training Loss: 1.151557 	Validation Loss: 1.349103 	 time: 0.3
Epoch: 2161 	Training Loss: 1.151731 	Validation Loss: 1.348903 	 time: 0.3
Epoch: 2162 	Training Loss: 1.151479 	Validation Loss: 1.348406 	 time: 0.3
Epoch: 2163 	Training Loss: 1.151267 	Validation Loss: 1.347987 	 time: 0.3
Epoch: 2164 	Training Loss: 1.151261 	Validation Loss: 1.347791 	 time: 0.3
Epoch: 2165 	Training Loss: 1.151324 	Validation Loss: 1.347584 	 time: 0.3
Epoch: 2166 	Training Loss: 1.151289 	Validation Loss: 1.347204 	 time: 0.3
Epoch: 2167 	Training Loss: 1.151151 	Validation Loss: 1.346930 	 time: 0.3
Epoch: 2168 	Training Loss: 1.151114 	Validation Loss: 1.347036 	 time: 0.3
Epoch: 2169 	Training Loss: 1.151110 	Validation Loss: 1.347326 	 time: 0.3
Epoch: 2170 	Training Loss: 1.151096 	Validation Loss: 1.347618 	 time: 0.3
Epoch: 2171 	Training Loss: 1.151051 	Validation Loss: 1.347853 	 time: 0.3
Epoch: 2172 	Training Loss: 1.150987 	Validation Loss: 1.347859 	 time: 0.3
Epoch: 2173 	Training Loss: 1.150981 	Validation Loss: 1.347572 	 time: 0.3
Epoch: 2174 	Training Loss: 1.150997 	Validation Loss: 1.347244 	 time: 0.3
Epoch: 2175 	Training Loss: 1.150956 	Validation Loss: 1.347068 	 time: 0.3
Epoch: 2176 	Training Loss: 1.150919 	Validation Loss: 1.346950 	 time: 0.3
Epoch: 2177 	Training Loss: 1.150929 	Validation Loss: 1.346680 	 time: 0.3
Epoch: 2178 	Training Loss: 1.150915 	Validation Loss: 1.346234 	 time: 0.3
Epoch: 2179 	Training Loss: 1.150876 	Validation Loss: 1.345803 	 time: 0.3
Epoch: 2180 	Training Loss: 1.150867 	Validation Loss: 1.345501 	 time: 0.3
Epoch: 2181 	Training Loss: 1.150865 	Validation Loss: 1.345302 	 time: 0.3
Epoch: 2182 	Training Loss: 1.150858 	Validation Loss: 1.345202 	 time: 0.3
Epoch: 2183 	Training Loss: 1.150832 	Validation Loss: 1.345316 	 time: 0.3
Epoch: 2184 	Training Loss: 1.150817 	Validation Loss: 1.345547 	 time: 0.3
Epoch: 2185 	Training Loss: 1.150820 	Validation Loss: 1.345694 	 time: 0.3
Epoch: 2186 	Training Loss: 1.150812 	Validation Loss: 1.345716 	 time: 0.3
Epoch: 2187 	Training Loss: 1.150800 	Validation Loss: 1.345654 	 time: 0.3
Epoch: 2188 	Training Loss: 1.150795 	Validation Loss: 1.345578 	 time: 0.3
Epoch: 2189 	Training Loss: 1.150801 	Validation Loss: 1.345513 	 time: 0.3
Epoch: 2190 	Training Loss: 1.150789 	Validation Loss: 1.345436 	 time: 0.3
Epoch: 2191 	Training Loss: 1.150770 	Validation Loss: 1.345322 	 time: 0.3
Epoch: 2192 	Training Loss: 1.150767 	Validation Loss: 1.345173 	 time: 0.3
Epoch: 2193 	Training Loss: 1.150766 	Validation Loss: 1.345049 	 time: 0.3
Epoch: 2194 	Training Loss: 1.150759 	Validation Loss: 1.345020 	 time: 0.3
Epoch: 2195 	Training Loss: 1.150759 	Validation Loss: 1.345074 	 time: 0.3
Epoch: 2196 	Training Loss: 1.150759 	Validation Loss: 1.345136 	 time: 0.3
Epoch: 2197 	Training Loss: 1.150752 	Validation Loss: 1.345160 	 time: 0.3
Epoch: 2198 	Training Loss: 1.150745 	Validation Loss: 1.345167 	 time: 0.3
Epoch: 2199 	Training Loss: 1.150742 	Validation Loss: 1.345187 	 time: 0.3
Epoch: 2200 	Training Loss: 1.150739 	Validation Loss: 1.345228 	 time: 0.3
Epoch: 2201 	Training Loss: 1.150737 	Validation Loss: 1.345273 	 time: 0.3
Epoch: 2202 	Training Loss: 1.150735 	Validation Loss: 1.345285 	 time: 0.3
Epoch: 2203 	Training Loss: 1.150732 	Validation Loss: 1.345258 	 time: 0.3
Epoch: 2204 	Training Loss: 1.150727 	Validation Loss: 1.345236 	 time: 0.3
Epoch: 2205 	Training Loss: 1.150721 	Validation Loss: 1.345239 	 time: 0.3
Epoch: 2206 	Training Loss: 1.150715 	Validation Loss: 1.345243 	 time: 0.3
Epoch: 2207 	Training Loss: 1.150711 	Validation Loss: 1.345219 	 time: 0.3
Epoch: 2208 	Training Loss: 1.150705 	Validation Loss: 1.345174 	 time: 0.3
Epoch: 2209 	Training Loss: 1.150699 	Validation Loss: 1.345149 	 time: 0.3
Epoch: 2210 	Training Loss: 1.150697 	Validation Loss: 1.345177 	 time: 0.3
Epoch: 2211 	Training Loss: 1.150694 	Validation Loss: 1.345240 	 time: 0.3
Epoch: 2212 	Training Loss: 1.150690 	Validation Loss: 1.345289 	 time: 0.3
Epoch: 2213 	Training Loss: 1.150687 	Validation Loss: 1.345293 	 time: 0.3
Epoch: 2214 	Training Loss: 1.150683 	Validation Loss: 1.345257 	 time: 0.3
Epoch: 2215 	Training Loss: 1.150677 	Validation Loss: 1.345211 	 time: 0.3
Epoch: 2216 	Training Loss: 1.150669 	Validation Loss: 1.345176 	 time: 0.3
Epoch: 2217 	Training Loss: 1.150660 	Validation Loss: 1.345157 	 time: 0.3
Epoch: 2218 	Training Loss: 1.150652 	Validation Loss: 1.345147 	 time: 0.3
Epoch: 2219 	Training Loss: 1.150647 	Validation Loss: 1.345148 	 time: 0.3
Epoch: 2220 	Training Loss: 1.150643 	Validation Loss: 1.345158 	 time: 0.3
Epoch: 2221 	Training Loss: 1.150641 	Validation Loss: 1.345171 	 time: 0.3
Epoch: 2222 	Training Loss: 1.150639 	Validation Loss: 1.345181 	 time: 0.3
Epoch: 2223 	Training Loss: 1.150638 	Validation Loss: 1.345181 	 time: 0.3
Epoch: 2224 	Training Loss: 1.150636 	Validation Loss: 1.345166 	 time: 0.3
Epoch: 2225 	Training Loss: 1.150634 	Validation Loss: 1.345140 	 time: 0.3
Epoch: 2226 	Training Loss: 1.150632 	Validation Loss: 1.345114 	 time: 0.3
Epoch: 2227 	Training Loss: 1.150629 	Validation Loss: 1.345101 	 time: 0.3
Epoch: 2228 	Training Loss: 1.150626 	Validation Loss: 1.345102 	 time: 0.3
Epoch: 2229 	Training Loss: 1.150624 	Validation Loss: 1.345110 	 time: 0.3
Epoch: 2230 	Training Loss: 1.150622 	Validation Loss: 1.345113 	 time: 0.3
Epoch: 2231 	Training Loss: 1.150620 	Validation Loss: 1.345106 	 time: 0.3
Epoch: 2232 	Training Loss: 1.150618 	Validation Loss: 1.345096 	 time: 0.3
Epoch: 2233 	Training Loss: 1.150616 	Validation Loss: 1.345091 	 time: 0.3
Epoch: 2234 	Training Loss: 1.150614 	Validation Loss: 1.345088 	 time: 0.3
Epoch: 2235 	Training Loss: 1.150612 	Validation Loss: 1.345081 	 time: 0.3
Epoch: 2236 	Training Loss: 1.150609 	Validation Loss: 1.345067 	 time: 0.3
Epoch: 2237 	Training Loss: 1.150607 	Validation Loss: 1.345056 	 time: 0.3
Epoch: 2238 	Training Loss: 1.150605 	Validation Loss: 1.345052 	 time: 0.3
Epoch: 2239 	Training Loss: 1.150602 	Validation Loss: 1.345054 	 time: 0.3
Epoch: 2240 	Training Loss: 1.150599 	Validation Loss: 1.345053 	 time: 0.3
Epoch: 2241 	Training Loss: 1.150595 	Validation Loss: 1.345052 	 time: 0.3
Epoch: 2242 	Training Loss: 1.150590 	Validation Loss: 1.345059 	 time: 0.3
Epoch: 2243 	Training Loss: 1.150583 	Validation Loss: 1.345079 	 time: 0.3
Epoch: 2244 	Training Loss: 1.150572 	Validation Loss: 1.345104 	 time: 0.3
Epoch: 2245 	Training Loss: 1.150556 	Validation Loss: 1.345128 	 time: 0.3
Epoch: 2246 	Training Loss: 1.150539 	Validation Loss: 1.345144 	 time: 0.3
Epoch: 2247 	Training Loss: 1.150528 	Validation Loss: 1.345146 	 time: 0.3
Epoch: 2248 	Training Loss: 1.150521 	Validation Loss: 1.345135 	 time: 0.3
Epoch: 2249 	Training Loss: 1.150514 	Validation Loss: 1.345112 	 time: 0.3
Epoch: 2250 	Training Loss: 1.150504 	Validation Loss: 1.345073 	 time: 0.3
Epoch: 2251 	Training Loss: 1.150492 	Validation Loss: 1.344996 	 time: 0.3
Epoch: 2252 	Training Loss: 1.150488 	Validation Loss: 1.344888 	 time: 0.3
Epoch: 2253 	Training Loss: 1.150486 	Validation Loss: 1.344784 	 time: 0.3
Epoch: 2254 	Training Loss: 1.150481 	Validation Loss: 1.344703 	 time: 0.3
Epoch: 2255 	Training Loss: 1.150476 	Validation Loss: 1.344640 	 time: 0.3
Epoch: 2256 	Training Loss: 1.150470 	Validation Loss: 1.344578 	 time: 0.3
Epoch: 2257 	Training Loss: 1.150465 	Validation Loss: 1.344516 	 time: 0.3
Epoch: 2258 	Training Loss: 1.150461 	Validation Loss: 1.344470 	 time: 0.3
Epoch: 2259 	Training Loss: 1.150458 	Validation Loss: 1.344452 	 time: 0.3
Epoch: 2260 	Training Loss: 1.150454 	Validation Loss: 1.344452 	 time: 0.3
Epoch: 2261 	Training Loss: 1.150450 	Validation Loss: 1.344446 	 time: 0.3
Epoch: 2262 	Training Loss: 1.150447 	Validation Loss: 1.344434 	 time: 0.3
Epoch: 2263 	Training Loss: 1.150443 	Validation Loss: 1.344445 	 time: 0.3
Epoch: 2264 	Training Loss: 1.150440 	Validation Loss: 1.344498 	 time: 0.3
Epoch: 2265 	Training Loss: 1.150434 	Validation Loss: 1.344573 	 time: 0.3
Epoch: 2266 	Training Loss: 1.150429 	Validation Loss: 1.344627 	 time: 0.3
Epoch: 2267 	Training Loss: 1.150426 	Validation Loss: 1.344649 	 time: 0.3
Epoch: 2268 	Training Loss: 1.150425 	Validation Loss: 1.344664 	 time: 0.3
Epoch: 2269 	Training Loss: 1.150424 	Validation Loss: 1.344692 	 time: 0.3
Epoch: 2270 	Training Loss: 1.150424 	Validation Loss: 1.344720 	 time: 0.3
Epoch: 2271 	Training Loss: 1.150421 	Validation Loss: 1.344720 	 time: 0.3
Epoch: 2272 	Training Loss: 1.150418 	Validation Loss: 1.344687 	 time: 0.3
Epoch: 2273 	Training Loss: 1.150414 	Validation Loss: 1.344643 	 time: 0.3
Epoch: 2274 	Training Loss: 1.150412 	Validation Loss: 1.344604 	 time: 0.3
Epoch: 2275 	Training Loss: 1.150410 	Validation Loss: 1.344571 	 time: 0.3
Epoch: 2276 	Training Loss: 1.150408 	Validation Loss: 1.344539 	 time: 0.3
Epoch: 2277 	Training Loss: 1.150406 	Validation Loss: 1.344512 	 time: 0.3
Epoch: 2278 	Training Loss: 1.150405 	Validation Loss: 1.344503 	 time: 0.3
Epoch: 2279 	Training Loss: 1.150403 	Validation Loss: 1.344516 	 time: 0.3
Epoch: 2280 	Training Loss: 1.150402 	Validation Loss: 1.344540 	 time: 0.3
Epoch: 2281 	Training Loss: 1.150400 	Validation Loss: 1.344559 	 time: 0.3
Epoch: 2282 	Training Loss: 1.150398 	Validation Loss: 1.344570 	 time: 0.3
Epoch: 2283 	Training Loss: 1.150397 	Validation Loss: 1.344577 	 time: 0.3
Epoch: 2284 	Training Loss: 1.150395 	Validation Loss: 1.344591 	 time: 0.3
Epoch: 2285 	Training Loss: 1.150393 	Validation Loss: 1.344610 	 time: 0.3
Epoch: 2286 	Training Loss: 1.150392 	Validation Loss: 1.344624 	 time: 0.3
Epoch: 2287 	Training Loss: 1.150390 	Validation Loss: 1.344629 	 time: 0.3
Epoch: 2288 	Training Loss: 1.150388 	Validation Loss: 1.344629 	 time: 0.3
Epoch: 2289 	Training Loss: 1.150387 	Validation Loss: 1.344632 	 time: 0.3
Epoch: 2290 	Training Loss: 1.150386 	Validation Loss: 1.344642 	 time: 0.3
Epoch: 2291 	Training Loss: 1.150384 	Validation Loss: 1.344656 	 time: 0.3
Epoch: 2292 	Training Loss: 1.150382 	Validation Loss: 1.344672 	 time: 0.3
Epoch: 2293 	Training Loss: 1.150381 	Validation Loss: 1.344686 	 time: 0.3
Epoch: 2294 	Training Loss: 1.150380 	Validation Loss: 1.344699 	 time: 0.3
Epoch: 2295 	Training Loss: 1.150378 	Validation Loss: 1.344710 	 time: 0.3
Epoch: 2296 	Training Loss: 1.150376 	Validation Loss: 1.344717 	 time: 0.3
Epoch: 2297 	Training Loss: 1.150375 	Validation Loss: 1.344720 	 time: 0.3
Epoch: 2298 	Training Loss: 1.150374 	Validation Loss: 1.344721 	 time: 0.3
Epoch: 2299 	Training Loss: 1.150372 	Validation Loss: 1.344723 	 time: 0.3
Epoch: 2300 	Training Loss: 1.150370 	Validation Loss: 1.344726 	 time: 0.3
Epoch: 2301 	Training Loss: 1.150369 	Validation Loss: 1.344727 	 time: 0.3
Epoch: 2302 	Training Loss: 1.150367 	Validation Loss: 1.344725 	 time: 0.3
Epoch: 2303 	Training Loss: 1.150366 	Validation Loss: 1.344722 	 time: 0.3
Epoch: 2304 	Training Loss: 1.150364 	Validation Loss: 1.344723 	 time: 0.3
Epoch: 2305 	Training Loss: 1.150363 	Validation Loss: 1.344730 	 time: 0.3
Epoch: 2306 	Training Loss: 1.150361 	Validation Loss: 1.344741 	 time: 0.3
Epoch: 2307 	Training Loss: 1.150360 	Validation Loss: 1.344754 	 time: 0.3
Epoch: 2308 	Training Loss: 1.150358 	Validation Loss: 1.344766 	 time: 0.3
Epoch: 2309 	Training Loss: 1.150357 	Validation Loss: 1.344779 	 time: 0.3
Epoch: 2310 	Training Loss: 1.150355 	Validation Loss: 1.344793 	 time: 0.3
Epoch: 2311 	Training Loss: 1.150353 	Validation Loss: 1.344805 	 time: 0.3
Epoch: 2312 	Training Loss: 1.150352 	Validation Loss: 1.344814 	 time: 0.3
Epoch: 2313 	Training Loss: 1.150350 	Validation Loss: 1.344822 	 time: 0.3
Epoch: 2314 	Training Loss: 1.150348 	Validation Loss: 1.344832 	 time: 0.3
Epoch: 2315 	Training Loss: 1.150347 	Validation Loss: 1.344846 	 time: 0.3
Epoch: 2316 	Training Loss: 1.150345 	Validation Loss: 1.344861 	 time: 0.3
Epoch: 2317 	Training Loss: 1.150344 	Validation Loss: 1.344876 	 time: 0.3
Epoch: 2318 	Training Loss: 1.150342 	Validation Loss: 1.344892 	 time: 0.3
Epoch: 2319 	Training Loss: 1.150340 	Validation Loss: 1.344910 	 time: 0.3
Epoch: 2320 	Training Loss: 1.150338 	Validation Loss: 1.344932 	 time: 0.3
Epoch: 2321 	Training Loss: 1.150336 	Validation Loss: 1.344957 	 time: 0.3
Epoch: 2322 	Training Loss: 1.150334 	Validation Loss: 1.344983 	 time: 0.3
Epoch: 2323 	Training Loss: 1.150332 	Validation Loss: 1.345011 	 time: 0.3
Epoch: 2324 	Training Loss: 1.150329 	Validation Loss: 1.345041 	 time: 0.3
Epoch: 2325 	Training Loss: 1.150326 	Validation Loss: 1.345076 	 time: 0.3
Epoch: 2326 	Training Loss: 1.150323 	Validation Loss: 1.345116 	 time: 0.3
Epoch: 2327 	Training Loss: 1.150319 	Validation Loss: 1.345162 	 time: 0.3
Epoch: 2328 	Training Loss: 1.150314 	Validation Loss: 1.345215 	 time: 0.3
Epoch: 2329 	Training Loss: 1.150307 	Validation Loss: 1.345280 	 time: 0.3
Epoch: 2330 	Training Loss: 1.150298 	Validation Loss: 1.345360 	 time: 0.3
Epoch: 2331 	Training Loss: 1.150285 	Validation Loss: 1.345462 	 time: 0.3
Epoch: 2332 	Training Loss: 1.150266 	Validation Loss: 1.345590 	 time: 0.3
Epoch: 2333 	Training Loss: 1.150243 	Validation Loss: 1.345738 	 time: 0.3
Epoch: 2334 	Training Loss: 1.150224 	Validation Loss: 1.345892 	 time: 0.3
Epoch: 2335 	Training Loss: 1.150216 	Validation Loss: 1.346055 	 time: 0.3
Epoch: 2336 	Training Loss: 1.150211 	Validation Loss: 1.346227 	 time: 0.3
Epoch: 2337 	Training Loss: 1.150208 	Validation Loss: 1.346382 	 time: 0.3
Epoch: 2338 	Training Loss: 1.150202 	Validation Loss: 1.346496 	 time: 0.3
Epoch: 2339 	Training Loss: 1.150193 	Validation Loss: 1.346571 	 time: 0.3
Epoch: 2340 	Training Loss: 1.150172 	Validation Loss: 1.346612 	 time: 0.3
Epoch: 2341 	Training Loss: 1.150117 	Validation Loss: 1.346607 	 time: 0.3
Epoch: 2342 	Training Loss: 1.150069 	Validation Loss: 1.346546 	 time: 0.3
Epoch: 2343 	Training Loss: 1.150054 	Validation Loss: 1.346547 	 time: 0.3
Epoch: 2344 	Training Loss: 1.150054 	Validation Loss: 1.346595 	 time: 0.3
Epoch: 2345 	Training Loss: 1.150048 	Validation Loss: 1.346575 	 time: 0.3
Epoch: 2346 	Training Loss: 1.150049 	Validation Loss: 1.346440 	 time: 0.3
Epoch: 2347 	Training Loss: 1.150044 	Validation Loss: 1.346320 	 time: 0.3
Epoch: 2348 	Training Loss: 1.150044 	Validation Loss: 1.346260 	 time: 0.3
Epoch: 2349 	Training Loss: 1.150039 	Validation Loss: 1.346210 	 time: 0.3
Epoch: 2350 	Training Loss: 1.150037 	Validation Loss: 1.346111 	 time: 0.3
Epoch: 2351 	Training Loss: 1.150031 	Validation Loss: 1.345989 	 time: 0.3
Epoch: 2352 	Training Loss: 1.150029 	Validation Loss: 1.345888 	 time: 0.3
Epoch: 2353 	Training Loss: 1.150025 	Validation Loss: 1.345806 	 time: 0.3
Epoch: 2354 	Training Loss: 1.150019 	Validation Loss: 1.345686 	 time: 0.3
Epoch: 2355 	Training Loss: 1.150012 	Validation Loss: 1.345548 	 time: 0.3
Epoch: 2356 	Training Loss: 1.150003 	Validation Loss: 1.345454 	 time: 0.3
Epoch: 2357 	Training Loss: 1.149988 	Validation Loss: 1.345408 	 time: 0.3
Epoch: 2358 	Training Loss: 1.149971 	Validation Loss: 1.345341 	 time: 0.3
Epoch: 2359 	Training Loss: 1.149962 	Validation Loss: 1.345205 	 time: 0.3
Epoch: 2360 	Training Loss: 1.149956 	Validation Loss: 1.345026 	 time: 0.3
Epoch: 2361 	Training Loss: 1.149952 	Validation Loss: 1.344839 	 time: 0.3
Epoch: 2362 	Training Loss: 1.149949 	Validation Loss: 1.344675 	 time: 0.3
Epoch: 2363 	Training Loss: 1.149947 	Validation Loss: 1.344551 	 time: 0.3
Epoch: 2364 	Training Loss: 1.149944 	Validation Loss: 1.344470 	 time: 0.3
Epoch: 2365 	Training Loss: 1.149942 	Validation Loss: 1.344409 	 time: 0.3
Epoch: 2366 	Training Loss: 1.149939 	Validation Loss: 1.344356 	 time: 0.3
Epoch: 2367 	Training Loss: 1.149937 	Validation Loss: 1.344312 	 time: 0.3
Epoch: 2368 	Training Loss: 1.149934 	Validation Loss: 1.344285 	 time: 0.3
Epoch: 2369 	Training Loss: 1.149932 	Validation Loss: 1.344279 	 time: 0.3
Epoch: 2370 	Training Loss: 1.149929 	Validation Loss: 1.344296 	 time: 0.3
Epoch: 2371 	Training Loss: 1.149927 	Validation Loss: 1.344326 	 time: 0.3
Epoch: 2372 	Training Loss: 1.149925 	Validation Loss: 1.344360 	 time: 0.3
Epoch: 2373 	Training Loss: 1.149923 	Validation Loss: 1.344386 	 time: 0.3
Epoch: 2374 	Training Loss: 1.149920 	Validation Loss: 1.344402 	 time: 0.3
Epoch: 2375 	Training Loss: 1.149917 	Validation Loss: 1.344421 	 time: 0.3
Epoch: 2376 	Training Loss: 1.149914 	Validation Loss: 1.344453 	 time: 0.3
Epoch: 2377 	Training Loss: 1.149912 	Validation Loss: 1.344502 	 time: 0.3
Epoch: 2378 	Training Loss: 1.149909 	Validation Loss: 1.344566 	 time: 0.3
Epoch: 2379 	Training Loss: 1.149906 	Validation Loss: 1.344628 	 time: 0.3
Epoch: 2380 	Training Loss: 1.149904 	Validation Loss: 1.344676 	 time: 0.3
Epoch: 2381 	Training Loss: 1.149900 	Validation Loss: 1.344713 	 time: 0.3
Epoch: 2382 	Training Loss: 1.149896 	Validation Loss: 1.344749 	 time: 0.3
Epoch: 2383 	Training Loss: 1.149893 	Validation Loss: 1.344790 	 time: 0.3
Epoch: 2384 	Training Loss: 1.149890 	Validation Loss: 1.344833 	 time: 0.3
Epoch: 2385 	Training Loss: 1.149887 	Validation Loss: 1.344873 	 time: 0.3
Epoch: 2386 	Training Loss: 1.149883 	Validation Loss: 1.344901 	 time: 0.3
Epoch: 2387 	Training Loss: 1.149879 	Validation Loss: 1.344918 	 time: 0.3
Epoch: 2388 	Training Loss: 1.149875 	Validation Loss: 1.344926 	 time: 0.3
Epoch: 2389 	Training Loss: 1.149870 	Validation Loss: 1.344933 	 time: 0.3
Epoch: 2390 	Training Loss: 1.149863 	Validation Loss: 1.344939 	 time: 0.3
Epoch: 2391 	Training Loss: 1.149851 	Validation Loss: 1.344937 	 time: 0.3
Epoch: 2392 	Training Loss: 1.149835 	Validation Loss: 1.344930 	 time: 0.3
Epoch: 2393 	Training Loss: 1.149817 	Validation Loss: 1.344931 	 time: 0.3
Epoch: 2394 	Training Loss: 1.149802 	Validation Loss: 1.344950 	 time: 0.3
Epoch: 2395 	Training Loss: 1.149794 	Validation Loss: 1.344985 	 time: 0.3
Epoch: 2396 	Training Loss: 1.149788 	Validation Loss: 1.345025 	 time: 0.3
Epoch: 2397 	Training Loss: 1.149784 	Validation Loss: 1.345062 	 time: 0.3
Epoch: 2398 	Training Loss: 1.149780 	Validation Loss: 1.345087 	 time: 0.3
Epoch: 2399 	Training Loss: 1.149776 	Validation Loss: 1.345091 	 time: 0.3
Epoch: 2400 	Training Loss: 1.149771 	Validation Loss: 1.345070 	 time: 0.3
Epoch: 2401 	Training Loss: 1.149767 	Validation Loss: 1.345035 	 time: 0.3
Epoch: 2402 	Training Loss: 1.149762 	Validation Loss: 1.345007 	 time: 0.3
Epoch: 2403 	Training Loss: 1.149757 	Validation Loss: 1.344998 	 time: 0.3
Epoch: 2404 	Training Loss: 1.149753 	Validation Loss: 1.345005 	 time: 0.3
Epoch: 2405 	Training Loss: 1.149748 	Validation Loss: 1.345022 	 time: 0.3
Epoch: 2406 	Training Loss: 1.149743 	Validation Loss: 1.345040 	 time: 0.3
Epoch: 2407 	Training Loss: 1.149736 	Validation Loss: 1.345052 	 time: 0.3
Epoch: 2408 	Training Loss: 1.149727 	Validation Loss: 1.345044 	 time: 0.3
Epoch: 2409 	Training Loss: 1.149711 	Validation Loss: 1.345004 	 time: 0.3
Epoch: 2410 	Training Loss: 1.149687 	Validation Loss: 1.344948 	 time: 0.3
Epoch: 2411 	Training Loss: 1.149663 	Validation Loss: 1.344925 	 time: 0.3
Epoch: 2412 	Training Loss: 1.149653 	Validation Loss: 1.344947 	 time: 0.3
Epoch: 2413 	Training Loss: 1.149650 	Validation Loss: 1.345001 	 time: 0.3
Epoch: 2414 	Training Loss: 1.149648 	Validation Loss: 1.345085 	 time: 0.3
Epoch: 2415 	Training Loss: 1.149646 	Validation Loss: 1.345195 	 time: 0.3
Epoch: 2416 	Training Loss: 1.149644 	Validation Loss: 1.345299 	 time: 0.3
Epoch: 2417 	Training Loss: 1.149642 	Validation Loss: 1.345371 	 time: 0.3
Epoch: 2418 	Training Loss: 1.149639 	Validation Loss: 1.345410 	 time: 0.3
Epoch: 2419 	Training Loss: 1.149635 	Validation Loss: 1.345438 	 time: 0.3
Epoch: 2420 	Training Loss: 1.149631 	Validation Loss: 1.345464 	 time: 0.3
Epoch: 2421 	Training Loss: 1.149628 	Validation Loss: 1.345495 	 time: 0.3
Epoch: 2422 	Training Loss: 1.149624 	Validation Loss: 1.345529 	 time: 0.3
Epoch: 2423 	Training Loss: 1.149622 	Validation Loss: 1.345563 	 time: 0.3
Epoch: 2424 	Training Loss: 1.149619 	Validation Loss: 1.345592 	 time: 0.3
Epoch: 2425 	Training Loss: 1.149616 	Validation Loss: 1.345614 	 time: 0.3
Epoch: 2426 	Training Loss: 1.149613 	Validation Loss: 1.345624 	 time: 0.3
Epoch: 2427 	Training Loss: 1.149610 	Validation Loss: 1.345607 	 time: 0.3
Epoch: 2428 	Training Loss: 1.149607 	Validation Loss: 1.345552 	 time: 0.3
Epoch: 2429 	Training Loss: 1.149604 	Validation Loss: 1.345476 	 time: 0.3
Epoch: 2430 	Training Loss: 1.149601 	Validation Loss: 1.345409 	 time: 0.3
Epoch: 2431 	Training Loss: 1.149597 	Validation Loss: 1.345366 	 time: 0.3
Epoch: 2432 	Training Loss: 1.149593 	Validation Loss: 1.345335 	 time: 0.3
Epoch: 2433 	Training Loss: 1.149588 	Validation Loss: 1.345306 	 time: 0.3
Epoch: 2434 	Training Loss: 1.149582 	Validation Loss: 1.345278 	 time: 0.3
Epoch: 2435 	Training Loss: 1.149576 	Validation Loss: 1.345253 	 time: 0.3
Epoch: 2436 	Training Loss: 1.149570 	Validation Loss: 1.345229 	 time: 0.3
Epoch: 2437 	Training Loss: 1.149564 	Validation Loss: 1.345205 	 time: 0.3
Epoch: 2438 	Training Loss: 1.149559 	Validation Loss: 1.345184 	 time: 0.3
Epoch: 2439 	Training Loss: 1.149556 	Validation Loss: 1.345165 	 time: 0.3
Epoch: 2440 	Training Loss: 1.149553 	Validation Loss: 1.345145 	 time: 0.3
Epoch: 2441 	Training Loss: 1.149550 	Validation Loss: 1.345121 	 time: 0.3
Epoch: 2442 	Training Loss: 1.149548 	Validation Loss: 1.345097 	 time: 0.3
Epoch: 2443 	Training Loss: 1.149546 	Validation Loss: 1.345075 	 time: 0.4
Epoch: 2444 	Training Loss: 1.149543 	Validation Loss: 1.345055 	 time: 0.3
Epoch: 2445 	Training Loss: 1.149541 	Validation Loss: 1.345038 	 time: 0.3
Epoch: 2446 	Training Loss: 1.149538 	Validation Loss: 1.345021 	 time: 0.3
Epoch: 2447 	Training Loss: 1.149535 	Validation Loss: 1.345003 	 time: 0.3
Epoch: 2448 	Training Loss: 1.149533 	Validation Loss: 1.344987 	 time: 0.3
Epoch: 2449 	Training Loss: 1.149530 	Validation Loss: 1.344971 	 time: 0.3
Epoch: 2450 	Training Loss: 1.149526 	Validation Loss: 1.344955 	 time: 0.3
Epoch: 2451 	Training Loss: 1.149524 	Validation Loss: 1.344937 	 time: 0.3
Epoch: 2452 	Training Loss: 1.149520 	Validation Loss: 1.344916 	 time: 0.3
Epoch: 2453 	Training Loss: 1.149516 	Validation Loss: 1.344894 	 time: 0.3
Epoch: 2454 	Training Loss: 1.149513 	Validation Loss: 1.344874 	 time: 0.3
Epoch: 2455 	Training Loss: 1.149509 	Validation Loss: 1.344857 	 time: 0.3
Epoch: 2456 	Training Loss: 1.149506 	Validation Loss: 1.344839 	 time: 0.3
Epoch: 2457 	Training Loss: 1.149502 	Validation Loss: 1.344822 	 time: 0.3
Epoch: 2458 	Training Loss: 1.149499 	Validation Loss: 1.344807 	 time: 0.3
Epoch: 2459 	Training Loss: 1.149496 	Validation Loss: 1.344794 	 time: 0.3
Epoch: 2460 	Training Loss: 1.149492 	Validation Loss: 1.344783 	 time: 0.3
Epoch: 2461 	Training Loss: 1.149490 	Validation Loss: 1.344772 	 time: 0.3
Epoch: 2462 	Training Loss: 1.149486 	Validation Loss: 1.344763 	 time: 0.3
Epoch: 2463 	Training Loss: 1.149483 	Validation Loss: 1.344753 	 time: 0.3
Epoch: 2464 	Training Loss: 1.149480 	Validation Loss: 1.344744 	 time: 0.3
Epoch: 2465 	Training Loss: 1.149477 	Validation Loss: 1.344737 	 time: 0.3
Epoch: 2466 	Training Loss: 1.149474 	Validation Loss: 1.344731 	 time: 0.3
Epoch: 2467 	Training Loss: 1.149471 	Validation Loss: 1.344726 	 time: 0.3
Epoch: 2468 	Training Loss: 1.149468 	Validation Loss: 1.344721 	 time: 0.3
Epoch: 2469 	Training Loss: 1.149464 	Validation Loss: 1.344717 	 time: 0.3
Epoch: 2470 	Training Loss: 1.149461 	Validation Loss: 1.344714 	 time: 0.3
Epoch: 2471 	Training Loss: 1.149457 	Validation Loss: 1.344714 	 time: 0.3
Epoch: 2472 	Training Loss: 1.149453 	Validation Loss: 1.344714 	 time: 0.3
Epoch: 2473 	Training Loss: 1.149449 	Validation Loss: 1.344715 	 time: 0.3
Epoch: 2474 	Training Loss: 1.149445 	Validation Loss: 1.344715 	 time: 0.3
Epoch: 2475 	Training Loss: 1.149441 	Validation Loss: 1.344714 	 time: 0.3
Epoch: 2476 	Training Loss: 1.149437 	Validation Loss: 1.344714 	 time: 0.3
Epoch: 2477 	Training Loss: 1.149433 	Validation Loss: 1.344715 	 time: 0.3
Epoch: 2478 	Training Loss: 1.149430 	Validation Loss: 1.344717 	 time: 0.3
Epoch: 2479 	Training Loss: 1.149426 	Validation Loss: 1.344719 	 time: 0.3
Epoch: 2480 	Training Loss: 1.149423 	Validation Loss: 1.344720 	 time: 0.3
Epoch: 2481 	Training Loss: 1.149419 	Validation Loss: 1.344721 	 time: 0.3
Epoch: 2482 	Training Loss: 1.149416 	Validation Loss: 1.344722 	 time: 0.3
Epoch: 2483 	Training Loss: 1.149413 	Validation Loss: 1.344721 	 time: 0.3
Epoch: 2484 	Training Loss: 1.149409 	Validation Loss: 1.344718 	 time: 0.3
Epoch: 2485 	Training Loss: 1.149406 	Validation Loss: 1.344714 	 time: 0.3
Epoch: 2486 	Training Loss: 1.149402 	Validation Loss: 1.344709 	 time: 0.3
Epoch: 2487 	Training Loss: 1.149399 	Validation Loss: 1.344703 	 time: 0.3
Epoch: 2488 	Training Loss: 1.149395 	Validation Loss: 1.344698 	 time: 0.3
Epoch: 2489 	Training Loss: 1.149392 	Validation Loss: 1.344694 	 time: 0.3
Epoch: 2490 	Training Loss: 1.149388 	Validation Loss: 1.344691 	 time: 0.3
Epoch: 2491 	Training Loss: 1.149385 	Validation Loss: 1.344690 	 time: 0.3
Epoch: 2492 	Training Loss: 1.149382 	Validation Loss: 1.344691 	 time: 0.3
Epoch: 2493 	Training Loss: 1.149379 	Validation Loss: 1.344695 	 time: 0.3
Epoch: 2494 	Training Loss: 1.149375 	Validation Loss: 1.344703 	 time: 0.3
Epoch: 2495 	Training Loss: 1.149371 	Validation Loss: 1.344715 	 time: 0.3
Epoch: 2496 	Training Loss: 1.149365 	Validation Loss: 1.344732 	 time: 0.3
Epoch: 2497 	Training Loss: 1.149355 	Validation Loss: 1.344753 	 time: 0.3
Epoch: 2498 	Training Loss: 1.149337 	Validation Loss: 1.344772 	 time: 0.3
Epoch: 2499 	Training Loss: 1.149314 	Validation Loss: 1.344778 	 time: 0.3
Epoch: 2500 	Training Loss: 1.149295 	Validation Loss: 1.344768 	 time: 0.3
Epoch: 2501 	Training Loss: 1.149279 	Validation Loss: 1.344762 	 time: 0.3
Epoch: 2502 	Training Loss: 1.149269 	Validation Loss: 1.344765 	 time: 0.3
Epoch: 2503 	Training Loss: 1.149260 	Validation Loss: 1.344765 	 time: 0.3
Epoch: 2504 	Training Loss: 1.149250 	Validation Loss: 1.344750 	 time: 0.3
Epoch: 2505 	Training Loss: 1.149235 	Validation Loss: 1.344738 	 time: 0.3
Epoch: 2506 	Training Loss: 1.149217 	Validation Loss: 1.344757 	 time: 0.3
Epoch: 2507 	Training Loss: 1.149199 	Validation Loss: 1.344811 	 time: 0.3
Epoch: 2508 	Training Loss: 1.149190 	Validation Loss: 1.344884 	 time: 0.3
Epoch: 2509 	Training Loss: 1.149184 	Validation Loss: 1.344977 	 time: 0.3
Epoch: 2510 	Training Loss: 1.149179 	Validation Loss: 1.345090 	 time: 0.3
Epoch: 2511 	Training Loss: 1.149171 	Validation Loss: 1.345192 	 time: 0.3
Epoch: 2512 	Training Loss: 1.149161 	Validation Loss: 1.345244 	 time: 0.3
Epoch: 2513 	Training Loss: 1.149151 	Validation Loss: 1.345251 	 time: 0.3
Epoch: 2514 	Training Loss: 1.149146 	Validation Loss: 1.345253 	 time: 0.3
Epoch: 2515 	Training Loss: 1.149143 	Validation Loss: 1.345273 	 time: 0.3
Epoch: 2516 	Training Loss: 1.149140 	Validation Loss: 1.345305 	 time: 0.3
Epoch: 2517 	Training Loss: 1.149137 	Validation Loss: 1.345324 	 time: 0.3
Epoch: 2518 	Training Loss: 1.149135 	Validation Loss: 1.345311 	 time: 0.3
Epoch: 2519 	Training Loss: 1.149132 	Validation Loss: 1.345266 	 time: 0.3
Epoch: 2520 	Training Loss: 1.149128 	Validation Loss: 1.345205 	 time: 0.3
Epoch: 2521 	Training Loss: 1.149125 	Validation Loss: 1.345143 	 time: 0.3
Epoch: 2522 	Training Loss: 1.149121 	Validation Loss: 1.345092 	 time: 0.3
Epoch: 2523 	Training Loss: 1.149117 	Validation Loss: 1.345054 	 time: 0.3
Epoch: 2524 	Training Loss: 1.149113 	Validation Loss: 1.345026 	 time: 0.3
Epoch: 2525 	Training Loss: 1.149109 	Validation Loss: 1.344998 	 time: 0.3
Epoch: 2526 	Training Loss: 1.149105 	Validation Loss: 1.344969 	 time: 0.3
Epoch: 2527 	Training Loss: 1.149100 	Validation Loss: 1.344957 	 time: 0.3
Epoch: 2528 	Training Loss: 1.149096 	Validation Loss: 1.344972 	 time: 0.3
Epoch: 2529 	Training Loss: 1.149091 	Validation Loss: 1.345004 	 time: 0.3
Epoch: 2530 	Training Loss: 1.149087 	Validation Loss: 1.345034 	 time: 0.3
Epoch: 2531 	Training Loss: 1.149082 	Validation Loss: 1.345057 	 time: 0.3
Epoch: 2532 	Training Loss: 1.149077 	Validation Loss: 1.345078 	 time: 0.3
Epoch: 2533 	Training Loss: 1.149072 	Validation Loss: 1.345097 	 time: 0.3
Epoch: 2534 	Training Loss: 1.149066 	Validation Loss: 1.345112 	 time: 0.3
Epoch: 2535 	Training Loss: 1.149057 	Validation Loss: 1.345125 	 time: 0.3
Epoch: 2536 	Training Loss: 1.149043 	Validation Loss: 1.345135 	 time: 0.3
Epoch: 2537 	Training Loss: 1.149027 	Validation Loss: 1.345134 	 time: 0.3
Epoch: 2538 	Training Loss: 1.149019 	Validation Loss: 1.345117 	 time: 0.3
Epoch: 2539 	Training Loss: 1.149015 	Validation Loss: 1.345083 	 time: 0.3
Epoch: 2540 	Training Loss: 1.149012 	Validation Loss: 1.345036 	 time: 0.3
Epoch: 2541 	Training Loss: 1.149010 	Validation Loss: 1.344987 	 time: 0.4
Epoch: 2542 	Training Loss: 1.149007 	Validation Loss: 1.344943 	 time: 0.3
Epoch: 2543 	Training Loss: 1.149005 	Validation Loss: 1.344903 	 time: 0.3
Epoch: 2544 	Training Loss: 1.149002 	Validation Loss: 1.344860 	 time: 0.3
Epoch: 2545 	Training Loss: 1.148998 	Validation Loss: 1.344810 	 time: 0.3
Epoch: 2546 	Training Loss: 1.148993 	Validation Loss: 1.344762 	 time: 0.3
Epoch: 2547 	Training Loss: 1.148985 	Validation Loss: 1.344728 	 time: 0.3
Epoch: 2548 	Training Loss: 1.148974 	Validation Loss: 1.344689 	 time: 0.3
Epoch: 2549 	Training Loss: 1.148968 	Validation Loss: 1.344636 	 time: 0.3
Epoch: 2550 	Training Loss: 1.148965 	Validation Loss: 1.344588 	 time: 0.3
Epoch: 2551 	Training Loss: 1.148963 	Validation Loss: 1.344554 	 time: 0.3
Epoch: 2552 	Training Loss: 1.148959 	Validation Loss: 1.344516 	 time: 0.3
Epoch: 2553 	Training Loss: 1.148955 	Validation Loss: 1.344473 	 time: 0.3
Epoch: 2554 	Training Loss: 1.148952 	Validation Loss: 1.344435 	 time: 0.3
Epoch: 2555 	Training Loss: 1.148949 	Validation Loss: 1.344403 	 time: 0.3
Epoch: 2556 	Training Loss: 1.148946 	Validation Loss: 1.344365 	 time: 0.3
Epoch: 2557 	Training Loss: 1.148944 	Validation Loss: 1.344325 	 time: 0.3
Epoch: 2558 	Training Loss: 1.148941 	Validation Loss: 1.344307 	 time: 0.3
Epoch: 2559 	Training Loss: 1.148938 	Validation Loss: 1.344323 	 time: 0.3
Epoch: 2560 	Training Loss: 1.148934 	Validation Loss: 1.344357 	 time: 0.3
Epoch: 2561 	Training Loss: 1.148931 	Validation Loss: 1.344379 	 time: 0.3
Epoch: 2562 	Training Loss: 1.148928 	Validation Loss: 1.344383 	 time: 0.3
Epoch: 2563 	Training Loss: 1.148924 	Validation Loss: 1.344391 	 time: 0.3
Epoch: 2564 	Training Loss: 1.148920 	Validation Loss: 1.344414 	 time: 0.3
Epoch: 2565 	Training Loss: 1.148916 	Validation Loss: 1.344435 	 time: 0.3
Epoch: 2566 	Training Loss: 1.148912 	Validation Loss: 1.344434 	 time: 0.3
Epoch: 2567 	Training Loss: 1.148908 	Validation Loss: 1.344425 	 time: 0.3
Epoch: 2568 	Training Loss: 1.148904 	Validation Loss: 1.344434 	 time: 0.3
Epoch: 2569 	Training Loss: 1.148900 	Validation Loss: 1.344463 	 time: 0.3
Epoch: 2570 	Training Loss: 1.148897 	Validation Loss: 1.344495 	 time: 0.3
Epoch: 2571 	Training Loss: 1.148894 	Validation Loss: 1.344525 	 time: 0.3
Epoch: 2572 	Training Loss: 1.148891 	Validation Loss: 1.344555 	 time: 0.3
Epoch: 2573 	Training Loss: 1.148889 	Validation Loss: 1.344584 	 time: 0.3
Epoch: 2574 	Training Loss: 1.148886 	Validation Loss: 1.344604 	 time: 0.3
Epoch: 2575 	Training Loss: 1.148884 	Validation Loss: 1.344614 	 time: 0.3
Epoch: 2576 	Training Loss: 1.148882 	Validation Loss: 1.344623 	 time: 0.3
Epoch: 2577 	Training Loss: 1.148880 	Validation Loss: 1.344641 	 time: 0.3
Epoch: 2578 	Training Loss: 1.148878 	Validation Loss: 1.344667 	 time: 0.3
Epoch: 2579 	Training Loss: 1.148876 	Validation Loss: 1.344690 	 time: 0.3
Epoch: 2580 	Training Loss: 1.148875 	Validation Loss: 1.344708 	 time: 0.3
Epoch: 2581 	Training Loss: 1.148873 	Validation Loss: 1.344725 	 time: 0.3
Epoch: 2582 	Training Loss: 1.148871 	Validation Loss: 1.344740 	 time: 0.3
Epoch: 2583 	Training Loss: 1.148870 	Validation Loss: 1.344749 	 time: 0.3
Epoch: 2584 	Training Loss: 1.148868 	Validation Loss: 1.344755 	 time: 0.3
Epoch: 2585 	Training Loss: 1.148866 	Validation Loss: 1.344760 	 time: 0.3
Epoch: 2586 	Training Loss: 1.148864 	Validation Loss: 1.344765 	 time: 0.3
Epoch: 2587 	Training Loss: 1.148862 	Validation Loss: 1.344767 	 time: 0.3
Epoch: 2588 	Training Loss: 1.148859 	Validation Loss: 1.344766 	 time: 0.3
Epoch: 2589 	Training Loss: 1.148856 	Validation Loss: 1.344764 	 time: 0.3
Epoch: 2590 	Training Loss: 1.148852 	Validation Loss: 1.344759 	 time: 0.3
Epoch: 2591 	Training Loss: 1.148849 	Validation Loss: 1.344747 	 time: 0.3
Epoch: 2592 	Training Loss: 1.148846 	Validation Loss: 1.344730 	 time: 0.3
Epoch: 2593 	Training Loss: 1.148844 	Validation Loss: 1.344718 	 time: 0.3
Epoch: 2594 	Training Loss: 1.148842 	Validation Loss: 1.344717 	 time: 0.3
Epoch: 2595 	Training Loss: 1.148840 	Validation Loss: 1.344720 	 time: 0.3
Epoch: 2596 	Training Loss: 1.148837 	Validation Loss: 1.344715 	 time: 0.3
Epoch: 2597 	Training Loss: 1.148834 	Validation Loss: 1.344694 	 time: 0.3
Epoch: 2598 	Training Loss: 1.148831 	Validation Loss: 1.344657 	 time: 0.3
Epoch: 2599 	Training Loss: 1.148829 	Validation Loss: 1.344605 	 time: 0.3
Epoch: 2600 	Training Loss: 1.148826 	Validation Loss: 1.344542 	 time: 0.3
Epoch: 2601 	Training Loss: 1.148824 	Validation Loss: 1.344471 	 time: 0.3
Epoch: 2602 	Training Loss: 1.148821 	Validation Loss: 1.344386 	 time: 0.3
Epoch: 2603 	Training Loss: 1.148818 	Validation Loss: 1.344282 	 time: 0.3
Epoch: 2604 	Training Loss: 1.148814 	Validation Loss: 1.344152 	 time: 0.3
Epoch: 2605 	Training Loss: 1.148808 	Validation Loss: 1.343996 	 time: 0.3
Epoch: 2606 	Training Loss: 1.148802 	Validation Loss: 1.343820 	 time: 0.3
Epoch: 2607 	Training Loss: 1.148795 	Validation Loss: 1.343638 	 time: 0.3
Epoch: 2608 	Training Loss: 1.148789 	Validation Loss: 1.343473 	 time: 0.3
Epoch: 2609 	Training Loss: 1.148785 	Validation Loss: 1.343345 	 time: 0.3
Epoch: 2610 	Training Loss: 1.148781 	Validation Loss: 1.343265 	 time: 0.3
Epoch: 2611 	Training Loss: 1.148778 	Validation Loss: 1.343230 	 time: 0.3
Epoch: 2612 	Training Loss: 1.148774 	Validation Loss: 1.343228 	 time: 0.3
Epoch: 2613 	Training Loss: 1.148769 	Validation Loss: 1.343249 	 time: 0.3
Epoch: 2614 	Training Loss: 1.148763 	Validation Loss: 1.343289 	 time: 0.3
Epoch: 2615 	Training Loss: 1.148758 	Validation Loss: 1.343349 	 time: 0.3
Epoch: 2616 	Training Loss: 1.148755 	Validation Loss: 1.343427 	 time: 0.3
Epoch: 2617 	Training Loss: 1.148752 	Validation Loss: 1.343521 	 time: 0.3
Epoch: 2618 	Training Loss: 1.148749 	Validation Loss: 1.343625 	 time: 0.3
Epoch: 2619 	Training Loss: 1.148747 	Validation Loss: 1.343732 	 time: 0.3
Epoch: 2620 	Training Loss: 1.148745 	Validation Loss: 1.343838 	 time: 0.3
Epoch: 2621 	Training Loss: 1.148743 	Validation Loss: 1.343941 	 time: 0.3
Epoch: 2622 	Training Loss: 1.148741 	Validation Loss: 1.344039 	 time: 0.3
Epoch: 2623 	Training Loss: 1.148739 	Validation Loss: 1.344133 	 time: 0.3
Epoch: 2624 	Training Loss: 1.148738 	Validation Loss: 1.344217 	 time: 0.3
Epoch: 2625 	Training Loss: 1.148736 	Validation Loss: 1.344286 	 time: 0.3
Epoch: 2626 	Training Loss: 1.148734 	Validation Loss: 1.344337 	 time: 0.3
Epoch: 2627 	Training Loss: 1.148733 	Validation Loss: 1.344372 	 time: 0.3
Epoch: 2628 	Training Loss: 1.148731 	Validation Loss: 1.344401 	 time: 0.3
Epoch: 2629 	Training Loss: 1.148729 	Validation Loss: 1.344431 	 time: 0.3
Epoch: 2630 	Training Loss: 1.148727 	Validation Loss: 1.344466 	 time: 0.3
Epoch: 2631 	Training Loss: 1.148725 	Validation Loss: 1.344501 	 time: 0.3
Epoch: 2632 	Training Loss: 1.148723 	Validation Loss: 1.344530 	 time: 0.3
Epoch: 2633 	Training Loss: 1.148721 	Validation Loss: 1.344552 	 time: 0.3
Epoch: 2634 	Training Loss: 1.148718 	Validation Loss: 1.344570 	 time: 0.3
Epoch: 2635 	Training Loss: 1.148716 	Validation Loss: 1.344588 	 time: 0.3
Epoch: 2636 	Training Loss: 1.148714 	Validation Loss: 1.344607 	 time: 0.3
Epoch: 2637 	Training Loss: 1.148710 	Validation Loss: 1.344630 	 time: 0.3
Epoch: 2638 	Training Loss: 1.148706 	Validation Loss: 1.344656 	 time: 0.3
Epoch: 2639 	Training Loss: 1.148702 	Validation Loss: 1.344687 	 time: 0.3
Epoch: 2640 	Training Loss: 1.148696 	Validation Loss: 1.344721 	 time: 0.3
Epoch: 2641 	Training Loss: 1.148689 	Validation Loss: 1.344757 	 time: 0.3
Epoch: 2642 	Training Loss: 1.148679 	Validation Loss: 1.344798 	 time: 0.3
Epoch: 2643 	Training Loss: 1.148664 	Validation Loss: 1.344849 	 time: 0.3
Epoch: 2644 	Training Loss: 1.148644 	Validation Loss: 1.344909 	 time: 0.3
Epoch: 2645 	Training Loss: 1.148618 	Validation Loss: 1.344941 	 time: 0.3
Epoch: 2646 	Training Loss: 1.148599 	Validation Loss: 1.344929 	 time: 0.3
Epoch: 2647 	Training Loss: 1.148590 	Validation Loss: 1.344904 	 time: 0.3
Epoch: 2648 	Training Loss: 1.148586 	Validation Loss: 1.344891 	 time: 0.3
Epoch: 2649 	Training Loss: 1.148584 	Validation Loss: 1.344907 	 time: 0.3
Epoch: 2650 	Training Loss: 1.148582 	Validation Loss: 1.344940 	 time: 0.3
Epoch: 2651 	Training Loss: 1.148579 	Validation Loss: 1.344965 	 time: 0.3
Epoch: 2652 	Training Loss: 1.148576 	Validation Loss: 1.344991 	 time: 0.3
Epoch: 2653 	Training Loss: 1.148572 	Validation Loss: 1.345022 	 time: 0.3
Epoch: 2654 	Training Loss: 1.148569 	Validation Loss: 1.345039 	 time: 0.3
Epoch: 2655 	Training Loss: 1.148565 	Validation Loss: 1.345044 	 time: 0.3
Epoch: 2656 	Training Loss: 1.148561 	Validation Loss: 1.345044 	 time: 0.3
Epoch: 2657 	Training Loss: 1.148558 	Validation Loss: 1.345038 	 time: 0.3
Epoch: 2658 	Training Loss: 1.148555 	Validation Loss: 1.345043 	 time: 0.3
Epoch: 2659 	Training Loss: 1.148553 	Validation Loss: 1.345059 	 time: 0.3
Epoch: 2660 	Training Loss: 1.148550 	Validation Loss: 1.345077 	 time: 0.3
Epoch: 2661 	Training Loss: 1.148547 	Validation Loss: 1.345106 	 time: 0.3
Epoch: 2662 	Training Loss: 1.148545 	Validation Loss: 1.345131 	 time: 0.3
Epoch: 2663 	Training Loss: 1.148543 	Validation Loss: 1.345148 	 time: 0.3
Epoch: 2664 	Training Loss: 1.148541 	Validation Loss: 1.345164 	 time: 0.3
Epoch: 2665 	Training Loss: 1.148539 	Validation Loss: 1.345170 	 time: 0.3
Epoch: 2666 	Training Loss: 1.148537 	Validation Loss: 1.345183 	 time: 0.3
Epoch: 2667 	Training Loss: 1.148535 	Validation Loss: 1.345193 	 time: 0.3
Epoch: 2668 	Training Loss: 1.148533 	Validation Loss: 1.345199 	 time: 0.3
Epoch: 2669 	Training Loss: 1.148532 	Validation Loss: 1.345202 	 time: 0.3
Epoch: 2670 	Training Loss: 1.148529 	Validation Loss: 1.345202 	 time: 0.3
Epoch: 2671 	Training Loss: 1.148528 	Validation Loss: 1.345214 	 time: 0.3
Epoch: 2672 	Training Loss: 1.148526 	Validation Loss: 1.345224 	 time: 0.3
Epoch: 2673 	Training Loss: 1.148524 	Validation Loss: 1.345246 	 time: 0.3
Epoch: 2674 	Training Loss: 1.148523 	Validation Loss: 1.345258 	 time: 0.3
Epoch: 2675 	Training Loss: 1.148521 	Validation Loss: 1.345268 	 time: 0.3
Epoch: 2676 	Training Loss: 1.148520 	Validation Loss: 1.345284 	 time: 0.3
Epoch: 2677 	Training Loss: 1.148518 	Validation Loss: 1.345302 	 time: 0.3
Epoch: 2678 	Training Loss: 1.148516 	Validation Loss: 1.345329 	 time: 0.3
Epoch: 2679 	Training Loss: 1.148515 	Validation Loss: 1.345344 	 time: 0.3
Epoch: 2680 	Training Loss: 1.148514 	Validation Loss: 1.345365 	 time: 0.3
Epoch: 2681 	Training Loss: 1.148512 	Validation Loss: 1.345380 	 time: 0.3
Epoch: 2682 	Training Loss: 1.148511 	Validation Loss: 1.345399 	 time: 0.3
Epoch: 2683 	Training Loss: 1.148509 	Validation Loss: 1.345420 	 time: 0.3
Epoch: 2684 	Training Loss: 1.148508 	Validation Loss: 1.345436 	 time: 0.3
Epoch: 2685 	Training Loss: 1.148507 	Validation Loss: 1.345457 	 time: 0.3
Epoch: 2686 	Training Loss: 1.148505 	Validation Loss: 1.345472 	 time: 0.3
Epoch: 2687 	Training Loss: 1.148504 	Validation Loss: 1.345496 	 time: 0.3
Epoch: 2688 	Training Loss: 1.148502 	Validation Loss: 1.345511 	 time: 0.3
Epoch: 2689 	Training Loss: 1.148500 	Validation Loss: 1.345533 	 time: 0.3
Epoch: 2690 	Training Loss: 1.148499 	Validation Loss: 1.345550 	 time: 0.3
Epoch: 2691 	Training Loss: 1.148497 	Validation Loss: 1.345574 	 time: 0.3
Epoch: 2692 	Training Loss: 1.148496 	Validation Loss: 1.345600 	 time: 0.3
Epoch: 2693 	Training Loss: 1.148494 	Validation Loss: 1.345633 	 time: 0.3
Epoch: 2694 	Training Loss: 1.148492 	Validation Loss: 1.345669 	 time: 0.3
Epoch: 2695 	Training Loss: 1.148489 	Validation Loss: 1.345716 	 time: 0.3
Epoch: 2696 	Training Loss: 1.148486 	Validation Loss: 1.345773 	 time: 0.3
Epoch: 2697 	Training Loss: 1.148481 	Validation Loss: 1.345852 	 time: 0.3
Epoch: 2698 	Training Loss: 1.148471 	Validation Loss: 1.345940 	 time: 0.3
Epoch: 2699 	Training Loss: 1.148457 	Validation Loss: 1.345990 	 time: 0.3
Epoch: 2700 	Training Loss: 1.148451 	Validation Loss: 1.345959 	 time: 0.3
Epoch: 2701 	Training Loss: 1.148451 	Validation Loss: 1.345986 	 time: 0.3
Epoch: 2702 	Training Loss: 1.148447 	Validation Loss: 1.346037 	 time: 0.3
Epoch: 2703 	Training Loss: 1.148441 	Validation Loss: 1.346025 	 time: 0.3
Epoch: 2704 	Training Loss: 1.148432 	Validation Loss: 1.345984 	 time: 0.3
Epoch: 2705 	Training Loss: 1.148424 	Validation Loss: 1.345970 	 time: 0.3
Epoch: 2706 	Training Loss: 1.148417 	Validation Loss: 1.345990 	 time: 0.3
Epoch: 2707 	Training Loss: 1.148413 	Validation Loss: 1.345962 	 time: 0.3
Epoch: 2708 	Training Loss: 1.148411 	Validation Loss: 1.345906 	 time: 0.3
Epoch: 2709 	Training Loss: 1.148409 	Validation Loss: 1.345930 	 time: 0.3
Epoch: 2710 	Training Loss: 1.148407 	Validation Loss: 1.345992 	 time: 0.3
Epoch: 2711 	Training Loss: 1.148404 	Validation Loss: 1.346031 	 time: 0.3
Epoch: 2712 	Training Loss: 1.148402 	Validation Loss: 1.346053 	 time: 0.3
Epoch: 2713 	Training Loss: 1.148400 	Validation Loss: 1.346077 	 time: 0.3
Epoch: 2714 	Training Loss: 1.148397 	Validation Loss: 1.346095 	 time: 0.3
Epoch: 2715 	Training Loss: 1.148395 	Validation Loss: 1.346056 	 time: 0.3
Epoch: 2716 	Training Loss: 1.148392 	Validation Loss: 1.345986 	 time: 0.3
Epoch: 2717 	Training Loss: 1.148391 	Validation Loss: 1.345947 	 time: 0.3
Epoch: 2718 	Training Loss: 1.148388 	Validation Loss: 1.345931 	 time: 0.3
Epoch: 2719 	Training Loss: 1.148386 	Validation Loss: 1.345915 	 time: 0.3
Epoch: 2720 	Training Loss: 1.148384 	Validation Loss: 1.345890 	 time: 0.3
Epoch: 2721 	Training Loss: 1.148381 	Validation Loss: 1.345874 	 time: 0.3
Epoch: 2722 	Training Loss: 1.148378 	Validation Loss: 1.345852 	 time: 0.3
Epoch: 2723 	Training Loss: 1.148375 	Validation Loss: 1.345806 	 time: 0.3
Epoch: 2724 	Training Loss: 1.148371 	Validation Loss: 1.345755 	 time: 0.3
Epoch: 2725 	Training Loss: 1.148367 	Validation Loss: 1.345718 	 time: 0.3
Epoch: 2726 	Training Loss: 1.148362 	Validation Loss: 1.345699 	 time: 0.3
Epoch: 2727 	Training Loss: 1.148356 	Validation Loss: 1.345673 	 time: 0.3
Epoch: 2728 	Training Loss: 1.148350 	Validation Loss: 1.345671 	 time: 0.3
Epoch: 2729 	Training Loss: 1.148341 	Validation Loss: 1.345682 	 time: 0.3
Epoch: 2730 	Training Loss: 1.148334 	Validation Loss: 1.345715 	 time: 0.3
Epoch: 2731 	Training Loss: 1.148330 	Validation Loss: 1.345710 	 time: 0.3
Epoch: 2732 	Training Loss: 1.148329 	Validation Loss: 1.345716 	 time: 0.3
Epoch: 2733 	Training Loss: 1.148326 	Validation Loss: 1.345722 	 time: 0.3
Epoch: 2734 	Training Loss: 1.148323 	Validation Loss: 1.345744 	 time: 0.3
Epoch: 2735 	Training Loss: 1.148319 	Validation Loss: 1.345765 	 time: 0.3
Epoch: 2736 	Training Loss: 1.148317 	Validation Loss: 1.345779 	 time: 0.3
Epoch: 2737 	Training Loss: 1.148314 	Validation Loss: 1.345796 	 time: 0.3
Epoch: 2738 	Training Loss: 1.148312 	Validation Loss: 1.345783 	 time: 0.3
Epoch: 2739 	Training Loss: 1.148310 	Validation Loss: 1.345781 	 time: 0.3
Epoch: 2740 	Training Loss: 1.148308 	Validation Loss: 1.345753 	 time: 0.3
Epoch: 2741 	Training Loss: 1.148306 	Validation Loss: 1.345763 	 time: 0.3
Epoch: 2742 	Training Loss: 1.148303 	Validation Loss: 1.345739 	 time: 0.3
Epoch: 2743 	Training Loss: 1.148301 	Validation Loss: 1.345734 	 time: 0.3
Epoch: 2744 	Training Loss: 1.148298 	Validation Loss: 1.345715 	 time: 0.3
Epoch: 2745 	Training Loss: 1.148295 	Validation Loss: 1.345708 	 time: 0.3
Epoch: 2746 	Training Loss: 1.148291 	Validation Loss: 1.345689 	 time: 0.3
Epoch: 2747 	Training Loss: 1.148286 	Validation Loss: 1.345668 	 time: 0.3
Epoch: 2748 	Training Loss: 1.148279 	Validation Loss: 1.345658 	 time: 0.3
Epoch: 2749 	Training Loss: 1.148267 	Validation Loss: 1.345642 	 time: 0.3
Epoch: 2750 	Training Loss: 1.148245 	Validation Loss: 1.345628 	 time: 0.3
Epoch: 2751 	Training Loss: 1.148201 	Validation Loss: 1.345551 	 time: 0.3
Epoch: 2752 	Training Loss: 1.148149 	Validation Loss: 1.345440 	 time: 0.3
Epoch: 2753 	Training Loss: 1.148121 	Validation Loss: 1.345317 	 time: 0.3
Epoch: 2754 	Training Loss: 1.148114 	Validation Loss: 1.345248 	 time: 0.3
Epoch: 2755 	Training Loss: 1.148116 	Validation Loss: 1.345203 	 time: 0.3
Epoch: 2756 	Training Loss: 1.148117 	Validation Loss: 1.345265 	 time: 0.3
Epoch: 2757 	Training Loss: 1.148107 	Validation Loss: 1.345381 	 time: 0.3
Epoch: 2758 	Training Loss: 1.148091 	Validation Loss: 1.345595 	 time: 0.3
Epoch: 2759 	Training Loss: 1.148073 	Validation Loss: 1.345811 	 time: 0.3
Epoch: 2760 	Training Loss: 1.148048 	Validation Loss: 1.345932 	 time: 0.3
Epoch: 2761 	Training Loss: 1.148001 	Validation Loss: 1.346029 	 time: 0.3
Epoch: 2762 	Training Loss: 1.147937 	Validation Loss: 1.346218 	 time: 0.3
Epoch: 2763 	Training Loss: 1.147900 	Validation Loss: 1.346559 	 time: 0.3
Epoch: 2764 	Training Loss: 1.147890 	Validation Loss: 1.346985 	 time: 0.3
Epoch: 2765 	Training Loss: 1.147883 	Validation Loss: 1.347415 	 time: 0.3
Epoch: 2766 	Training Loss: 1.147877 	Validation Loss: 1.347744 	 time: 0.3
Epoch: 2767 	Training Loss: 1.147868 	Validation Loss: 1.347852 	 time: 0.3
Epoch: 2768 	Training Loss: 1.147856 	Validation Loss: 1.347749 	 time: 0.3
Epoch: 2769 	Training Loss: 1.147835 	Validation Loss: 1.347536 	 time: 0.3
Epoch: 2770 	Training Loss: 1.147805 	Validation Loss: 1.347360 	 time: 0.3
Epoch: 2771 	Training Loss: 1.147780 	Validation Loss: 1.347398 	 time: 0.3
Epoch: 2772 	Training Loss: 1.147768 	Validation Loss: 1.347623 	 time: 0.3
Epoch: 2773 	Training Loss: 1.147753 	Validation Loss: 1.347904 	 time: 0.3
Epoch: 2774 	Training Loss: 1.147739 	Validation Loss: 1.348050 	 time: 0.3
Epoch: 2775 	Training Loss: 1.147727 	Validation Loss: 1.348017 	 time: 0.3
Epoch: 2776 	Training Loss: 1.147714 	Validation Loss: 1.347757 	 time: 0.3
Epoch: 2777 	Training Loss: 1.147698 	Validation Loss: 1.347447 	 time: 0.3
Epoch: 2778 	Training Loss: 1.147686 	Validation Loss: 1.347220 	 time: 0.3
Epoch: 2779 	Training Loss: 1.147679 	Validation Loss: 1.347152 	 time: 0.3
Epoch: 2780 	Training Loss: 1.147675 	Validation Loss: 1.347250 	 time: 0.3
Epoch: 2781 	Training Loss: 1.147671 	Validation Loss: 1.347441 	 time: 0.3
Epoch: 2782 	Training Loss: 1.147666 	Validation Loss: 1.347721 	 time: 0.3
Epoch: 2783 	Training Loss: 1.147663 	Validation Loss: 1.347895 	 time: 0.3
Epoch: 2784 	Training Loss: 1.147661 	Validation Loss: 1.347965 	 time: 0.3
Epoch: 2785 	Training Loss: 1.147657 	Validation Loss: 1.347872 	 time: 0.3
Epoch: 2786 	Training Loss: 1.147655 	Validation Loss: 1.347731 	 time: 0.3
Epoch: 2787 	Training Loss: 1.147652 	Validation Loss: 1.347590 	 time: 0.3
Epoch: 2788 	Training Loss: 1.147648 	Validation Loss: 1.347492 	 time: 0.3
Epoch: 2789 	Training Loss: 1.147645 	Validation Loss: 1.347458 	 time: 0.3
Epoch: 2790 	Training Loss: 1.147642 	Validation Loss: 1.347465 	 time: 0.3
Epoch: 2791 	Training Loss: 1.147640 	Validation Loss: 1.347496 	 time: 0.3
Epoch: 2792 	Training Loss: 1.147638 	Validation Loss: 1.347454 	 time: 0.3
Epoch: 2793 	Training Loss: 1.147636 	Validation Loss: 1.347400 	 time: 0.3
Epoch: 2794 	Training Loss: 1.147633 	Validation Loss: 1.347300 	 time: 0.3
Epoch: 2795 	Training Loss: 1.147631 	Validation Loss: 1.347224 	 time: 0.3
Epoch: 2796 	Training Loss: 1.147629 	Validation Loss: 1.347164 	 time: 0.3
Epoch: 2797 	Training Loss: 1.147627 	Validation Loss: 1.347156 	 time: 0.3
Epoch: 2798 	Training Loss: 1.147624 	Validation Loss: 1.347147 	 time: 0.3
Epoch: 2799 	Training Loss: 1.147622 	Validation Loss: 1.347133 	 time: 0.3
Epoch: 2800 	Training Loss: 1.147619 	Validation Loss: 1.347099 	 time: 0.3
Epoch: 2801 	Training Loss: 1.147617 	Validation Loss: 1.347061 	 time: 0.3
Epoch: 2802 	Training Loss: 1.147614 	Validation Loss: 1.347057 	 time: 0.3
Epoch: 2803 	Training Loss: 1.147611 	Validation Loss: 1.347059 	 time: 0.3
Epoch: 2804 	Training Loss: 1.147607 	Validation Loss: 1.347096 	 time: 0.3
Epoch: 2805 	Training Loss: 1.147604 	Validation Loss: 1.347098 	 time: 0.3
Epoch: 2806 	Training Loss: 1.147599 	Validation Loss: 1.347143 	 time: 0.3
Epoch: 2807 	Training Loss: 1.147592 	Validation Loss: 1.347096 	 time: 0.3
Epoch: 2808 	Training Loss: 1.147581 	Validation Loss: 1.347127 	 time: 0.3
Epoch: 2809 	Training Loss: 1.147563 	Validation Loss: 1.347158 	 time: 0.3
Epoch: 2810 	Training Loss: 1.147535 	Validation Loss: 1.347306 	 time: 0.3
Epoch: 2811 	Training Loss: 1.147503 	Validation Loss: 1.347419 	 time: 0.3
Epoch: 2812 	Training Loss: 1.147488 	Validation Loss: 1.347562 	 time: 0.3
Epoch: 2813 	Training Loss: 1.147481 	Validation Loss: 1.347683 	 time: 0.3
Epoch: 2814 	Training Loss: 1.147475 	Validation Loss: 1.347800 	 time: 0.3
Epoch: 2815 	Training Loss: 1.147464 	Validation Loss: 1.347930 	 time: 0.3
Epoch: 2816 	Training Loss: 1.147444 	Validation Loss: 1.348053 	 time: 0.3
Epoch: 2817 	Training Loss: 1.147420 	Validation Loss: 1.348146 	 time: 0.3
Epoch: 2818 	Training Loss: 1.147401 	Validation Loss: 1.348192 	 time: 0.3
Epoch: 2819 	Training Loss: 1.147384 	Validation Loss: 1.348235 	 time: 0.3
Epoch: 2820 	Training Loss: 1.147364 	Validation Loss: 1.348186 	 time: 0.3
Epoch: 2821 	Training Loss: 1.147331 	Validation Loss: 1.348153 	 time: 0.3
Epoch: 2822 	Training Loss: 1.147298 	Validation Loss: 1.348229 	 time: 0.3
Epoch: 2823 	Training Loss: 1.147283 	Validation Loss: 1.348305 	 time: 0.3
Epoch: 2824 	Training Loss: 1.147275 	Validation Loss: 1.348366 	 time: 0.3
Epoch: 2825 	Training Loss: 1.147273 	Validation Loss: 1.348538 	 time: 0.3
Epoch: 2826 	Training Loss: 1.147270 	Validation Loss: 1.348716 	 time: 0.3
Epoch: 2827 	Training Loss: 1.147264 	Validation Loss: 1.348805 	 time: 0.3
Epoch: 2828 	Training Loss: 1.147256 	Validation Loss: 1.348846 	 time: 0.3
Epoch: 2829 	Training Loss: 1.147251 	Validation Loss: 1.348876 	 time: 0.3
Epoch: 2830 	Training Loss: 1.147246 	Validation Loss: 1.348849 	 time: 0.3
Epoch: 2831 	Training Loss: 1.147241 	Validation Loss: 1.348820 	 time: 0.3
Epoch: 2832 	Training Loss: 1.147235 	Validation Loss: 1.348865 	 time: 0.3
Epoch: 2833 	Training Loss: 1.147229 	Validation Loss: 1.348940 	 time: 0.3
Epoch: 2834 	Training Loss: 1.147223 	Validation Loss: 1.348971 	 time: 0.3
Epoch: 2835 	Training Loss: 1.147218 	Validation Loss: 1.348966 	 time: 0.3
Epoch: 2836 	Training Loss: 1.147212 	Validation Loss: 1.348945 	 time: 0.3
Epoch: 2837 	Training Loss: 1.147203 	Validation Loss: 1.348911 	 time: 0.3
Epoch: 2838 	Training Loss: 1.147192 	Validation Loss: 1.348962 	 time: 0.3
Epoch: 2839 	Training Loss: 1.147173 	Validation Loss: 1.349149 	 time: 0.3
Epoch: 2840 	Training Loss: 1.147153 	Validation Loss: 1.349257 	 time: 0.3
Epoch: 2841 	Training Loss: 1.147132 	Validation Loss: 1.349555 	 time: 0.3
Epoch: 2842 	Training Loss: 1.147105 	Validation Loss: 1.349704 	 time: 0.3
Epoch: 2843 	Training Loss: 1.147071 	Validation Loss: 1.349793 	 time: 0.3
Epoch: 2844 	Training Loss: 1.147029 	Validation Loss: 1.349785 	 time: 0.3
Epoch: 2845 	Training Loss: 1.146991 	Validation Loss: 1.350182 	 time: 0.3
Epoch: 2846 	Training Loss: 1.146969 	Validation Loss: 1.350224 	 time: 0.3
Epoch: 2847 	Training Loss: 1.146972 	Validation Loss: 1.350690 	 time: 0.3
Epoch: 2848 	Training Loss: 1.146973 	Validation Loss: 1.350731 	 time: 0.3
Epoch: 2849 	Training Loss: 1.146956 	Validation Loss: 1.351072 	 time: 0.3
Epoch: 2850 	Training Loss: 1.146931 	Validation Loss: 1.351163 	 time: 0.3
Epoch: 2851 	Training Loss: 1.146923 	Validation Loss: 1.350952 	 time: 0.3
Epoch: 2852 	Training Loss: 1.146932 	Validation Loss: 1.350999 	 time: 0.3
Epoch: 2853 	Training Loss: 1.146949 	Validation Loss: 1.350552 	 time: 0.3
Epoch: 2854 	Training Loss: 1.146955 	Validation Loss: 1.350653 	 time: 0.3
Epoch: 2855 	Training Loss: 1.146928 	Validation Loss: 1.350451 	 time: 0.3
Epoch: 2856 	Training Loss: 1.146901 	Validation Loss: 1.350123 	 time: 0.3
Epoch: 2857 	Training Loss: 1.146910 	Validation Loss: 1.349794 	 time: 0.3
Epoch: 2858 	Training Loss: 1.146944 	Validation Loss: 1.349096 	 time: 0.3
Epoch: 2859 	Training Loss: 1.146979 	Validation Loss: 1.348722 	 time: 0.3
Epoch: 2860 	Training Loss: 1.146952 	Validation Loss: 1.348472 	 time: 0.3
Epoch: 2861 	Training Loss: 1.146891 	Validation Loss: 1.348302 	 time: 0.3
Epoch: 2862 	Training Loss: 1.146937 	Validation Loss: 1.347713 	 time: 0.3
Epoch: 2863 	Training Loss: 1.147048 	Validation Loss: 1.347183 	 time: 0.3
Epoch: 2864 	Training Loss: 1.147053 	Validation Loss: 1.346740 	 time: 0.3
Epoch: 2865 	Training Loss: 1.146919 	Validation Loss: 1.347065 	 time: 0.3
Epoch: 2866 	Training Loss: 1.147012 	Validation Loss: 1.346962 	 time: 0.3
Epoch: 2867 	Training Loss: 1.146995 	Validation Loss: 1.345905 	 time: 0.3
Epoch: 2868 	Training Loss: 1.146982 	Validation Loss: 1.346182 	 time: 0.3
Epoch: 2869 	Training Loss: 1.146933 	Validation Loss: 1.346539 	 time: 0.3
Epoch: 2870 	Training Loss: 1.147000 	Validation Loss: 1.345190 	 time: 0.3
Epoch: 2871 	Training Loss: 1.146899 	Validation Loss: 1.344563 	 time: 0.3
Epoch: 2872 	Training Loss: 1.146915 	Validation Loss: 1.345285 	 time: 0.3
Epoch: 2873 	Training Loss: 1.146935 	Validation Loss: 1.345116 	 time: 0.3
Epoch: 2874 	Training Loss: 1.146940 	Validation Loss: 1.344599 	 time: 0.3
Epoch: 2875 	Training Loss: 1.146915 	Validation Loss: 1.343789 	 time: 0.3
Epoch: 2876 	Training Loss: 1.146894 	Validation Loss: 1.344387 	 time: 0.3
Epoch: 2877 	Training Loss: 1.146874 	Validation Loss: 1.345270 	 time: 0.3
Epoch: 2878 	Training Loss: 1.146867 	Validation Loss: 1.344274 	 time: 0.3
Epoch: 2879 	Training Loss: 1.146826 	Validation Loss: 1.343609 	 time: 0.3
Epoch: 2880 	Training Loss: 1.146804 	Validation Loss: 1.344445 	 time: 0.3
Epoch: 2881 	Training Loss: 1.146804 	Validation Loss: 1.344945 	 time: 0.3
Epoch: 2882 	Training Loss: 1.146786 	Validation Loss: 1.345354 	 time: 0.3
Epoch: 2883 	Training Loss: 1.146752 	Validation Loss: 1.345339 	 time: 0.3
Epoch: 2884 	Training Loss: 1.146667 	Validation Loss: 1.345755 	 time: 0.3
Epoch: 2885 	Training Loss: 1.146662 	Validation Loss: 1.345958 	 time: 0.3
Epoch: 2886 	Training Loss: 1.146639 	Validation Loss: 1.346002 	 time: 0.3
Epoch: 2887 	Training Loss: 1.146644 	Validation Loss: 1.346569 	 time: 0.3
Epoch: 2888 	Training Loss: 1.146634 	Validation Loss: 1.346918 	 time: 0.3
Epoch: 2889 	Training Loss: 1.146638 	Validation Loss: 1.346601 	 time: 0.3
Epoch: 2890 	Training Loss: 1.146627 	Validation Loss: 1.346161 	 time: 0.3
Epoch: 2891 	Training Loss: 1.146622 	Validation Loss: 1.345765 	 time: 0.3
Epoch: 2892 	Training Loss: 1.146617 	Validation Loss: 1.345227 	 time: 0.3
Epoch: 2893 	Training Loss: 1.146610 	Validation Loss: 1.344507 	 time: 0.3
Epoch: 2894 	Training Loss: 1.146607 	Validation Loss: 1.344347 	 time: 0.3
Epoch: 2895 	Training Loss: 1.146602 	Validation Loss: 1.344403 	 time: 0.3
Epoch: 2896 	Training Loss: 1.146599 	Validation Loss: 1.343903 	 time: 0.3
Epoch: 2897 	Training Loss: 1.146595 	Validation Loss: 1.343366 	 time: 0.3
Epoch: 2898 	Training Loss: 1.146596 	Validation Loss: 1.343258 	 time: 0.3
Epoch: 2899 	Training Loss: 1.146590 	Validation Loss: 1.343298 	 time: 0.3
Epoch: 2900 	Training Loss: 1.146588 	Validation Loss: 1.342899 	 time: 0.3
Epoch: 2901 	Training Loss: 1.146584 	Validation Loss: 1.342509 	 time: 0.3
Epoch: 2902 	Training Loss: 1.146583 	Validation Loss: 1.342801 	 time: 0.3
Epoch: 2903 	Training Loss: 1.146577 	Validation Loss: 1.343051 	 time: 0.3
Epoch: 2904 	Training Loss: 1.146573 	Validation Loss: 1.342990 	 time: 0.3
Epoch: 2905 	Training Loss: 1.146564 	Validation Loss: 1.342883 	 time: 0.3
Epoch: 2906 	Training Loss: 1.146553 	Validation Loss: 1.343052 	 time: 0.3
Epoch: 2907 	Training Loss: 1.146540 	Validation Loss: 1.343142 	 time: 0.3
Epoch: 2908 	Training Loss: 1.146538 	Validation Loss: 1.342898 	 time: 0.3
Epoch: 2909 	Training Loss: 1.146538 	Validation Loss: 1.342900 	 time: 0.3
Epoch: 2910 	Training Loss: 1.146534 	Validation Loss: 1.342951 	 time: 0.3
Epoch: 2911 	Training Loss: 1.146531 	Validation Loss: 1.342893 	 time: 0.3
Epoch: 2912 	Training Loss: 1.146524 	Validation Loss: 1.342688 	 time: 0.3
Epoch: 2913 	Training Loss: 1.146518 	Validation Loss: 1.342654 	 time: 0.3
Epoch: 2914 	Training Loss: 1.146513 	Validation Loss: 1.342565 	 time: 0.3
Epoch: 2915 	Training Loss: 1.146510 	Validation Loss: 1.342489 	 time: 0.3
Epoch: 2916 	Training Loss: 1.146507 	Validation Loss: 1.342581 	 time: 0.3
Epoch: 2917 	Training Loss: 1.146503 	Validation Loss: 1.342785 	 time: 0.3
Epoch: 2918 	Training Loss: 1.146500 	Validation Loss: 1.342873 	 time: 0.3
Epoch: 2919 	Training Loss: 1.146495 	Validation Loss: 1.343025 	 time: 0.3
Epoch: 2920 	Training Loss: 1.146492 	Validation Loss: 1.343339 	 time: 0.3
Epoch: 2921 	Training Loss: 1.146488 	Validation Loss: 1.343480 	 time: 0.3
Epoch: 2922 	Training Loss: 1.146485 	Validation Loss: 1.343460 	 time: 0.3
Epoch: 2923 	Training Loss: 1.146482 	Validation Loss: 1.343441 	 time: 0.3
Epoch: 2924 	Training Loss: 1.146479 	Validation Loss: 1.343533 	 time: 0.3
Epoch: 2925 	Training Loss: 1.146476 	Validation Loss: 1.343478 	 time: 0.3
Epoch: 2926 	Training Loss: 1.146472 	Validation Loss: 1.343353 	 time: 0.3
Epoch: 2927 	Training Loss: 1.146469 	Validation Loss: 1.343269 	 time: 0.3
Epoch: 2928 	Training Loss: 1.146466 	Validation Loss: 1.343212 	 time: 0.3
Epoch: 2929 	Training Loss: 1.146462 	Validation Loss: 1.343084 	 time: 0.3
Epoch: 2930 	Training Loss: 1.146459 	Validation Loss: 1.342972 	 time: 0.3
Epoch: 2931 	Training Loss: 1.146456 	Validation Loss: 1.342949 	 time: 0.3
Epoch: 2932 	Training Loss: 1.146453 	Validation Loss: 1.342915 	 time: 0.3
Epoch: 2933 	Training Loss: 1.146451 	Validation Loss: 1.342847 	 time: 0.3
Epoch: 2934 	Training Loss: 1.146448 	Validation Loss: 1.342766 	 time: 0.3
Epoch: 2935 	Training Loss: 1.146446 	Validation Loss: 1.342739 	 time: 0.3
Epoch: 2936 	Training Loss: 1.146443 	Validation Loss: 1.342711 	 time: 0.3
Epoch: 2937 	Training Loss: 1.146440 	Validation Loss: 1.342750 	 time: 0.3
Epoch: 2938 	Training Loss: 1.146437 	Validation Loss: 1.342752 	 time: 0.3
Epoch: 2939 	Training Loss: 1.146434 	Validation Loss: 1.342758 	 time: 0.3
Epoch: 2940 	Training Loss: 1.146431 	Validation Loss: 1.342718 	 time: 0.3
Epoch: 2941 	Training Loss: 1.146427 	Validation Loss: 1.342785 	 time: 0.3
Epoch: 2942 	Training Loss: 1.146423 	Validation Loss: 1.342788 	 time: 0.3
Epoch: 2943 	Training Loss: 1.146417 	Validation Loss: 1.342838 	 time: 0.3
Epoch: 2944 	Training Loss: 1.146412 	Validation Loss: 1.342807 	 time: 0.3
Epoch: 2945 	Training Loss: 1.146407 	Validation Loss: 1.342959 	 time: 0.3
Epoch: 2946 	Training Loss: 1.146403 	Validation Loss: 1.342950 	 time: 0.3
Epoch: 2947 	Training Loss: 1.146400 	Validation Loss: 1.343114 	 time: 0.3
Epoch: 2948 	Training Loss: 1.146398 	Validation Loss: 1.343015 	 time: 0.3
Epoch: 2949 	Training Loss: 1.146396 	Validation Loss: 1.343249 	 time: 0.3
Epoch: 2950 	Training Loss: 1.146394 	Validation Loss: 1.343036 	 time: 0.3
Epoch: 2951 	Training Loss: 1.146393 	Validation Loss: 1.343293 	 time: 0.3
Epoch: 2952 	Training Loss: 1.146391 	Validation Loss: 1.343006 	 time: 0.3
Epoch: 2953 	Training Loss: 1.146387 	Validation Loss: 1.343379 	 time: 0.3
Epoch: 2954 	Training Loss: 1.146375 	Validation Loss: 1.343163 	 time: 0.3
Epoch: 2955 	Training Loss: 1.146360 	Validation Loss: 1.343514 	 time: 0.3
Epoch: 2956 	Training Loss: 1.146339 	Validation Loss: 1.343479 	 time: 0.3
Epoch: 2957 	Training Loss: 1.146326 	Validation Loss: 1.343691 	 time: 0.3
Epoch: 2958 	Training Loss: 1.146319 	Validation Loss: 1.343668 	 time: 0.3
Epoch: 2959 	Training Loss: 1.146315 	Validation Loss: 1.343669 	 time: 0.3
Epoch: 2960 	Training Loss: 1.146312 	Validation Loss: 1.343614 	 time: 0.3
Epoch: 2961 	Training Loss: 1.146309 	Validation Loss: 1.343551 	 time: 0.3
Epoch: 2962 	Training Loss: 1.146306 	Validation Loss: 1.343523 	 time: 0.3
Epoch: 2963 	Training Loss: 1.146303 	Validation Loss: 1.343455 	 time: 0.3
Epoch: 2964 	Training Loss: 1.146301 	Validation Loss: 1.343464 	 time: 0.3
Epoch: 2965 	Training Loss: 1.146300 	Validation Loss: 1.343256 	 time: 0.3
Epoch: 2966 	Training Loss: 1.146302 	Validation Loss: 1.343298 	 time: 0.3
Epoch: 2967 	Training Loss: 1.146308 	Validation Loss: 1.342947 	 time: 0.3
Epoch: 2968 	Training Loss: 1.146326 	Validation Loss: 1.343122 	 time: 0.3
Epoch: 2969 	Training Loss: 1.146327 	Validation Loss: 1.342736 	 time: 0.3
Epoch: 2970 	Training Loss: 1.146310 	Validation Loss: 1.343102 	 time: 0.3
Epoch: 2971 	Training Loss: 1.146229 	Validation Loss: 1.343099 	 time: 0.3
Epoch: 2972 	Training Loss: 1.146188 	Validation Loss: 1.343019 	 time: 0.3
Epoch: 2973 	Training Loss: 1.146204 	Validation Loss: 1.343209 	 time: 0.3
Epoch: 2974 	Training Loss: 1.146260 	Validation Loss: 1.342675 	 time: 0.3
Epoch: 2975 	Training Loss: 1.146339 	Validation Loss: 1.342752 	 time: 0.3
Epoch: 2976 	Training Loss: 1.146273 	Validation Loss: 1.342564 	 time: 0.3
Epoch: 2977 	Training Loss: 1.146189 	Validation Loss: 1.342406 	 time: 0.3
Epoch: 2978 	Training Loss: 1.146270 	Validation Loss: 1.342074 	 time: 0.3
Epoch: 2979 	Training Loss: 1.146261 	Validation Loss: 1.341218 	 time: 0.3
Validation loss decreased from 1.341981 to 1.341218. Model was saved
Epoch: 2980 	Training Loss: 1.146237 	Validation Loss: 1.341624 	 time: 0.3
Epoch: 2981 	Training Loss: 1.146180 	Validation Loss: 1.341931 	 time: 0.3
Epoch: 2982 	Training Loss: 1.146265 	Validation Loss: 1.340982 	 time: 0.3
Validation loss decreased from 1.341218 to 1.340982. Model was saved
Epoch: 2983 	Training Loss: 1.146356 	Validation Loss: 1.340651 	 time: 0.3
Validation loss decreased from 1.340982 to 1.340651. Model was saved
Epoch: 2984 	Training Loss: 1.146312 	Validation Loss: 1.341681 	 time: 0.3
Epoch: 2985 	Training Loss: 1.146246 	Validation Loss: 1.341293 	 time: 0.3
Epoch: 2986 	Training Loss: 1.146407 	Validation Loss: 1.340607 	 time: 0.3
Validation loss decreased from 1.340651 to 1.340607. Model was saved
Epoch: 2987 	Training Loss: 1.146375 	Validation Loss: 1.339679 	 time: 0.3
Validation loss decreased from 1.340607 to 1.339679. Model was saved
Epoch: 2988 	Training Loss: 1.146308 	Validation Loss: 1.342641 	 time: 0.3
Epoch: 2989 	Training Loss: 1.146298 	Validation Loss: 1.343301 	 time: 0.3
Epoch: 2990 	Training Loss: 1.146386 	Validation Loss: 1.341147 	 time: 0.3
Epoch: 2991 	Training Loss: 1.146406 	Validation Loss: 1.340672 	 time: 0.3
Epoch: 2992 	Training Loss: 1.146296 	Validation Loss: 1.342790 	 time: 0.3
Epoch: 2993 	Training Loss: 1.146422 	Validation Loss: 1.343378 	 time: 0.3
Epoch: 2994 	Training Loss: 1.146289 	Validation Loss: 1.341761 	 time: 0.3
Epoch: 2995 	Training Loss: 1.146208 	Validation Loss: 1.340766 	 time: 0.3
Epoch: 2996 	Training Loss: 1.146391 	Validation Loss: 1.341877 	 time: 0.3
Epoch: 2997 	Training Loss: 1.146364 	Validation Loss: 1.342162 	 time: 0.3
Epoch: 2998 	Training Loss: 1.146297 	Validation Loss: 1.341337 	 time: 0.3
Epoch: 2999 	Training Loss: 1.146265 	Validation Loss: 1.340413 	 time: 0.3
Epoch: 3000 	Training Loss: 1.146319 	Validation Loss: 1.342626 	 time: 0.3
Epoch: 3001 	Training Loss: 1.146207 	Validation Loss: 1.343608 	 time: 0.3
Epoch: 3002 	Training Loss: 1.146227 	Validation Loss: 1.342155 	 time: 0.3
Epoch: 3003 	Training Loss: 1.146297 	Validation Loss: 1.339241 	 time: 0.3
Validation loss decreased from 1.339679 to 1.339241. Model was saved
Epoch: 3004 	Training Loss: 1.146252 	Validation Loss: 1.339240 	 time: 0.3
Validation loss decreased from 1.339241 to 1.339240. Model was saved
Epoch: 3005 	Training Loss: 1.146227 	Validation Loss: 1.340789 	 time: 0.3
Epoch: 3006 	Training Loss: 1.146191 	Validation Loss: 1.343124 	 time: 0.3
Epoch: 3007 	Training Loss: 1.146237 	Validation Loss: 1.343384 	 time: 0.3
Epoch: 3008 	Training Loss: 1.146197 	Validation Loss: 1.342503 	 time: 0.3
Epoch: 3009 	Training Loss: 1.146156 	Validation Loss: 1.341721 	 time: 0.3
Epoch: 3010 	Training Loss: 1.146203 	Validation Loss: 1.340512 	 time: 0.3
Epoch: 3011 	Training Loss: 1.146179 	Validation Loss: 1.339977 	 time: 0.3
Epoch: 3012 	Training Loss: 1.146181 	Validation Loss: 1.340261 	 time: 0.3
Epoch: 3013 	Training Loss: 1.146134 	Validation Loss: 1.340490 	 time: 0.3
Epoch: 3014 	Training Loss: 1.146154 	Validation Loss: 1.340742 	 time: 0.3
Epoch: 3015 	Training Loss: 1.146176 	Validation Loss: 1.341227 	 time: 0.3
Epoch: 3016 	Training Loss: 1.146176 	Validation Loss: 1.342485 	 time: 0.3
Epoch: 3017 	Training Loss: 1.146135 	Validation Loss: 1.342104 	 time: 0.3
Epoch: 3018 	Training Loss: 1.146161 	Validation Loss: 1.339944 	 time: 0.3
Epoch: 3019 	Training Loss: 1.146190 	Validation Loss: 1.339070 	 time: 0.3
Validation loss decreased from 1.339240 to 1.339070. Model was saved
Epoch: 3020 	Training Loss: 1.146140 	Validation Loss: 1.339679 	 time: 0.3
Epoch: 3021 	Training Loss: 1.146104 	Validation Loss: 1.340625 	 time: 0.3
Epoch: 3022 	Training Loss: 1.146167 	Validation Loss: 1.340789 	 time: 0.3
Epoch: 3023 	Training Loss: 1.146181 	Validation Loss: 1.340092 	 time: 0.3
Epoch: 3024 	Training Loss: 1.146118 	Validation Loss: 1.341143 	 time: 0.3
Epoch: 3025 	Training Loss: 1.146128 	Validation Loss: 1.341998 	 time: 0.3
Epoch: 3026 	Training Loss: 1.146181 	Validation Loss: 1.340721 	 time: 0.3
Epoch: 3027 	Training Loss: 1.146235 	Validation Loss: 1.339920 	 time: 0.3
Epoch: 3028 	Training Loss: 1.146101 	Validation Loss: 1.341091 	 time: 0.3
Epoch: 3029 	Training Loss: 1.146296 	Validation Loss: 1.341263 	 time: 0.3
Epoch: 3030 	Training Loss: 1.146370 	Validation Loss: 1.340210 	 time: 0.3
Epoch: 3031 	Training Loss: 1.146291 	Validation Loss: 1.340119 	 time: 0.3
Epoch: 3032 	Training Loss: 1.146286 	Validation Loss: 1.340705 	 time: 0.3
Epoch: 3033 	Training Loss: 1.146398 	Validation Loss: 1.342238 	 time: 0.3
Epoch: 3034 	Training Loss: 1.146155 	Validation Loss: 1.342566 	 time: 0.3
Epoch: 3035 	Training Loss: 1.146273 	Validation Loss: 1.341071 	 time: 0.3
Epoch: 3036 	Training Loss: 1.146323 	Validation Loss: 1.342275 	 time: 0.3
Epoch: 3037 	Training Loss: 1.146196 	Validation Loss: 1.343533 	 time: 0.3
Epoch: 3038 	Training Loss: 1.146330 	Validation Loss: 1.341144 	 time: 0.3
Epoch: 3039 	Training Loss: 1.146194 	Validation Loss: 1.340207 	 time: 0.3
Epoch: 3040 	Training Loss: 1.146199 	Validation Loss: 1.342687 	 time: 0.3
Epoch: 3041 	Training Loss: 1.146263 	Validation Loss: 1.342670 	 time: 0.3
Epoch: 3042 	Training Loss: 1.146271 	Validation Loss: 1.340149 	 time: 0.3
Epoch: 3043 	Training Loss: 1.146189 	Validation Loss: 1.338975 	 time: 0.3
Validation loss decreased from 1.339070 to 1.338975. Model was saved
Epoch: 3044 	Training Loss: 1.146239 	Validation Loss: 1.341151 	 time: 0.3
Epoch: 3045 	Training Loss: 1.146271 	Validation Loss: 1.343150 	 time: 0.3
Epoch: 3046 	Training Loss: 1.146243 	Validation Loss: 1.341148 	 time: 0.3
Epoch: 3047 	Training Loss: 1.146124 	Validation Loss: 1.339861 	 time: 0.3
Epoch: 3048 	Training Loss: 1.146228 	Validation Loss: 1.341403 	 time: 0.3
Epoch: 3049 	Training Loss: 1.146168 	Validation Loss: 1.342421 	 time: 0.3
Epoch: 3050 	Training Loss: 1.146205 	Validation Loss: 1.342213 	 time: 0.3
Epoch: 3051 	Training Loss: 1.146091 	Validation Loss: 1.342201 	 time: 0.3
Epoch: 3052 	Training Loss: 1.146214 	Validation Loss: 1.341417 	 time: 0.3
Epoch: 3053 	Training Loss: 1.146162 	Validation Loss: 1.341352 	 time: 0.3
Epoch: 3054 	Training Loss: 1.146127 	Validation Loss: 1.340751 	 time: 0.3
Epoch: 3055 	Training Loss: 1.146147 	Validation Loss: 1.340808 	 time: 0.3
Epoch: 3056 	Training Loss: 1.146256 	Validation Loss: 1.341774 	 time: 0.3
Epoch: 3057 	Training Loss: 1.146105 	Validation Loss: 1.342981 	 time: 0.3
Epoch: 3058 	Training Loss: 1.146251 	Validation Loss: 1.340938 	 time: 0.3
Epoch: 3059 	Training Loss: 1.146265 	Validation Loss: 1.339774 	 time: 0.3
Epoch: 3060 	Training Loss: 1.146089 	Validation Loss: 1.341465 	 time: 0.3
Epoch: 3061 	Training Loss: 1.146214 	Validation Loss: 1.342819 	 time: 0.3
Epoch: 3062 	Training Loss: 1.146341 	Validation Loss: 1.340909 	 time: 0.3
Epoch: 3063 	Training Loss: 1.146072 	Validation Loss: 1.342185 	 time: 0.3
Epoch: 3064 	Training Loss: 1.146402 	Validation Loss: 1.341767 	 time: 0.3
Epoch: 3065 	Training Loss: 1.146366 	Validation Loss: 1.343008 	 time: 0.3
Epoch: 3066 	Training Loss: 1.146443 	Validation Loss: 1.343519 	 time: 0.3
Epoch: 3067 	Training Loss: 1.146285 	Validation Loss: 1.341849 	 time: 0.3
Epoch: 3068 	Training Loss: 1.146332 	Validation Loss: 1.341207 	 time: 0.3
Epoch: 3069 	Training Loss: 1.146214 	Validation Loss: 1.344878 	 time: 0.3
Epoch: 3070 	Training Loss: 1.146052 	Validation Loss: 1.346052 	 time: 0.3
Epoch: 3071 	Training Loss: 1.146144 	Validation Loss: 1.342245 	 time: 0.3
Epoch: 3072 	Training Loss: 1.146026 	Validation Loss: 1.340004 	 time: 0.3
Epoch: 3073 	Training Loss: 1.145972 	Validation Loss: 1.343777 	 time: 0.3
Epoch: 3074 	Training Loss: 1.145993 	Validation Loss: 1.345722 	 time: 0.3
Epoch: 3075 	Training Loss: 1.146022 	Validation Loss: 1.346022 	 time: 0.3
Epoch: 3076 	Training Loss: 1.145878 	Validation Loss: 1.345654 	 time: 0.3
Epoch: 3077 	Training Loss: 1.145988 	Validation Loss: 1.344287 	 time: 0.3
Epoch: 3078 	Training Loss: 1.145973 	Validation Loss: 1.345330 	 time: 0.3
Epoch: 3079 	Training Loss: 1.145894 	Validation Loss: 1.345023 	 time: 0.3
Epoch: 3080 	Training Loss: 1.146094 	Validation Loss: 1.341942 	 time: 0.3
Epoch: 3081 	Training Loss: 1.146165 	Validation Loss: 1.342433 	 time: 0.3
Epoch: 3082 	Training Loss: 1.146112 	Validation Loss: 1.346239 	 time: 0.3
Epoch: 3083 	Training Loss: 1.146039 	Validation Loss: 1.348514 	 time: 0.3
Epoch: 3084 	Training Loss: 1.146073 	Validation Loss: 1.346575 	 time: 0.3
Epoch: 3085 	Training Loss: 1.146154 	Validation Loss: 1.344090 	 time: 0.3
Epoch: 3086 	Training Loss: 1.146114 	Validation Loss: 1.344404 	 time: 0.3
Epoch: 3087 	Training Loss: 1.146108 	Validation Loss: 1.344488 	 time: 0.3
Epoch: 3088 	Training Loss: 1.146106 	Validation Loss: 1.343039 	 time: 0.3
Epoch: 3089 	Training Loss: 1.145957 	Validation Loss: 1.342856 	 time: 0.3
Epoch: 3090 	Training Loss: 1.146055 	Validation Loss: 1.344204 	 time: 0.3
Epoch: 3091 	Training Loss: 1.145923 	Validation Loss: 1.346753 	 time: 0.3
Epoch: 3092 	Training Loss: 1.145992 	Validation Loss: 1.345204 	 time: 0.3
Epoch: 3093 	Training Loss: 1.145992 	Validation Loss: 1.341564 	 time: 0.3
Epoch: 3094 	Training Loss: 1.145908 	Validation Loss: 1.340533 	 time: 0.3
Epoch: 3095 	Training Loss: 1.145921 	Validation Loss: 1.342932 	 time: 0.3
Epoch: 3096 	Training Loss: 1.145933 	Validation Loss: 1.343926 	 time: 0.3
Epoch: 3097 	Training Loss: 1.145906 	Validation Loss: 1.343392 	 time: 0.3
Epoch: 3098 	Training Loss: 1.145856 	Validation Loss: 1.342425 	 time: 0.3
Epoch: 3099 	Training Loss: 1.145878 	Validation Loss: 1.341547 	 time: 0.3
Epoch: 3100 	Training Loss: 1.145828 	Validation Loss: 1.341968 	 time: 0.3
Epoch: 3101 	Training Loss: 1.145769 	Validation Loss: 1.342801 	 time: 0.3
Epoch: 3102 	Training Loss: 1.145826 	Validation Loss: 1.341987 	 time: 0.3
Epoch: 3103 	Training Loss: 1.145791 	Validation Loss: 1.341733 	 time: 0.3
Epoch: 3104 	Training Loss: 1.145759 	Validation Loss: 1.342565 	 time: 0.3
Epoch: 3105 	Training Loss: 1.145779 	Validation Loss: 1.342522 	 time: 0.3
Epoch: 3106 	Training Loss: 1.145731 	Validation Loss: 1.341587 	 time: 0.3
Epoch: 3107 	Training Loss: 1.145735 	Validation Loss: 1.340392 	 time: 0.3
Epoch: 3108 	Training Loss: 1.145739 	Validation Loss: 1.339463 	 time: 0.3
Epoch: 3109 	Training Loss: 1.145742 	Validation Loss: 1.339483 	 time: 0.3
Epoch: 3110 	Training Loss: 1.145702 	Validation Loss: 1.340135 	 time: 0.3
Epoch: 3111 	Training Loss: 1.145717 	Validation Loss: 1.341179 	 time: 0.3
Epoch: 3112 	Training Loss: 1.145705 	Validation Loss: 1.341908 	 time: 0.3
Epoch: 3113 	Training Loss: 1.145682 	Validation Loss: 1.341639 	 time: 0.3
Epoch: 3114 	Training Loss: 1.145693 	Validation Loss: 1.341089 	 time: 0.3
Epoch: 3115 	Training Loss: 1.145681 	Validation Loss: 1.341453 	 time: 0.3
Epoch: 3116 	Training Loss: 1.145675 	Validation Loss: 1.341982 	 time: 0.3
Epoch: 3117 	Training Loss: 1.145673 	Validation Loss: 1.341183 	 time: 0.3
Epoch: 3118 	Training Loss: 1.145662 	Validation Loss: 1.340195 	 time: 0.3
Epoch: 3119 	Training Loss: 1.145672 	Validation Loss: 1.340487 	 time: 0.3
Epoch: 3120 	Training Loss: 1.145656 	Validation Loss: 1.340649 	 time: 0.3
Epoch: 3121 	Training Loss: 1.145652 	Validation Loss: 1.340476 	 time: 0.3
Epoch: 3122 	Training Loss: 1.145648 	Validation Loss: 1.340663 	 time: 0.3
Epoch: 3123 	Training Loss: 1.145640 	Validation Loss: 1.341147 	 time: 0.3
Epoch: 3124 	Training Loss: 1.145643 	Validation Loss: 1.341310 	 time: 0.3
Epoch: 3125 	Training Loss: 1.145640 	Validation Loss: 1.340728 	 time: 0.3
Epoch: 3126 	Training Loss: 1.145637 	Validation Loss: 1.340266 	 time: 0.3
Epoch: 3127 	Training Loss: 1.145635 	Validation Loss: 1.339930 	 time: 0.3
Epoch: 3128 	Training Loss: 1.145626 	Validation Loss: 1.340310 	 time: 0.3
Epoch: 3129 	Training Loss: 1.145621 	Validation Loss: 1.340416 	 time: 0.3
Epoch: 3130 	Training Loss: 1.145617 	Validation Loss: 1.340215 	 time: 0.3
Epoch: 3131 	Training Loss: 1.145610 	Validation Loss: 1.339984 	 time: 0.3
Epoch: 3132 	Training Loss: 1.145608 	Validation Loss: 1.340480 	 time: 0.3
Epoch: 3133 	Training Loss: 1.145605 	Validation Loss: 1.340813 	 time: 0.3
Epoch: 3134 	Training Loss: 1.145603 	Validation Loss: 1.341035 	 time: 0.3
Epoch: 3135 	Training Loss: 1.145604 	Validation Loss: 1.340784 	 time: 0.3
Epoch: 3136 	Training Loss: 1.145608 	Validation Loss: 1.340917 	 time: 0.3
Epoch: 3137 	Training Loss: 1.145620 	Validation Loss: 1.340650 	 time: 0.3
Epoch: 3138 	Training Loss: 1.145652 	Validation Loss: 1.340966 	 time: 0.3
Epoch: 3139 	Training Loss: 1.145659 	Validation Loss: 1.340948 	 time: 0.3
Epoch: 3140 	Training Loss: 1.145638 	Validation Loss: 1.341545 	 time: 0.3
Epoch: 3141 	Training Loss: 1.145586 	Validation Loss: 1.341697 	 time: 0.3
Epoch: 3142 	Training Loss: 1.145593 	Validation Loss: 1.341336 	 time: 0.3
Epoch: 3143 	Training Loss: 1.145654 	Validation Loss: 1.341692 	 time: 0.3
Epoch: 3144 	Training Loss: 1.145671 	Validation Loss: 1.341369 	 time: 0.3
Epoch: 3145 	Training Loss: 1.145657 	Validation Loss: 1.341400 	 time: 0.3
Epoch: 3146 	Training Loss: 1.145579 	Validation Loss: 1.341878 	 time: 0.3
Epoch: 3147 	Training Loss: 1.145648 	Validation Loss: 1.341476 	 time: 0.3
Epoch: 3148 	Training Loss: 1.145804 	Validation Loss: 1.341707 	 time: 0.3
Epoch: 3149 	Training Loss: 1.145633 	Validation Loss: 1.342817 	 time: 0.3
Epoch: 3150 	Training Loss: 1.145764 	Validation Loss: 1.341484 	 time: 0.3
Epoch: 3151 	Training Loss: 1.146132 	Validation Loss: 1.340441 	 time: 0.3
Epoch: 3152 	Training Loss: 1.145884 	Validation Loss: 1.342755 	 time: 0.3
Epoch: 3153 	Training Loss: 1.146151 	Validation Loss: 1.342421 	 time: 0.3
Epoch: 3154 	Training Loss: 1.145949 	Validation Loss: 1.344279 	 time: 0.3
Epoch: 3155 	Training Loss: 1.146092 	Validation Loss: 1.344898 	 time: 0.3
Epoch: 3156 	Training Loss: 1.145921 	Validation Loss: 1.342545 	 time: 0.3
Epoch: 3157 	Training Loss: 1.145813 	Validation Loss: 1.342909 	 time: 0.3
Epoch: 3158 	Training Loss: 1.145895 	Validation Loss: 1.344537 	 time: 0.3
Epoch: 3159 	Training Loss: 1.145751 	Validation Loss: 1.345376 	 time: 0.3
Epoch: 3160 	Training Loss: 1.145791 	Validation Loss: 1.342849 	 time: 0.3
Epoch: 3161 	Training Loss: 1.145712 	Validation Loss: 1.341957 	 time: 0.3
Epoch: 3162 	Training Loss: 1.145738 	Validation Loss: 1.342212 	 time: 0.3
Epoch: 3163 	Training Loss: 1.145684 	Validation Loss: 1.342471 	 time: 0.3
Epoch: 3164 	Training Loss: 1.145704 	Validation Loss: 1.341644 	 time: 0.3
Epoch: 3165 	Training Loss: 1.145658 	Validation Loss: 1.341217 	 time: 0.3
Epoch: 3166 	Training Loss: 1.145658 	Validation Loss: 1.343222 	 time: 0.3
Epoch: 3167 	Training Loss: 1.145623 	Validation Loss: 1.344841 	 time: 0.3
Epoch: 3168 	Training Loss: 1.145651 	Validation Loss: 1.342943 	 time: 0.3
Epoch: 3169 	Training Loss: 1.145616 	Validation Loss: 1.340573 	 time: 0.3
Epoch: 3170 	Training Loss: 1.145615 	Validation Loss: 1.340760 	 time: 0.3
Epoch: 3171 	Training Loss: 1.145593 	Validation Loss: 1.342410 	 time: 0.3
Epoch: 3172 	Training Loss: 1.145566 	Validation Loss: 1.342371 	 time: 0.3
Epoch: 3173 	Training Loss: 1.145577 	Validation Loss: 1.340655 	 time: 0.3
Epoch: 3174 	Training Loss: 1.145584 	Validation Loss: 1.339792 	 time: 0.3
Epoch: 3175 	Training Loss: 1.145580 	Validation Loss: 1.340709 	 time: 0.3
Epoch: 3176 	Training Loss: 1.145549 	Validation Loss: 1.341141 	 time: 0.3
Epoch: 3177 	Training Loss: 1.145534 	Validation Loss: 1.340241 	 time: 0.3
Epoch: 3178 	Training Loss: 1.145539 	Validation Loss: 1.339661 	 time: 0.3
Epoch: 3179 	Training Loss: 1.145545 	Validation Loss: 1.339307 	 time: 0.3
Epoch: 3180 	Training Loss: 1.145526 	Validation Loss: 1.338928 	 time: 0.3
Validation loss decreased from 1.338975 to 1.338928. Model was saved
Epoch: 3181 	Training Loss: 1.145541 	Validation Loss: 1.338307 	 time: 0.3
Validation loss decreased from 1.338928 to 1.338307. Model was saved
Epoch: 3182 	Training Loss: 1.145517 	Validation Loss: 1.338185 	 time: 0.3
Validation loss decreased from 1.338307 to 1.338185. Model was saved
Epoch: 3183 	Training Loss: 1.145508 	Validation Loss: 1.338442 	 time: 0.3
Epoch: 3184 	Training Loss: 1.145509 	Validation Loss: 1.338903 	 time: 0.3
Epoch: 3185 	Training Loss: 1.145502 	Validation Loss: 1.338659 	 time: 0.3
Epoch: 3186 	Training Loss: 1.145501 	Validation Loss: 1.338117 	 time: 0.3
Validation loss decreased from 1.338185 to 1.338117. Model was saved
Epoch: 3187 	Training Loss: 1.145499 	Validation Loss: 1.338065 	 time: 0.3
Validation loss decreased from 1.338117 to 1.338065. Model was saved
Epoch: 3188 	Training Loss: 1.145492 	Validation Loss: 1.338535 	 time: 0.3
Epoch: 3189 	Training Loss: 1.145492 	Validation Loss: 1.338407 	 time: 0.3
Epoch: 3190 	Training Loss: 1.145489 	Validation Loss: 1.338257 	 time: 0.3
Epoch: 3191 	Training Loss: 1.145487 	Validation Loss: 1.337951 	 time: 0.3
Validation loss decreased from 1.338065 to 1.337951. Model was saved
Epoch: 3192 	Training Loss: 1.145492 	Validation Loss: 1.338316 	 time: 0.3
Epoch: 3193 	Training Loss: 1.145508 	Validation Loss: 1.337796 	 time: 0.3
Validation loss decreased from 1.337951 to 1.337796. Model was saved
Epoch: 3194 	Training Loss: 1.145548 	Validation Loss: 1.338595 	 time: 0.3
Epoch: 3195 	Training Loss: 1.145525 	Validation Loss: 1.338156 	 time: 0.3
Epoch: 3196 	Training Loss: 1.145515 	Validation Loss: 1.338551 	 time: 0.3
Epoch: 3197 	Training Loss: 1.145485 	Validation Loss: 1.338371 	 time: 0.3
Epoch: 3198 	Training Loss: 1.145467 	Validation Loss: 1.338476 	 time: 0.3
Epoch: 3199 	Training Loss: 1.145466 	Validation Loss: 1.338468 	 time: 0.3
Epoch: 3200 	Training Loss: 1.145470 	Validation Loss: 1.338138 	 time: 0.3
Epoch: 3201 	Training Loss: 1.145497 	Validation Loss: 1.338778 	 time: 0.3
Epoch: 3202 	Training Loss: 1.145529 	Validation Loss: 1.338179 	 time: 0.3
Epoch: 3203 	Training Loss: 1.145586 	Validation Loss: 1.338785 	 time: 0.3
Epoch: 3204 	Training Loss: 1.145488 	Validation Loss: 1.339132 	 time: 0.3
Epoch: 3205 	Training Loss: 1.145476 	Validation Loss: 1.338681 	 time: 0.3
Epoch: 3206 	Training Loss: 1.145598 	Validation Loss: 1.339371 	 time: 0.3
Epoch: 3207 	Training Loss: 1.145562 	Validation Loss: 1.338160 	 time: 0.3
Epoch: 3208 	Training Loss: 1.145480 	Validation Loss: 1.339162 	 time: 0.3
Epoch: 3209 	Training Loss: 1.145449 	Validation Loss: 1.339561 	 time: 0.3
Epoch: 3210 	Training Loss: 1.145495 	Validation Loss: 1.338073 	 time: 0.3
Epoch: 3211 	Training Loss: 1.145690 	Validation Loss: 1.339136 	 time: 0.3
Epoch: 3212 	Training Loss: 1.145461 	Validation Loss: 1.341482 	 time: 0.3
Epoch: 3213 	Training Loss: 1.145782 	Validation Loss: 1.340093 	 time: 0.3
Epoch: 3214 	Training Loss: 1.146068 	Validation Loss: 1.338983 	 time: 0.3
Epoch: 3215 	Training Loss: 1.145942 	Validation Loss: 1.341960 	 time: 0.3
Epoch: 3216 	Training Loss: 1.145832 	Validation Loss: 1.344672 	 time: 0.3
Epoch: 3217 	Training Loss: 1.145983 	Validation Loss: 1.341929 	 time: 0.3
Epoch: 3218 	Training Loss: 1.145857 	Validation Loss: 1.339546 	 time: 0.3
Epoch: 3219 	Training Loss: 1.145927 	Validation Loss: 1.339288 	 time: 0.3
Epoch: 3220 	Training Loss: 1.145767 	Validation Loss: 1.341683 	 time: 0.3
Epoch: 3221 	Training Loss: 1.145806 	Validation Loss: 1.340541 	 time: 0.3
Epoch: 3222 	Training Loss: 1.145579 	Validation Loss: 1.343163 	 time: 0.3
Epoch: 3223 	Training Loss: 1.145715 	Validation Loss: 1.341694 	 time: 0.3
Epoch: 3224 	Training Loss: 1.145593 	Validation Loss: 1.343499 	 time: 0.3
Epoch: 3225 	Training Loss: 1.145520 	Validation Loss: 1.345014 	 time: 0.3
Epoch: 3226 	Training Loss: 1.145563 	Validation Loss: 1.343156 	 time: 0.3
Epoch: 3227 	Training Loss: 1.145486 	Validation Loss: 1.341333 	 time: 0.3
Epoch: 3228 	Training Loss: 1.145555 	Validation Loss: 1.341527 	 time: 0.3
Epoch: 3229 	Training Loss: 1.145460 	Validation Loss: 1.343864 	 time: 0.3
Epoch: 3230 	Training Loss: 1.145478 	Validation Loss: 1.344864 	 time: 0.3
Epoch: 3231 	Training Loss: 1.145493 	Validation Loss: 1.343197 	 time: 0.3
Epoch: 3232 	Training Loss: 1.145408 	Validation Loss: 1.341513 	 time: 0.3
Epoch: 3233 	Training Loss: 1.145422 	Validation Loss: 1.340034 	 time: 0.3
Epoch: 3234 	Training Loss: 1.145481 	Validation Loss: 1.340664 	 time: 0.3
Epoch: 3235 	Training Loss: 1.145512 	Validation Loss: 1.339869 	 time: 0.3
Epoch: 3236 	Training Loss: 1.145586 	Validation Loss: 1.340496 	 time: 0.3
Epoch: 3237 	Training Loss: 1.145422 	Validation Loss: 1.341570 	 time: 0.3
Epoch: 3238 	Training Loss: 1.145525 	Validation Loss: 1.339848 	 time: 0.3
Epoch: 3239 	Training Loss: 1.145789 	Validation Loss: 1.338773 	 time: 0.3
Epoch: 3240 	Training Loss: 1.145715 	Validation Loss: 1.339868 	 time: 0.3
Epoch: 3241 	Training Loss: 1.145792 	Validation Loss: 1.339280 	 time: 0.3
Epoch: 3242 	Training Loss: 1.145591 	Validation Loss: 1.339491 	 time: 0.3
Epoch: 3243 	Training Loss: 1.145882 	Validation Loss: 1.341640 	 time: 0.3
Epoch: 3244 	Training Loss: 1.145756 	Validation Loss: 1.340998 	 time: 0.3
Epoch: 3245 	Training Loss: 1.145709 	Validation Loss: 1.339064 	 time: 0.3
Epoch: 3246 	Training Loss: 1.145610 	Validation Loss: 1.338490 	 time: 0.3
Epoch: 3247 	Training Loss: 1.145598 	Validation Loss: 1.339834 	 time: 0.3
Epoch: 3248 	Training Loss: 1.145791 	Validation Loss: 1.338741 	 time: 0.3
Epoch: 3249 	Training Loss: 1.145729 	Validation Loss: 1.339963 	 time: 0.3
Epoch: 3250 	Training Loss: 1.145557 	Validation Loss: 1.341354 	 time: 0.3
Epoch: 3251 	Training Loss: 1.145815 	Validation Loss: 1.337307 	 time: 0.3
Validation loss decreased from 1.337796 to 1.337307. Model was saved
Epoch: 3252 	Training Loss: 1.145653 	Validation Loss: 1.337918 	 time: 0.3
Epoch: 3253 	Training Loss: 1.145560 	Validation Loss: 1.341128 	 time: 0.3
Epoch: 3254 	Training Loss: 1.145774 	Validation Loss: 1.341407 	 time: 0.3
Epoch: 3255 	Training Loss: 1.145676 	Validation Loss: 1.339331 	 time: 0.3
Epoch: 3256 	Training Loss: 1.145619 	Validation Loss: 1.340664 	 time: 0.3
Epoch: 3257 	Training Loss: 1.145715 	Validation Loss: 1.340920 	 time: 0.3
Epoch: 3258 	Training Loss: 1.145560 	Validation Loss: 1.343109 	 time: 0.3
Epoch: 3259 	Training Loss: 1.145520 	Validation Loss: 1.343368 	 time: 0.3
Epoch: 3260 	Training Loss: 1.145546 	Validation Loss: 1.339847 	 time: 0.3
Epoch: 3261 	Training Loss: 1.145465 	Validation Loss: 1.339139 	 time: 0.3
Epoch: 3262 	Training Loss: 1.145432 	Validation Loss: 1.339949 	 time: 0.3
Epoch: 3263 	Training Loss: 1.145377 	Validation Loss: 1.341585 	 time: 0.3
Epoch: 3264 	Training Loss: 1.145407 	Validation Loss: 1.341410 	 time: 0.3
Epoch: 3265 	Training Loss: 1.145373 	Validation Loss: 1.340109 	 time: 0.3
Epoch: 3266 	Training Loss: 1.145411 	Validation Loss: 1.340605 	 time: 0.3
Epoch: 3267 	Training Loss: 1.145417 	Validation Loss: 1.340413 	 time: 0.3
Epoch: 3268 	Training Loss: 1.145458 	Validation Loss: 1.340159 	 time: 0.3
Epoch: 3269 	Training Loss: 1.145346 	Validation Loss: 1.339484 	 time: 0.3
Epoch: 3270 	Training Loss: 1.145407 	Validation Loss: 1.338511 	 time: 0.3
Epoch: 3271 	Training Loss: 1.145601 	Validation Loss: 1.339453 	 time: 0.3
Epoch: 3272 	Training Loss: 1.145376 	Validation Loss: 1.339435 	 time: 0.3
Epoch: 3273 	Training Loss: 1.145444 	Validation Loss: 1.337722 	 time: 0.3
Epoch: 3274 	Training Loss: 1.145663 	Validation Loss: 1.337028 	 time: 0.3
Validation loss decreased from 1.337307 to 1.337028. Model was saved
Epoch: 3275 	Training Loss: 1.145532 	Validation Loss: 1.339252 	 time: 0.3
Epoch: 3276 	Training Loss: 1.145708 	Validation Loss: 1.339783 	 time: 0.3
Epoch: 3277 	Training Loss: 1.145525 	Validation Loss: 1.338636 	 time: 0.3
Epoch: 3278 	Training Loss: 1.145542 	Validation Loss: 1.340282 	 time: 0.3
Epoch: 3279 	Training Loss: 1.145592 	Validation Loss: 1.339735 	 time: 0.3
Epoch: 3280 	Training Loss: 1.145424 	Validation Loss: 1.340775 	 time: 0.3
Epoch: 3281 	Training Loss: 1.145398 	Validation Loss: 1.341542 	 time: 0.3
Epoch: 3282 	Training Loss: 1.145440 	Validation Loss: 1.339529 	 time: 0.3
Epoch: 3283 	Training Loss: 1.145515 	Validation Loss: 1.340458 	 time: 0.3
Epoch: 3284 	Training Loss: 1.145334 	Validation Loss: 1.339847 	 time: 0.3
Epoch: 3285 	Training Loss: 1.145299 	Validation Loss: 1.338068 	 time: 0.3
Epoch: 3286 	Training Loss: 1.145325 	Validation Loss: 1.338407 	 time: 0.3
Epoch: 3287 	Training Loss: 1.145348 	Validation Loss: 1.338533 	 time: 0.3
Epoch: 3288 	Training Loss: 1.145508 	Validation Loss: 1.340302 	 time: 0.3
Epoch: 3289 	Training Loss: 1.145365 	Validation Loss: 1.339207 	 time: 0.3
Epoch: 3290 	Training Loss: 1.145278 	Validation Loss: 1.338083 	 time: 0.3
Epoch: 3291 	Training Loss: 1.145330 	Validation Loss: 1.338375 	 time: 0.3
Epoch: 3292 	Training Loss: 1.145339 	Validation Loss: 1.338700 	 time: 0.3
Epoch: 3293 	Training Loss: 1.145459 	Validation Loss: 1.340076 	 time: 0.3
Epoch: 3294 	Training Loss: 1.145349 	Validation Loss: 1.339157 	 time: 0.3
Epoch: 3295 	Training Loss: 1.145311 	Validation Loss: 1.337572 	 time: 0.3
Epoch: 3296 	Training Loss: 1.145471 	Validation Loss: 1.339686 	 time: 0.3
Epoch: 3297 	Training Loss: 1.145353 	Validation Loss: 1.341365 	 time: 0.3
Epoch: 3298 	Training Loss: 1.145356 	Validation Loss: 1.341050 	 time: 0.3
Epoch: 3299 	Training Loss: 1.145288 	Validation Loss: 1.339880 	 time: 0.3
Epoch: 3300 	Training Loss: 1.145307 	Validation Loss: 1.339526 	 time: 0.3
Epoch: 3301 	Training Loss: 1.145405 	Validation Loss: 1.342233 	 time: 0.3
Epoch: 3302 	Training Loss: 1.145288 	Validation Loss: 1.343269 	 time: 0.3
Epoch: 3303 	Training Loss: 1.145308 	Validation Loss: 1.340084 	 time: 0.3
Epoch: 3304 	Training Loss: 1.145282 	Validation Loss: 1.339441 	 time: 0.3
Epoch: 3305 	Training Loss: 1.145447 	Validation Loss: 1.339811 	 time: 0.3
Epoch: 3306 	Training Loss: 1.145594 	Validation Loss: 1.341510 	 time: 0.3
Epoch: 3307 	Training Loss: 1.145373 	Validation Loss: 1.341984 	 time: 0.3
Epoch: 3308 	Training Loss: 1.145516 	Validation Loss: 1.338616 	 time: 0.3
Epoch: 3309 	Training Loss: 1.145929 	Validation Loss: 1.340715 	 time: 0.3
Epoch: 3310 	Training Loss: 1.145641 	Validation Loss: 1.343812 	 time: 0.3
Epoch: 3311 	Training Loss: 1.145730 	Validation Loss: 1.341152 	 time: 0.3
Epoch: 3312 	Training Loss: 1.145592 	Validation Loss: 1.337364 	 time: 0.3
Epoch: 3313 	Training Loss: 1.145649 	Validation Loss: 1.338206 	 time: 0.3
Epoch: 3314 	Training Loss: 1.145449 	Validation Loss: 1.341361 	 time: 0.3
Epoch: 3315 	Training Loss: 1.146007 	Validation Loss: 1.341107 	 time: 0.3
Epoch: 3316 	Training Loss: 1.145873 	Validation Loss: 1.338177 	 time: 0.3
Epoch: 3317 	Training Loss: 1.145613 	Validation Loss: 1.341467 	 time: 0.3
Epoch: 3318 	Training Loss: 1.145856 	Validation Loss: 1.340905 	 time: 0.3
Epoch: 3319 	Training Loss: 1.145715 	Validation Loss: 1.338772 	 time: 0.3
Epoch: 3320 	Training Loss: 1.145602 	Validation Loss: 1.341720 	 time: 0.3
Epoch: 3321 	Training Loss: 1.145625 	Validation Loss: 1.344623 	 time: 0.3
Epoch: 3322 	Training Loss: 1.145665 	Validation Loss: 1.342117 	 time: 0.3
Epoch: 3323 	Training Loss: 1.145396 	Validation Loss: 1.341829 	 time: 0.3
Epoch: 3324 	Training Loss: 1.145415 	Validation Loss: 1.342876 	 time: 0.3
Epoch: 3325 	Training Loss: 1.145528 	Validation Loss: 1.341307 	 time: 0.3
Epoch: 3326 	Training Loss: 1.145428 	Validation Loss: 1.341869 	 time: 0.3
Epoch: 3327 	Training Loss: 1.145365 	Validation Loss: 1.342439 	 time: 0.3
Epoch: 3328 	Training Loss: 1.145495 	Validation Loss: 1.340299 	 time: 0.3
Epoch: 3329 	Training Loss: 1.145285 	Validation Loss: 1.341085 	 time: 0.3
Epoch: 3330 	Training Loss: 1.145168 	Validation Loss: 1.344102 	 time: 0.3
Epoch: 3331 	Training Loss: 1.145309 	Validation Loss: 1.341175 	 time: 0.3
Epoch: 3332 	Training Loss: 1.145280 	Validation Loss: 1.341087 	 time: 0.3
Epoch: 3333 	Training Loss: 1.145176 	Validation Loss: 1.341224 	 time: 0.3
Epoch: 3334 	Training Loss: 1.145185 	Validation Loss: 1.339570 	 time: 0.3
Epoch: 3335 	Training Loss: 1.145345 	Validation Loss: 1.339622 	 time: 0.3
Epoch: 3336 	Training Loss: 1.145193 	Validation Loss: 1.341589 	 time: 0.3
Epoch: 3337 	Training Loss: 1.145420 	Validation Loss: 1.340042 	 time: 0.3
Epoch: 3338 	Training Loss: 1.145156 	Validation Loss: 1.340353 	 time: 0.3
Epoch: 3339 	Training Loss: 1.145247 	Validation Loss: 1.342727 	 time: 0.3
Epoch: 3340 	Training Loss: 1.145211 	Validation Loss: 1.340161 	 time: 0.3
Epoch: 3341 	Training Loss: 1.145071 	Validation Loss: 1.338910 	 time: 0.3
Epoch: 3342 	Training Loss: 1.145088 	Validation Loss: 1.339849 	 time: 0.3
Epoch: 3343 	Training Loss: 1.145092 	Validation Loss: 1.339851 	 time: 0.3
Epoch: 3344 	Training Loss: 1.144989 	Validation Loss: 1.341354 	 time: 0.3
Epoch: 3345 	Training Loss: 1.145054 	Validation Loss: 1.340443 	 time: 0.3
Epoch: 3346 	Training Loss: 1.144984 	Validation Loss: 1.339990 	 time: 0.3
Epoch: 3347 	Training Loss: 1.144953 	Validation Loss: 1.339653 	 time: 0.3
Epoch: 3348 	Training Loss: 1.144973 	Validation Loss: 1.340530 	 time: 0.3
Epoch: 3349 	Training Loss: 1.144948 	Validation Loss: 1.341496 	 time: 0.3
Epoch: 3350 	Training Loss: 1.144952 	Validation Loss: 1.341303 	 time: 0.3
Epoch: 3351 	Training Loss: 1.144937 	Validation Loss: 1.340972 	 time: 0.3
Epoch: 3352 	Training Loss: 1.144910 	Validation Loss: 1.340318 	 time: 0.3
Epoch: 3353 	Training Loss: 1.144915 	Validation Loss: 1.340591 	 time: 0.3
Epoch: 3354 	Training Loss: 1.144901 	Validation Loss: 1.340710 	 time: 0.3
Epoch: 3355 	Training Loss: 1.144893 	Validation Loss: 1.341406 	 time: 0.3
Epoch: 3356 	Training Loss: 1.144913 	Validation Loss: 1.340779 	 time: 0.3
Epoch: 3357 	Training Loss: 1.144910 	Validation Loss: 1.341375 	 time: 0.3
Epoch: 3358 	Training Loss: 1.144915 	Validation Loss: 1.341274 	 time: 0.3
Epoch: 3359 	Training Loss: 1.144921 	Validation Loss: 1.341640 	 time: 0.3
Epoch: 3360 	Training Loss: 1.144904 	Validation Loss: 1.340820 	 time: 0.3
Epoch: 3361 	Training Loss: 1.144870 	Validation Loss: 1.341590 	 time: 0.3
Epoch: 3362 	Training Loss: 1.144842 	Validation Loss: 1.341833 	 time: 0.3
Epoch: 3363 	Training Loss: 1.144817 	Validation Loss: 1.341721 	 time: 0.3
Epoch: 3364 	Training Loss: 1.144841 	Validation Loss: 1.342095 	 time: 0.3
Epoch: 3365 	Training Loss: 1.144843 	Validation Loss: 1.341061 	 time: 0.3
Epoch: 3366 	Training Loss: 1.144871 	Validation Loss: 1.342396 	 time: 0.3
Epoch: 3367 	Training Loss: 1.144900 	Validation Loss: 1.341707 	 time: 0.3
Epoch: 3368 	Training Loss: 1.144931 	Validation Loss: 1.342906 	 time: 0.3
Epoch: 3369 	Training Loss: 1.144835 	Validation Loss: 1.342803 	 time: 0.3
Epoch: 3370 	Training Loss: 1.144795 	Validation Loss: 1.342202 	 time: 0.3
Epoch: 3371 	Training Loss: 1.144875 	Validation Loss: 1.343133 	 time: 0.3
Epoch: 3372 	Training Loss: 1.145004 	Validation Loss: 1.341081 	 time: 0.3
Epoch: 3373 	Training Loss: 1.145189 	Validation Loss: 1.342279 	 time: 0.3
Epoch: 3374 	Training Loss: 1.144925 	Validation Loss: 1.345078 	 time: 0.3
Epoch: 3375 	Training Loss: 1.145292 	Validation Loss: 1.341114 	 time: 0.3
Epoch: 3376 	Training Loss: 1.145288 	Validation Loss: 1.341545 	 time: 0.3
Epoch: 3377 	Training Loss: 1.145066 	Validation Loss: 1.345263 	 time: 0.3
Epoch: 3378 	Training Loss: 1.145258 	Validation Loss: 1.343176 	 time: 0.3
Epoch: 3379 	Training Loss: 1.144923 	Validation Loss: 1.340970 	 time: 0.3
Epoch: 3380 	Training Loss: 1.145064 	Validation Loss: 1.342928 	 time: 0.3
Epoch: 3381 	Training Loss: 1.145220 	Validation Loss: 1.340807 	 time: 0.3
Epoch: 3382 	Training Loss: 1.145081 	Validation Loss: 1.343174 	 time: 0.3
Epoch: 3383 	Training Loss: 1.145050 	Validation Loss: 1.346999 	 time: 0.3
Epoch: 3384 	Training Loss: 1.144972 	Validation Loss: 1.343752 	 time: 0.3
Epoch: 3385 	Training Loss: 1.144827 	Validation Loss: 1.341298 	 time: 0.3
Epoch: 3386 	Training Loss: 1.144880 	Validation Loss: 1.341607 	 time: 0.3
Epoch: 3387 	Training Loss: 1.144860 	Validation Loss: 1.340846 	 time: 0.3
Epoch: 3388 	Training Loss: 1.144936 	Validation Loss: 1.341267 	 time: 0.3
Epoch: 3389 	Training Loss: 1.144883 	Validation Loss: 1.340419 	 time: 0.3
Epoch: 3390 	Training Loss: 1.144816 	Validation Loss: 1.341500 	 time: 0.3
Epoch: 3391 	Training Loss: 1.144764 	Validation Loss: 1.343281 	 time: 0.3
Epoch: 3392 	Training Loss: 1.144817 	Validation Loss: 1.341982 	 time: 0.3
Epoch: 3393 	Training Loss: 1.144848 	Validation Loss: 1.341680 	 time: 0.3
Epoch: 3394 	Training Loss: 1.144852 	Validation Loss: 1.339704 	 time: 0.3
Epoch: 3395 	Training Loss: 1.144914 	Validation Loss: 1.340653 	 time: 0.3
Epoch: 3396 	Training Loss: 1.144736 	Validation Loss: 1.341967 	 time: 0.3
Epoch: 3397 	Training Loss: 1.144923 	Validation Loss: 1.339647 	 time: 0.3
Epoch: 3398 	Training Loss: 1.145448 	Validation Loss: 1.341517 	 time: 0.3
Epoch: 3399 	Training Loss: 1.144994 	Validation Loss: 1.344547 	 time: 0.3
Epoch: 3400 	Training Loss: 1.145383 	Validation Loss: 1.341270 	 time: 0.3
Epoch: 3401 	Training Loss: 1.145289 	Validation Loss: 1.340869 	 time: 0.3
Epoch: 3402 	Training Loss: 1.145477 	Validation Loss: 1.345223 	 time: 0.3
Epoch: 3403 	Training Loss: 1.145087 	Validation Loss: 1.348107 	 time: 0.3
Epoch: 3404 	Training Loss: 1.145647 	Validation Loss: 1.340873 	 time: 0.3
Epoch: 3405 	Training Loss: 1.145253 	Validation Loss: 1.339430 	 time: 0.3
Epoch: 3406 	Training Loss: 1.145858 	Validation Loss: 1.344231 	 time: 0.3
Epoch: 3407 	Training Loss: 1.145089 	Validation Loss: 1.349823 	 time: 0.3
Epoch: 3408 	Training Loss: 1.146222 	Validation Loss: 1.343143 	 time: 0.3
Epoch: 3409 	Training Loss: 1.145438 	Validation Loss: 1.340536 	 time: 0.3
Epoch: 3410 	Training Loss: 1.147711 	Validation Loss: 1.350030 	 time: 0.3
Epoch: 3411 	Training Loss: 1.145398 	Validation Loss: 1.350729 	 time: 0.3
Epoch: 3412 	Training Loss: 1.147575 	Validation Loss: 1.346345 	 time: 0.3
Epoch: 3413 	Training Loss: 1.145676 	Validation Loss: 1.341579 	 time: 0.3
Epoch: 3414 	Training Loss: 1.147007 	Validation Loss: 1.341101 	 time: 0.3
Epoch: 3415 	Training Loss: 1.146437 	Validation Loss: 1.342483 	 time: 0.3
Epoch: 3416 	Training Loss: 1.145909 	Validation Loss: 1.343789 	 time: 0.3
Epoch: 3417 	Training Loss: 1.146399 	Validation Loss: 1.345616 	 time: 0.3
Epoch: 3418 	Training Loss: 1.145522 	Validation Loss: 1.348596 	 time: 0.3
Epoch: 3419 	Training Loss: 1.145521 	Validation Loss: 1.348551 	 time: 0.3
Epoch: 3420 	Training Loss: 1.145698 	Validation Loss: 1.348332 	 time: 0.3
Epoch: 3421 	Training Loss: 1.145659 	Validation Loss: 1.348479 	 time: 0.3
Epoch: 3422 	Training Loss: 1.145361 	Validation Loss: 1.347772 	 time: 0.3
Epoch: 3423 	Training Loss: 1.145606 	Validation Loss: 1.347723 	 time: 0.3
Epoch: 3424 	Training Loss: 1.145354 	Validation Loss: 1.349503 	 time: 0.3
Epoch: 3425 	Training Loss: 1.145416 	Validation Loss: 1.349025 	 time: 0.3
Epoch: 3426 	Training Loss: 1.145355 	Validation Loss: 1.347678 	 time: 0.3
Epoch: 3427 	Training Loss: 1.145123 	Validation Loss: 1.348070 	 time: 0.3
Epoch: 3428 	Training Loss: 1.145196 	Validation Loss: 1.346299 	 time: 0.3
Epoch: 3429 	Training Loss: 1.145154 	Validation Loss: 1.344717 	 time: 0.3
Epoch: 3430 	Training Loss: 1.145080 	Validation Loss: 1.344386 	 time: 0.3
Epoch: 3431 	Training Loss: 1.145058 	Validation Loss: 1.344067 	 time: 0.3
Epoch: 3432 	Training Loss: 1.144955 	Validation Loss: 1.343219 	 time: 0.3
Epoch: 3433 	Training Loss: 1.144916 	Validation Loss: 1.341998 	 time: 0.3
Epoch: 3434 	Training Loss: 1.144944 	Validation Loss: 1.342234 	 time: 0.3
Epoch: 3435 	Training Loss: 1.144839 	Validation Loss: 1.342934 	 time: 0.3
Epoch: 3436 	Training Loss: 1.144777 	Validation Loss: 1.343058 	 time: 0.3
Epoch: 3437 	Training Loss: 1.144725 	Validation Loss: 1.343018 	 time: 0.3
Epoch: 3438 	Training Loss: 1.144737 	Validation Loss: 1.343106 	 time: 0.3
Epoch: 3439 	Training Loss: 1.144743 	Validation Loss: 1.342590 	 time: 0.3
Epoch: 3440 	Training Loss: 1.144670 	Validation Loss: 1.342201 	 time: 0.3
Epoch: 3441 	Training Loss: 1.144645 	Validation Loss: 1.342307 	 time: 0.3
Epoch: 3442 	Training Loss: 1.144608 	Validation Loss: 1.341841 	 time: 0.3
Epoch: 3443 	Training Loss: 1.144606 	Validation Loss: 1.341120 	 time: 0.3
Epoch: 3444 	Training Loss: 1.144609 	Validation Loss: 1.341620 	 time: 0.3
Epoch: 3445 	Training Loss: 1.144569 	Validation Loss: 1.341969 	 time: 0.3
Epoch: 3446 	Training Loss: 1.144534 	Validation Loss: 1.341800 	 time: 0.3
Epoch: 3447 	Training Loss: 1.144507 	Validation Loss: 1.342345 	 time: 0.3
Epoch: 3448 	Training Loss: 1.144500 	Validation Loss: 1.342665 	 time: 0.3
Epoch: 3449 	Training Loss: 1.144504 	Validation Loss: 1.342808 	 time: 0.3
Epoch: 3450 	Training Loss: 1.144496 	Validation Loss: 1.343207 	 time: 0.3
Epoch: 3451 	Training Loss: 1.144470 	Validation Loss: 1.343096 	 time: 0.3
Epoch: 3452 	Training Loss: 1.144460 	Validation Loss: 1.342552 	 time: 0.3
Epoch: 3453 	Training Loss: 1.144456 	Validation Loss: 1.342929 	 time: 0.3
Epoch: 3454 	Training Loss: 1.144448 	Validation Loss: 1.343432 	 time: 0.3
Epoch: 3455 	Training Loss: 1.144442 	Validation Loss: 1.343372 	 time: 0.3
Epoch: 3456 	Training Loss: 1.144439 	Validation Loss: 1.343643 	 time: 0.3
Epoch: 3457 	Training Loss: 1.144419 	Validation Loss: 1.343582 	 time: 0.3
Epoch: 3458 	Training Loss: 1.144413 	Validation Loss: 1.343411 	 time: 0.3
Epoch: 3459 	Training Loss: 1.144411 	Validation Loss: 1.343625 	 time: 0.3
Epoch: 3460 	Training Loss: 1.144409 	Validation Loss: 1.343646 	 time: 0.3
Epoch: 3461 	Training Loss: 1.144401 	Validation Loss: 1.343335 	 time: 0.3
Epoch: 3462 	Training Loss: 1.144390 	Validation Loss: 1.343353 	 time: 0.3
Epoch: 3463 	Training Loss: 1.144382 	Validation Loss: 1.343532 	 time: 0.3
Epoch: 3464 	Training Loss: 1.144371 	Validation Loss: 1.343550 	 time: 0.3
Epoch: 3465 	Training Loss: 1.144371 	Validation Loss: 1.343468 	 time: 0.3
Epoch: 3466 	Training Loss: 1.144360 	Validation Loss: 1.343129 	 time: 0.3
Epoch: 3467 	Training Loss: 1.144358 	Validation Loss: 1.343101 	 time: 0.3
Epoch: 3468 	Training Loss: 1.144356 	Validation Loss: 1.343540 	 time: 0.3
Epoch: 3469 	Training Loss: 1.144350 	Validation Loss: 1.343761 	 time: 0.3
Epoch: 3470 	Training Loss: 1.144343 	Validation Loss: 1.343557 	 time: 0.3
Epoch: 3471 	Training Loss: 1.144338 	Validation Loss: 1.343401 	 time: 0.3
Epoch: 3472 	Training Loss: 1.144328 	Validation Loss: 1.343373 	 time: 0.3
Epoch: 3473 	Training Loss: 1.144314 	Validation Loss: 1.343719 	 time: 0.3
Epoch: 3474 	Training Loss: 1.144292 	Validation Loss: 1.344007 	 time: 0.3
Epoch: 3475 	Training Loss: 1.144276 	Validation Loss: 1.343843 	 time: 0.3
Epoch: 3476 	Training Loss: 1.144267 	Validation Loss: 1.343779 	 time: 0.3
Epoch: 3477 	Training Loss: 1.144258 	Validation Loss: 1.343662 	 time: 0.3
Epoch: 3478 	Training Loss: 1.144255 	Validation Loss: 1.343435 	 time: 0.3
Epoch: 3479 	Training Loss: 1.144255 	Validation Loss: 1.343517 	 time: 0.3
Epoch: 3480 	Training Loss: 1.144251 	Validation Loss: 1.343402 	 time: 0.3
Epoch: 3481 	Training Loss: 1.144249 	Validation Loss: 1.343349 	 time: 0.3
Epoch: 3482 	Training Loss: 1.144248 	Validation Loss: 1.343567 	 time: 0.3
Epoch: 3483 	Training Loss: 1.144246 	Validation Loss: 1.343545 	 time: 0.3
Epoch: 3484 	Training Loss: 1.144243 	Validation Loss: 1.343582 	 time: 0.3
Epoch: 3485 	Training Loss: 1.144238 	Validation Loss: 1.343706 	 time: 0.3
Epoch: 3486 	Training Loss: 1.144237 	Validation Loss: 1.343652 	 time: 0.3
Epoch: 3487 	Training Loss: 1.144233 	Validation Loss: 1.343686 	 time: 0.3
Epoch: 3488 	Training Loss: 1.144231 	Validation Loss: 1.343613 	 time: 0.3
Epoch: 3489 	Training Loss: 1.144229 	Validation Loss: 1.343426 	 time: 0.3
Epoch: 3490 	Training Loss: 1.144229 	Validation Loss: 1.343503 	 time: 0.3
Epoch: 3491 	Training Loss: 1.144227 	Validation Loss: 1.343521 	 time: 0.3
Epoch: 3492 	Training Loss: 1.144225 	Validation Loss: 1.343487 	 time: 0.3
Epoch: 3493 	Training Loss: 1.144222 	Validation Loss: 1.343577 	 time: 0.3
Epoch: 3494 	Training Loss: 1.144220 	Validation Loss: 1.343548 	 time: 0.3
Epoch: 3495 	Training Loss: 1.144218 	Validation Loss: 1.343622 	 time: 0.3
Epoch: 3496 	Training Loss: 1.144216 	Validation Loss: 1.343692 	 time: 0.3
Epoch: 3497 	Training Loss: 1.144214 	Validation Loss: 1.343580 	 time: 0.3
Epoch: 3498 	Training Loss: 1.144212 	Validation Loss: 1.343613 	 time: 0.3
Epoch: 3499 	Training Loss: 1.144211 	Validation Loss: 1.343674 	 time: 0.3
Epoch: 3500 	Training Loss: 1.144209 	Validation Loss: 1.343704 	 time: 0.3
Epoch: 3501 	Training Loss: 1.144208 	Validation Loss: 1.343779 	 time: 0.3
Epoch: 3502 	Training Loss: 1.144206 	Validation Loss: 1.343678 	 time: 0.3
Epoch: 3503 	Training Loss: 1.144204 	Validation Loss: 1.343631 	 time: 0.3
Epoch: 3504 	Training Loss: 1.144202 	Validation Loss: 1.343713 	 time: 0.3
Epoch: 3505 	Training Loss: 1.144200 	Validation Loss: 1.343725 	 time: 0.3
Epoch: 3506 	Training Loss: 1.144198 	Validation Loss: 1.343772 	 time: 0.3
Epoch: 3507 	Training Loss: 1.144195 	Validation Loss: 1.343761 	 time: 0.3
Epoch: 3508 	Training Loss: 1.144193 	Validation Loss: 1.343704 	 time: 0.3
Epoch: 3509 	Training Loss: 1.144190 	Validation Loss: 1.343744 	 time: 0.3
Epoch: 3510 	Training Loss: 1.144189 	Validation Loss: 1.343700 	 time: 0.3
Epoch: 3511 	Training Loss: 1.144188 	Validation Loss: 1.343698 	 time: 0.3
Epoch: 3512 	Training Loss: 1.144187 	Validation Loss: 1.343754 	 time: 0.3
Epoch: 3513 	Training Loss: 1.144186 	Validation Loss: 1.343754 	 time: 0.3
Epoch: 3514 	Training Loss: 1.144184 	Validation Loss: 1.343829 	 time: 0.3
Epoch: 3515 	Training Loss: 1.144183 	Validation Loss: 1.343799 	 time: 0.3
Epoch: 3516 	Training Loss: 1.144182 	Validation Loss: 1.343764 	 time: 0.3
Epoch: 3517 	Training Loss: 1.144180 	Validation Loss: 1.343717 	 time: 0.3
Epoch: 3518 	Training Loss: 1.144178 	Validation Loss: 1.343661 	 time: 0.3
Epoch: 3519 	Training Loss: 1.144177 	Validation Loss: 1.343714 	 time: 0.3
Epoch: 3520 	Training Loss: 1.144176 	Validation Loss: 1.343742 	 time: 0.3
Epoch: 3521 	Training Loss: 1.144175 	Validation Loss: 1.343789 	 time: 0.3
Epoch: 3522 	Training Loss: 1.144174 	Validation Loss: 1.343820 	 time: 0.3
Epoch: 3523 	Training Loss: 1.144173 	Validation Loss: 1.343812 	 time: 0.3
Epoch: 3524 	Training Loss: 1.144172 	Validation Loss: 1.343853 	 time: 0.3
Epoch: 3525 	Training Loss: 1.144170 	Validation Loss: 1.343847 	 time: 0.3
Epoch: 3526 	Training Loss: 1.144169 	Validation Loss: 1.343836 	 time: 0.3
Epoch: 3527 	Training Loss: 1.144168 	Validation Loss: 1.343826 	 time: 0.3
Epoch: 3528 	Training Loss: 1.144167 	Validation Loss: 1.343808 	 time: 0.3
Epoch: 3529 	Training Loss: 1.144166 	Validation Loss: 1.343853 	 time: 0.3
Epoch: 3530 	Training Loss: 1.144165 	Validation Loss: 1.343863 	 time: 0.3
Epoch: 3531 	Training Loss: 1.144163 	Validation Loss: 1.343879 	 time: 0.3
Epoch: 3532 	Training Loss: 1.144162 	Validation Loss: 1.343889 	 time: 0.3
Epoch: 3533 	Training Loss: 1.144161 	Validation Loss: 1.343899 	 time: 0.3
Epoch: 3534 	Training Loss: 1.144160 	Validation Loss: 1.343933 	 time: 0.3
Epoch: 3535 	Training Loss: 1.144159 	Validation Loss: 1.343927 	 time: 0.3
Epoch: 3536 	Training Loss: 1.144158 	Validation Loss: 1.343946 	 time: 0.3
Epoch: 3537 	Training Loss: 1.144157 	Validation Loss: 1.343958 	 time: 0.3
Epoch: 3538 	Training Loss: 1.144156 	Validation Loss: 1.343972 	 time: 0.3
Epoch: 3539 	Training Loss: 1.144155 	Validation Loss: 1.343984 	 time: 0.3
Epoch: 3540 	Training Loss: 1.144154 	Validation Loss: 1.343973 	 time: 0.3
Epoch: 3541 	Training Loss: 1.144153 	Validation Loss: 1.343990 	 time: 0.3
Epoch: 3542 	Training Loss: 1.144152 	Validation Loss: 1.343996 	 time: 0.3
Epoch: 3543 	Training Loss: 1.144151 	Validation Loss: 1.344012 	 time: 0.3
Epoch: 3544 	Training Loss: 1.144150 	Validation Loss: 1.344025 	 time: 0.3
Epoch: 3545 	Training Loss: 1.144149 	Validation Loss: 1.344028 	 time: 0.3
Epoch: 3546 	Training Loss: 1.144148 	Validation Loss: 1.344044 	 time: 0.3
Epoch: 3547 	Training Loss: 1.144147 	Validation Loss: 1.344042 	 time: 0.3
Epoch: 3548 	Training Loss: 1.144146 	Validation Loss: 1.344051 	 time: 0.3
Epoch: 3549 	Training Loss: 1.144144 	Validation Loss: 1.344044 	 time: 0.3
Epoch: 3550 	Training Loss: 1.144143 	Validation Loss: 1.344035 	 time: 0.3
Epoch: 3551 	Training Loss: 1.144142 	Validation Loss: 1.344016 	 time: 0.3
Epoch: 3552 	Training Loss: 1.144139 	Validation Loss: 1.343977 	 time: 0.3
Epoch: 3553 	Training Loss: 1.144136 	Validation Loss: 1.343914 	 time: 0.3
Epoch: 3554 	Training Loss: 1.144126 	Validation Loss: 1.343750 	 time: 0.3
Epoch: 3555 	Training Loss: 1.144102 	Validation Loss: 1.343503 	 time: 0.3
Epoch: 3556 	Training Loss: 1.144073 	Validation Loss: 1.343334 	 time: 0.3
Epoch: 3557 	Training Loss: 1.144064 	Validation Loss: 1.343170 	 time: 0.3
Epoch: 3558 	Training Loss: 1.144062 	Validation Loss: 1.343234 	 time: 0.3
Epoch: 3559 	Training Loss: 1.144062 	Validation Loss: 1.343151 	 time: 0.3
Epoch: 3560 	Training Loss: 1.144061 	Validation Loss: 1.343122 	 time: 0.3
Epoch: 3561 	Training Loss: 1.144060 	Validation Loss: 1.343055 	 time: 0.3
Epoch: 3562 	Training Loss: 1.144059 	Validation Loss: 1.343091 	 time: 0.3
Epoch: 3563 	Training Loss: 1.144058 	Validation Loss: 1.343159 	 time: 0.3
Epoch: 3564 	Training Loss: 1.144057 	Validation Loss: 1.343176 	 time: 0.3
Epoch: 3565 	Training Loss: 1.144056 	Validation Loss: 1.343266 	 time: 0.3
Epoch: 3566 	Training Loss: 1.144054 	Validation Loss: 1.343289 	 time: 0.3
Epoch: 3567 	Training Loss: 1.144053 	Validation Loss: 1.343372 	 time: 0.3
Epoch: 3568 	Training Loss: 1.144052 	Validation Loss: 1.343332 	 time: 0.3
Epoch: 3569 	Training Loss: 1.144051 	Validation Loss: 1.343404 	 time: 0.3
Epoch: 3570 	Training Loss: 1.144050 	Validation Loss: 1.343472 	 time: 0.3
Epoch: 3571 	Training Loss: 1.144049 	Validation Loss: 1.343599 	 time: 0.3
Epoch: 3572 	Training Loss: 1.144047 	Validation Loss: 1.343664 	 time: 0.3
Epoch: 3573 	Training Loss: 1.144046 	Validation Loss: 1.343698 	 time: 0.3
Epoch: 3574 	Training Loss: 1.144045 	Validation Loss: 1.343752 	 time: 0.3
Epoch: 3575 	Training Loss: 1.144044 	Validation Loss: 1.343759 	 time: 0.3
Epoch: 3576 	Training Loss: 1.144043 	Validation Loss: 1.343810 	 time: 0.3
Epoch: 3577 	Training Loss: 1.144042 	Validation Loss: 1.343806 	 time: 0.3
Epoch: 3578 	Training Loss: 1.144041 	Validation Loss: 1.343884 	 time: 0.3
Epoch: 3579 	Training Loss: 1.144040 	Validation Loss: 1.343886 	 time: 0.3
Epoch: 3580 	Training Loss: 1.144038 	Validation Loss: 1.343939 	 time: 0.3
Epoch: 3581 	Training Loss: 1.144037 	Validation Loss: 1.343935 	 time: 0.3
Epoch: 3582 	Training Loss: 1.144036 	Validation Loss: 1.344005 	 time: 0.3
Epoch: 3583 	Training Loss: 1.144034 	Validation Loss: 1.344044 	 time: 0.3
Epoch: 3584 	Training Loss: 1.144032 	Validation Loss: 1.344150 	 time: 0.3
Epoch: 3585 	Training Loss: 1.144027 	Validation Loss: 1.344281 	 time: 0.3
Epoch: 3586 	Training Loss: 1.144015 	Validation Loss: 1.344527 	 time: 0.3
Epoch: 3587 	Training Loss: 1.143992 	Validation Loss: 1.344664 	 time: 0.3
Epoch: 3588 	Training Loss: 1.143979 	Validation Loss: 1.344639 	 time: 0.3
Epoch: 3589 	Training Loss: 1.143975 	Validation Loss: 1.344659 	 time: 0.3
Epoch: 3590 	Training Loss: 1.143975 	Validation Loss: 1.344836 	 time: 0.3
Epoch: 3591 	Training Loss: 1.143975 	Validation Loss: 1.345082 	 time: 0.3
Epoch: 3592 	Training Loss: 1.143973 	Validation Loss: 1.345283 	 time: 0.3
Epoch: 3593 	Training Loss: 1.143972 	Validation Loss: 1.345450 	 time: 0.3
Epoch: 3594 	Training Loss: 1.143972 	Validation Loss: 1.345526 	 time: 0.3
Epoch: 3595 	Training Loss: 1.143971 	Validation Loss: 1.345552 	 time: 0.3
Epoch: 3596 	Training Loss: 1.143970 	Validation Loss: 1.345512 	 time: 0.3
Epoch: 3597 	Training Loss: 1.143968 	Validation Loss: 1.345526 	 time: 0.3
Epoch: 3598 	Training Loss: 1.143967 	Validation Loss: 1.345540 	 time: 0.3
Epoch: 3599 	Training Loss: 1.143966 	Validation Loss: 1.345631 	 time: 0.3
Epoch: 3600 	Training Loss: 1.143965 	Validation Loss: 1.345633 	 time: 0.3
Epoch: 3601 	Training Loss: 1.143963 	Validation Loss: 1.345631 	 time: 0.3
Epoch: 3602 	Training Loss: 1.143962 	Validation Loss: 1.345523 	 time: 0.3
Epoch: 3603 	Training Loss: 1.143961 	Validation Loss: 1.345547 	 time: 0.3
Epoch: 3604 	Training Loss: 1.143960 	Validation Loss: 1.345519 	 time: 0.3
Epoch: 3605 	Training Loss: 1.143959 	Validation Loss: 1.345620 	 time: 0.3
Epoch: 3606 	Training Loss: 1.143957 	Validation Loss: 1.345497 	 time: 0.3
Epoch: 3607 	Training Loss: 1.143957 	Validation Loss: 1.345571 	 time: 0.3
Epoch: 3608 	Training Loss: 1.143956 	Validation Loss: 1.345355 	 time: 0.3
Epoch: 3609 	Training Loss: 1.143955 	Validation Loss: 1.345502 	 time: 0.3
Epoch: 3610 	Training Loss: 1.143956 	Validation Loss: 1.345138 	 time: 0.3
Epoch: 3611 	Training Loss: 1.143957 	Validation Loss: 1.345434 	 time: 0.3
Epoch: 3612 	Training Loss: 1.143960 	Validation Loss: 1.344903 	 time: 0.3
Epoch: 3613 	Training Loss: 1.143967 	Validation Loss: 1.345526 	 time: 0.3
Epoch: 3614 	Training Loss: 1.143974 	Validation Loss: 1.344683 	 time: 0.3
Epoch: 3615 	Training Loss: 1.143988 	Validation Loss: 1.345544 	 time: 0.3
Epoch: 3616 	Training Loss: 1.143990 	Validation Loss: 1.344596 	 time: 0.3
Epoch: 3617 	Training Loss: 1.143988 	Validation Loss: 1.345411 	 time: 0.3
Epoch: 3618 	Training Loss: 1.143969 	Validation Loss: 1.344847 	 time: 0.3
Epoch: 3619 	Training Loss: 1.143955 	Validation Loss: 1.345129 	 time: 0.3
Epoch: 3620 	Training Loss: 1.143945 	Validation Loss: 1.344985 	 time: 0.3
Epoch: 3621 	Training Loss: 1.143943 	Validation Loss: 1.344763 	 time: 0.3
Epoch: 3622 	Training Loss: 1.143945 	Validation Loss: 1.345114 	 time: 0.3
Epoch: 3623 	Training Loss: 1.143952 	Validation Loss: 1.344451 	 time: 0.3
Epoch: 3624 	Training Loss: 1.143968 	Validation Loss: 1.345263 	 time: 0.3
Epoch: 3625 	Training Loss: 1.143986 	Validation Loss: 1.344092 	 time: 0.3
Epoch: 3626 	Training Loss: 1.144017 	Validation Loss: 1.345387 	 time: 0.3
Epoch: 3627 	Training Loss: 1.144002 	Validation Loss: 1.344380 	 time: 0.3
Epoch: 3628 	Training Loss: 1.143978 	Validation Loss: 1.345164 	 time: 0.3
Epoch: 3629 	Training Loss: 1.143946 	Validation Loss: 1.344935 	 time: 0.3
Epoch: 3630 	Training Loss: 1.143936 	Validation Loss: 1.344603 	 time: 0.3
Epoch: 3631 	Training Loss: 1.143943 	Validation Loss: 1.345123 	 time: 0.3
Epoch: 3632 	Training Loss: 1.143964 	Validation Loss: 1.344164 	 time: 0.3
Epoch: 3633 	Training Loss: 1.144005 	Validation Loss: 1.345383 	 time: 0.3
Epoch: 3634 	Training Loss: 1.144033 	Validation Loss: 1.343857 	 time: 0.3
Epoch: 3635 	Training Loss: 1.144029 	Validation Loss: 1.345052 	 time: 0.3
Epoch: 3636 	Training Loss: 1.143945 	Validation Loss: 1.345446 	 time: 0.3
Epoch: 3637 	Training Loss: 1.143963 	Validation Loss: 1.343955 	 time: 0.3
Epoch: 3638 	Training Loss: 1.144092 	Validation Loss: 1.345590 	 time: 0.3
Epoch: 3639 	Training Loss: 1.144223 	Validation Loss: 1.343328 	 time: 0.3
Epoch: 3640 	Training Loss: 1.144090 	Validation Loss: 1.344264 	 time: 0.3
Epoch: 3641 	Training Loss: 1.144130 	Validation Loss: 1.347234 	 time: 0.3
Epoch: 3642 	Training Loss: 1.144520 	Validation Loss: 1.342934 	 time: 0.3
Epoch: 3643 	Training Loss: 1.144456 	Validation Loss: 1.342720 	 time: 0.3
Epoch: 3644 	Training Loss: 1.144502 	Validation Loss: 1.346117 	 time: 0.3
Epoch: 3645 	Training Loss: 1.144325 	Validation Loss: 1.348381 	 time: 0.3
Epoch: 3646 	Training Loss: 1.144536 	Validation Loss: 1.346284 	 time: 0.3
Epoch: 3647 	Training Loss: 1.144241 	Validation Loss: 1.345152 	 time: 0.3
Epoch: 3648 	Training Loss: 1.144256 	Validation Loss: 1.346605 	 time: 0.3
Epoch: 3649 	Training Loss: 1.144429 	Validation Loss: 1.344677 	 time: 0.3
Epoch: 3650 	Training Loss: 1.144255 	Validation Loss: 1.344715 	 time: 0.3
Epoch: 3651 	Training Loss: 1.144273 	Validation Loss: 1.345599 	 time: 0.3
Epoch: 3652 	Training Loss: 1.144344 	Validation Loss: 1.344984 	 time: 0.3
Epoch: 3653 	Training Loss: 1.144225 	Validation Loss: 1.344638 	 time: 0.3
Epoch: 3654 	Training Loss: 1.144313 	Validation Loss: 1.346701 	 time: 0.3
Epoch: 3655 	Training Loss: 1.144393 	Validation Loss: 1.345607 	 time: 0.3
Epoch: 3656 	Training Loss: 1.144243 	Validation Loss: 1.344193 	 time: 0.3
Epoch: 3657 	Training Loss: 1.144232 	Validation Loss: 1.345490 	 time: 0.3
Epoch: 3658 	Training Loss: 1.144179 	Validation Loss: 1.346405 	 time: 0.3
Epoch: 3659 	Training Loss: 1.144206 	Validation Loss: 1.344975 	 time: 0.3
Epoch: 3660 	Training Loss: 1.144085 	Validation Loss: 1.344486 	 time: 0.3
Epoch: 3661 	Training Loss: 1.144106 	Validation Loss: 1.345689 	 time: 0.3
Epoch: 3662 	Training Loss: 1.144195 	Validation Loss: 1.344272 	 time: 0.3
Epoch: 3663 	Training Loss: 1.144134 	Validation Loss: 1.343777 	 time: 0.3
Epoch: 3664 	Training Loss: 1.144060 	Validation Loss: 1.343895 	 time: 0.3
Epoch: 3665 	Training Loss: 1.144158 	Validation Loss: 1.342304 	 time: 0.3
Epoch: 3666 	Training Loss: 1.144171 	Validation Loss: 1.343515 	 time: 0.3
Epoch: 3667 	Training Loss: 1.144031 	Validation Loss: 1.344879 	 time: 0.3
Epoch: 3668 	Training Loss: 1.144088 	Validation Loss: 1.343289 	 time: 0.3
Epoch: 3669 	Training Loss: 1.144156 	Validation Loss: 1.344180 	 time: 0.3
Epoch: 3670 	Training Loss: 1.144033 	Validation Loss: 1.344133 	 time: 0.3
Epoch: 3671 	Training Loss: 1.144044 	Validation Loss: 1.342855 	 time: 0.3
Epoch: 3672 	Training Loss: 1.144121 	Validation Loss: 1.344409 	 time: 0.3
Epoch: 3673 	Training Loss: 1.144030 	Validation Loss: 1.343996 	 time: 0.3
Epoch: 3674 	Training Loss: 1.143998 	Validation Loss: 1.343304 	 time: 0.3
Epoch: 3675 	Training Loss: 1.144015 	Validation Loss: 1.344377 	 time: 0.3
Epoch: 3676 	Training Loss: 1.144002 	Validation Loss: 1.343908 	 time: 0.3
Epoch: 3677 	Training Loss: 1.143989 	Validation Loss: 1.344140 	 time: 0.3
Epoch: 3678 	Training Loss: 1.143968 	Validation Loss: 1.343444 	 time: 0.3
Epoch: 3679 	Training Loss: 1.143962 	Validation Loss: 1.343226 	 time: 0.3
Epoch: 3680 	Training Loss: 1.143935 	Validation Loss: 1.343384 	 time: 0.3
Epoch: 3681 	Training Loss: 1.143939 	Validation Loss: 1.343395 	 time: 0.3
Epoch: 3682 	Training Loss: 1.143922 	Validation Loss: 1.343742 	 time: 0.3
Epoch: 3683 	Training Loss: 1.143917 	Validation Loss: 1.343760 	 time: 0.3
Epoch: 3684 	Training Loss: 1.143916 	Validation Loss: 1.343725 	 time: 0.3
Epoch: 3685 	Training Loss: 1.143910 	Validation Loss: 1.343717 	 time: 0.3
Epoch: 3686 	Training Loss: 1.143915 	Validation Loss: 1.343551 	 time: 0.3
Epoch: 3687 	Training Loss: 1.143910 	Validation Loss: 1.343843 	 time: 0.3
Epoch: 3688 	Training Loss: 1.143906 	Validation Loss: 1.343641 	 time: 0.3
Epoch: 3689 	Training Loss: 1.143894 	Validation Loss: 1.343566 	 time: 0.3
Epoch: 3690 	Training Loss: 1.143899 	Validation Loss: 1.343429 	 time: 0.3
Epoch: 3691 	Training Loss: 1.143896 	Validation Loss: 1.343432 	 time: 0.3
Epoch: 3692 	Training Loss: 1.143893 	Validation Loss: 1.343920 	 time: 0.3
Epoch: 3693 	Training Loss: 1.143898 	Validation Loss: 1.343395 	 time: 0.3
Epoch: 3694 	Training Loss: 1.143898 	Validation Loss: 1.343818 	 time: 0.3
Epoch: 3695 	Training Loss: 1.143894 	Validation Loss: 1.343835 	 time: 0.3
Epoch: 3696 	Training Loss: 1.143885 	Validation Loss: 1.344154 	 time: 0.3
Epoch: 3697 	Training Loss: 1.143884 	Validation Loss: 1.343872 	 time: 0.3
Epoch: 3698 	Training Loss: 1.143879 	Validation Loss: 1.343478 	 time: 0.3
Epoch: 3699 	Training Loss: 1.143880 	Validation Loss: 1.343799 	 time: 0.3
Epoch: 3700 	Training Loss: 1.143880 	Validation Loss: 1.343669 	 time: 0.3
Epoch: 3701 	Training Loss: 1.143885 	Validation Loss: 1.343874 	 time: 0.3
Epoch: 3702 	Training Loss: 1.143887 	Validation Loss: 1.343316 	 time: 0.3
Epoch: 3703 	Training Loss: 1.143891 	Validation Loss: 1.344118 	 time: 0.3
Epoch: 3704 	Training Loss: 1.143893 	Validation Loss: 1.343616 	 time: 0.3
Epoch: 3705 	Training Loss: 1.143894 	Validation Loss: 1.344247 	 time: 0.3
Epoch: 3706 	Training Loss: 1.143892 	Validation Loss: 1.343617 	 time: 0.3
Epoch: 3707 	Training Loss: 1.143888 	Validation Loss: 1.344333 	 time: 0.3
Epoch: 3708 	Training Loss: 1.143875 	Validation Loss: 1.344055 	 time: 0.3
Epoch: 3709 	Training Loss: 1.143868 	Validation Loss: 1.344000 	 time: 0.3
Epoch: 3710 	Training Loss: 1.143863 	Validation Loss: 1.343985 	 time: 0.3
Epoch: 3711 	Training Loss: 1.143860 	Validation Loss: 1.343924 	 time: 0.3
Epoch: 3712 	Training Loss: 1.143861 	Validation Loss: 1.344122 	 time: 0.3
Epoch: 3713 	Training Loss: 1.143865 	Validation Loss: 1.343503 	 time: 0.3
Epoch: 3714 	Training Loss: 1.143879 	Validation Loss: 1.344401 	 time: 0.3
Epoch: 3715 	Training Loss: 1.143899 	Validation Loss: 1.343193 	 time: 0.3
Epoch: 3716 	Training Loss: 1.143958 	Validation Loss: 1.344710 	 time: 0.3
Epoch: 3717 	Training Loss: 1.144005 	Validation Loss: 1.342888 	 time: 0.3
Epoch: 3718 	Training Loss: 1.144003 	Validation Loss: 1.344349 	 time: 0.3
Epoch: 3719 	Training Loss: 1.143868 	Validation Loss: 1.345331 	 time: 0.3
Epoch: 3720 	Training Loss: 1.143959 	Validation Loss: 1.342918 	 time: 0.3
Epoch: 3721 	Training Loss: 1.144237 	Validation Loss: 1.344653 	 time: 0.3
Epoch: 3722 	Training Loss: 1.144195 	Validation Loss: 1.344658 	 time: 0.3
Epoch: 3723 	Training Loss: 1.144150 	Validation Loss: 1.342337 	 time: 0.3
Epoch: 3724 	Training Loss: 1.144424 	Validation Loss: 1.343835 	 time: 0.3
Epoch: 3725 	Training Loss: 1.144154 	Validation Loss: 1.347305 	 time: 0.3
Epoch: 3726 	Training Loss: 1.144261 	Validation Loss: 1.344935 	 time: 0.3
Epoch: 3727 	Training Loss: 1.144064 	Validation Loss: 1.344714 	 time: 0.3
Epoch: 3728 	Training Loss: 1.144072 	Validation Loss: 1.347398 	 time: 0.3
Epoch: 3729 	Training Loss: 1.144154 	Validation Loss: 1.344638 	 time: 0.3
Epoch: 3730 	Training Loss: 1.144060 	Validation Loss: 1.343973 	 time: 0.3
Epoch: 3731 	Training Loss: 1.143962 	Validation Loss: 1.344621 	 time: 0.3
Epoch: 3732 	Training Loss: 1.144129 	Validation Loss: 1.342694 	 time: 0.3
Epoch: 3733 	Training Loss: 1.144186 	Validation Loss: 1.344592 	 time: 0.3
Epoch: 3734 	Training Loss: 1.144051 	Validation Loss: 1.345718 	 time: 0.3
Epoch: 3735 	Training Loss: 1.144202 	Validation Loss: 1.342988 	 time: 0.3
Epoch: 3736 	Training Loss: 1.144565 	Validation Loss: 1.344748 	 time: 0.3
Epoch: 3737 	Training Loss: 1.144482 	Validation Loss: 1.348126 	 time: 0.3
Epoch: 3738 	Training Loss: 1.144595 	Validation Loss: 1.344139 	 time: 0.3
Epoch: 3739 	Training Loss: 1.144273 	Validation Loss: 1.342075 	 time: 0.3
Epoch: 3740 	Training Loss: 1.144565 	Validation Loss: 1.344521 	 time: 0.3
Epoch: 3741 	Training Loss: 1.144269 	Validation Loss: 1.345711 	 time: 0.3
Epoch: 3742 	Training Loss: 1.144517 	Validation Loss: 1.343417 	 time: 0.3
Epoch: 3743 	Training Loss: 1.144260 	Validation Loss: 1.343791 	 time: 0.3
Epoch: 3744 	Training Loss: 1.144175 	Validation Loss: 1.344391 	 time: 0.3
Epoch: 3745 	Training Loss: 1.144415 	Validation Loss: 1.343644 	 time: 0.3
Epoch: 3746 	Training Loss: 1.144287 	Validation Loss: 1.344917 	 time: 0.3
Epoch: 3747 	Training Loss: 1.144189 	Validation Loss: 1.346207 	 time: 0.3
Epoch: 3748 	Training Loss: 1.144465 	Validation Loss: 1.343811 	 time: 0.3
Epoch: 3749 	Training Loss: 1.144349 	Validation Loss: 1.344973 	 time: 0.3
Epoch: 3750 	Training Loss: 1.144332 	Validation Loss: 1.347526 	 time: 0.3
Epoch: 3751 	Training Loss: 1.144296 	Validation Loss: 1.345249 	 time: 0.3
Epoch: 3752 	Training Loss: 1.144079 	Validation Loss: 1.343006 	 time: 0.3
Epoch: 3753 	Training Loss: 1.144218 	Validation Loss: 1.343483 	 time: 0.3
Epoch: 3754 	Training Loss: 1.144203 	Validation Loss: 1.343164 	 time: 0.3
Epoch: 3755 	Training Loss: 1.144083 	Validation Loss: 1.342266 	 time: 0.3
Epoch: 3756 	Training Loss: 1.144125 	Validation Loss: 1.342870 	 time: 0.3
Epoch: 3757 	Training Loss: 1.144089 	Validation Loss: 1.342974 	 time: 0.3
Epoch: 3758 	Training Loss: 1.144008 	Validation Loss: 1.344299 	 time: 0.3
Epoch: 3759 	Training Loss: 1.144004 	Validation Loss: 1.345238 	 time: 0.3
Epoch: 3760 	Training Loss: 1.143990 	Validation Loss: 1.343910 	 time: 0.3
Epoch: 3761 	Training Loss: 1.143967 	Validation Loss: 1.342216 	 time: 0.3
Epoch: 3762 	Training Loss: 1.143957 	Validation Loss: 1.341556 	 time: 0.3
Epoch: 3763 	Training Loss: 1.143982 	Validation Loss: 1.342493 	 time: 0.3
Epoch: 3764 	Training Loss: 1.143929 	Validation Loss: 1.343385 	 time: 0.3
Epoch: 3765 	Training Loss: 1.143927 	Validation Loss: 1.342288 	 time: 0.3
Epoch: 3766 	Training Loss: 1.143911 	Validation Loss: 1.341773 	 time: 0.3
Epoch: 3767 	Training Loss: 1.143929 	Validation Loss: 1.341781 	 time: 0.3
Epoch: 3768 	Training Loss: 1.143914 	Validation Loss: 1.342407 	 time: 0.3
Epoch: 3769 	Training Loss: 1.143903 	Validation Loss: 1.342197 	 time: 0.3
Epoch: 3770 	Training Loss: 1.143886 	Validation Loss: 1.342149 	 time: 0.3
Epoch: 3771 	Training Loss: 1.143887 	Validation Loss: 1.342229 	 time: 0.3
Epoch: 3772 	Training Loss: 1.143886 	Validation Loss: 1.342221 	 time: 0.3
Epoch: 3773 	Training Loss: 1.143873 	Validation Loss: 1.342437 	 time: 0.3
Epoch: 3774 	Training Loss: 1.143882 	Validation Loss: 1.341882 	 time: 0.3
Epoch: 3775 	Training Loss: 1.143887 	Validation Loss: 1.342548 	 time: 0.3
Epoch: 3776 	Training Loss: 1.143901 	Validation Loss: 1.342262 	 time: 0.3
Epoch: 3777 	Training Loss: 1.143883 	Validation Loss: 1.342936 	 time: 0.3
Epoch: 3778 	Training Loss: 1.143863 	Validation Loss: 1.342760 	 time: 0.3
Epoch: 3779 	Training Loss: 1.143862 	Validation Loss: 1.343066 	 time: 0.3
Epoch: 3780 	Training Loss: 1.143847 	Validation Loss: 1.343292 	 time: 0.3
Epoch: 3781 	Training Loss: 1.143853 	Validation Loss: 1.343156 	 time: 0.3
Epoch: 3782 	Training Loss: 1.143842 	Validation Loss: 1.343066 	 time: 0.3
Epoch: 3783 	Training Loss: 1.143843 	Validation Loss: 1.342804 	 time: 0.3
Epoch: 3784 	Training Loss: 1.143840 	Validation Loss: 1.343159 	 time: 0.3
Epoch: 3785 	Training Loss: 1.143842 	Validation Loss: 1.342663 	 time: 0.3
Epoch: 3786 	Training Loss: 1.143852 	Validation Loss: 1.343367 	 time: 0.3
Epoch: 3787 	Training Loss: 1.143861 	Validation Loss: 1.342526 	 time: 0.3
Epoch: 3788 	Training Loss: 1.143914 	Validation Loss: 1.344032 	 time: 0.3
Epoch: 3789 	Training Loss: 1.143939 	Validation Loss: 1.342117 	 time: 0.3
Epoch: 3790 	Training Loss: 1.143998 	Validation Loss: 1.343673 	 time: 0.3
Epoch: 3791 	Training Loss: 1.143912 	Validation Loss: 1.343637 	 time: 0.3
Epoch: 3792 	Training Loss: 1.143833 	Validation Loss: 1.344214 	 time: 0.3
Epoch: 3793 	Training Loss: 1.143817 	Validation Loss: 1.345033 	 time: 0.3
Epoch: 3794 	Training Loss: 1.143829 	Validation Loss: 1.342911 	 time: 0.3
Epoch: 3795 	Training Loss: 1.143916 	Validation Loss: 1.344617 	 time: 0.3
Epoch: 3796 	Training Loss: 1.143777 	Validation Loss: 1.344553 	 time: 0.3
Epoch: 3797 	Training Loss: 1.143739 	Validation Loss: 1.343828 	 time: 0.3
Epoch: 3798 	Training Loss: 1.143820 	Validation Loss: 1.345063 	 time: 0.3
Epoch: 3799 	Training Loss: 1.143735 	Validation Loss: 1.344328 	 time: 0.3
Epoch: 3800 	Training Loss: 1.143653 	Validation Loss: 1.345604 	 time: 0.3
Epoch: 3801 	Training Loss: 1.143639 	Validation Loss: 1.346485 	 time: 0.3
Epoch: 3802 	Training Loss: 1.143708 	Validation Loss: 1.343568 	 time: 0.3
Epoch: 3803 	Training Loss: 1.143863 	Validation Loss: 1.344216 	 time: 0.3
Epoch: 3804 	Training Loss: 1.143679 	Validation Loss: 1.346015 	 time: 0.3
Epoch: 3805 	Training Loss: 1.143862 	Validation Loss: 1.344744 	 time: 0.3
Epoch: 3806 	Training Loss: 1.144305 	Validation Loss: 1.345298 	 time: 0.3
Epoch: 3807 	Training Loss: 1.144048 	Validation Loss: 1.347676 	 time: 0.3
Epoch: 3808 	Training Loss: 1.144096 	Validation Loss: 1.346205 	 time: 0.3
Epoch: 3809 	Training Loss: 1.143935 	Validation Loss: 1.345935 	 time: 0.3
Epoch: 3810 	Training Loss: 1.143896 	Validation Loss: 1.347735 	 time: 0.3
Epoch: 3811 	Training Loss: 1.143866 	Validation Loss: 1.347047 	 time: 0.3
Epoch: 3812 	Training Loss: 1.143771 	Validation Loss: 1.347597 	 time: 0.3
Epoch: 3813 	Training Loss: 1.143820 	Validation Loss: 1.347480 	 time: 0.3
Epoch: 3814 	Training Loss: 1.143864 	Validation Loss: 1.344838 	 time: 0.3
Epoch: 3815 	Training Loss: 1.144059 	Validation Loss: 1.344930 	 time: 0.3
Epoch: 3816 	Training Loss: 1.143874 	Validation Loss: 1.347756 	 time: 0.3
Epoch: 3817 	Training Loss: 1.144035 	Validation Loss: 1.347821 	 time: 0.3
Epoch: 3818 	Training Loss: 1.143822 	Validation Loss: 1.347901 	 time: 0.3
Epoch: 3819 	Training Loss: 1.143883 	Validation Loss: 1.349857 	 time: 0.3
Epoch: 3820 	Training Loss: 1.144120 	Validation Loss: 1.348175 	 time: 0.3
Epoch: 3821 	Training Loss: 1.144082 	Validation Loss: 1.345089 	 time: 0.3
Epoch: 3822 	Training Loss: 1.144043 	Validation Loss: 1.344828 	 time: 0.3
Epoch: 3823 	Training Loss: 1.144009 	Validation Loss: 1.346156 	 time: 0.3
Epoch: 3824 	Training Loss: 1.144016 	Validation Loss: 1.345963 	 time: 0.3
Epoch: 3825 	Training Loss: 1.144012 	Validation Loss: 1.347347 	 time: 0.3
Epoch: 3826 	Training Loss: 1.143931 	Validation Loss: 1.348778 	 time: 0.3
Epoch: 3827 	Training Loss: 1.144026 	Validation Loss: 1.348286 	 time: 0.3
Epoch: 3828 	Training Loss: 1.143891 	Validation Loss: 1.349459 	 time: 0.3
Epoch: 3829 	Training Loss: 1.143977 	Validation Loss: 1.347673 	 time: 0.3
Epoch: 3830 	Training Loss: 1.143876 	Validation Loss: 1.346648 	 time: 0.3
Epoch: 3831 	Training Loss: 1.143882 	Validation Loss: 1.345167 	 time: 0.3
Epoch: 3832 	Training Loss: 1.143943 	Validation Loss: 1.346216 	 time: 0.3
Epoch: 3833 	Training Loss: 1.143950 	Validation Loss: 1.346581 	 time: 0.3
Epoch: 3834 	Training Loss: 1.143857 	Validation Loss: 1.347658 	 time: 0.3
Epoch: 3835 	Training Loss: 1.143817 	Validation Loss: 1.347795 	 time: 0.3
Epoch: 3836 	Training Loss: 1.143809 	Validation Loss: 1.346821 	 time: 0.3
Epoch: 3837 	Training Loss: 1.143726 	Validation Loss: 1.347445 	 time: 0.3
Epoch: 3838 	Training Loss: 1.143729 	Validation Loss: 1.348452 	 time: 0.3
Epoch: 3839 	Training Loss: 1.143736 	Validation Loss: 1.347010 	 time: 0.3
Epoch: 3840 	Training Loss: 1.143735 	Validation Loss: 1.345793 	 time: 0.3
Epoch: 3841 	Training Loss: 1.143705 	Validation Loss: 1.344988 	 time: 0.3
Epoch: 3842 	Training Loss: 1.143695 	Validation Loss: 1.345295 	 time: 0.3
Epoch: 3843 	Training Loss: 1.143662 	Validation Loss: 1.346281 	 time: 0.3
Epoch: 3844 	Training Loss: 1.143601 	Validation Loss: 1.348227 	 time: 0.3
Epoch: 3845 	Training Loss: 1.143549 	Validation Loss: 1.348638 	 time: 0.3
Epoch: 3846 	Training Loss: 1.143532 	Validation Loss: 1.347020 	 time: 0.3
Epoch: 3847 	Training Loss: 1.143498 	Validation Loss: 1.346735 	 time: 0.3
Epoch: 3848 	Training Loss: 1.143498 	Validation Loss: 1.348101 	 time: 0.3
Epoch: 3849 	Training Loss: 1.143493 	Validation Loss: 1.349245 	 time: 0.3
Epoch: 3850 	Training Loss: 1.143502 	Validation Loss: 1.348687 	 time: 0.3
Epoch: 3851 	Training Loss: 1.143486 	Validation Loss: 1.348358 	 time: 0.3
Epoch: 3852 	Training Loss: 1.143463 	Validation Loss: 1.348274 	 time: 0.3
Epoch: 3853 	Training Loss: 1.143475 	Validation Loss: 1.347827 	 time: 0.3
Epoch: 3854 	Training Loss: 1.143482 	Validation Loss: 1.348378 	 time: 0.3
Epoch: 3855 	Training Loss: 1.143472 	Validation Loss: 1.348566 	 time: 0.3
Epoch: 3856 	Training Loss: 1.143457 	Validation Loss: 1.347787 	 time: 0.3
Epoch: 3857 	Training Loss: 1.143454 	Validation Loss: 1.347517 	 time: 0.3
Epoch: 3858 	Training Loss: 1.143466 	Validation Loss: 1.348547 	 time: 0.3
Epoch: 3859 	Training Loss: 1.143492 	Validation Loss: 1.347896 	 time: 0.3
Epoch: 3860 	Training Loss: 1.143520 	Validation Loss: 1.347660 	 time: 0.3
Epoch: 3861 	Training Loss: 1.143491 	Validation Loss: 1.347907 	 time: 0.3
Epoch: 3862 	Training Loss: 1.143440 	Validation Loss: 1.347136 	 time: 0.3
Epoch: 3863 	Training Loss: 1.143460 	Validation Loss: 1.346730 	 time: 0.3
Epoch: 3864 	Training Loss: 1.143538 	Validation Loss: 1.346633 	 time: 0.3
Epoch: 3865 	Training Loss: 1.143577 	Validation Loss: 1.348326 	 time: 0.3
Epoch: 3866 	Training Loss: 1.143458 	Validation Loss: 1.347952 	 time: 0.3
Epoch: 3867 	Training Loss: 1.143546 	Validation Loss: 1.346530 	 time: 0.3
Epoch: 3868 	Training Loss: 1.143907 	Validation Loss: 1.345973 	 time: 0.3
Epoch: 3869 	Training Loss: 1.143693 	Validation Loss: 1.347934 	 time: 0.3
Epoch: 3870 	Training Loss: 1.144045 	Validation Loss: 1.346594 	 time: 0.3
Epoch: 3871 	Training Loss: 1.143965 	Validation Loss: 1.347284 	 time: 0.3
Epoch: 3872 	Training Loss: 1.143957 	Validation Loss: 1.350619 	 time: 0.3
Epoch: 3873 	Training Loss: 1.143734 	Validation Loss: 1.349670 	 time: 0.3
Epoch: 3874 	Training Loss: 1.143664 	Validation Loss: 1.346810 	 time: 0.3
Epoch: 3875 	Training Loss: 1.143810 	Validation Loss: 1.346165 	 time: 0.3
Epoch: 3876 	Training Loss: 1.143900 	Validation Loss: 1.348444 	 time: 0.3
Epoch: 3877 	Training Loss: 1.143789 	Validation Loss: 1.348178 	 time: 0.3
Epoch: 3878 	Training Loss: 1.143572 	Validation Loss: 1.348846 	 time: 0.3
Epoch: 3879 	Training Loss: 1.143682 	Validation Loss: 1.348368 	 time: 0.3
Epoch: 3880 	Training Loss: 1.143791 	Validation Loss: 1.345392 	 time: 0.3
Epoch: 3881 	Training Loss: 1.143806 	Validation Loss: 1.345928 	 time: 0.3
Epoch: 3882 	Training Loss: 1.143582 	Validation Loss: 1.348075 	 time: 0.3
Epoch: 3883 	Training Loss: 1.143832 	Validation Loss: 1.348251 	 time: 0.3
Epoch: 3884 	Training Loss: 1.143789 	Validation Loss: 1.348462 	 time: 0.3
Epoch: 3885 	Training Loss: 1.143707 	Validation Loss: 1.350238 	 time: 0.3
Epoch: 3886 	Training Loss: 1.143771 	Validation Loss: 1.348898 	 time: 0.3
Epoch: 3887 	Training Loss: 1.143654 	Validation Loss: 1.346469 	 time: 0.3
Epoch: 3888 	Training Loss: 1.143580 	Validation Loss: 1.346304 	 time: 0.3
Epoch: 3889 	Training Loss: 1.143740 	Validation Loss: 1.346300 	 time: 0.3
Epoch: 3890 	Training Loss: 1.143722 	Validation Loss: 1.346295 	 time: 0.3
Epoch: 3891 	Training Loss: 1.143499 	Validation Loss: 1.347975 	 time: 0.3
Epoch: 3892 	Training Loss: 1.144002 	Validation Loss: 1.346631 	 time: 0.3
Epoch: 3893 	Training Loss: 1.144254 	Validation Loss: 1.346549 	 time: 0.3
Epoch: 3894 	Training Loss: 1.144125 	Validation Loss: 1.347376 	 time: 0.3
Epoch: 3895 	Training Loss: 1.144106 	Validation Loss: 1.349931 	 time: 0.3
Epoch: 3896 	Training Loss: 1.144195 	Validation Loss: 1.349769 	 time: 0.3
Epoch: 3897 	Training Loss: 1.143976 	Validation Loss: 1.347437 	 time: 0.3
Epoch: 3898 	Training Loss: 1.143832 	Validation Loss: 1.346683 	 time: 0.3
Epoch: 3899 	Training Loss: 1.143744 	Validation Loss: 1.347744 	 time: 0.3
Epoch: 3900 	Training Loss: 1.144028 	Validation Loss: 1.346846 	 time: 0.3
Epoch: 3901 	Training Loss: 1.143738 	Validation Loss: 1.346405 	 time: 0.3
Epoch: 3902 	Training Loss: 1.143701 	Validation Loss: 1.346316 	 time: 0.3
Epoch: 3903 	Training Loss: 1.143779 	Validation Loss: 1.346175 	 time: 0.3
Epoch: 3904 	Training Loss: 1.143812 	Validation Loss: 1.346900 	 time: 0.3
Epoch: 3905 	Training Loss: 1.143673 	Validation Loss: 1.346836 	 time: 0.3
Epoch: 3906 	Training Loss: 1.143509 	Validation Loss: 1.346283 	 time: 0.3
Epoch: 3907 	Training Loss: 1.143546 	Validation Loss: 1.343682 	 time: 0.3
Epoch: 3908 	Training Loss: 1.143603 	Validation Loss: 1.343554 	 time: 0.3
Epoch: 3909 	Training Loss: 1.143507 	Validation Loss: 1.343771 	 time: 0.3
Epoch: 3910 	Training Loss: 1.143486 	Validation Loss: 1.345051 	 time: 0.3
Epoch: 3911 	Training Loss: 1.143524 	Validation Loss: 1.345222 	 time: 0.3
Epoch: 3912 	Training Loss: 1.143500 	Validation Loss: 1.344934 	 time: 0.3
Epoch: 3913 	Training Loss: 1.143466 	Validation Loss: 1.345993 	 time: 0.3
Epoch: 3914 	Training Loss: 1.143475 	Validation Loss: 1.345984 	 time: 0.3
Epoch: 3915 	Training Loss: 1.143498 	Validation Loss: 1.346299 	 time: 0.3
Epoch: 3916 	Training Loss: 1.143450 	Validation Loss: 1.345182 	 time: 0.3
Epoch: 3917 	Training Loss: 1.143375 	Validation Loss: 1.344142 	 time: 0.3
Epoch: 3918 	Training Loss: 1.143357 	Validation Loss: 1.343786 	 time: 0.3
Epoch: 3919 	Training Loss: 1.143362 	Validation Loss: 1.343819 	 time: 0.3
Epoch: 3920 	Training Loss: 1.143345 	Validation Loss: 1.344534 	 time: 0.3
Epoch: 3921 	Training Loss: 1.143381 	Validation Loss: 1.343828 	 time: 0.3
Epoch: 3922 	Training Loss: 1.143365 	Validation Loss: 1.343182 	 time: 0.3
Epoch: 3923 	Training Loss: 1.143375 	Validation Loss: 1.342978 	 time: 0.3
Epoch: 3924 	Training Loss: 1.143411 	Validation Loss: 1.342581 	 time: 0.3
Epoch: 3925 	Training Loss: 1.143352 	Validation Loss: 1.343180 	 time: 0.3
Epoch: 3926 	Training Loss: 1.143288 	Validation Loss: 1.344203 	 time: 0.3
Epoch: 3927 	Training Loss: 1.143315 	Validation Loss: 1.344784 	 time: 0.3
Epoch: 3928 	Training Loss: 1.143386 	Validation Loss: 1.343076 	 time: 0.3
Epoch: 3929 	Training Loss: 1.143381 	Validation Loss: 1.343420 	 time: 0.3
Epoch: 3930 	Training Loss: 1.143274 	Validation Loss: 1.343434 	 time: 0.3
Epoch: 3931 	Training Loss: 1.143391 	Validation Loss: 1.342673 	 time: 0.3
Epoch: 3932 	Training Loss: 1.143617 	Validation Loss: 1.343330 	 time: 0.3
Epoch: 3933 	Training Loss: 1.143412 	Validation Loss: 1.345598 	 time: 0.3
Epoch: 3934 	Training Loss: 1.143651 	Validation Loss: 1.344655 	 time: 0.3
Epoch: 3935 	Training Loss: 1.144054 	Validation Loss: 1.343478 	 time: 0.3
Epoch: 3936 	Training Loss: 1.144059 	Validation Loss: 1.346578 	 time: 0.3
Epoch: 3937 	Training Loss: 1.143923 	Validation Loss: 1.346873 	 time: 0.3
Epoch: 3938 	Training Loss: 1.144066 	Validation Loss: 1.344372 	 time: 0.3
Epoch: 3939 	Training Loss: 1.143990 	Validation Loss: 1.344729 	 time: 0.3
Epoch: 3940 	Training Loss: 1.143946 	Validation Loss: 1.346314 	 time: 0.3
Epoch: 3941 	Training Loss: 1.143759 	Validation Loss: 1.347993 	 time: 0.3
Epoch: 3942 	Training Loss: 1.144185 	Validation Loss: 1.346735 	 time: 0.3
Epoch: 3943 	Training Loss: 1.143613 	Validation Loss: 1.345573 	 time: 0.3
Epoch: 3944 	Training Loss: 1.143583 	Validation Loss: 1.347188 	 time: 0.3
Epoch: 3945 	Training Loss: 1.143832 	Validation Loss: 1.346857 	 time: 0.3
Epoch: 3946 	Training Loss: 1.143606 	Validation Loss: 1.346390 	 time: 0.3
Epoch: 3947 	Training Loss: 1.143720 	Validation Loss: 1.347878 	 time: 0.3
Epoch: 3948 	Training Loss: 1.143657 	Validation Loss: 1.348839 	 time: 0.3
Epoch: 3949 	Training Loss: 1.143705 	Validation Loss: 1.346922 	 time: 0.3
Epoch: 3950 	Training Loss: 1.143497 	Validation Loss: 1.345686 	 time: 0.3
Epoch: 3951 	Training Loss: 1.143513 	Validation Loss: 1.345679 	 time: 0.3
Epoch: 3952 	Training Loss: 1.143535 	Validation Loss: 1.345497 	 time: 0.3
Epoch: 3953 	Training Loss: 1.143554 	Validation Loss: 1.345457 	 time: 0.3
Epoch: 3954 	Training Loss: 1.143531 	Validation Loss: 1.345129 	 time: 0.3
Epoch: 3955 	Training Loss: 1.143499 	Validation Loss: 1.345685 	 time: 0.3
Epoch: 3956 	Training Loss: 1.143464 	Validation Loss: 1.345873 	 time: 0.3
Epoch: 3957 	Training Loss: 1.143419 	Validation Loss: 1.345453 	 time: 0.3
Epoch: 3958 	Training Loss: 1.143388 	Validation Loss: 1.345152 	 time: 0.3
Epoch: 3959 	Training Loss: 1.143424 	Validation Loss: 1.345430 	 time: 0.3
Epoch: 3960 	Training Loss: 1.143380 	Validation Loss: 1.345683 	 time: 0.3
Epoch: 3961 	Training Loss: 1.143376 	Validation Loss: 1.344342 	 time: 0.3
Epoch: 3962 	Training Loss: 1.143358 	Validation Loss: 1.344070 	 time: 0.3
Epoch: 3963 	Training Loss: 1.143323 	Validation Loss: 1.344049 	 time: 0.3
Epoch: 3964 	Training Loss: 1.143344 	Validation Loss: 1.344423 	 time: 0.3
Epoch: 3965 	Training Loss: 1.143346 	Validation Loss: 1.344295 	 time: 0.3
Epoch: 3966 	Training Loss: 1.143337 	Validation Loss: 1.344642 	 time: 0.3
Epoch: 3967 	Training Loss: 1.143316 	Validation Loss: 1.344437 	 time: 0.3
Epoch: 3968 	Training Loss: 1.143288 	Validation Loss: 1.344209 	 time: 0.3
Epoch: 3969 	Training Loss: 1.143307 	Validation Loss: 1.344050 	 time: 0.3
Epoch: 3970 	Training Loss: 1.143300 	Validation Loss: 1.344348 	 time: 0.3
Epoch: 3971 	Training Loss: 1.143296 	Validation Loss: 1.344729 	 time: 0.3
Epoch: 3972 	Training Loss: 1.143297 	Validation Loss: 1.345751 	 time: 0.3
Epoch: 3973 	Training Loss: 1.143293 	Validation Loss: 1.345194 	 time: 0.3
Epoch: 3974 	Training Loss: 1.143289 	Validation Loss: 1.345223 	 time: 0.3
Epoch: 3975 	Training Loss: 1.143289 	Validation Loss: 1.344762 	 time: 0.3
Epoch: 3976 	Training Loss: 1.143279 	Validation Loss: 1.345078 	 time: 0.3
Epoch: 3977 	Training Loss: 1.143267 	Validation Loss: 1.344397 	 time: 0.3
Epoch: 3978 	Training Loss: 1.143261 	Validation Loss: 1.344317 	 time: 0.3
Epoch: 3979 	Training Loss: 1.143256 	Validation Loss: 1.344597 	 time: 0.3
Epoch: 3980 	Training Loss: 1.143260 	Validation Loss: 1.344110 	 time: 0.3
Epoch: 3981 	Training Loss: 1.143267 	Validation Loss: 1.344454 	 time: 0.3
Epoch: 3982 	Training Loss: 1.143268 	Validation Loss: 1.344391 	 time: 0.3
Epoch: 3983 	Training Loss: 1.143274 	Validation Loss: 1.345018 	 time: 0.3
Epoch: 3984 	Training Loss: 1.143278 	Validation Loss: 1.344294 	 time: 0.3
Epoch: 3985 	Training Loss: 1.143276 	Validation Loss: 1.344942 	 time: 0.3
Epoch: 3986 	Training Loss: 1.143273 	Validation Loss: 1.344073 	 time: 0.3
Epoch: 3987 	Training Loss: 1.143298 	Validation Loss: 1.344545 	 time: 0.3
Epoch: 3988 	Training Loss: 1.143265 	Validation Loss: 1.344558 	 time: 0.3
Epoch: 3989 	Training Loss: 1.143235 	Validation Loss: 1.344200 	 time: 0.3
Epoch: 3990 	Training Loss: 1.143246 	Validation Loss: 1.344894 	 time: 0.3
Epoch: 3991 	Training Loss: 1.143300 	Validation Loss: 1.344815 	 time: 0.3
Epoch: 3992 	Training Loss: 1.143366 	Validation Loss: 1.345191 	 time: 0.3
Epoch: 3993 	Training Loss: 1.143310 	Validation Loss: 1.344602 	 time: 0.3
Epoch: 3994 	Training Loss: 1.143239 	Validation Loss: 1.344441 	 time: 0.3
Epoch: 3995 	Training Loss: 1.143252 	Validation Loss: 1.344064 	 time: 0.3
Epoch: 3996 	Training Loss: 1.143294 	Validation Loss: 1.343267 	 time: 0.3
Epoch: 3997 	Training Loss: 1.143333 	Validation Loss: 1.345013 	 time: 0.3
Epoch: 3998 	Training Loss: 1.143327 	Validation Loss: 1.344152 	 time: 0.3
Epoch: 3999 	Training Loss: 1.143246 	Validation Loss: 1.344160 	 time: 0.3
Epoch: 4000 	Training Loss: 1.143259 	Validation Loss: 1.345031 	 time: 0.3
